% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Text as Data},
  pdfauthor={Jerid Francom},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% Make links footnotes instead of hotlinks:
\DeclareRobustCommand{\href}[2]{#2\footnote{\url{#1}}}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% default
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{float}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

% added
\usepackage{longtable}
\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\ifxetex
  \usepackage{letltxmacro}
  \setlength{\XeTeXLinkMargin}{2pt}
  \LetLtxMacro\SavedIncludeGraphics\includegraphics
  \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
    \IncludeGraphicsAux{#1}%
  }%
  \newcommand*{\IncludeGraphicsAux}[2]{%
    \XeTeXLinkBox{%
      \SavedIncludeGraphics#1{#2}%
    }%
  }%
\fi

\newenvironment{rmdblock}[1]
  {\begin{shaded*}
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.5\height}[0pt][0pt]{
      {\setkeys{Gin}{width=2em,keepaspectratio}\includegraphics{assets/images/#1}}
    }
  }
  \item
  }
  {
  \end{itemize}
  \end{shaded*}
  }
\newenvironment{rmdkey}
  {\begin{rmdblock}{key}}
  {\end{rmdblock}}
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\newenvironment{rmdactivity}
  {\begin{rmdblock}{code}}
  {\end{rmdblock}}
\newenvironment{rmdstudy}
  {\begin{rmdblock}{paper}}
  {\end{rmdblock}}
\newenvironment{rmdquestion}
  {\begin{rmdblock}{question}}
  {\end{rmdblock}}

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Text as Data}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{An introduction to quantitative text analysis and reproducible research with R}
\author{Jerid Francom}
\date{March 21, 2022 (latest version)}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{welcome}{%
\section*{Welcome}\label{welcome}}
\addcontentsline{toc}{section}{Welcome}

DRAFT

This is the coursebook to accompany Linguistics 380 ``Language Use and Technology'' at Wake Forest University. The working title for this coursebook is \emph{Text as Data: An Introduction to Quantitative Text Analysis and Reproducible Research with R}.

The content is currently under development. Feedback is welcome and can be provided through the \href{https://web.hypothes.is/}{hypothes.is} service. A toolbar interface to this service is located on the right sidebar. To register for a free account and join the ``text\_as\_data'' annotation group \href{https://hypothes.is/groups/WkoaXnBX/text-as-data}{follow this link}. Suggestions and changes that are incorporated will be \protect\hyperlink{acknowledgements}{acknowledged}.

\textbf{Author}

Dr.~Jerid Francom is Associate Professor of Spanish and Linguistics at Wake Forest University. His research interests are focused around quantitative approaches to language variation.

\hypertarget{license}{%
\subsection*{License}\label{license}}
\addcontentsline{toc}{subsection}{License}

This work by \href{https://francojc.github.io/}{Jerid C. Francom} is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States License.

\hypertarget{credits}{%
\subsection*{Credits}\label{credits}}
\addcontentsline{toc}{subsection}{Credits}

Icons made from Icon Fonts are licensed by CC BY 3.0

\hypertarget{acknowledgements}{%
\subsection*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{subsection}{Acknowledgements}

TAD has been reviewed by and suggestions and changes incorporated based on the feedback through \href{https://hypothes.is/groups/Q3o92MJg/tad}{the TAD Hypothes.is group} by the following people: \ldots{}

\hypertarget{preface}{%
\section*{Preface}\label{preface}}
\addcontentsline{toc}{section}{Preface}

DRAFT

\begin{quote}
The journey of a thousand miles begins with one step.

--- \href{https://en.wikipedia.org/wiki/Laozi}{Lao Tzu}
\end{quote}

\begin{rmdkey}
The essential questions for this chapter are:

\begin{itemize}
\tightlist
\item
  What is the rationale for creating a coursebook on quantitative text
  analysis?
\item
  What is the approach taken in this coursebook?
\item
  What are the learning goals and how does the coursebook aim to support
  attaining these goals?
\end{itemize}
\end{rmdkey}

This chapter aims to provide a brief summary of current research trends that form the context for the rationale for this coursebook. It also provides instructors and students an overview of the purpose and approach of the coursebook It will also include a description of the main components of each section and chapter and provide a guide to conventions used in the book and resources available.

\hypertarget{rationale}{%
\subsection*{Rationale}\label{rationale}}
\addcontentsline{toc}{subsection}{Rationale}

In recent years there has been a growing buzz around the term `Data Science' and related terms; data analytics, data mining, \emph{etc}. In a nutshell data science is the process by which an investigator leverages statistical methods and computational power to uncover insight from large datasets. Driven in large part by the increase in computing power available to the average individual and the increasing amount of electronic data that is now available through the internet, interest in data science has expanded to virtually all fields in academia and areas in the public sector. Data scientists are in high demand and this trend is expected to continue into the foreseeable future.

This coursebook is an introduction to the fundamental concepts and practical programming skills from Data Science that are increasingly employed in a variety of language-centered fields and sub-fields applied to the task of text analysis. It is geared towards advanced undergraduates and graduate students of linguistics and related fields. As quantitative research skills are quickly becoming a core aspect of many language programs, this coursebook aims to provide a fundamental understanding of theoretical concepts, programming skills, and statistical methods for doing quantitative text analysis.

\hypertarget{approach}{%
\subsection*{Approach}\label{approach}}
\addcontentsline{toc}{subsection}{Approach}

Many textbooks on doing `Data Science', even those that have a domain-centric approach, such as text analysis, tend to focus on the `tidy' approach, seen in Figure \ref{fig:tidy-workflow-img} from \citet{Wickham2017}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/01-course/standard-tidy-approach} 

}

\caption{Workflow diagram from R for Data Science.}\label{fig:tidy-workflow-img}
\end{figure}

However these resources tend to underrepresent the importance of establishing a research question and implementation plan. A big part, or perhaps the biggest part of doing quantitative research, and research in general is what is the question to be addressed. I think a central advantage to this coursebook for language researchers is to thread the project goals from a conceptual point of view without technical implementation in mind first. Then, after establishing a viable vision about what the data should look like, how it should be analyzed, and how the analysis will contribute to knowledge in the field, we can move towards implementing these preliminary formulations in R code in a reproducible fashion. In essence this approach reflects \href{https://en.wikipedia.org/wiki/Separation_of_content_and_presentation}{the classic separation between content and format} --the content of our research should precede the format it should or will take.

\hypertarget{learning-goals}{%
\subsubsection*{Learning goals}\label{learning-goals}}
\addcontentsline{toc}{subsubsection}{Learning goals}

This course you will:

\textbf{Data Literacy (DL):} learn to interpret, assess, and contextualize findings based on data.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ability to understand and apply data analysis to derive insight from data
\item
  ability to understand and apply data knowledge and skills across linguistic and language-related disciplines
\end{enumerate}

\textbf{Research Skills (RS):} learn to conduct original research (design, implementation, interpretation, and communication).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  identify an applicable area of investigation in a linguistic or language-related field
\item
  develop a viable research question or hypothesis
\item
  assess, acquire, and document data
\item
  curate and transform data for analysis
\item
  select and apply relevant analysis method
\item
  interpret and communicate findings
\end{enumerate}

\textbf{Programming Skills (PS):} learn to produce your own research and work collaboratively with others.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  demonstrate proficiency to implement research with R (RD points 3-5)
\item
  demonstrate ability to produce collaborative and reproducible research using R, RStudio, and GitHub
\end{enumerate}

\hypertarget{prerequisites}{%
\subsubsection*{Prerequisites}\label{prerequisites}}
\addcontentsline{toc}{subsubsection}{Prerequisites}

This coursebook is aimed at students that have some background in language-related studies or linguistics with a desire to expand their methodological toolbox. It does not assume a strong background in these areas, however. Furthermore, I will make no assumptions about students' experience with programming in general, or programming with R, in particular.

You will need reliable access to the internet and a computer to work with the code in this coursebook and the code found in the accompanying resource site (\href{https://lin380.github.io/tadr/}{tadr}).

\hypertarget{programming}{%
\subsubsection*{Programming}\label{programming}}
\addcontentsline{toc}{subsubsection}{Programming}

Programming is the backbone for modern quantitative research. Here are some of the reasons to program:

\begin{itemize}
\tightlist
\item
  \emph{Flexibility} Graphical User Interface (GUI) based software is inherently limited. What you see is what you get. If you have another need, you need to find a tool. If another tool does not implement what you think you need, you are out of luck.
\item
  \emph{Transparency} By taking a programming approach to research analysis you make your decisions explicit and leave a breadcrumb trail to everything you do.
\item
  \emph{Reproducibility} What you do will be clearer to you but also allow you to share the process with others (including your future self!). Insight grows much faster when exposed to light. Sharing your research with collaborators or on sites such as GitHub or BitBucket brings makes your work visible and accessible to the world. Reproducibility is gaining momentum and is fueled by programmatic approaches to research.
\end{itemize}

R is a popular programming language with statiticians and was adopted by many other fields in natural and social sciences. There are various reasons why using R for this coursebook is a good choice:

\begin{itemize}
\tightlist
\item
  \emph{One stop shopping} Once known specifically as a statistical programming language, R can now be a round trip tool to acquire, curate, transform, visualize, \emph{and} statistically analyze data. It also allows for robust communication in reports and data and analysis sharing (reproducibility).
\item
  \emph{You are not alone} There is a sizable R programming community, especially in academics. This has two tangible benefits; first, you will likely be able to find user contributed R packages that will satisfy many of the more sophisticated programming goals you will have and second, you will be able to get answers to any of your programming questions on popular sites like StackOverflow.
\item
  \emph{RStudio} RStudio is the envy of many other programmers. It is a very capable interface to R and provides convenient access powerful tools to allow you to be a more efficient and productive R programmer.
\end{itemize}

\hypertarget{coursebook-structure}{%
\subsection*{Coursebook structure}\label{coursebook-structure}}
\addcontentsline{toc}{subsection}{Coursebook structure}

This coursebook is divided into four parts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In ``Foundations'', an environmental survey of quantitative research across disciplines and orient language-based research is provided.
\item
  ``Orientation'' aims to build your knowledge about what data is, how text is organized into datasets, what role statistics play in quantitative research and the types of statistical approaches that are commonly found in text analysis research, and finally how to develop a research question and a research blueprint for conducting a quantitative text analysis research project.
\item
  ``Preparation'' covers a variety of implementation approaches for each stage for deriving a dataset ready for statistical analysis which includes acquiring, curating, and transforming data.
\item
  ``Analysis'' elaborates various statistical approaches for data analysis and contextualizes their application in for different types of research questions and aims.
\end{enumerate}

\hypertarget{conventions}{%
\subsection*{Conventions}\label{conventions}}
\addcontentsline{toc}{subsection}{Conventions}

This coursebook is about the concepts for understanding and the techniques for doing quantitative text analysis with R. Therefore there will be an intermingling of prose and code presented. As such, an attempt to establish consistent conventions throughout the text has been made to signal reader's attention as appropriate. As we explore concepts, R code itself will be incorporated into the text. This may be a unique coursebook compared to others you have seen. It has been created using R itself --specifically using an R language package called \texttt{bookdown} \citep{R-bookdown}. This R package makes it possible to write, execute (`run'), and display code and results within the text.

For example, the following text block shows actual R code and the results that are generated when running this code. Note that the hashtag \texttt{\#} signals a \textbf{code comment}. The code follows within the same text block and a subsequent text block displays the output of the code.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add 1 plus 1}
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}
\CommentTok{\#\textgreater{} [1] 2}
\end{Highlighting}
\end{Shaded}

Inline code will be used when code blocks are short and the results are not needed for display. For example, the same code as above will sometimes appear as \texttt{1\ +\ 1}.

When necessary meta-description of code will appear. This is particularly relevant for R Markdown documents.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r test{-}code\}}
\NormalTok{1 + 1}
\NormalTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

In terms of prose, key concepts will be signaled using \textbf{\emph{bold italics}}. Terms that appear in this typeface will also appear in the {[}glossary{]} at the end of the text (TODO). Furthermore, there are four pose text blocks that will be used to signal the reader's attention: \emph{key points}, \emph{notes}, \emph{tips}, \emph{questions}, and \emph{warnings}.

Key points summarize the main points to be covered in a chapter or a subsection of the text.

\begin{rmdkey}
In this chapter you will learn:

\begin{itemize}
\tightlist
\item
  the goals of this coursebook
\item
  the reasoning for using the R programming language
\item
  important text conventions employed in this coursebook
\end{itemize}
\end{rmdkey}

Case studies are provided in-line which highlight key concepts and/ or methodological approaches relevant to the current topic or section.

\begin{rmdstudy}
\citet{Eisenstein2012} track the geographic spread of neologisms from city to city in the United States using Twitter data collected between 6/2009 and 5/2011. They only used tweets with geolocation data and then associated each tweet with a zipcode using the US Census. The most populous metropolitan areas were used. They also used the demographics from these areas to make associations between lexical innovations and demographic attributes. From this analysis they are able to reconstruct a network of linguistic influence. One of the main findings is that demographically-similar cities are more likely to share linguistic influence. At the individual level, there is a strong, potentially stronger role of demographics than geographical location.
\end{rmdstudy}

Notes provide a bit more information on the topic or where to find more information.

\begin{rmdnote}
R is more than a powerful statistical programming language, it also can
be used to perform all the necessary steps in a data science project;
including reporting. A relatively new addition to the reporting
capabilities of R is the \texttt{bookdown} package (this coursebook was
created using this very package). You can find out more
\href{https://bookdown.org/}{here}.
\end{rmdnote}

Tips are used to signal helpful hints that might otherwise be overlooked.

\begin{rmdtip}
During a the course of an exploratory work session, many R objects are
often created to test ideas. At some point inspecting the workspace
becomes difficult due to the number of objects displayed using
\texttt{ls()}.

To remove all objects from the workspace, use
\texttt{rm(list\ =\ ls())}.
\end{rmdtip}

From time to time there will be points for you to consider and questions to explore.

\begin{rmdquestion}
Consider the objectives in this course: what ways can the knowledge and
skills you will learn benefit you in your academic studies and/ or
professional and personal life?
\end{rmdquestion}

Activities\ldots.

\begin{rmdactivity}
This is an activity\ldots.
\end{rmdactivity}

Errors will be an inevitable part of learning, but some errors can be avoided. The text will used the warning text block to highlight typical pitfalls and errors.

\begin{rmdwarning}
Hello world!\\
This is a warning.
\end{rmdwarning}

Although this is not intended to be a in-depth introduction to statistical techniques, mathematical formulas will be included in the text. These formulas will appear either inline \(1 + 1 = 2\) or as block equations.

\begin{equation}
  \hat{c} = \underset{c \in C} {\mathrm{argmax}} ~\hat{P}(c) \prod_i \hat{P}(w_i|c)
  \label{eq:example-formula}
\end{equation}

Data analysis leans heavily on graphical representations. Figures will appear numbered, as in Figure \ref{fig:test-fig}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2) }\CommentTok{\# load graphics package}
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ hp, }\AttributeTok{y =}\NormalTok{ mpg)) }\SpecialCharTok{+} \CommentTok{\# map \textquotesingle{}hp\textquotesingle{} and \textquotesingle{}mpg\textquotesingle{} to coordinate space}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# add points}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# draw linear trend line}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Horsepower"}\NormalTok{, }\CommentTok{\# label x axis}
       \AttributeTok{y =} \StringTok{"Miles per gallon"}\NormalTok{, }\CommentTok{\# label y axis}
       \AttributeTok{title =} \StringTok{"Test plot"}\NormalTok{, }\CommentTok{\# add title}
       \AttributeTok{subtitle =} \StringTok{"From mtcars dataset"}\NormalTok{) }\CommentTok{\# add subtitle}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{01-course_files/figure-latex/test-fig-1} 

}

\caption{Test plot from mtcars dataset}\label{fig:test-fig}
\end{figure}

Tables, such as Table \ref{tab:test-tab} will be numbered separately from figures.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(iris, }\DecValTok{20}\NormalTok{), }\AttributeTok{caption =} \StringTok{"Here is a nice table!"}\NormalTok{, }\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:test-tab}Here is a nice table!}
\centering
\begin{tabular}[t]{rrrrl}
\toprule
Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\
\midrule
5.1 & 3.5 & 1.4 & 0.2 & setosa\\
4.9 & 3.0 & 1.4 & 0.2 & setosa\\
4.7 & 3.2 & 1.3 & 0.2 & setosa\\
4.6 & 3.1 & 1.5 & 0.2 & setosa\\
5.0 & 3.6 & 1.4 & 0.2 & setosa\\
\addlinespace
5.4 & 3.9 & 1.7 & 0.4 & setosa\\
4.6 & 3.4 & 1.4 & 0.3 & setosa\\
5.0 & 3.4 & 1.5 & 0.2 & setosa\\
4.4 & 2.9 & 1.4 & 0.2 & setosa\\
4.9 & 3.1 & 1.5 & 0.1 & setosa\\
\addlinespace
5.4 & 3.7 & 1.5 & 0.2 & setosa\\
4.8 & 3.4 & 1.6 & 0.2 & setosa\\
4.8 & 3.0 & 1.4 & 0.1 & setosa\\
4.3 & 3.0 & 1.1 & 0.1 & setosa\\
5.8 & 4.0 & 1.2 & 0.2 & setosa\\
\addlinespace
5.7 & 4.4 & 1.5 & 0.4 & setosa\\
5.4 & 3.9 & 1.3 & 0.4 & setosa\\
5.1 & 3.5 & 1.4 & 0.3 & setosa\\
5.7 & 3.8 & 1.7 & 0.3 & setosa\\
5.1 & 3.8 & 1.5 & 0.3 & setosa\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{resources}{%
\subsection*{Resources}\label{resources}}
\addcontentsline{toc}{subsection}{Resources}

This coursebook includes the \href{https://lin380.github.io/tadr/}{Text as Data Resources} accompany website. This site itself includes resources to learn and extend R skills relevant for conducting reproducible text analysis research. \href{https://lin380.github.io/tadr/tutorials/index.html}{Tutorials} are provided which provide video, questions, and interactive coding practice. \href{https://lin380.github.io/tadr/articles/index.html}{Recipes} includes worked demonstrations of targeted aspects of R programming. Each of these resources are coordinated to provide the programming skills necessary for the stages of text analysis covered in the coursebook.

In addition to the Tutorials and Recipes, students are encouraged to engage with the interactive coding \texttt{swirl} activities. In contrast to Tutorials, swirl activities will be performed in an RStudio session in the R console. This provides a more authentic experience for learning to use R.

\hypertarget{build-information}{%
\subsection*{Build information}\label{build-information}}
\addcontentsline{toc}{subsection}{Build information}

This coursebook was written in \href{http://bookdown.org/}{bookdown} inside \href{http://www.rstudio.com/ide/}{RStudio}. The website is hosted with \href{https://pages.github.com/}{GitHub Pages} and the complete source is available from \href{https://github.com/lin380}{GitHub}.

This version of the coursebook was built with R version 4.1.2 (2021-11-01).

\hypertarget{part-foundations}{%
\part{Foundations}\label{part-foundations}}

\hypertarget{foundations-overview}{%
\section*{Overview}\label{foundations-overview}}
\addcontentsline{toc}{section}{Overview}

\textbf{FOUNDATIONS}

In this section the aims are to: (1) provide an overview of quantitative research and their applications, by both highlighting visible applications and notable research in various fields. (2) We will under the hood a bit and consider how quantitative research contributes to language research. (3) I will layout the main types of research and situate quantitative text analysis inside these. (4) I will provide an overview of the rest of the coursebook highlighting the learning goals, the structure of the coursebook, and how this structure will support a robust knowledge of what text analysis is, why it is used, and how to conduct your own research.

\hypertarget{data-language-and-text-analysis}{%
\section{Data, language, and text analysis}\label{data-language-and-text-analysis}}

DRAFT

\begin{quote}
Science walks forward on two feet, namely theory and experiment\ldots Sometimes it is one foot which is put forward first, sometimes the other, but continuous progress is only made by the use of both.

--- \href{https://www.nobelprize.org/uploads/2018/06/millikan-lecture.pdf}{Robert A. Millikan} \citeyearpar{Millikan1923}
\end{quote}

\begin{rmdkey}
The essential questions for this chapter are:

\begin{itemize}
\tightlist
\item
  What is the role and goals of data analysis in and outside of
  academia?
\item
  In what ways is quantitative language research approached?
\item
  What are some of the applications of text analysis?
\item
  How is this coursebook structured and what are the target learning
  goals?
\end{itemize}
\end{rmdkey}

In this chapter I will aim to introduce the topic of text analysis and text analytics and frame the approach of this coursebook. The goals of this section are to work from the general field of data science/ data analysis to the particular sub-field of text analysis (where text is defined broadly as corpus). The aim is to introduce the context needed to understand how text analysis fits in a larger universe of data analysis and see the commonalities in the ever-ubiquitous field of data analysis, with attention to how language and linguistics studies employ data analysis down to the particular area of text analysis. To round out this chapter, I will provide a general overview of the rest of the coursebook motivating the general structure and sequencing as well as setting the foundation for programmatic approaches to data analysis.

\hypertarget{making-sense-of-a-complex-world}{%
\subsection{Making sense of a complex world}\label{making-sense-of-a-complex-world}}

The world around us is full of actions and interactions so numerous that it is difficult to really comprehend. Through the lens each individual sees and experiences this world. We gain knowledge about this world and build up heuristic knowledge about how it works and how we do and can interact with it. This happens regardless of your educational background. As humans we are built for this. Our minds process countless sensory inputs many of which never make it to our conscious mind. They underlie skills and abilities that we take for granted like being able to predict what will happen if you see someone about to knock a wine glass off a table and onto a concrete floor. You've never seen this object before and this is the first time you've been to this winery, but somehow and from somewhere you `instinctively' make an effort to warn the would-be-glass-breaker before it is too late. You most likely have not stopped to consider where this predictive knowledge has come from, or if you have, you may have just chalked it up to `common sense'. As common as it may be, it is an incredible display of the brain's capacity to monitor your environment, relate the events and observations that take place, and store that information all the time not making a big fuss to tell your conscious mind what it's up to.

So wait, this is a coursebook on text analytics and language, right? So what does all this have to do with that? Well, there are two points to make that are relevant for framing our journey: (1) the world is full of countless information which unfold in real-time at a scale that is daunting and (2) for all the power of the brain that works so efficiently behind the scene making sense of the world, we are one individual living one life that has a limited view of the world at large. Let me expand on these two points a little more.

First let's be clear. There is no way for any one to experience all things at all times, i.e.~omnipotence. But even extremely reduced slices of reality are still vastly outside of our experiental capacity, at least in real-time. One can make the point that since the inception of the internet an individual's ability to experience larger slices of the world has increased. But could you imagine reading, watching, and listening to every file that is currently accessible on the web? Or has been? (See the \href{https://web.archive.org/}{Wayback Machine}.) Scale this down even further; let's take Wikipedia, the world's largest encyclopedia. Can you imagine reading every wiki entry? As large as a resource such as Wikipedia is \footnote{As of 22 July 2021, there are 6,341,359 articles in the \href{https://en.wikipedia.org/wiki/English_Wikipedia}{English Wikipedia} containing over 3.9 billion words occupying around 19 gigabytes of information.}, it is still a small fragment of the written language that is produced on the web, just the web \footnote{For reference, \href{https://commoncrawl.org/big-picture/}{Common Crawl} has millions of gigabytes collected since 2008}. Consider that for a moment.

To my second framing point, which is actually two points in one. I made underscored the efficiency of our brain's capacity to make sense of the world. That efficiency comes from some clever evolutionary twists that lead our brain to take in the world but it makes some shortcuts that compress the raw experience into heuristic understanding. What that means is that the brain is not a supercomputer. It does not store every experience in raw form, we do not have access to the records of our experience like we would imagine a computer would have access to the records logged in a database. Where our brains do excel is in making associations and predictions that help us (most of the time) navigate the complex world we inhabit. This point is key --our brains are doing some amazing work, but that work can give us the impression that we understand the world in more detail that we actually do. Let's do a little thought experiment. Close your eyes and think about the last time you saw your best friend. What were they wearing? Can you remember the colors? If your like me, or any other human, you probably will have a pretty confident feeling that you know the answers to these questions and there is a chance you a right. But it has been demonstrated in numerous experiments on human memory that our confidence does not correlate with accuracy \citep{Talarico2003, roediger:2000}. You've experienced an event, but there is no real reason that we should be our lives on what we experienced. It's a little bit scary, for sure, but the magic is that it works `good enough' for practical purposes.

So here's the deal: as humans we are (1) clearly unable to experience large swaths of experience by the simple fact that we are individuals living individual lives and (2) the experiences we do live are not recorded with precision and therefore we cannot `trust' our intuitions, at least in an absolute sense.

What does that mean for our human curiosity about the world around us and our ability to reliably make sense of it? In short it means that we need to approach understanding our world with the tools of science. Science is so powerful because it makes strides to overcome our inherit limitations as humans (breadth of our experience and recall and relational abilities) and bring a complex world into a more digestible perspective. Science starts with question, identifies and collects data, careful selected slices of the complex world, submits this data to analysis through clearly defined and reproducible procedures, and reports the results for others to evaluate. This process is repeated, modifying, and manipulating the procedures, asking new questions and positing new explanations, all in an effort to make inroads to bring the complex into tangible view.

In essence what science does is attempt to subvert our inherent limitations in understanding by drawing on carefully and purposefully collected slices of experience and letting the analysis of this experience speak, even if it goes against our intuitions (those powerful but sometime spurious heuristics that our brains use to make sense of the world).

\hypertarget{data-analysis}{%
\subsection{Data analysis}\label{data-analysis}}

At this point I've sketched an outline strengths and limitations of humans' ability to make sense of the world and why science to address these limitations. This science I've described is the one you are familiar with and it has been an indespensible tool to make sense of the world. If you are like me, this description of science may be associated with visions of white coats, labs, and petri dishes. While science's foundation still stands strong in the 21st century, a series of intellectual and technological events mid-20th century set in motion changes that have changed aspects about how science is done, not why it is done. We could call this Science 2.0, but let's use the more popularized term ``Data Science''. The recognized beginnings of Data Science are attributed to work in the ``Statistics and Data Analysis Research'' department at Bell Labs during the 1960s. Although primarily conceptual and theoretic at the time, a framework for quantitative data analysis took shape that would anticipate what would come: sizable datasets which would ``\protect\hyperlink{section-1}{\ldots{}}require advanced statistical and computational techniques \protect\hyperlink{section-1}{\ldots{}} and the software to implement them.'' \citep{Chambers2020} This framework emphasized both the inference-based research of traditional science, but also embraced exploratory research and recognized the need to address practical considerations that would arise when working with and deriving insight from an abundance of data.

Fast-forward to the 21st century a world in which machine readable data is truly in abundance. With increased computing power and innovative uses of this technology the world wide web took flight. To put this in perspective, in 2019 it was estimated that every minute 511 thousand tweets were posted, 18.1 million text messages were sent, and 188 million emails were sent \citep{DataNeverSleeps08-2021}. The data flood has not been limited to language, there are more sensors and recording devices than ever before which capture evermore swaths of the world we live in \citep{Desjardins2019}. Where increased computing power gave rise to the influx of data, it is also on of the primary methods for gathering, preparing, transforming, analyzing, and communicating insight derived from this data \citep{Donoho2017}. The vision laid out in the 1960s at Bell Labs had come to fruition.

The interest in deriving insight from the available data is now almost ubiquitous. The science of data has now reached deep into all aspects of life where making sense of the world is sought. Predicting whether a loan applicant will get a loan \citep{Bao2019}, whether a lump is cancerous \citep{Saxena2020}, what films to recommend based on your previous viewing history \citep{Gomez-Uribe2015}, what players a sports team should sign \citep{Lewis2004} all now incorporate a common set of data analysis tools.

These advances, however, are not predicated on data alone. As envisioned by researchers at Bell Labs, turning data into insight it takes computing skills (i.e.~programming), knowledge of statistics, and, importantly, substantive/ domain expertise. This triad has been popularly represented in a Venn diagram \ref{fig:intro-data-science-venn}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/02-introduction/data-science-venn-paper} 

}

\caption{Data Science Venn Diagram adapted from [Drew Conway](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram).}\label{fig:intro-data-science-venn}
\end{figure}

This same toolbelt underlies well-known public-facing language applications. From the language-capable personal assistant applications, plagiarism detection software, machine translation and search, tangible results of quantitative approaches to language are becoming standard fixtures in our lives.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/02-introduction/well-known-language-applications} 

}

\caption{Well-known language applications}\label{fig:intro-language-applications}
\end{figure}

The spread of quantitative data analysis too has taken root in academia. Even in areas that on first blush don't appear to be approached in a quantitative manner such as fields in the social sciences and humanities, data science is making important and sometimes disisplinary changes to the way that academic research is conducted. This coursebook focuses in on a domain that cuts across many of these fields; namely language. At this point let's turn to quantitative approaches to language.

\hypertarget{language-analysis}{%
\subsection{Language analysis}\label{language-analysis}}

Language is a defining characteristic of our species. As such, the study of language is of key concern to a wide variety of fields, not just linguists. The goals of various fields, however, and as such approaches to language research, vary. On the one hand some language research traditions, namely those closely associated with Noam Chomsky, eschewed quantitative approaches to language research during the later half of the 20th century and instead turned to qualitative assessment of language structure through introspective methods. On the other hand many language research programs turned to and/or developed quantitative research methods either by necessity or through theoretical principles. These quantitative research trajectories share much of the common data analysis toolbox described in the previous section. This means to a large extent language analysis projects share a common research language with other language research but also with research beyond outside of language. However, there is never a one-size-fits all approach to anything --much less data analysis. And in quantitative analysis there is a key distinction in data collection that has downstream effects in terms of procedure but also in terms of interpretation.

The key distinction, that we need to make at this point, which will provide context for our exploration of text analysis, comes down to the approach to collecting language data and the nature of that data. This distinction is between experimental and observational data collection. Experimental approaches start with a intentionally designed hypothesis and lay out a research methodology with appropriate instruments and a plan to collect data that shows promise for shedding light on the validity of the hypothesis. Experimental approaches are conducted under controlled contexts, usually a lab environment, in which participants are recruited to perform a language related task with stimuli that have been carefully curated by researchers to elicit some aspect of language behavior of interest. Experimental approaches to language research are heavily influenced by procedures adapted from psychology. This link is logical as language is a central area of study in cognitive psychology. This approach looks a much like the white-coat science that we made reference to earlier but, as in most quantitative research, has now taken advantage of the data analysis tool belt to collect and organize much larger quantities of data and conduct statistically more robust analysis procedures and communicate findings more efficiently.

Observational approaches are a bit more of a mixed bag in terms of the rationale for the study; they may either start with a testable hypothesis or in other cases may start with a more open-ended research question to explore. But a more fundamental distinction between the two is drawn in the amount of control the researcher has on contexts and conditions in which the language behavior data to be collected is produced. Observational approaches seek out records of language behavior that is produced by language speakers for communicative purposes in natural(istic) contexts. This may take place in labs (language development, language disorders, etc.), but more often than not, language is collected from sources where speakers are performing language as part of their daily lives --whether that be posting on social media, speaking on the telephone, making political speeches, writing class essays, reporting the latest news for a newspaper, or crafting the next novel destined to be a New York Times best-seller. What is more, data collected from the `wild' is varies in more in structure relative to data collected in experimental approaches and requires a number of steps to prepare the data to synch up with the data analysis tool belt.

I liken this distinction between experimental and observational data collection to the difference between farming and foraging. Experimental approaches are like farming; the groundwork for a research plan is designed, much as a field is prepared for seeding, then the researcher performs as series of tasks to produce data, just as a farmer waters and cares for the crops, the results of the process bear fruit, data in our case, and this data is harvested. Observational approaches are like foraging; the researcher scans the available environmental landscape for viable sources of data from all the naturally existing sources, these sources are assessed as to their usefulness and value to address the research question, the most viable is selected, and then the data is collected.

The data acquired from both of these approaches have their trade-offs, just as farming and foraging. Experimental approaches directly elicit language behavior in highly controlled conditions. This directness and level of control has the benefit of allowing researchers to precisely track how particular experimental conditions effect language behavior. As these conditions are an explicit part of the design and therefore the resulting language behavior can be more precisely attributed to the experimental manipulation. The primary shortcoming of experimental approaches is that there is a level of artificialness to this directness and control. Whether it is the language materials used in the task, the task itself, or the fact that the procedure takes place under supervision the language behavior elicited can diverge quite significantly from language behavior performed in natural communicative settings. Observational approaches show complementary strengths and shortcomings. Whereas experimental approaches may diverge from natural language use, observational approaches strive to identify and collected language behavior data in natural, uncontrolled, and unmonitored contexts. In this way observational approaches do not have to question to what extent the language behavior data is or is not performed as a natural communicative act. On the flipside, the contexts in which natural language communication take place are complex relative to experimental contexts. Language collected from natural contexts are nested within the complex workings of a complex world and as such inevitably include a host of factors and conditions which can prove challenging to disentangle from the language phenomenon of interest but must be addressed in order to draw reliable associations and conclusions.

The upshot, then, is twofold: (1) data collection methods matter for research design and interpretation and (2) there is no single best approach to data collection, each have their strengths and shortcomings. In the ideal, a robust science of language will include insight from both experimental and observational approaches \citep{Gilquin:2009}. And evermore there is greater appreciation for the complementary nature of experimental and observational approaches and a growing body of research which highlights this recognition. Given their particular trade-offs observational data is often used as an exploratory starting point to help build insight and form predictions that can then be submitted to experimental conditions. In this way studies based on observational data serve as an exploratory tool to gather a better and more externally valid view of language use which can then serve to make prediction that can be explore with more precision in an experimental paradigm. However, this is not always the case. Observational data is also often used in hypothesis-testing contexts as well. And furthermore, some in some language-related fields, a hypothesis-testing is not the ultimate goal for deriving knowledge and insight.

\hypertarget{text-analysis}{%
\subsection{Text analysis}\label{text-analysis}}

Text analysis is the application of data analysis procedures from data science to derive insight from textual data collected through observational methods. I have deliberately chosen the term `text analysis' to avoid what I see are the pitfalls of using some other common terms in the literature such as Corpus Linguistics, Computational Linguistics, or Digital Humanities. There are plenty of learning resources that focus specifically on one of these three fields when discussing the quantitative analysis of text. But from my perspective what is missing is a resource which underscores the fact that text analysis research and the methods employed span across a wide variety of academic fields and applications in industry. This coursebook aims to introduce you to these areas through the lens of the data and analysis procedures and not through a particular field. This approach, I hope, provides a wider view of the potential applications of using text as data and inspires you to either employ quantitative text analysis in your research and/ or to raise your awareness of the advantages of text analysis for making sense of language-related and linguistic-based phenomenon.

So what are some applications of text analysis? For most the public facing applications that stem from Computational Linguistic research, often known as Natural Language Processing by practitioners, are the most well-known applications of text analysis. Whether it be using search engines, online translators, submitting your paper to plagiarism detection software, etc. the text analysis methods we will cover are at play. These uses of text analysis are production-level applications and there is big money behind developing evermore robust text analysis methods.

In academia the use of quantitative text analysis is even more widespread, despite the lack of public fanfare. Let's run through some select studies to give you an idea of the areas that are employing text analysis, of what researchers are doing with text analysis, and to whet your interest for conducting your own text analysis project.

\begin{rmdstudy}
\citet{Eisenstein2012} track the geographic spread of neologisms from city to city in the United States using Twitter data collected between 6/2009 and 5/2011. They only used tweets with geolocation data and then associated each tweet with a zipcode using the US Census. The most populous metropolitan areas were used. They also used the demographics from these areas to make associations between lexical innovations and demographic attributes. From this analysis they are able to reconstruct a network of linguistic influence. One of the main findings is that demographically-similar cities are more likely to share linguistic influence. At the individual level, there is a strong, potentially stronger role of demographics than geographical location.
\end{rmdstudy}

\begin{rmdstudy}
\citet{Voigt2017} explore potential racial disparities in officer respect in police body camera footage. The dataset is based on body camera footage from the Oakland Police Department during April 2014. At total of 981 stops by 245 different officers were included (black 682, white 299) and resulted in 36,738 officer utterances. The authors found evidence for racial disparities in respect but not formality of utterances, with less respectful language used with the black community members.
\end{rmdstudy}

\begin{rmdstudy}
\citet{Conway2012} investigate whether the established drop in language complexity of rhetoric in election seasons is associated with election outcomes. The authors used US Democratic Primary Debates from 2004. The results suggest that although there was no overall difference in complexity between winners and losers, their pattern differed over time. Winners tended to drop the complexity of their language closer to the upcoming election.
\end{rmdstudy}

\begin{rmdstudy}
\citet{Kloumann2012} explore the extent to which languages are positively, neutrally, or negatively biased. Using Twitter, Google Books (1520-2008), NY Times newspaper (1987-2007), and music lyrics (1960-2007) the authors extract the top 5,000 most frequent words from each source and have participants rate each word for happiness (9-point scale). The results show that positive words strongly outnumber negative words overall suggesting English is positive-, and pro-social- biased.
\end{rmdstudy}

\begin{rmdstudy}
\citet{Bychkovska2017} investigates possible differences between L1-English and L1-Chinese undergraduate students' use of lexical bundles, multiword sequences which are extended collocations (i.e.~as the result of), in argumentative essays. The authors used the Michigan Corpus of Upper-Level Student Papers (MICUSP) corpus using the argumentative essay section for L1-English and the Corpus of Ohio Learner and Teacher English (COLTE) for the L1-Chinese English essays. They found that L1-Chinese writers used more than 2 times as many bundle types than L1-English peers which they attribute to L1-Chinese writers attempt to avoid uncommon expressions and/or due to their lack of register awareness (conversation has more bundles than writing generally).
\end{rmdstudy}

\begin{rmdstudy}
\citet{Jaeger:2007a} use a corpus study to investigate the phenomenon of syntactic persistence, the increased tendency for speakers to use a particular syntactic form over an alternate when the syntactic form is recently processed. The authors attempt to distinguish between two competing explanations for the phenomenon: (1) transient activation, where the increased tendency is short-lived and time-bound and (2) implicit learning, where the increased tendency is a reflect of learning mechanisms. The use of a speech corpora (Switchboard and spoken BNC) were used to avoid the artificialness that typically occurs in experimental settings. The authors investigated the ditransitive alternation (NP PP/ NP NP), voice alternation (active/ passive), and complementizer/ relativizer omission. In these alternations structural bias was established by measuring the probability for a verb form to appear in one of the two syntactic forms. Then the probability that that form (target) would change given previous exposure to the alternative form (prime) was calculated; what the authors called surprisal. Distance between the prime structure and the target verb were considered in the analysis. In these alternations, the less common structure was used in the target more often when the when it corresponded to the prime form (higher surprisal) suggesting that implicit learning underlies syntactic persistence effects.
\end{rmdstudy}

\begin{rmdstudy}
\citet{Wulff2007} explore differences between British and American English at the lexico-syntactic level in the \emph{into}-causative construction (ex. `He tricked me into employing him.'). The analysis uses newspaper text (The Guardian and LA Times) and the findings suggest that American English uses this construction in verbal persuasion verbs whereas British English uses physical force verbs.
\end{rmdstudy}

\begin{rmdstudy}
\citet{Mosteller1963} provide a method for solving the authorship debate surrounding The Federalist papers \footnote{\url{https://guides.loc.gov/federalist-papers/full-text}}. They employ a probabilistic approach using the word frequency profiles of the articles with known authors to predict authorship of the disputed 12 papers. The results suggest that the disputed papers were most likely authored by Madison.
\end{rmdstudy}

\begin{rmdstudy}
\citet{Olohan2008} investigate the extent to which translated texts differ from native texts. In particular the author explores the notion of explicitation in translated texts (the tendency to make information in the source text explicit in the target translation). The study makes use of the Translational English Corpus (TEC) for translation samples and comparable sections of the British National Corpus (BNC) for the native samples. The results suggest that there is a tendency for syntactic explicitation in the translational corpus (TEC) which is assumed to be a subconscious process employed unwittingly by translators.
\end{rmdstudy}

This sample of studies include research from areas such as translation, stylistics, language variation, dialectology, psychology, psycholinguistics, political science, and sociolinguistics which highlights the diversity of fields and subareas which employ quantitative text analysis. Text analysis is at the center of these studies as they share a set of common goals:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To detect and retrieve patterns from text too subtle or too numerous to be done by hand
\item
  To challenge assumptions and/or provide other views from textual sources
\item
  To explore new questions and/or provide novel insight
\end{enumerate}

Let's now turn to the last section of this chapter which will provide an overview of the rationale for doing learning to do text analysis, the structure of the content covered, and a justification for the approach we will take to perform text analysis.

\hypertarget{coursebook-overview}{%
\subsection{Coursebook overview}\label{coursebook-overview}}

In this section I will provide a general overview of the rest of the coursebook motivating the general structure and sequencing as well as setting the foundation for programmatic approaches to data analysis. Let me highlight why I think this is a valuable area of study, what I hope you gain from this coursebook, and how the structure of this coursebook is configured to help scaffold your conceptual and practical knowledge of text analysis.

The target learning outcomes in this coursebook are the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data Literacy
\item
  Research Skills
\item
  Programming Skills
\end{enumerate}

\textbf{Data Literacy} refers to the ability to interpret, assess, and contextualize findings based on data. Throughout this coursebook we will explore topics which will help you understand how data analysis methods derive insight from data. In this process you will be encouraged to critically evaluate connections across linguistic and language-related disciplines using data analysis knowledge and skills. Data Literacy is an invaluable skillset for academics and professionals (cite) but also is an indispensable aptitude for in the 21st century citizens to navigate and actively participate in the `Information Age' in which we live \citep{Carmi2020}.

\textbf{Research skills} covers the ability to conduct original research, communicate findings, and make meaningful connections with findings in the literature of the field. This target area does not differ significantly, in spirit, from common learning outcomes in a research methods course: identify an area of investigation, develop a viable research question or hypothesis, collect relevant data, analyze data with relevant statistical methods, and interpret and communicate findings. However, working with text will incur a series of key steps in the selection, collection, and preparation of the data that are unique to text analysis projects. In addition, I will stress the importance of research documentation and creating reproducible research as an integral part of modern scientific inquiry \citep{Buckheit1995}.

\textbf{Programming skills} aims to develop your ability to implement research skills programmatically and produce research that is replicable and collaborative. Modern data analysis, and by extension, text analysis is conducted using programming. There are various key reasons for this: (1) programming affords researchers unlimited research freedom --if you can envision it, you can program it. The same cannot be said for off-the-shelf software which is either proprietary or unmaintained --or both. (2) programming underlies well-documented and reproducible research --documenting button clicks and menu option selections leads to research which is not readily reproduced, either by some other researcher or by your future self! (3) programming forces researchers to engage more intimately with the data and the methods for analysis. The more familiar you are with the data and the methods the more likely you are to produce higher quality work.

Now let me turn to how these learning goals integrate and shape the structure and sequencing of the following chapters.

In Part II ``Orientation'' we will build our Data Literacy skills working from data to insight. This progression is visualized in Figure \ref{fig:diki-hierarchy} \footnote{Adapted from \citet{Ackoff1989}}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/02-introduction/diki-hierarchy-paper} 

}

\caption{Data to Insight Hierarchy (DIKI)}\label{fig:diki-hierarchy}
\end{figure}

The DIKI Hierarchy highlights the stages and intermediate steps required to derive insight from data. \protect\hyperlink{understanding-data}{Chapter 2 ``Understanding data''} will cover both Data and Information covering the conceptual topics of populations versus samples and how language data samples are converted to information and the forms that they can take. In \protect\hyperlink{approaching-analysis}{Chapter 3 ``Approaching analysis''} I will discuss the distinction between descriptive and analytic statistics. In brief they are important for data analysis, but descriptive statistics serve as a sanity check on the dataset before submitting it to interrogation --which is the goal of analytic statistics. We will also cover some of the main distinctions between analytics approaches including inference-, exploration-, and prediction-based methods. With a fundamental understanding of data, information, and knowledge we will then move to \protect\hyperlink{framing-research}{Chapter 4 ``Framing research''} where we will discuss how to develop a research plan, or what I will call a `research blueprint'. At this point we will directly address Research Skills and elaborate on how research really comes together; how to bring yourself up to speed with the literature on a topic, how to develop a research goal or hypothesis, how to select data which is viable to address the research goal or hypothesis, how to determine the necessary information and appropriate measures to prepare for analysis, how to perform diagnostic statistics on the data and make adjustments before analysis, how to select and perform the relevant analytic statistics given the research goals, how to report your findings, and finally, how to structure your project so that it is well-documented and reproducible.

Part III ``Preparation'' and Part IV ``Analysis'' serve as practical and more detailed guides to the R programming strategies to conduct text analysis research and as such develop your Programming Skills. In \href{}{Chapter 5 ``Acquire data''} I will discuss three main strategies for accessing data: direct downloads, Automatic Programming Interfaces (APIs), and web scraping. In \href{}{Chapter 6 ``Curate data''} I will outline the process for converting or augmenting the acquired data into a structured format, therefore creating information. This will include organizing linguistic and non-linguistic metadata into one dataset. In \href{}{Chapter 7 ``Transform data''} I describe how to work with a curated dataset to derive more detailed information and appropriate dataset structures that are appropriate for the upcoming analysis.

\href{}{Chapters 8 ``Inference''}, \href{}{9 ``Prediction''}, and \href{}{10 ``Exploration''} focus on different categories of statistical analysis each associated with distinct research goals. Inference deals with analysis methods associated with standard hypothesis-testing. This will include some common statistical models employed in text analysis: chi-squared, logistic regression, and linear regression. Prediction covers methods for modeling associations in data with the aim to accurately predict outcomes on new textual data. I will cover some standard methods for text classification including Näive Bayes, \emph{k}-nearest neighbors (\emph{k}-NN), and decisions tree and random forest models. Exploration covers a variety of analysis methods such as association measures, clustering, topic modeling, and vector-space models. These methods are aligned with research goals that aim to interpret patterns that arise in from the data itself.

\hypertarget{summary}{%
\subsection*{Summary}\label{summary}}
\addcontentsline{toc}{subsection}{Summary}

In this chapter I started with some general observations about the difficulty of making sense of a complex world. The standard approach to overcoming inherent human limitations in sense making is science. In the 21st century the toolbelt for doing scientific research and exploration has grown in terms of the amount of data available, the statistical methods for analyzing the data, and the computational power to manage, store, and share the data, methods, and results from quantitative research. The methods and tools for deriving insight from data have made significant inroads in and outside academia, and increasingly figure in the quantitative investigation of language. Text analysis is a particular branch of this enterprise based on observational data from real-world language and is used in a wide variety of fields. This coursebook aims to develop your knowledge and skills in three fundamental areas: Data Literacy, Research Skills, and Programming Skills.

In the end I hope that you enjoy this exploration into text analysis. Although learning curve at times may seem steep --the experience you will gain will not only improve your data literacy, research skills, and programmings skills but also enhance your appreciation for the richness of human language and its important role in our everyday lives.

\hypertarget{activities}{%
\subsection*{Activities}\label{activities}}
\addcontentsline{toc}{subsection}{Activities}

\hypertarget{part-orientation}{%
\part{Orientation}\label{part-orientation}}

\hypertarget{orientation-overview}{%
\section*{Overview}\label{orientation-overview}}
\addcontentsline{toc}{section}{Overview}

\textbf{ORIENTATION}

Before we begin working on the specifics of our data project, it is important to establish a fundamental understanding of the characteristics of each of the levels in the DIKI Hierarchy (Figure \ref{fig:diki-hierarchy}) and the roles each of these levels have in deriving insight from data. In \protect\hyperlink{understanding-data}{Chapter 2} we will explore the Data and Information levels drawing a distinction between two main types of data (populations and samples) and then cover how data is structured and transformed to generate information (datasets) that is fit for statistical analysis. In \protect\hyperlink{approaching-analysis}{Chapter 3} I will outline the importance and distinct types of statistical procedures (descriptive and analytic) that are commonly used in text analysis. \protect\hyperlink{framing-research}{Chapter 4} aims to tie these concepts together and cover the required steps for preparing a research blueprint to conduct an original text analysis project.

\hypertarget{understanding-data}{%
\section{Understanding data}\label{understanding-data}}

DRAFT

\begin{quote}
The plural of anecdote is not data.

--- Marc Bekoff
\end{quote}

\begin{rmdkey}
The essential questions for this chapter are:

\begin{itemize}
\tightlist
\item
  What are the distinct types of data and how do they differ?
\item
  What is information and what form does it take?
\item
  What is the importance of documentation in quantitative research?
\end{itemize}
\end{rmdkey}

In this chapter I cover the starting concepts in our journey to understand how to derive insight from data, illustrated in the DIKI Hierarchy (Figure \ref{fig:diki-hierarchy}), focusing specifically on the first two levels: Data and Information. We will see that what is commonly referred to as `data' everyday uses is broken into three distinct categories, two of which are referred to as data and the third is known as information. We will also cover the importance of documentation of data and datasets in quantitative research.

\hypertarget{data}{%
\subsection{Data}\label{data}}

Data is data, right? The term `data' is so common in popular vernacular it is easy to assume we know what we mean when we say `data'. But as in most things, where there are common assumptions there are important details the require more careful consideration. Let's turn to the first key distinction that we need to make to start to break down the term `data': the difference between populations and samples.

\hypertarget{populations}{%
\subsubsection{Populations}\label{populations}}

The first thing that comes to many people's mind when the term population is used is human populations. Say for example --What's the population of Milwuakee? When we speak of a population in these terms we are talking about the total sum of people living within the geographical boundaries of Milwaukee. In concrete terms, a \textbf{population} is the objective make up of an idealized set of objects and events in reality. Key terms here are objective and idealized. Although we can look up the US Census report for Milwaukee and retrieve a figure for the population, this cannot truly be the population. Why is that? Well, whatever method that was used to derive this numerical figure was surely incomplete. If not incomplete, by the time someone recorded the figure some number of residents of Milwaukee moved out, moved in, were born, or passed away --the figure is no longer the true population.

Likewise when we talk about populations in terms of language we dealing with an objective and idealized aspect of reality. Let's take the words of the English language as an analog to our previous example population. In this case the words are the people and English is the bounding characteristic. Just as people, words move out, move in, are born, and pass away. Any compendium of the words of English at any moment is almost instananeously incomplete. This is true for all populations, save those in which the bounding characteristics select a narrow slice of reality which is objectively measurable and whose membership is fixed (the complete works of Shakespeare, for example).

In sum, (most) populations are amorphous moving targets. We objectively hold them to exist, but in practical terms we often cannot nail down the specifics of populations. So how do researchers go about studying populations if they are theoretically impossible to access directly? The strategy employed is called sampling.

\hypertarget{sampling}{%
\subsubsection{Sampling}\label{sampling}}

A \textbf{sample} is the product of a subjective process of selecting a finite set of observations from an objective population with the goal of capturing the relevant characteristics of the target population. Although there are strategies to minimize the mismatch between the characteristics of the subjective sample and objective population, it is important to note that it is almost certainly true that any given sample diverges from the population it aims to represent to some degree. The aim, however, is to employ a series of sampling decisions, which are collectively known as a sampling frame, that maximize the chance of representing the population.

What are the most common sampling strategies? First \textbf{sample size}. A larger sample will always be more representative than a smaller sample. Sample size, however, is not enough. It is not hard to imagine a large sample which by chance captures only a subset of the features of the population. A next step to enhance sample representativeness is apply \textbf{random sampling}. Together a large random sample has an even better chance of reflecting the main characteristics of the population better than a large or random sample. But, random as random is, we still run the risk of acquiring a skewed sample (i.e a sample which does not mirror the target population).

To help mitigate these issues, there are two more strategies that can be applied to improve sample representativeness. Note, however, that while size and random samples can be applied to any sample with little information about internal characteristics of the population, these next two strategies require decisions depend on the presumed internal characteristics of the population. The first of these more informed sampling strategies is called \textbf{stratified sampling}. Stratified samples make (educated) assumptions about sub-components within the population of interest. With these sub-populations in mind, large random samples are acquired for each sub-population, or strata. At a minimum, stratified samples can be no less representative than random sampling alone, but the chances that the sample is better increases. Can there be problems in the approach? Yes, and on two fronts. First knowledge of the internal components of a population are often based on a limited or incomplete knowledge of the population. In other words, strata are selected subjectively by researchers using various heuristics some of which are based on some sense of `common knowledge'. The second front that stratified sampling can err concerns the relative sizes of the sub-components relative to the whole population. Even if the relevant sub-components are identified, their relative size adds another challenge in which researchers must face in order to maximize the representativeness of a sample. To attempt to align, or \textbf{balance}, the relative sizes of the samples for the strata is the second population-informed sampling strategy.

A key feature of a sample is that it is purposely selected. Samples are not simply a collection or set of data from the population. Samples are rigorously selected with an explicit target population in mind. In text analysis a purposely sampled collection of texts, of the type defined here, is known as a \textbf{corpus.} For this same reason a set of texts or documents which have not been selected along a purposely selected sampling frame is not a corpus. The sampling frame, and therefore the populations modeled, in any given corpus most likely will vary and for this reason it is not a safe assumption that any given corpus is equally applicable for any and every research question. Corpus development (i.e.~sampling) is purposeful, and the characteristics of the corpus development process should be made explicit through documentation. Therefore vetting a corpus sample for its applicability to a research goal is a key step in that a research must take to ensure the integrity of the research findings.

\begin{rmdquestion}
The Brown Corpus is widely recognized as one of the first large, machine-readable corpora. It was compiled by \citet{Kucera1967}. Consult the \href{http://korpus.uib.no/icame/brown/bcm.html}{documentation for this corpus}. Can you determine what language population this corpus aims to represent? Given the sampling frame for this corpus (in the documentation and summarized in Figure \ref{fig:brown-distribution}), what types of research might this corpus support or not support?
\end{rmdquestion}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{03-understanding-data_files/figure-latex/brown-distribution-1} 

}

\caption{Brown Corpus of Written American English}\label{fig:brown-distribution}
\end{figure}

\hypertarget{corpora}{%
\subsubsection{Corpora}\label{corpora}}

\hypertarget{types}{%
\paragraph{Types}\label{types}}

With the notion of sampling frames in mind, some corpora are compiled with the aim to be of general purpose (general or \textbf{reference corpora}), and some with much more specialized sampling frames (\textbf{specialized corpora}). For example, the \href{https://www.anc.org/}{American National Corpus (ANC)} or the \href{http://www.natcorp.ox.ac.uk/}{British National Corpus (BNC)} are corpora which aim to model (represent/ reflect) the general characteristics of the English language, the former of American English and the later British English. These are ambitious projects, and require significant investments of time in corpus design and then in implementation (and continued development) that are usually undertaken by research teams \citep{Adel2020}.

Specialized corpora aim to represent more specific populations. The \href{https://www.linguistics.ucsb.edu/research/santa-barbara-corpus}{Santa Barbara Corpus of Spoken American English (SBCSAE)}, as you can imagine from the name of the resource, aims to model spoken American English. No claim to written English is included. There are even more specific types of corpora which attempt to model other types of sub-populations such as scientific writing, \href{https://www.clarin.eu/resource-families/cmc-corpora}{computer-mediated communication (CMC)}, language use in specific \href{http://ice-corpora.net/ice/index.html}{regions of the world}, a \href{https://cesa.arizona.edu}{country}, or a \href{https://cesa.arizona.edu}{region}, etc.

Another set of specialized corpora are resources which aim to compile texts from different languages or different language varieties for direct or indirect comparison. Corpora that are directly comparable, that is they include source and translated texts, are called \textbf{parallel corpora}. Parallel corpora include different languages or language varieties that are indexed and aligned at some linguistic level (i.e.~word, phrase, sentence, paragraph, or document), see \href{https://opus.nlpl.eu/}{OPUS}. Corpora that are compiled with different languages or language varieties but are not directly aligned are called \textbf{comparable corpora}. The comparable language or language varieties are sampled with the same or similar sampling frame, for example \href{https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/0402}{Brown} and \href{https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/0167}{LOB} corpora.

The aim of the quantitative text researcher is to select the corpus or corpora (plural of corpus) which best aligns with the purpose of the research. Therefore a general corpus such as the ANC may be better suited to address a question dealing with the way American English works, but this general resource may lack detail in certain areas, such as \href{http://www.hd.uib.no/icame/ij22/vihla.pdf}{medical language}, that may be vital for a research project aimed at understanding changes in medical terminology.

\hypertarget{sources}{%
\paragraph{Sources}\label{sources}}

The most common source of data used in contemporary quantitative research is the internet. On the web an investigator can access corpora published for research purposes and language used in natural settings that can be coerced by the investigator into a corpus. Many organizations exist around the globe that provide access to corpora in browsable catalogs, or \textbf{repositories}. There are repositories dedicated to language research, in general, such as the \href{https://www.ldc.upenn.edu/}{Language Data Consortium} or for specific language domains, such as the language acquisition repository \href{http://talkbank.org/}{TalkBank}. It is always advisable to start looking for the available language data in a repository. The advantage of beginning your data search in repositories is that a repository, especially those geared towards the linguistic community, will make identifying language corpora faster than through a general web search. Furthermore, repositories often require certain standards for corpus format and documentation for publication. A standardized resource many times will be easier to interpret and evaluate for its appropriateness for a particular research project.

In the table below I've compiled a list of some corpus repositories to help you get started.

\begin{table}

\caption{\label{tab:pinboard-repositories}A list of some corpus repositories}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="https://corpus.byu.edu/">BYU corpora</a> & A repository of corpora that includes billions of words of data.\\
\hline
<a href="http://corporafromtheweb.org/">COW (COrpora from the Web)</a> & A collection of linguistically processed gigatoken web corpora\\
\hline
<a href="http://wortschatz.uni-leipzig.de/en/download/">Leipzig Corpora Collection</a> & Corpora in different languages using the same format and comparable sources.\\
\hline
<a href="https://www.ldc.upenn.edu/">Linguistic Data Consortium</a> & Repository of language corpora\\
\hline
<a href="http://www.resourcebook.eu/searchll.php">LRE Map</a> & Repository of language resources collected during the submission process for the Language Resource and Evaluation Conference (LREC).\\
\hline
<a href="http://www.nltk.org/nltk\_data/">NLTK language data</a> & Repository of corpora and language datasets included with the Python package NLTK.\\
\hline
<a href="http://opus.lingfil.uu.se/">OPUS - an open source parallel corpus</a> & Repository of translated texts from the web.\\
\hline
<a href="http://talkbank.org/">TalkBank</a> & Repository of language collections dealing with conversation, acquisition, multilingualism, and clinical contexts.\\
\hline
<a href="https://corpus1.mpi.nl/ds/asv/?4">The Language Archive</a> & Various corpora and language datasets\\
\hline
<a href="http://ota.ox.ac.uk/">The Oxford Text Archive (OTA)</a> & A collection of thousands of texts in more than 25 different languages.\\
\hline
\end{tabular}
\end{table}

Repositories are by no means the only source of corpora on the web. Researchers from around the world provide access to corpora and other data sources on their own sites or through data sharing platforms. Corpora of various sizes and scopes will often be accessible on a dedicated homepage or appear on the homepage of a sponsoring institution. Finding these resources is a matter of doing a web search with the word `corpus' and a list of desired attributes, including language, modality, register, etc. As part of a general movement towards reproducibility more corpora are available on the web than ever before. Therefore data sharing platforms supporting reproducible research, such as \href{https://github.com/}{GitHub}, \href{https://zenodo.org/}{Zenodo}, \href{http://www.re3data.org/}{Re3data}, \href{https://osf.io/}{OSF}, etc., are a good place to look as well, if searching repositories and targeted web searches do not yield results.

In the table below you will find a list of corpus resources and datasets.

\begin{table}

\caption{\label{tab:pinboard-corpora}Corpora and language datasets.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="http://www.socsci.uci.edu/\textasciitilde{}lpearl/CoLaLab/CHILDESTreebank/childestreebank.html">CHILDES Treebank</a> & A corpus derived from several corpora from the American English section of CHILDES with the goal to annotate child-directed speech utterance transcriptions with phrase structure tree information.\\
\hline
<a href="http://www.cs.cornell.edu/\textasciitilde{}cristian/Cornell\_Movie-Dialogs\_Corpus.html">Cornell Movie-Dialogs Corpus</a> & A corpus containing a large metadata-rich collection of fictional conversations extracted from raw movie scripts.\\
\hline
<a href="http://www.lllf.uam.es/\textasciitilde{}fmarcos/informes/corpus/coarginl.html">Corpus Argentino</a> & Corpus of Argentine Spanish\\
\hline
<a href="https://cesa.arizona.edu/">Corpus of Spanish in Southern Arizona</a> & Spanish varieties spoken in Arizona.\\
\hline
<a href="https://www.statmt.org/europarl/">Europarl Parallel Corpus</a> & A parallel corpus extracted from the proceedings of the European Parliament Proceedings between 1996-2011.\\
\hline
<a href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html">Google Ngram Viewer</a> & Google web corpus\\
\hline
<a href="http://ice-corpora.net/ice/">International Corpus of English (ICE)</a> & The International Corpus of English (ICE) began in 1990 with the primary aim of collecting material for comparative studies of English worldwide.\\
\hline
<a href="http://opus.lingfil.uu.se/OpenSubtitles\_v2.php">OpenSubtitles2011</a> & A collection of documents from http://www.opensubtitles.org/.\\
\hline
<a href="http://www.ruscorpora.ru/en/">Russian National Corpus</a> & A corpus of modern Russian language incorporating over 300 million words.\\
\hline
<a href="https://quantumstat.com//dataset">The Big Bad NLP Database - Quantum Stat</a> & NLP datasets\\
\hline
<a href="https://catalog.ldc.upenn.edu/docs/LDC97S62/">The Switchboard Dialog Act Corpus</a> & A corpus of 1155 5-minute conversations in American English, comprising 205,000 utterances and 1.4 million words, from the Switchboard corpus of telephone conversations.\\
\hline
<a href="http://langsnap.soton.ac.uk/">Welcome to LANGSNAP - LANGSNAP</a> & The aim of this repository is to promote research on the learning of French and Spanish as L2, by making parallel learner corpora for each language freely available to the research community.\\
\hline
<a href="http://www.psych.ualberta.ca/\textasciitilde{}westburylab/downloads/usenetcorpus.download.html">Westbury Lab Web Site: Usenet Corpus Download</a> & This corpus is a collection of public USENET postings. This corpus was collected between Oct 2005 and Jan 2011, and covers 47,860 English language, non-binary-file news groups (see list of newsgroups included with the corpus for details)\\
\hline
\end{tabular}
\end{table}

Language corpora prepared by researchers and research groups listed on repositories or hosted by the researchers themselves is often the first place to look for data. The web, however, contains a wealth of language and language-related data that can be accessed by researcher to compile their own corpus. There are two primary ways to attain language data from the web. The first is through the process of web scraping. Web scraping is the process of harvesting data from the web either manually or (semi-)automatically from the actual public-facing web. The second way to acquire data from the web is through an Application Programming Interface (API). APIs are, as the title suggests, programming interfaces which allow access, under certain conditions, to information that a website or database accessible via the web contains.

The table below lists some R packages that serve to interface language data directly through R.

\begin{table}

\caption{\label{tab:pinboard-apis}R Package interfaces to language corpora and datasets.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="https://ropensci.org/tutorials/arxiv\_tutorial.html">aRxiv</a> & R package interface to query arXiv, a repository of electronic preprints for computer science, mathematics, physics, quantitative biology, quantitative finance, and statistics.\\
\hline
<a href="https://github.com/ropensci/crminer">crminer</a> & R package interface focusing on getting the user full text via the Crossref search API.\\
\hline
<a href="https://github.com/ropensci/dvn">dvn</a> & R package interface to access to the Dataverse Network APIs.\\
\hline
<a href="https://ropensci.org/tutorials/fulltext\_tutorial.html">fulltext</a> & R package interface to query open access journals, such as PLOS.\\
\hline
<a href="https://ropensci.org/tutorials/gutenbergr\_tutorial.html">gutenbergr</a> & R package interface to download and process public domain works from the Project Gutenberg collection.\\
\hline
<a href="https://ropensci.org/tutorials/internetarchive\_tutorial.html">internetarchive</a> & R package interface to query the Internet Archive.\\
\hline
<a href="https://github.com/hrbrmstr/newsflash">newsflash</a> & R package interface to query the Internet Archive and GDELT Television Explorer\\
\hline
<a href="https://github.com/ropensci/oai">oai</a> & R package interface to query any OAI-PMH repository, including Zenodo.\\
\hline
<a href="https://github.com/ropensci/rfigshare">rfigshare</a> & R package interface to query the data sharing platform FigShare.\\
\hline
<a href="https://github.com/ropensci/rtweet">rtweet</a> & R client for interacting with Twitter's APIs\\
\hline
\end{tabular}
\end{table}

Data for language research is not limited to (primary) text sources. Other sources may include processed data from previous research; word lists, linguistic features, etc.. Alone or in combination with text sources this data can be a rich and viable source of data for a research project.

Below I've included some processed language resources.

\begin{table}

\caption{\label{tab:pinboard-experimental}Language data from previous research and meta-studies.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="http://elexicon.wustl.edu/WordStart.asp">English Lexicon Project</a> & Access to a large set of lexical characteristics, along with behavioral data from visual lexical decision and naming studies.\\
\hline
<a href="https://github.com/ropensci/lingtypology">lingtypology</a> & R package interface to connect with the Glottolog database and provides additional functionality for linguistic mapping.\\
\hline
<a href="https://nyu-mll.github.io/CoLA/">The Corpus of Linguistic Acceptability (CoLA)</a> & A corpus that consists of 10657 sentences from 23 linguistics publications, expertly annotated for acceptability (grammaticality) by their original authors.\\
\hline
<a href="http://icon.shef.ac.uk/Moby/">The Moby lexicon project</a> & Language wordlists and resources from the Moby project.\\
\hline
\end{tabular}
\end{table}

The list of data available for language research is constantly growing. I've document very few of the wide variety of resources. Below I've included attempts by others to provide a summary of the corpus data and language resources available.

\begin{table}

\caption{\label{tab:pinboard-listings}Lists of corpus resources.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="https://uclouvain.be/en/research-institutes/ilc/cecl/learner-corpora-around-the-world.html">Learner corpora around the world</a> & A listing of learner corpora around the world\\
\hline
<a href="https://paperswithcode.com/datasets">Machine Learning Datasets | Papers With Code</a> & A free and open resource with Machine Learning papers, code, and evaluation tables.\\
\hline
<a href="http://nlp.stanford.edu/links/statnlp.html\#Corpora">Stanford NLP corpora</a> & Listing of corpora and language resources aimed at the NLP community.\\
\hline
<a href="https://makingnoiseandhearingthings.com/2017/09/20/where-can-you-find-language-data-on-the-web/">Where can you find language data on the web?</a> & Listing of various corpora and language datasets.\\
\hline
\end{tabular}
\end{table}

\begin{rmdquestion}
Explore some of the resources listed above and consider their sampling frames. Can you think of a research question or questions that this resource may be well-suited to support research into? What types of questions would be less-than-adequate for a given resource?
\end{rmdquestion}

\hypertarget{formats}{%
\paragraph{Formats}\label{formats}}

A corpus will often include various types of non-linguistic attributes, or \textbf{meta-data}, as well. Ideally this will include information regarding the source(s) of the data, dates when it was acquired or published, and other author or speaker information. It may also include any number of other attributes that were identified as potentially important in order to appropriately document the target population. Again, it is key to match the available meta-data with the goals of your research. In some cases a corpus may be ideal in some aspects but not contain all the key information to address your research question. This may mean you will need to compile your own corpus if there are fundamental attributes missing. Before you consider compiling your own corpus, however, it is worth investigating the possibility of augmenting an available corpus to bring it inline with your particular goals. This may include adding new language sources, harnessing software for linguistic annotation (part-of-speech, syntactic structure, named entities, etc.), or linking available corpus meta-data to other resources, linguistic or non-linguistic.

Corpora come in various formats, the main three being: running text, structured documents, and databases. The format of a corpus is often influenced by characteristics of the data but may also reflect an author's individual preferences as well. It is typical for corpora with few meta-data characteristics to take the form of running text.

Running text sample from the \href{https://www.statmt.org/europarl/}{Europarle Parallel Corpus}.

\begin{verbatim}
> Resumption of the session
> I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.
> Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.
> You have requested a debate on this subject in the course of the next few days, during this part-session.
> In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.
> Please rise, then, for this minute' s silence.
> (The House rose and observed a minute' s silence)
> Madam President, on a point of order.
> You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.
> One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.
\end{verbatim}

In corpora with more meta-data, a header may be appended to the top of each running text document or the meta-data may be contained in a separate file with appropriate coding to coordinate meta-data attributes with each text in the corpus.

Meta-data header sample from the \href{}{Switchboard Dialog Act Corpus}.

\begin{verbatim}
> FILENAME: 4325_1632_1519
> TOPIC#:       323
> DATE:     920323
> TRANSCRIBER:  glp
> UTT_CODER:    tc
> DIFFICULTY:   1
> TOPICALITY:   3
> NATURALNESS:  2
> ECHO_FROM_B:  1
> ECHO_FROM_A:  4
> STATIC_ON_A:  1
> STATIC_ON_B:  1
> BACKGROUND_A: 1
> BACKGROUND_B: 2
> REMARKS:        None.
> 
> =========================================================================
> 
> 
> o          A.1 utt1: Okay.  /
> qw          A.1 utt2: {D So, }
> 
> qy^d          B.2 utt1: [ [ I guess, +
> 
> +          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /
> 
> +          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /
> 
> qy          A.5 utt1: Does it say something? /
> 
> sd          B.6 utt1: I think it usually does.  /
> ad          B.6 utt2: You might try, {F uh, }  /
> h          B.6 utt3: I don't know,  /
> ad          B.6 utt4: hold it down a little longer,  /
> ad          B.6 utt5: {C and } see if it, {F uh, } -/
\end{verbatim}

When meta-data and/ or linguistic annotation increases in complexity it is common to structure each corpus document more explicitly with a markup language such as XML (Extensible Markup Language) or organize relationships between language and meta-data attributes in a database.

XML format for meta-data (and linguistic annotation) from the \href{http://www.nltk.org/nltk_data/}{Brown Corpus}.

\begin{verbatim}
> <TEI xmlns="http://www.tei-c.org/ns/1.0"><teiHeader><fileDesc><titleStmt><title>Sample A01 from  The Atlanta Constitution</title><title type="sub"> November 4, 1961, p.1 "Atlanta Primary ..."
>  "Hartsfield Files"
>  August 17, 1961, "Urged strongly ..."
>  "Sam Caldwell Joins"
>  March 6,1961, p.1 "Legislators Are Moving" by Reg Murphy
>  "Legislator to fight" by Richard Ashworth
>  "House Due Bid..."
>  p.18 "Harry Miller Wins..."
> </title></titleStmt><editionStmt><edition>A part  of the XML version of the Brown Corpus</edition></editionStmt><extent>1,988 words 431 (21.7%) quotes 2 symbols</extent><publicationStmt><idno>A01</idno><availability><p>Used by permission of The Atlanta ConstitutionState News Service (H), and Reg Murphy (E).</p></availability></publicationStmt><sourceDesc><bibl> The Atlanta Constitution</bibl></sourceDesc></fileDesc><encodingDesc><p>Arbitrary Hyphen: multi-million [0520]</p></encodingDesc><revisionDesc><change when="2008-04-27">Header auto-generated for TEI version</change></revisionDesc></teiHeader>
> <text xml:id="A01" decls="A">
> <body><p><s n="1"><w type="AT">The</w> <w type="NP" subtype="TL">Fulton</w> <w type="NN" subtype="TL">County</w> <w type="JJ" subtype="TL">Grand</w> <w type="NN" subtype="TL">Jury</w> <w type="VBD">said</w> <w type="NR">Friday</w> <w type="AT">an</w> <w type="NN">investigation</w> <w type="IN">of</w> <w type="NPg">Atlanta's</w> <w type="JJ">recent</w> <w type="NN">primary</w> <w type="NN">election</w> <w type="VBD">produced</w> <c type="pct">``</c> <w type="AT">no</w> <w type="NN">evidence</w> <c type="pct">''</c> <w type="CS">that</w> <w type="DTI">any</w> <w type="NNS">irregularities</w> <w type="VBD">took</w> <w type="NN">place</w> <c type="pct">.</c> </s>
> </p>
\end{verbatim}

Although there has been a push towards standardization of corpus formats, most available resources display some degree of idiosyncrasy. Being able to parse the structure of a corpus is a skill that will develop with time. With more experience working with corpora you will become more adept at identifying how the data is stored and whether its content and format will serve the needs of your analysis.

\hypertarget{information}{%
\subsection{Information}\label{information}}

Identifying an adequate corpus resource for the target research question is the first step in moving a quantitative text research project forward. The next step is to select the components or characteristics of this resource that are relevant for the research and then move to organize the attributes of this data into a more useful and informative format. This is the process of converting a corpus into a \textbf{dataset} --a tabular representation of the information to be leveraged in the analysis.

\hypertarget{structure}{%
\subsubsection{Structure}\label{structure}}

Data alone is not informative. Only through explicit organization of the data in a way that makes relationships accessible does the data become information. This is a particularly salient hurdle in text analysis research. Some textual data is \emph{unstructured} --that is, the relationships that will be used in the analysis have yet to be explicitly drawn and organized from the text to make the relationships meaningful and useful for analysis.

For the running text in the Europarle Corpus, we know that there are files which are the source text (original) and files that correspond to the target text (translation). In Table \ref{tab:structure-europarle} we see that this text has been organized so that there are columns corresponding to the \texttt{type} and \texttt{sentence} with an additional \texttt{sentence\_id} column to keep an index of how the sentences are aligned.

\begin{rmdtip}
It is conventional to work with column names for datasets in R using the
same conventions that are used for naming objects. It is a matter of
taste which convention is used, but I have adopted
\href{https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/names.html\#snake_case}{snake
case} as my personal preference. There are also
\href{https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/names.html}{alternatives}.
Regardless of the convention you choose, it is good practice to be
consistent.

It is also of note that the column names should be balanced for
meaningfulness and brevity. This brevity is of practical concern but can
be somewhat opaque. For questions into the meaning of the column and is
values consult the resource's documentation.
\end{rmdtip}

\begin{table}

\caption{\label{tab:structure-europarle}First 10 source and target sentences in the Europarle Corpus.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 1 & Resumption of the session\\
Target & 1 & Reanudación del período de sesiones\\
Source & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Target & 2 & Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\\
Source & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
\addlinespace
Target & 3 & Como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\
Source & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
Target & 4 & Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\
Source & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Target & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\\
\addlinespace
Source & 6 & Please rise, then, for this minute' s silence.\\
Target & 6 & Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
Source & 7 & (The House rose and observed a minute' s silence)\\
Target & 7 & (El Parlamento, de pie, guarda un minuto de silencio)\\
Source & 8 & Madam President, on a point of order.\\
\addlinespace
Target & 8 & Señora Presidenta, una cuestión de procedimiento.\\
Source & 9 & You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\\
Target & 9 & Sabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en Sri Lanka.\\
Source & 10 & One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\\
Target & 10 & Una de las personas que recientemente han asesinado en Sri Lanka ha sido al Sr. Kumar Ponnambalam, quien hace pocos meses visitó el Parlamento Europeo.\\
\bottomrule
\end{tabular}
\end{table}

Other corpus resources are \emph{semi-structured} --that is, there are some characteristics which are structured, but other which are not.

The Switchboard Dialog Act Corpus is an example of a semi-structured resource. It has meta-data associated with each of the 1,155 conversations in the corpus. In Table \ref{tab:structure-swda} a language-relevant sub-set of the meta-data is associated with each utterance.

\begin{table}

\caption{\label{tab:structure-swda}First 5 utterances from the Switchboard Dialog Act Corpus.}
\centering
\begin{tabular}[t]{lrrllllll}
\toprule
doc\_id & speaker\_id & topic\_num & topicality & naturalness & damsl\_tag & speaker & utterance\_num & utterance\_text\\
\midrule
4325 & 1632 & 323 & 3 & 2 & o & A & 1 & Okay.  /\\
4325 & 1632 & 323 & 3 & 2 & qw & A & 2 & \{D So, \}\\
4325 & 1519 & 323 & 3 & 2 & qy\textasciicircum{}d & B & 1 & {}[ [ I guess, +\\
4325 & 1632 & 323 & 3 & 2 & + & A & 1 & What kind of experience [ do you, + do you ] have, then with child care? /\\
4325 & 1519 & 323 & 3 & 2 & + & B & 1 & I think, ] + \{F uh, \} I wonder ] if that worked. /\\
\bottomrule
\end{tabular}
\end{table}

Relatively fewer resources are \emph{structured}. In these cases a high amount of meta-data and/ or linguistic annotation is included in the corpus. The format convention, however, varies from resource to resource. Some of the formats are programming general (.csv, .xml, .json, etc.) and others are resource specific (.cha, .utt, .prd, etc.). In Table \ref{tab:structure-brown} the XML version of the Brown Corpus is represented in tabular format. Note that along with other meta-data variables, it also contains a variable with linguistic annotation for grammatical category (\texttt{pos} part-of-speech) of each word.

\begin{table}

\caption{\label{tab:structure-brown}First 10 words from the Brown Corpus.}
\centering
\begin{tabular}[t]{llll}
\toprule
document\_id & category & words & pos\\
\midrule
01 & A & The & AT\\
01 & A & Fulton & NP\\
01 & A & County & NN\\
01 & A & Grand & JJ\\
01 & A & Jury & NN\\
\addlinespace
01 & A & said & VBD\\
01 & A & Friday & NR\\
01 & A & an & AT\\
01 & A & investigation & NN\\
01 & A & of & IN\\
\bottomrule
\end{tabular}
\end{table}

In this coursebook, the selection of the attributes from a corpus and the juxtaposition of these attributes in a relational format, or dataset, that converts data into information will be referred to as \textbf{data curation}. The process of data curation minimally involves creating a base dataset, or \emph{derived dataset}, which establishes the main informational associations according to philosophical approach outlined by \citet{Wickham2014a}. In this work, a `tidy' dataset refers both to the structural (physical) and informational (semantic) organization of the dataset. Physically, a tidy dataset is a tabular data structure where each \emph{row} is an observation and each \emph{column} is a variable that contains measures of a feature or attribute of each observation. Each cell where a given row-column intersect contains a \emph{value} which is a particular attribute of a particular observation for the particular observation-feature pair also known as a \emph{data point}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/03-understanding-data/tidy-format-paper} 

}

\caption{Visual summary of the tidy format.}\label{fig:tidy-format-image}
\end{figure}

Semantic value in a tidy dataset is derived from the association of this physical structure along the two dimensions of this rectangular format. First, each column is a \textbf{variable} which reflects measures for a particular attribute. In the Europarle Corpus dataset, in Table \ref{tab:structure-europarle}, for example, the \texttt{type} column measures the type of text, either \texttt{Source} or \texttt{Target}. Columns can contain measures which are qualitative or quantitative, that is character-based or numeric. Second, each row is an \textbf{observation} that contains all of the variables associated with the primary unit of observation. The primary unit of observation the variable that is the essential focus of the informational structure. In this same dataset the first observation contains the \texttt{type}, \texttt{sentence\_id}, and the \texttt{sentence}. As this dataset is currently structured the primary unit of investigation is the \texttt{sentence} as each of the other variables have measures that characterize each value of \texttt{sentence}.

The decision as to what the primary unit of observation is is fundamentally guided by the research question, and therefore highly specific to the particular research project. Say instead we wanted to focus on words instead of sentences. The dataset would need to be transformed such that a new variable (\texttt{words}) would be created to contain each word in the corpus.

\begin{table}

\caption{\label{tab:tidy-words-europarle}Europarle Paralle Corpus with `words` as primary unit of investigation.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & words\\
\midrule
Source & 1 & Resumption\\
Source & 1 & of\\
Source & 1 & the\\
Source & 1 & session\\
Target & 1 & Reanudación\\
\addlinespace
Target & 1 & del\\
Target & 1 & período\\
Target & 1 & de\\
Target & 1 & sesiones\\
\bottomrule
\end{tabular}
\end{table}

The values for the variables \texttt{type} and \texttt{sentence\_id} maintain the necessary description for each \texttt{word} to ensure the required semantic relationships to identify the particular attributes for each word observation. This dataset may seem redundant in that the values for \texttt{type} and \texttt{sentence\_id} are repeated numerous times but this `redundancy' makes the relationship between each variable associated with the primary unit of investigation explicit. This format makes a tidy dataset a versatile format for researchers to conduct analyses in a powerful and flexible way, as we will see throughout this coursebook.

It is important to make clear that data in tabular format in itself does not constitute a dataset, in the tidy sense we will be using. Data can be organized in many ways which do not make relationships between variables and observations explicit.

\begin{rmdquestion}
All tabular data does not have the `tidy' format that I have described
here. Can you think of examples of tabular information that would not be
in a tidy format?
\end{rmdquestion}

\hypertarget{transformation}{%
\subsubsection{Transformation}\label{transformation}}

At this point have introduced the first step in data curation in which the original data is converted into a relational dataset (derived dataset) and highlighted the importance of this informational structure for setting the stage for data analysis. However, the primary derived dataset is often not the final organizational step before proceeding to statistical analysis. Many times, if not always, the derived dataset requires some manipulation or transformation to prepare the dataset for the specific analysis approach to be taken. This is another level of human intervention and informational organization, and therefore another step forward in our journey from data to insight and as such a step up in the DIKI hierarchy. Common types of transformations include cleaning variables (normalization), separating or eliminating variables (recoding), creating new variables (generation), or incorporating others datasets which integrate with the existing variables (merging). The results of these transformations build on and manipulate the derived dataset and produce an \emph{analysis dataset}. Let's now turn to provide a select set of examples of each of these transformations using the datasets we have introduced in this chapter.

\hypertarget{normalization}{%
\paragraph{Normalization}\label{normalization}}

The process of normalization aims to \emph{sanitize} the values within a variable or set of variables. This may include removing whitespace, punctuation, numerals, or special characters or substituting uppercase for lowercase characters, numerals for word versions, acronyms for their full forms, irregular or incorrect spelling for accepted forms, or removing common words (stopwords), etc.

On inspecting the Europarle dataset (Table \ref{tab:structure-europarle}) we will see that there are sentence lines which do not represent actual parliment speeches. In Table \ref{tab:normalize-non-speech-identify-europarle} we see these lines.

\begin{table}

\caption{\label{tab:normalize-non-speech-identify-europarle}Non-speech lines in the Europarle dataset.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 1 & Resumption of the session\\
Target & 1 & Reanudación del período de sesiones\\
Source & 7 & (The House rose and observed a minute' s silence)\\
Target & 7 & (El Parlamento, de pie, guarda un minuto de silencio)\\
\bottomrule
\end{tabular}
\end{table}

A research project aiming to analyze speech would want to normalize this dataset removing these lines, as seen in Table \ref{tab:normalize-non-speech-remove-europarle}.

\begin{table}

\caption{\label{tab:normalize-non-speech-remove-europarle}The Europarle dataset with non-speech lines removed.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Target & 2 & Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\\
Source & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Target & 3 & Como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\
Source & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
\addlinespace
Target & 4 & Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\
Source & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Target & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\\
Source & 6 & Please rise, then, for this minute' s silence.\\
Target & 6 & Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
\addlinespace
Source & 8 & Madam President, on a point of order.\\
Target & 8 & Señora Presidenta, una cuestión de procedimiento.\\
Source & 9 & You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\\
Target & 9 & Sabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en Sri Lanka.\\
Source & 10 & One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\\
\addlinespace
Target & 10 & Una de las personas que recientemente han asesinado en Sri Lanka ha sido al Sr. Kumar Ponnambalam, quien hace pocos meses visitó el Parlamento Europeo.\\
\bottomrule
\end{tabular}
\end{table}

Another feature of this dataset which may require attention is the fact that the English lines include whitespace between possessive nouns.

\begin{table}

\caption{\label{tab:normalize-whitespace-identify-europarle}Lines with possessives with extra whitespace in the Europarle dataset.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Source & 6 & Please rise, then, for this minute' s silence.\\
\bottomrule
\end{tabular}
\end{table}

This may affect another transformation process or subsequent analysis, so it may be a good idea to normalize these forms by removing the extra whitespace.

\begin{table}

\caption{\label{tab:normalize-whitespace-remove-europarle}The Europarle dataset with whitespace from possessives removed.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 5 & In the meantime, I should like to observe a minute's silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Source & 6 & Please rise, then, for this minute's silence.\\
\bottomrule
\end{tabular}
\end{table}

A final normalization case scenario involves changing converting all the text to lowercase. If the goal for the research is to count words at some point the fact that a word starts a sentence and by convention the first letter is capitalized will result distinct counts for words that are in essence the same (i.e.~``In'' vs.~``in'').

\begin{table}

\caption{\label{tab:normalize-lowercase-europarle}The Europarle dataset with lowercasing applied.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 2 & i declare resumed the session of the european parliament adjourned on friday 17 december 1999, and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Target & 2 & declaro reanudado el período de sesiones del parlamento europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a sus señorías mi deseo de que hayan tenido unas buenas vacaciones.\\
Source & 3 & although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Target & 3 & como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. en cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\
Source & 4 & you have requested a debate on this subject in the course of the next few days, during this part-session.\\
\addlinespace
Target & 4 & sus señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\
Source & 5 & in the meantime, i should like to observe a minute's silence, as a number of members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the european union.\\
Target & 5 & a la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la unión europea afectados.\\
Source & 6 & please rise, then, for this minute's silence.\\
Target & 6 & invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
\addlinespace
Source & 8 & madam president, on a point of order.\\
Target & 8 & señora presidenta, una cuestión de procedimiento.\\
Source & 9 & you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka.\\
Target & 9 & sabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en sri lanka.\\
Source & 10 & one of the people assassinated very recently in sri lanka was mr kumar ponnambalam, who had visited the european parliament just a few months ago.\\
\addlinespace
Target & 10 & una de las personas que recientemente han asesinado en sri lanka ha sido al sr. kumar ponnambalam, quien hace pocos meses visitó el parlamento europeo.\\
\bottomrule
\end{tabular}
\end{table}

Note that lowercasing text, and normalization steps in general, can come at a cost. For example, lowercasing the Europarle dataset sentences means we lose potentially valuable information; namely the ability to identify proper names (i.e.~``Mr Kumar Ponnambalam'') and titles (i.e.~``European Parliament'') directly from the orthographic forms. There are, however, transformation steps that can be applied which aim to recover `lost' information in situations such as this and others.

\hypertarget{recoding}{%
\paragraph{Recoding}\label{recoding}}

The process of recoding aims to \emph{recast} the values of a variable or set of variables to a new variable or set of variables to enable more direct access. This may include extracting values from a variable, stemming or lemmatization of words, tokenization of linguistic forms (words, ngrams, sentences, etc.), calculating the lengths of linguistic units, removing variables that will not be used in the analysis, etc.

Words that we intuitively associate with a `base' word can take many forms in language use. For example the word forms `investigation', `investigation', `investigate', `investigated', etc. are intuitively linked. There are two common methods that can be applied to create a new variable to facilitate the identification of these associations. The first is stemming. Stemming is a rule-based heuristic to reduce word forms to their stem or root form.

\begin{table}

\caption{\label{tab:recoding-stemming-brown-example}Results for stemming the first words in the Brown Corpus.}
\centering
\begin{tabular}[t]{lllll}
\toprule
document\_id & category & words & pos & word\_stems\\
\midrule
01 & A & The & AT & The\\
01 & A & Fulton & NP & Fulton\\
01 & A & County & NN & Counti\\
01 & A & Grand & JJ & Grand\\
01 & A & Jury & NN & Juri\\
\addlinespace
01 & A & said & VBD & said\\
01 & A & Friday & NR & Fridai\\
01 & A & an & AT & an\\
01 & A & investigation & NN & investig\\
01 & A & of & IN & of\\
\bottomrule
\end{tabular}
\end{table}

A few things to note here. First there are a number of stemming algorithms both for individual languages and distinct languages \footnote{\url{https://snowballstem.org/algorithms/}}. Second not all words can be stemmed as they do not have alternate morphological forms (i.e.~``The'', ``of'', etc.). This generally related to the distinction between closed-class (articles, prepositions, conjunctions, etc.) and open-class (nouns, verbs, adjectives, etc.) grammatical categories. Third the stem generated for those words that can be stemmed result in forms that are not words themselves. Nonetheless, stems can be very useful for more easily extracting a set of related word forms.

As an example, let's identify all the word forms for the stem `investig'.

\begin{table}

\caption{\label{tab:recoding-stemming-brown-search}Results for filter word stems for "investig" in the Brown Corpus.}
\centering
\begin{tabular}[t]{lllll}
\toprule
document\_id & category & words & pos & word\_stems\\
\midrule
01 & A & investigation & NN & investig\\
01 & A & investigate & VB & investig\\
03 & A & investigation & NN & investig\\
03 & A & investigation & NN & investig\\
07 & A & investigations & NNS & investig\\
\addlinespace
07 & A & investigate & VB & investig\\
08 & A & investigation & NN & investig\\
09 & A & investigation & NN & investig\\
09 & A & investigating & VBG & investig\\
09 & A & investigation & NN & investig\\
\bottomrule
\end{tabular}
\end{table}

We can see from the results in Table \ref{tab:recoding-stemming-brown-search} that searching for \texttt{word\_stems} that match `investig' returns a set of stem-related forms. But it is worth noting that these forms cut across a number of grammatical categories. If instead you want to draw a distinction between grammatical categories, we can apply lemmatization. This process is distinct from stemming in two important ways: (1) inflectional forms are grouped by grammatical category and (2) the resulting forms are lemmas or `base' forms of words.

\begin{table}

\caption{\label{tab:recoding-lemmatization-brown-example}Results for lemmatization of the first words in the Brown Corpus.}
\centering
\begin{tabular}[t]{lllll}
\toprule
document\_id & category & words & pos & word\_lemmas\\
\midrule
01 & A & The & AT & The\\
01 & A & Fulton & NP & Fulton\\
01 & A & County & NN & County\\
01 & A & Grand & JJ & Grand\\
01 & A & Jury & NN & Jury\\
\addlinespace
01 & A & said & VBD & say\\
01 & A & Friday & NR & Friday\\
01 & A & an & AT & a\\
01 & A & investigation & NN & investigation\\
01 & A & of & IN & of\\
\bottomrule
\end{tabular}
\end{table}

To appreciate the difference between stemming and lemmatization, let's compare a filter for \texttt{word\_lemmas} which match `investigation'.

\begin{table}

\caption{\label{tab:recoding-lemmatization-brown-investigation}Results for filter word stems for "investigation" in the Brown Corpus.}
\centering
\begin{tabular}[t]{lllll}
\toprule
document\_id & category & words & pos & word\_lemmas\\
\midrule
01 & A & investigation & NN & investigation\\
03 & A & investigation & NN & investigation\\
03 & A & investigation & NN & investigation\\
07 & A & investigations & NNS & investigation\\
08 & A & investigation & NN & investigation\\
\addlinespace
09 & A & investigation & NN & investigation\\
09 & A & investigation & NN & investigation\\
23 & A & investigation & NN & investigation\\
25 & A & investigation & NN & investigation\\
41 & A & investigation & NN & investigation\\
\bottomrule
\end{tabular}
\end{table}

Only lemma forms of `investigate' which are nouns appear. Let's run a similar search but for the lemma `be'.

\begin{table}

\caption{\label{tab:recoding-lemmatization-brown-be}Results for filter word stems for "be" in the Brown Corpus.}
\centering
\begin{tabular}[t]{lllll}
\toprule
document\_id & category & words & pos & word\_lemmas\\
\midrule
01 & A & was & BEDZ & be\\
01 & A & been & BEN & be\\
01 & A & was & BEDZ & be\\
01 & A & was & BEDZ & be\\
01 & A & are & BER & be\\
\addlinespace
01 & A & are & BER & be\\
01 & A & be & BE & be\\
01 & A & is & BEZ & be\\
01 & A & was & BEDZ & be\\
01 & A & be & BE & be\\
\bottomrule
\end{tabular}
\end{table}

Again only words of the same grammatical category are returned. In this case the verb `be' has many more inflectional forms than `investigate'.

Another form of recoding is to detect a pattern in the values of an existing variable and create a new variable whose values are the extracted pattern or register that the pattern occurs and/ or how many times it occurs. As an example, let's count the number of disfluencies (`uh' or `um') that occur in each utterance in \texttt{utterance\_text} from the Switchboard Dialog Act Corpus. \emph{Note I've simplified the dataset dropping the non-relevant variables for this example.}

\begin{table}

\caption{\label{tab:recoding-extract-switchboard}Disfluency counts in the first 10 utterance text values from the Switchboard Corpus.}
\centering
\begin{tabular}[t]{lr}
\toprule
utterance\_text & disfluency\_count\\
\midrule
Okay.  / & 0\\
\{D So, \} & 0\\
{}[ [ I guess, + & 0\\
What kind of experience [ do you, + do you ] have, then with child care? / & 0\\
I think, ] + \{F uh, \} I wonder ] if that worked. / & 1\\
\addlinespace
Does it say something? / & 0\\
I think it usually does.  / & 0\\
You might try, \{F uh, \}  / & 1\\
I don't know,  / & 0\\
hold it down a little longer,  / & 0\\
\bottomrule
\end{tabular}
\end{table}

One of the most common forms of recoding in text analysis is tokenization. Tokenization is the process of recasting the text into smaller linguistic units. When working with text that has not been linguistically annotated, the most feasible linguistic tokens are words, ngrams, and sentences. While word and sentence tokens are easily understandable, ngram tokens need some explanation. An ngram is a sequence of either characters or words where \emph{n} is the length of this sequence. The ngram sequences are drawn incrementally, so the bigrams (two-word sequences) for the sentence ``This is an input sentence.'' are:

this is, is an, an input, input sentence

We've already seen word tokenization exemplified with the Europarle Corpus in subsection \protect\hyperlink{structure}{Structure} in Table \ref{tab:tidy-words-europarle}, so let's create (word) bigram tokens for this corpus.

\begin{table}

\caption{\label{tab:recoding-tokenization-europarle-bigram-words}The first 10 word bigrams of the Europarle Corpus.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & word\_bigrams\\
\midrule
Source & 2 & i declare\\
Source & 2 & declare resumed\\
Source & 2 & resumed the\\
Source & 2 & the session\\
Source & 2 & session of\\
\addlinespace
Source & 2 & of the\\
Source & 2 & the european\\
Source & 2 & european parliament\\
Source & 2 & parliament adjourned\\
Source & 2 & adjourned on\\
\bottomrule
\end{tabular}
\end{table}

As I just mentioned, ngrams sequences can be formed of characters as well. Here are character trigram (three-character) sequences.

\begin{table}

\caption{\label{tab:recoding-tokenization-europarle-trigram-chars}The first 10 character trigrams of the Europarle Corpus.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & char\_trigrams\\
\midrule
Source & 2 & ide\\
Source & 2 & dec\\
Source & 2 & ecl\\
Source & 2 & cla\\
Source & 2 & lar\\
\addlinespace
Source & 2 & are\\
Source & 2 & rer\\
Source & 2 & ere\\
Source & 2 & res\\
Source & 2 & esu\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{generation}{%
\paragraph{Generation}\label{generation}}

The process of generation aims to \emph{augment} a variable or set of variables. In essence this aims to make implicit attributes explicit to that they are directly accessible. This often targeted at the automatic generation of linguistic annotations such as grammatical category (part-of-speech) or syntactic structure.

In the examples below I've added linguistic annotation to a target (English) and source (Spanish) example sentence from the Europarle Parallel Corpus. First, note the variables that are added to our dataset that correspond to grammatical category. In addition to the \texttt{type} and \texttt{sentence\_id} we have an assortment of variables which replace the \texttt{sentence} variable. As part of the process of annotation the input text to be annotated \texttt{sentence} is tokenized \texttt{token} and indexed \texttt{token\_id}. Then \texttt{upos} contains the Universal Part of Speech tags\footnote{\href{https://universaldependencies.org/u/pos/}{Descriptions of the UPOS tagset}}, and a detailed list of features is included in \texttt{feats}. The syntactic annotation is reflected in the \texttt{token\_id\_source} and \texttt{syntactic\_relation} variables. These variables correspond to the type of syntactic parsing that has been done, in this case Dependency Parsing (using the \href{https://universaldependencies.org/}{Universal Dependencies} framework). Another common syntactic parsing framework is phrase constituency parsing \citep{Jurafsky2020}.

\begin{table}

\caption{\label{tab:generation-europarle-en-example}Automatic linguistic annotation for grammatical category and syntactic structure for an example English sentence from the Europarle Corpus}
\centering
\begin{tabular}[t]{lrllllll}
\toprule
type & sentence\_id & token\_id & token & upos & feats & token\_id\_source & syntactic\_relation\\
\midrule
Target & 6 & 1 & Invito & ADP & NA & 3 & case\\
Target & 6 & 2 & a & DET & Definite=Ind|PronType=Art & 3 & det\\
Target & 6 & 3 & todos & NOUN & Number=Plur & 6 & nmod\\
Target & 6 & 4 & a & DET & Definite=Ind|PronType=Art & 6 & det\\
Target & 6 & 5 & que & ADJ & Degree=Pos & 6 & amod\\
\addlinespace
Target & 6 & 6 & nos & NOUN & Number=Plur & 0 & root\\
Target & 6 & 7 & pongamos & X & NA & 13 & goeswith\\
Target & 6 & 8 & de & X & Foreign=Yes & 13 & goeswith\\
Target & 6 & 9 & pie & X & NA & 13 & goeswith\\
Target & 6 & 10 & para & X & NA & 13 & goeswith\\
\addlinespace
Target & 6 & 11 & guardar & X & NA & 13 & goeswith\\
Target & 6 & 12 & un & X & NA & 13 & goeswith\\
Target & 6 & 13 & minuto & NOUN & Number=Sing & 6 & appos\\
Target & 6 & 14 & de & PROPN & Number=Sing & 15 & compound\\
Target & 6 & 15 & silencio & PROPN & Number=Sing & 13 & flat\\
\addlinespace
Target & 6 & 16 & . & PUNCT & NA & 6 & punct\\
\bottomrule
\end{tabular}
\end{table}

Now compare the English example sentence dataset in Table \ref{tab:generation-europarle-en-example} with the parallel sentence in Spanish. Note that the grammatical features are language specific. For example, Spanish has gender which is apparent when scanning the \texttt{feats} variable.

\begin{table}

\caption{\label{tab:generation-europarle-es-example}Automatic linguistic annotation for grammatical category and syntactic structure for an example Spanish sentence from the Europarle Corpus}
\centering
\begin{tabular}[t]{lrllllll}
\toprule
type & sentence\_id & token\_id & token & upos & feats & token\_id\_source & syntactic\_relation\\
\midrule
Source & 6 & 1 & Please & PROPN & Gender=Fem|Number=Sing & 4 & nsubj\\
Source & 6 & 2 & rise & PROPN & Number=Sing & 1 & flat\\
Source & 6 & 3 & , & PUNCT & NA & 1 & punct\\
Source & 6 & 4 & then & VERB & Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin & 0 & root\\
Source & 6 & 5 & , & PUNCT & NA & 9 & punct\\
\addlinespace
Source & 6 & 6 & for & ADP & NA & 9 & compound\\
Source & 6 & 7 & this & X & NA & 9 & compound\\
Source & 6 & 8 & minute's & X & Gender=Masc|Number=Sing & 9 & compound\\
Source & 6 & 9 & silence & X & Gender=Masc|Number=Sing & 4 & conj\\
Source & 6 & 10 & . & PUNCT & NA & 4 & punct\\
\bottomrule
\end{tabular}
\end{table}

There is much more to explore with linguistic annotation, and syntactic parsing in particular, but at this point it will suffice to note that it is possible to augment a dataset with grammatical information automatically.

There are strengths and shortcomings with automatic linguistic annotation that a research should be aware of. First, automatic linguistic annotation provides quick access to rich and highly reliable linguistic information for a large number of languages. However, part of speech taggers and syntactic parsers are not magic. They are resources that are built by training a computational algorithm to recognize patterns in manually annotated datasets producing a language model. This model is then used to predict the linguistic annotations for new language (as we just did in the previous examples). The shortcomings of automatic linguistic annotation is first, not all languages have trained language models and second, the data used to train the model inevitably reflect a particular variety, register, modality, etc. The accuracy of the linguistic annotation is highly dependent on alignment between the language sampling frame of the trained data and the language data to be automatically annotated. Many (most) of the language models available for automatic linguistic annotation are based on language that is most readily available and for most languages this has traditionally been newswire text. It is important to be aware of these characteristics when using linguistic annotation tools.

\hypertarget{merging}{%
\paragraph{Merging}\label{merging}}

The process of merging aims to \emph{join} a variable or set of variables with another variable or set of variables from another dataset. The option to merge two (or more) datasets requires that there is a shared variable that indexes and aligns the datasets.

To provide an example let's look at the Switchboard Diaglog Act Corpus. Our existing, disfluency recoded, version includes the following variables.

\begin{verbatim}
#> Rows: 5
#> Columns: 11
#> $ doc_id           <chr> "4325", "4325", "4325", "4325", "4325"
#> $ speaker_id       <dbl> 1632, 1632, 1519, 1632, 1519
#> $ topic_num        <dbl> 323, 323, 323, 323, 323
#> $ topicality       <chr> "3", "3", "3", "3", "3"
#> $ naturalness      <chr> "2", "2", "2", "2", "2"
#> $ damsl_tag        <chr> "o", "qw", "qy^d", "+", "+"
#> $ speaker          <chr> "A", "A", "B", "A", "B"
#> $ turn_num         <chr> "1", "1", "2", "3", "4"
#> $ utterance_num    <chr> "1", "2", "1", "1", "1"
#> $ utterance_text   <chr> "Okay.  /", "{D So, }", "[ [ I guess, +", "What kind ~
#> $ disfluency_count <int> 0, 0, 0, 0, 1
\end{verbatim}

It turns out that on the \href{https://catalog.ldc.upenn.edu/docs/LDC97S62/}{corpus website} a number of meta-data files are available, including files pertaining to speakers and the topics of the conversations.

The speaker meta-data for this corpus is in a the \texttt{caller\_tab.csv} file and contains a \texttt{speaker\_id} variable which corresponds to each speaker in the corpus and other potentially relevant variables for a language research project including \texttt{sex}, \texttt{birth\_year}, \texttt{dialect\_area}, and \texttt{education}.

\begin{table}

\caption{\label{tab:merging-swda-speaker-table}Speaker meta-data for the Switchboard Dialog Act Corpus.}
\centering
\begin{tabular}[t]{rlrlr}
\toprule
speaker\_id & sex & birth\_year & dialect\_area & education\\
\midrule
1632 & FEMALE & 1962 & WESTERN & 2\\
1632 & FEMALE & 1962 & WESTERN & 2\\
1519 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
1632 & FEMALE & 1962 & WESTERN & 2\\
1519 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
\bottomrule
\end{tabular}
\end{table}

Since both datasets contain a shared index, \texttt{speaker\_id} we can merge these two datasets. The result is found in Table \ref{tab:merging-swda-speaker-added}.

\begin{table}

\caption{\label{tab:merging-swda-speaker-added}Merged conversations and speaker meta-data for the Switchboard Dialog Act Corpus.}
\centering
\begin{tabular}[t]{lrlrlrrlllllllr}
\toprule
doc\_id & speaker\_id & sex & birth\_year & dialect\_area & education & topic\_num & topicality & naturalness & damsl\_tag & speaker & turn\_num & utterance\_num & utterance\_text & disfluency\_count\\
\midrule
4325 & 1632 & FEMALE & 1962 & WESTERN & 2 & 323 & 3 & 2 & o & A & 1 & 1 & Okay.  / & 0\\
4325 & 1632 & FEMALE & 1962 & WESTERN & 2 & 323 & 3 & 2 & qw & A & 1 & 2 & \{D So, \} & 0\\
4325 & 1519 & FEMALE & 1971 & SOUTH MIDLAND & 1 & 323 & 3 & 2 & qy\textasciicircum{}d & B & 2 & 1 & {}[ [ I guess, + & 0\\
4325 & 1632 & FEMALE & 1962 & WESTERN & 2 & 323 & 3 & 2 & + & A & 3 & 1 & What kind of experience [ do you, + do you ] have, then with child care? / & 0\\
4325 & 1519 & FEMALE & 1971 & SOUTH MIDLAND & 1 & 323 & 3 & 2 & + & B & 4 & 1 & I think, ] + \{F uh, \} I wonder ] if that worked. / & 1\\
\bottomrule
\end{tabular}
\end{table}

In this example case the dataset that was merged was already in a structured format (.csv). Many corpus resources contain meta-data in stand-off files that are structured.

In some cases a researcher would like to merge information that does not already accompany the corpus resource. This is possible as long as a dataset can be created that contains a variable that is shared. Without a shared variable to index the datasets the merge cannot take place.

In sum, the transformation steps described here collectively aim to produce higher quality datasets that are relevant in content and structure to submit to analysis. The process may include one or more of the previous transformations but is rarely linear and is most often iterative. It is typical to do some normalization then generation, then recoding, and then return to normalizing, and so forth. This process is highly idiosyncratic given the characteristics of the derived dataset and the ultimate goals for the analysis dataset.

\begin{rmdtip}
Note in some cases we may convert our tidy tabular dataset to other data
formats that may be required for some particular statistic approaches
but at all times the relationship between the variables should be
maintained in line with our research purpose. We will touch on examples
of other types of data formats (e.g.~Corpus and Document-Term Matrix
(DTM) objects in R) when we dive into particular statistical approaches
that require them later in the coursebook.
\end{rmdtip}

\hypertarget{documentation}{%
\subsection{Documentation}\label{documentation}}

As we have seen in this chapter that acquiring data and converting that data into information involves a number of conscious decisions and implementation steps. As a favor to ourselves as researchers and to the research community, it is crucial to document these decisions and steps. This makes it both possible to retrace our own steps and also provides a guide for future researchers that want to reproduce and/ or build on your research. A programmatic approach to quantitative research helps ensure that the implementation steps are documented and reproducible but it is also vital that the decisions that are made are documented as well. This includes the creation/ selection of the corpus data, the description of the variables chosen from the corpus for the derived dataset, and the description of the variables created from the derived dataset for the analysis dataset.

For an existing corpus sample acquired from a repository (e.g.~\href{https://catalog.ldc.upenn.edu/LDC97S62}{Switchboard Dialog Act Corpus}, Language Data Consortium), a research group (e.g.~\href{http://cedel2.learnercorpora.com/}{CEDEL2}), or an individual researcher (e.g.~\href{https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/}{SMS Spam Collection}), there is often documentation provided describing key attributes of the resource. This documentation should be included with the acquisition of the corpus and added to the research project. For a corpus that a researcher compiles themselves, they will need to generate this documentation.

The curation and transformation steps conducted on the original corpus data to produce the datasets should also be documented. The steps themselves can be included in the programming scripts as code comments (or in prose if using a literate programming strategy (e.g.~RMarkdown)). The structure of each resulting dataset should include what is called a \textbf{data dictionary}. This is a table which includes the variable names, the values they contain, and a short prose description of each variable (e.g.~\href{https://osf.io/9jafz/}{ACTIV-ES Corpus}).

\hypertarget{summary-1}{%
\subsection*{Summary}\label{summary-1}}
\addcontentsline{toc}{subsection}{Summary}

In this chapter we have focused on data and information --the first two components of DIKI Hierarchy. This process is visualized in Figure \ref{fig:understanding-data-vis-sum}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/03-understanding-data/understanding-data_visual-summary-paper} 

}

\caption{Understanding data: visual summary}\label{fig:understanding-data-vis-sum}
\end{figure}

First a distinction is made between populations and samples, the latter being a intentional and subjective selection of observations from the world which attempt to represent the population of interest. The result of this process is known as a corpus. Whether developing a corpus or selecting an existing a corpus it is important to vet the sampling frame for its applicability and viability as a resource for a given research project.

Once a viable corpus is identified, then that corpus is converted into a derived dataset which adopts the `tidy' dataset format where each column is a variable, each row is an observation, and the intersection of columns and rows contain values. This derived dataset serves to establish the base informational relationships from which your research will stem.

The derived dataset will most likely require transformations including normalization, recoding, generation, and/ or merging to enhance the usefulness of the information to analysis. An analysis dataset is the result of this process.

Although covered at the end of this chapter, documentation should be implemented at each stage of the process. Employing a programmatic approach establishes documentation of the implementation steps but the motivation behind the decisions taken and the content of the corpus data and datasets generated also need documentation to ensure transparent and reproducible research.

\hypertarget{approaching-analysis}{%
\section{Approaching analysis}\label{approaching-analysis}}

DRAFT

\begin{quote}
Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write.

--- H.G. Wells
\end{quote}

\begin{rmdkey}
The essential questions for this chapter are:

\begin{itemize}
\tightlist
\item
  What is the role of statistics in data analysis?
\item
  What is the importance of descriptive assessment in data analysis?
\item
  In what ways are main approaches to data analysis similar and
  different?
\end{itemize}
\end{rmdkey}

In this chapter I will build on the notions of data and information from the previous chapter. The aim of statistics in quantitative analysis is to uncover patterns in datasets. Thus statistics is aimed at deriving knowledge from information, the next step in the DIKI Hierarchy (Figure \ref{fig:understanding-data-vis-sum}). Where the creation of information from data involves human intervention and conscious decisions, as we have seen, deriving knowledge from information involves even more conscious subjective decisions on what information to assess, and what method to select to interrogate the information, and ultimately how to interpret the findings. The first step is to conduct a descriptive assessment of the information, both at the individual variable level and also between variables, the second is to interrogate the dataset either through inferential, predictive, or exploratory analysis methods, and the third is to interpret and report the findings.

\hypertarget{description}{%
\subsection{Description}\label{description}}

A descriptive assessment of the dataset includes a set of diagnostic measures and tabular and visual summaries which provide researchers a better understanding of the structure of a dataset, prepare the researcher to make decisions about which statistical methods and/ or tests are most appropriate, and to safeguard against false assumptions (missing data, data distributions, etc.). In this section we will first cover the importance of understanding the informational value that variables can represent and then move to use this understanding to approach summarizing individual variables and relationships between variables.

To ground this discussion I will introduce a new dataset. This dataset is drawn from the \href{https://slabank.talkbank.org/access/English/BELC.html}{Barcelona English Language Corpus (BELC)}, which is found in the \href{http://talkbank.org/}{TalkBank repository}. I've selected the ``Written composition'' task from this corpus which contains writing samples from second language learners of English at different ages. Participants were given the task of writing for 15 minutes on the topic of ``Me: my past, present and future''. Data was collected for many (but not all) participants up to four times over the course of seven years. In Table \ref{tab:belc-overview} I've included the first 10 observations from the dataset which reflects structural and transformational steps I've done so we start with a tidy dataset.

\begin{table}

\caption{\label{tab:belc-overview}First 10 observations of the BELC dataset for demonstration.}
\centering
\begin{tabular}[t]{lllrrr}
\toprule
participant\_id & age\_group & sex & num\_tokens & num\_types & ttr\\
\midrule
L02 & 10-year-olds & female & 12 & 12 & 1.000\\
L05 & 10-year-olds & female & 18 & 15 & 0.833\\
L10 & 10-year-olds & female & 36 & 26 & 0.722\\
L11 & 10-year-olds & female & 10 & 8 & 0.800\\
L12 & 10-year-olds & female & 41 & 23 & 0.561\\
\addlinespace
L16 & 10-year-olds & female & 13 & 12 & 0.923\\
L22 & 10-year-olds & female & 47 & 30 & 0.638\\
L27 & 10-year-olds & female & 8 & 8 & 1.000\\
L28 & 10-year-olds & female & 84 & 34 & 0.405\\
L29 & 10-year-olds & female & 53 & 34 & 0.642\\
\bottomrule
\end{tabular}
\end{table}

The entire dataset includes 79 observations from 36 participants. Each observation in the BELC dataset corresponds to an individual learner's composition. It includes which participant wrote the composition (\texttt{participant\_id}), the age group they were part of at the time (\texttt{age\_group}), their sex (\texttt{sex}), and the number of English words they produced (\texttt{num\_tokens}), the number of unique English words they produced (\texttt{num\_types}). The final variable (\texttt{ttr}) is the calculated ratio of number of unique words (\texttt{num\_types}) to total words (\texttt{num\_tokens}) for each composition. This is known as the Type-Token Ratio and it is a standard metric for measuring lexical diversity.

\hypertarget{information-values}{%
\subsubsection{Information values}\label{information-values}}

Understanding the informational value, or \textbf{level of measurement}, of a variable or set of variables in key to preparing for analysis as it has implications for what visualization techniques and statistical measures we can use to interrogate the dataset. There are two main levels of measurement a variable can take: categorical and continuous. \textbf{Categorical variables} reflect class or group values. \textbf{Continuous variables} reflect values that are measured along a continuum.

The BELC dataset contains three categorical variables (\texttt{participant\_id}, \texttt{age\_group}, and \texttt{sex}) and three continuous variables (\texttt{num\_tokens}, \texttt{num\_types}, and \texttt{ttr}). The categorical variables identify class or group membership; which participant wrote the composition, what age group they were in, and their biological sex. The continuous variables measure attributes that can take a range of values without a fixed limit and the differences between each value are regular. The number of words and number of unique words for each composition can range from 1 to \(n\) and the Type-Token Ratio being derived from these two variables is also continuous for the same reason. Furthermore, the differences between the each of values of these measures is on a defined interval, so for example a composition which has a word count (\texttt{num\_tokens}) of 40 is exactly two times as large as a composition with a word count of 20.

The distinction between categorical an continuous levels of measurement, as mentioned above, are the main two and for some statistical approaches the only distinction that needs to be made to conduct an analysis. However, categorical and continuous can each be broken down into subcategories and for some descriptive and analytic purposes these distinctions are important. For categorical variables a distinction can be made between variables in which there is a structured relationship between the values and those in which there is not. \emph{Nominal variables} contain values which are labels denoting the membership in a class in which there is no relationship between the labels. \emph{Ordinal variables} also contain labels of classes, but in contrast to nominal variables, there is a relationship between the classes, namely one in which there is a precedence relationship or order. With this in mind, our categorical variables be sub-classified. There is no order between the values of \texttt{participant\_id} and \texttt{sex} and they are therefore nominal whereas the values of \texttt{age\_group} are ordered, each value refers to a sequential age group, and therefore it is ordinal.

Turning to continuous variables, another subdivision can be made which hinges on the existence of a non-arbitrary zero or not. \emph{Interval variables} contain values in which the difference between the values is regular and defined, but the measure has an arbitrary zero value. A typically cited example of an interval variable is temperature measurements on the Fahrenheit scale. A value of 0 on this scale does not mean there is 0 temperature. \emph{Ratio variables} have all the properties of interval variables but also include a non-arbitrary definition of zero. All of the continuous variables in the BELC dataset (\texttt{num\_tokens}, \texttt{num\_types}, and \texttt{ttr}) are ratio variables as a value of 0 would indicate the lack of this attribute.

An hierarchical overview of the relationship between the two main and four sub-types of levels of measurement appear in Figure \ref{fig:info-values-graphic}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/04-approaching-analysis/Informational-values-paper} 

}

\caption{Levels of measurement graphic representation.}\label{fig:info-values-graphic}
\end{figure}

A few notes of practical importance; First, the distinction between interval and ratio variables is often not applicable in text analysis and therefore often treated together as continuous variables. Second, the distinction between ordinal and interval/continuous variables is not as clear cut as it may seem. Both variables contain values which have an ordered relationship. By definition the values of an ordinal variable do not reflect regular intervals between the units of measurement. But in practice interval/continuous variables with a defined number of values (say from a Likert scale used on a survey) may be treated as an ordinal variable as they may be better understood as reflecting class membership. Third, all continuous variables can be converted to categorical variables, but the reverse is not true. We could, for example, define a criterion for binning the word counts in \texttt{num\_tokens} for each composition into ordered classes such as ``low'', ``mid'', ``high''. On the other hand, \texttt{sex} (as it has been measured here) cannot take intermediate values on a unfixed range. The upshot is that variables can be down-typed but not up-typed. In most cases it is preferred to treat continuous variables as such, if the nature of the variable permits it, as the down-typing of continuous data to categorical data results in a loss of information --which will result in a loss of information and hence statistical power which may lead to results that obscure meaningful patterns in the data \citep{Baayen2004}.

\hypertarget{summaries}{%
\subsubsection{Summaries}\label{summaries}}

It is always key to gain insight into shape of the information through numeric, tabular and/ or visual summaries before jumping in to analytic statistical approaches. The most appropriate form of summarizing information will depend on the number and informational value(s) of our target variables. To get a sense of how this looks, let's continue to work with the BELC dataset and pose different questions to the data with an eye towards seeing how various combinations of variables are descriptively explored.

\hypertarget{single-variables}{%
\paragraph{Single variables}\label{single-variables}}

The way to statistically summarize a variable into a single measure is to derive a \textbf{measure of central tendency}. For a continuous variable the most common measure is the (arithmetic) \emph{mean}, or average, which is simply the sum of all the values divided by the number of values. As a measure of central tendency, however, the mean can be less-than-reliable as it is sensitive to outliers which is to say that data points in the variable that are extreme relative to the overall distribution of the other values in the variable affect the value of the mean depending on how extreme the deviate. One way to assess the effects of outliers is to calculate a \textbf{measure of dispersion}. The most common of these is the \emph{standard deviation} which estimates the average amount of variability between the values in a continuous variable. Another way to assess, or rather side-step, outliers is to calculate another measure of central tendency, the \emph{median}. A median is calculated by sorting all the values in the variable and then selecting the value which falls in the middle of all the other values. A median is less sensitive to outliers as extreme values (if there are few) only indirectly affect the selection of the middle value. Another measure of dispersion is to calculate quantiles. A \emph{quantile} slices the data in four percentile ranges providing a five value numeric summary of the spread of the values in a continuous variable. The spread between the first and third quantile is known as the Interquartile Range (IQR) and is also used as a single statistic to summarize variability between values in a continuous variable.

Below is a list of central tendency and dispersion scores for the continuous variables in the BELC dataset.

\textbf{Variable type: numeric}

\begin{tabular}{l|r|r|r|r|r|r|r|r|r|r}
\hline
skim\_variable & n\_missing & complete\_rate & mean & sd & p0 & p25 & p50 & p75 & p100 & iqr\\
\hline
num\_tokens & 0 & 1 & 66.23 & 43.90 & 1.00 & 29.00 & 55.00 & 90.00 & 185 & 61.00\\
\hline
num\_types & 0 & 1 & 40.25 & 22.80 & 1.00 & 22.00 & 38.00 & 54.00 & 97 & 32.00\\
\hline
ttr & 0 & 1 & 0.67 & 0.13 & 0.41 & 0.57 & 0.64 & 0.73 & 1 & 0.16\\
\hline
\end{tabular}

\begin{rmdtip}
The descriptive statistics returned above were generated by the
\texttt{skimr} package.
\end{rmdtip}

In the above summary, we see the mean, standard deviation (sd), and the quantiles (the five-number summary, p0, p25, p50, p75, and p100). The middle quantile (p50) is the median and the IQR is listed last.

These are important measures for assessing the central tendency and dispersion and will be useful for reporting purposes, but to get a better feel of how a variable is distributed, nothing beats a visual summary. A boxplot graphically summarizes many of these metrics. In Figure \ref{fig:summaries-boxplots-belc} we see the same three continuous variables, but now in graphical form.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-boxplots-belc-1} 

}

\caption{Boxplots for each of the continuous variables in the BELC dataset.}\label{fig:summaries-boxplots-belc}
\end{figure}

In a boxplot, the bold line is the median. The surrounding box around the median is the interquantile range. The extending lines above and below the IQR mark the largest and lowest value that is within 1.5 times either the 3rd (top of the box) or 1st (bottom of the box). Any values that fall outside, above or below, the extending lines are considered statistical outliers and are marked as dots (in this case red dots). \footnote{Note that each of these three variables are to be considered separately here (vertically). Later we will see the use of boxplots to compare a continuous variable across levels of a categorical variable (horizontally).}

Boxplots provide a robust and visually intuitive way of assessing central tendency and variability in a continuous variable but this type of plot can be complemented by looking at the overall distribution of the values in terms of their frequencies. A histogram provides a visualization of the frequency (and density in this case with the blue overlay) of the values across a continuous variable binned at regular intervals.

In Figure \ref{fig:summaries-histograms-belc} I've plotted histograms in the top row and density plots in the bottom row for the same three continuous variables from the BELC dataset.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-histograms-belc-1} 

}

\caption{Histograms and density plots for the continuous variables in the BELC dataset.}\label{fig:summaries-histograms-belc}
\end{figure}

Histograms provide insight into the distribution of the data. For our three continuous variables, the distributions happen not to be too strikingly distinct. They are, however, not the same either. When we explore continuous variables with histograms we are often trying to assess whether there is skew or not. There are three general types of skew, visualized in Figure \ref{fig:summaries-skew-graphic}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/04-approaching-analysis/skew-types-paper} 

}

\caption{Examples of skew types in density plots.}\label{fig:summaries-skew-graphic}
\end{figure}

In histograms/ density plots in which the distribution is either left or right, the median and the mean are not aligned. The \emph{mode}, which indicates the most frequent value in the variable is also not aligned with the other two measures. In a left-skewed distribution the mean will be to the left of the median which is left of the mode whereas in a right-skewed distribution the opposite occurs. In a distribution with absolutely no skew these three measures are the same. In practice these measures rarely align perfectly but it is very typical for these three measures to approximate alignment. It is common enough that this distribution is called the Normal Distribution \footnote{formally known as a Gaussian Distribution} as it is very common in real-world data.

Another and potentially more informative way to inspect the normality of a distribution is to create Quantile-Quantile plots (QQ Plot). In Figure \ref{fig:summaries-qqnorm-plot-belc} I've created QQ plots for our three continuous variables. The line in each plot is the normal distribution and the more points that fall off of this line, the less likely that the distribution is normal.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-qqnorm-plot-belc-1} 

}

\caption{QQ Plots for the continuous variables in the BELC dataset.}\label{fig:summaries-qqnorm-plot-belc}
\end{figure}

A visual inspection can often be enough to detect non-normality, but in cases which visually approximate the normal distribution (such as these) we can perform the Shapiro-Wilk test of normality. This is an inferential test that compares a variable's distribution to the normal distribution. The likelihood that the distribution differs from the normal distribution is reflected in a \(p\)-value. A \(p\)-value below the .05 threshold suggests the distribution is non-normal. In Table \ref{tab:summaries-normality-test-belc} we see that given this criterion only the distribution of \texttt{num\_types} is normally distributed.

\begin{table}

\caption{\label{tab:summaries-normality-test-belc}Results from Shapiro-Wilk test of normality for continuous variables in the BELC dataset.}
\centering
\begin{tabular}[t]{lrr}
\toprule
variable & statistic & p\_value\\
\midrule
Number of tokens & 0.942 & 0.001\\
Number of types & 0.970 & 0.058\\
Type-Token Ratio & 0.947 & 0.003\\
\bottomrule
\end{tabular}
\end{table}

Downstream in the analytic analysis, the distribution of continuous variables will need to be taken into account for certain statistical tests. Tests that assume `normality' are parametric tests, those that do not are non-parametric. Distributions which approximate the normal distribution can sometimes be transformed to conform to the normal distribution either by outlier trimming or through statistical procedures (e.g.~square root, log, or inverse transformation), if necessary. At this stage, however, the most important thing is to recognize whether the distributions approximate or wildly diverge from the normal distribution.

Before we leave continuous variables, let's consider another approach for visually summarizing a single continuous variable. The Empirical Cumulative Distribution Frequency, or \emph{ECDF}, is a summary of the cumulative proportion of each of the values of a continuous variable. An ECDF plot can be useful in determining what proportion of the values fall above or below a certain percentage of the data.

In Figure \ref{fig:summarize-ecdf-belc} we see ECDF plots for our three continuous variables.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summarize-ecdf-belc-1} 

}

\caption{ECDF plots for the continuous variables in the BELC dataset.}\label{fig:summarize-ecdf-belc}
\end{figure}

Take, for example, the number of tokens (\texttt{num\_tokens}) per composition. The ECDF plot tells us that 50\% of the values in this variable are 56 words or less. In the three variables plotted, the cumulative growth is quite steady. In some cases it is not. When it is not, an ECDF goes a long way to provide us a glimpse into key bends in the proportions of values in a variable.

Now let's turn to the descriptive assessment of categorical variables. For categorical variables, central tendency can be calculated as well but only a subset of measures given the reduced informational value of categorical variables. For nominal variables where there is no relationship between the levels the central tendency is simply the mode. The levels of ordinal variables, however, are relational and therefore the median, in addition to the mode, can also be used as a measure of central tendency. Note that a variable with one mode is unimodal, two modes, bimmodal, and in variables that have two or more modes multimodal.

\begin{rmdwarning}
To get numeric value of the median for an ordinal variable the levels of
the variable will need to be numeric as well. Non-numeric levels can be
recoded to numeric for this purpose if necessary.
\end{rmdwarning}

Below is a list of the central tendency metrics for the categorical variables in the BELC dataset.

\textbf{Variable type: factor}

\begin{tabular}{l|r|r|l|r|l}
\hline
skim\_variable & n\_missing & complete\_rate & ordered & n\_unique & top\_counts\\
\hline
participant\_id & 0 & 1 & FALSE & 36 & L05: 3, L10: 3, L11: 3, L12: 3\\
\hline
age\_group & 0 & 1 & TRUE & 4 & 10-: 24, 16-: 24, 12-: 16, 17-: 15\\
\hline
sex & 0 & 1 & FALSE & 2 & fem: 48, mal: 31\\
\hline
\end{tabular}

In practice when a categorical variable has few levels it is common to simply summarize the counts of each level in a table to get an overview of the variable. With ordinal variables with more numerous levels, the five-score summary (quantiles) can be useful to summarize the distribution. In contrast to continuous variables where a graphical representation is very helpful to get perspective on the shape of the distribution of the values, the exploration of single categorical variables is rarely enhanced by plots.

\hypertarget{multiple-variables}{%
\paragraph{Multiple variables}\label{multiple-variables}}

In addition to the single variable summaries (univariate), it is very useful to understand how two (bivariate) or more variables (multivariate) are related to add to our understanding of the shape of the relationships in the dataset. Just as with univariate summaries, the informational values of the variables frame our approach.

To explore the relationship between two continuous variables we can statistically summarize a relationship with a \textbf{coefficient of correlation} which is a measure of \textbf{effect size} between continuous variables. If the continuous variables approximate the normal distribution \emph{Pearson's r} is used, if not \emph{Kendall's tau} is the appropriate measure. A correlation coefficient ranges from -1 to 1 where 0 is no correlation and -1 or 1 is perfect correlation (either negative or positive). Let's assess the correlation coefficient for the variables \texttt{num\_tokens} and \texttt{ttr}. Since these variables are not normally distributed, we use Kendall's tau. Using this measure the correlation coefficient is \(-0.563\) suggesting there is a correlation, but not a particularly strong one.

Correlation measures are important for reporting but to really appreciate a relationship it is best to graphically represent the variables in a \emph{scatterplot}. In Figure \ref{fig:summaries-bivariate-scatterplot-belc} we see the relationship between \texttt{num\_tokens} and \texttt{ttr}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-bivariate-scatterplot-belc-1} 

}

\caption{Scatterplot...}\label{fig:summaries-bivariate-scatterplot-belc}
\end{figure}

In both plots \texttt{ttr} is on the y-axis and \texttt{num\_tokens} on the x-axis. The points correspond to the intersection between these variables for each single observation. In the left pane only the points are represented. Visually (and given the correlation coefficient) we can see that there is a negative relationship between the number of tokens and the Type-Token ratio: in other words, the more tokens a composition has the lower the Type-Token Ratio. In this case this trend is quite apparent, but in other cases is may not be. To provide an additional visual cue a trend line is often added to a scatterplot. In the right pane I've added a linear trend line. This line demarcates the optimal central tendency across the relationship, assuming a linear relationship. The steeper the line, or slope, the more likely the correlation is strong. The band, or ribbon, around this trend line indicates the \textbf{confidence interval} which means that real central tendency could fall anywhere within this space. The wider the ribbon, the larger the variation between the observations. In this case we see that the ribbon widens when the number of tokens is either low or high. This means that the trend line could be potentially be drawn either steeper (more strongly correlated) or flatter (less strongly correlated).

\begin{rmdtip}
In plots comparing two or more variables, the choice of which variable
to plot on the x- and y-axis is contingent on the research question and/
or the statistical approach. The language varies between statistical
approaches: in inferential methods the x-axis is used to plot what is
known as the dependent variable and the y-axis an independent variable.
In predictive methods the dependent variable is known as the outcome and
the independent variable a predictor. Exploratory methods do not draw
distinctions between variables along these lines so the choice between
which variable to plot along the x- and y-axis is often arbitrary.
\end{rmdtip}

Let's add another variable to the mix, in this case the categorical variable \texttt{sex}, taking our bivariate exploration to a multivariate exploration. Again each point corresponds to an observation where the values for \texttt{num\_tokens} and \texttt{ttr} intersect. But now each of these points is given a color that reflects which level of \texttt{sex} it is associated with.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-multivariate-scatterplot-belc-1} 

}

\caption{Scatterplot visualizing the relationship between `num\_tokens` and `ttr`.}\label{fig:summaries-multivariate-scatterplot-belc}
\end{figure}

In this multivariate case, the scatterplot without the trend line is more difficult to interpret. The trend lines for the levels of \texttt{sex} help visually understand the variation of the relationship of \texttt{num\_tokens}and \texttt{ttr} much better. But it is important to note that when there are multiple trend lines there is more than one slope to evaluate. The correlation coefficient can be calculated for each level of \texttt{sex} (i.e.~`male' and `female') independently but the relationship between the each slope can be visually inspected and provide important information regarding each level's relative distribution. If the trend lines are parallel (ignoring the ribbons for the moment), as it appears in this case, this suggests that the relationship between the continuous variables is stable across the levels of the categorical variable, with males showing more lexical diversity than females declining at a similar rate. If the lines were to cross, or suggest that they would cross at some point, then there would be a potentially important difference between the levels of the categorical variable (known as an interaction). Now let's consider the meaning of the ribbons. Since the ribbons reflect the range in which the real trend line could fall, and these ribbons overlap, the differences between the levels of our categorical variable are likely not distinct. So at a descriptive level, this visual summary would suggest that there are no differences between the relationship between \texttt{num\_tokens} and \texttt{ttr} for the distinct levels of \texttt{sex}.

Characterizing the relationship between two continuous variables, as we have seen is either performed through a correlation coefficient metric or visually. The approach for summarizing a bivariate relationship which combines a continuous and categorical variable is distinct. Since a categorical variable is by definition a class-oriented variable, a descriptive evaluation can include a tabular representation, with some type of summary statistic. For example, if we consider the relationship between \texttt{num\_tokens} and \texttt{age\_group} we can calculate the mean for \texttt{num\_tokens} for each level of \texttt{age\_group}. To provide a metric of dispersion we can include either the standard error of the mean (SEM) and/ or the confidence interval (CI).

In Table \ref{tab:summarize-bivariate-cont-cat-table} we see each of these summary statistics.

\begin{table}

\caption{\label{tab:summarize-bivariate-cont-cat-table}Summary table for `tokens` by `age_group`.}
\centering
\begin{tabular}[t]{lrrr}
\toprule
age\_group & mean\_num\_tokens & sem & ci\\
\midrule
10-year-olds & 27.8 & 3.69 & 6.07\\
12-year-olds & 57.4 & 7.12 & 11.71\\
16-year-olds & 81.7 & 6.15 & 10.11\\
17-year-olds & 112.4 & 12.98 & 21.35\\
\bottomrule
\end{tabular}
\end{table}

The SEM is a metric which summarizes variation based on the number of values and the CI, as we have seen, summarizes the potential range of in which the mean may fall given a likelihood criterion (usually the same as the \(p\)-value, .05).

Because we are assessing a categorical variable in combination with a continuous variable a table is an available visual summary. But as I have said before, a graphic summary is hard to beat. In the following figure (\ref{fig:summaries-bivariate-barplot-belc}) a barplot is provided which includes the means of \texttt{num\_tokens} for each level of \texttt{age\_group}. The overlaid bars represent the confidence interval for each mean score.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-bivariate-barplot-belc-1} 

}

\caption{Barplot comparing the mean `num\_tokens` by `age\_group` from the BELC dataset.}\label{fig:summaries-bivariate-barplot-belc}
\end{figure}

When CI ranges overlap, just as with ribbons in scatterplots, the likelihood that the differences between levels are `real' is diminished.

To gauge the effect size of this relationship we can use \emph{Spearman's rho} for rank-based coefficients. The score is 0.708 indicating that the relationship between \texttt{age\_group} and \texttt{num\_tokens} is quite strong. \footnote{To calculate effect sizes for the difference between two means, \emph{Cohen's d} is used.}

Now, if we want to explore a multivariate relationship and add \texttt{sex} to the current descriptive summary, we can create a summary table, but let's jump straight to a barplot.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-multivariate-barplot-belc-1} 

}

\caption{Barplot comparing the mean `num_tokens` by `age_group` and `sex` from the BELC dataset.}\label{fig:summaries-multivariate-barplot-belc}
\end{figure}

We see in Figure \ref{fig:summaries-multivariate-barplot-belc} that on the whole, the appears to be general trend towards more tokens in a composition for more advanced learner levels. However, the non-overlap in CI bars for the `12-year-olds' for the levels of \texttt{sex} (`male' and `female') suggest that 12-year-old females may produce more tokens per composition than males --a potential divergence from the overall trend.

Barplots are a familiar and common visualization for summaries of continuous variables across levels of categorical variables, but a boxplot is another useful visualization of this type of relationship.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-bivariate-boxplots-belc-1} 

}

\caption{Boxplot of the relationship between `age_group` and `num_tokens` from the BELC dataset.}\label{fig:summaries-bivariate-boxplots-belc}
\end{figure}

As seen when summarizing single continuous variables, boxplots provide a rich set of information concerning the distribution of a continuous variable. In this case we can visually compare the continuous variable \texttt{num\_tokens} with the categorical variable \texttt{age\_group}. The plot in the right pane includes `notches'. Notches represent the confidence interval, in boxplots this interval surrounds the median. When compared horizontally across levels of a categorical variable the overlap of notched spaces suggest that the true median may be within the same range.
Additionally, when the confidence interval goes outside the interquantile range (the box) the notches hinge back to the either the 1st (lower) or the 3rd (higher) IQR range and suggests that the variability is high.

We can also add a third variable to our exploration. As in the barplot in Figure \ref{fig:summaries-multivariate-barplot-belc}, the boxplot in Figure \ref{fig:summaries-multivariate-boxplots-belc} suggests that there is an overall trend towards more tokens per composition as a learner advances in experience, except at the `12-year-old' level where there appears to be a difference between `males' and `females'.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-multivariate-boxplots-belc-1} 

}

\caption{Boxplot of the relationship between `age_group`, `num_tokens` and `sex` from the BELC dataset.}\label{fig:summaries-multivariate-boxplots-belc}
\end{figure}

Up to this point in our exploration of multiple variables we have always included at least one continuous variable. The central tendency for continuous variables can be summarized in multiple ways (mean, median, and mode) and when calculating means and medians, measures of dispersion are also provide helpful information summarize variability. When working with categorical variables, however, measures of central tendency and dispersion are more limited. For ordinal variables central tendency can be summarized by the median or mode and dispersion can be assessed with an interquantile range. For nominal variables the mode is the only measure of central tendency and dispersion is not applicable. For this reason relationships between categorical variables are typically summarized using \textbf{contingency tables} which provide cross-variable counts for each level of the target categorical variables.

Let's explore the relationship between the categorical variables \texttt{sex} and \texttt{age\_group}. In Table \ref{tab:summaries-bivariate-categorical-table-belc} we see the contingency table with summary counts and percentages.

\begin{table}

\caption{\label{tab:summaries-bivariate-categorical-table-belc}Contingency table for `age_group` and `sex`.}
\centering
\begin{tabular}[t]{llllll}
\toprule
sex/age\_group & 10-year-olds & 12-year-olds & 16-year-olds & 17-year-olds & Total\\
\midrule
female & 58\% (14) & 69\% (11) & 54\% (13) & 67\% (10) & 61\% (48)\\
male & 42\% (10) & 31\%  (5) & 46\% (11) & 33\%  (5) & 39\% (31)\\
Total & 100\% (24) & 100\% (16) & 100\% (24) & 100\% (15) & 100\% (79)\\
\bottomrule
\end{tabular}
\end{table}

As the size of the contingency table increases, visual inspection becomes more difficult. As we have seen, a graphical summary often proves more helpful to detect patterns.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-bivariate-categorical-barplot-belc-1} 

}

\caption{Barplot...}\label{fig:summaries-bivariate-categorical-barplot-belc}
\end{figure}

In Figure \ref{fig:summaries-bivariate-categorical-barplot-belc} the left pane shows the counts. Counts alone can be tricky to evaluate and adjusting the barplot to account for the proportions of males to females in each group, as shown in the right pane, provides a clearer picture of the relationship. From these barplots we can see there were more females in the study overall and particularly in the 12-year-olds and 17-year-olds groups. To gauge the association strength between \texttt{sex} and \texttt{age\_group} we can calculate \emph{Cramer's V} which, in spirit, is like our correlation coefficients for the relationship between continuous variables. The Cramer's V score for this relationship is 0.12 which is low, suggesting that there is not a strong association between \texttt{sex} and \texttt{age\_group} --in other words, the relationship is stable.

Let's look at a more complex case in which we have three categorical variables. Now the dataset, as is, does not have a third categorical variable for us to explore but we can recast the continuous \texttt{num\_tokens} variable as a categorical variable if we bin the scores into groups. I've binned tokens into three score groups with equal ranges in a new variable called \texttt{rank\_tokens}.

Adding a second categorical independent variable ups the complexity of our analysis and as a result our visualization strategy will change. Our numerical summary will include individual two-way cross-tabulations for each of the levels for the third variable. In this case it is often best to use the variable with the fewest levels as the third variable, in this case \texttt{sex}.

\begin{table}

\caption{\label{tab:summaries-multivariate-categorical-table-belc-female}Contingency table for `age_group`, `rank_tokens`, and `sex` (female).}
\centering
\begin{tabular}[t]{llllll}
\toprule
rank\_tokens/age\_group & 10-year-olds & 12-year-olds & 16-year-olds & 17-year-olds & Total\\
\midrule
low & 27\% (13) & 10\%  (5) & 4\%  (2) & 6\%  (3) & 48\% (23)\\
mid & 2\%  (1) & 13\%  (6) & 21\% (10) & 6\%  (3) & 42\% (20)\\
high & 0\%  (0) & 0\%  (0) & 2\%  (1) & 8\%  (4) & 10\%  (5)\\
Total & 29\% (14) & 23\% (11) & 27\% (13) & 21\% (10) & 100\% (48)\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:summaries-multivariate-categorical-table-belc-male}Contingency table for `age_group`, `rank_tokens`, and `sex` (male).}
\centering
\begin{tabular}[t]{llllll}
\toprule
rank\_tokens/age\_group & 10-year-olds & 12-year-olds & 16-year-olds & 17-year-olds & Total\\
\midrule
low & 32\% (10) & 13\% (4) & 13\%  (4) & 3\% (1) & 61\% (19)\\
mid & 0\%  (0) & 3\% (1) & 23\%  (7) & 6\% (2) & 32\% (10)\\
high & 0\%  (0) & 0\% (0) & 0\%  (0) & 6\% (2) & 6\%  (2)\\
Total & 32\% (10) & 16\% (5) & 35\% (11) & 16\% (5) & 100\% (31)\\
\bottomrule
\end{tabular}
\end{table}

Contingency tables with this many levels are notoriously difficult to interpret. A plot that is often used for three-way contingency table summaries is a mosaic plot. In Figure \ref{fig:summaries-multivariate-mosaic-belc} I have created a mosaic plot for the three categorical variables in the previous contingency tables.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-multivariate-mosaic-belc-1} 

}

\caption{Mosaic plot for three categorical variables `age_group`, `rank_tokens`, and `sex` in the BELC dataset.}\label{fig:summaries-multivariate-mosaic-belc}
\end{figure}

The mosaic plot suggests that the number of tokens per composition increase as the learner age group increases and that females show more tokens earlier.

In sum, a dataset is information but when the observations become numerous or complex they are visually difficult to inspect and understand at a pattern level. The descriptive methods described in this section are indispensable for providing the researcher an overview of the nature of each variable and any (potential) relationships between variables in a dataset. Importantly, the understanding derived from this exploration underlies all subsequent investigation and will counted on to frame your approach to analysis regardless of the research goals and the methods employed to derive more substantial knowledge.

\hypertarget{analysis}{%
\subsection{Analysis}\label{analysis}}

From identifying a target population, to selecting a data sample that represents that population, and then to structuring the sample into a dataset, the goals of a research project inform and frame the process. So it will be unsurprising to know that the process of selecting an approach to analysis is also intimately linked with a researcher's objectives. The goal of analysis, generally, is to generate knowledge from information. The type of knowledge generated and the process by which it is generated, however, differ and can be broadly grouped into three analysis types: inferential, predictive, and exploratory. In this section I will provide an overview of how each of these analysis types are tied to research goals and how the general goals of teach type affect: (1) how to \emph{identify} the variables of interest, (2) how to \emph{interrogate} these variables, and (3) how to \emph{interpret} the results. I will structure the discussion of these analysis types moving from the most structured (deductive) to least structured (inductive) approach to deriving knowledge from information with the aim to provide enough information to the would-be-researcher to identify these research approaches in the literature and to make appropriate decisions as to which approach their research should adopt.

\hypertarget{inferential-data-analysis}{%
\subsubsection{Inferential data analysis}\label{inferential-data-analysis}}

The most commonly recognized of the three data analysis approaches, inferential data analysis (IDA) is the bread-and-butter of science. IDA is a deductive, or top-down, approach to investigation in which every step in research stems from a premise, or hypothesis, about the nature of a relationship in the world and then aims to test whether this relationship is statistically supported given the evidence. The aim is to infer conclusions about a certain relationship in the population based on a statistical evaluation of a (corpus) sample. So, if a researcher's aim is to draw conclusions that generalize, then, this is the analysis approach a researcher will take.

Given the fact that this approach aims at making claims that can be generalized to the larger population, the IDA approach has the most rigorous set of methodological restrictions. First and foremost of these is the fact that a testable hypothesis must be formulated \emph{before} research begins. The hypothesis guides the collection of data, the organization of the data into a dataset and the transformation, selection of the variables to be used to address the hypothesis, and the interpretation of the results. To conduct an analysis and then draw a hypothesis which conforms to the results is known as ``Hypothesis After Result is Known'' (HARKing) \citep{Kerr1998} and this practice violates the principles of significance testing. A second key stipulation is that the reliability of the sample data, the corpus in text analysis, to provide evidence to test the hypothesis must be representative of the population. A corpus used in a study which is misaligned with the hypothesis undermines the ability of the researcher to make valid claims about the population. In essence, IDA is only as good as the primary data is is based on.

At this point, let me elaborate on the potentially counterintuitive nature of hypothesis formulation and testing. The IDA, or Null-Hypothesis Significance Testing (NHST), paradigm is in fact approached by proposing two mutually exclusive hypotheses. The first is the \textbf{Alternative Hypothesis} (\(H_1\)). \(H_1\) is a precise statement grounded in the previous literature outlining a predicted relationship (and in some cases the directionality of a relationship). This is the effect that the research aims to investigate. The second hypothesis is the \textbf{Null Hypothesis} (\(H_0\)). \(H_0\) is the flip-side of the hypothesis testing coin and states that there is no difference or relationship. Together \(H_1\) and \(H_0\) cover all logical outcomes.

So to provide an example consider a hypothetical study which is aimed at investigating the claim that men and women differ in terms of the number of questions they use in spontaneous conversations. The Alternative Hypothesis would be formulated in this way:

\(H_1\): Men and women differ in the frequency of the use of questions in spontaneous conversations.

The Null Hypothesis, then, would be a statement describing the remaining logical outcomes. Formally:

\(H_0\): There is no difference between how men and women use questions in spontaneous conversations.

Note that stated in this way our hypothesis makes no prediction about the directionality of the difference between men and women, only that there is a difference. It is a likely scenario that a hypothesis will stake a claim on the direction of the difference. A directional hypothesis would look like this:

\(H_1\): Women use more questions than men in spontaneous conversations.

\(H_0\): There is no difference between how men and women use questions in spontaneous conversations or men use more questions than women.

A further aspect which may run counter to expectations is that the aim of hypothesis testing is not to find evidence in support of \(H_1\), but rather the aim is to assess the likelihood that we can reliably reject \(H_0\). The default assumption is that \(H_0\) is true until there is sufficient evidence to reject it and accept \(H_1\), the \emph{alternative}. The metric used to determine if there is sufficient evidence is based on the probability that given the nature of the relationship and the characteristics of the data, the likelihood of there being no difference or relationship is low. The threshold for likelihood has traditionally been summarized in the p-value statistic. In the Social Sciences, a p-value lower that .05 is considered \emph{statistically significant} which when interpreted correctly means that there is more than a 95\% chance that the observed relationship would not be predicted by \(H_0\). Note that we are working in the realm of probability, not in absolutes, therefore an analysis that produces a significant result does not prove \(H_1\) is correct or that \(H_0\) is incorrect, for that matter. A margin of error is always present.

Let's now turn to the identification of variables, the statistical interrogation of these variables, and the interpretation of the statistical results. First, since a clearly defined and testable hypothesis is at the center of the IDA approach, the variables are in some sense pre-defined. The goal of the researcher is to select data and curate that data to produce variables that are operationalized (practically measured) to test the hypothesis. A second consideration are the roles that the variables will play in the analysis. In standard IDA one variable will be the \textbf{dependent variable} and one or more variables will be \textbf{independent variables}. The dependent variable, sometimes referred to as the outcome or response variable, is the variable which contains the information which is predicted to depend on the information in the independent variable(s). It is the variable whose variation a research study seeks to explain. An independent variable, sometimes referred to as a predictor or explanatory variable, is a variable whose variation is predicted to explain the variation in the dependent variable.

Returning to our hypothetical study on the use of questions between men and women in spontaneous conversation, the frequency of questions used by each speaker would be our dependent variable and the biological sex of the speakers our independent variable. This is so because hypothesis (\(H_1\)) states the proposition that a speaker's sex will predict the frequency of questions used.

In our hypothetical study we've identified two variables, one dependent and one independent. It is important keep in mind that there can be multiple independent variables in cases where the dependent variable's variation is predicted to be related to multiple variables. This relationship would need to be explicitly part of the original hypothesis, however.

Say we formulate a more complex relationship where the educational level of our speakers is also related to the number of questions. We can update our hypothesis to reflect such a scenario.

\(H_1\): Less educated women use more questions than men in spontaneous conversations.

\(H_0\): There is no difference between how men and women use questions in spontaneous conversations regardless of educational level, or more educated women use more questions than less educated women, or men use more questions than women.

The hypothesis we have described predicts what is known as an \emph{interaction}; the relationship between our independent variables predict different variational patterns in the dependent variable. As you most likely can appreciate the more independent variables we include in our hypothesis, and by extension our analysis, the more difficult it becomes to interpret. Due to the increasing difficulty for interpretation, in practice, IDA studies rarely include more than two or three independent variables in the same analysis.

Independent variables add to the complexity of a study because they are part of our research focus, specifically our hypothesis. It is, however, common to include other variables which are not of central focus, but are commonly assumed to contribute to the explanation of the variation of the dependent variable. Let's assume that the background literature suggests that the age of speakers also plays a role in the number of questions that men and women use in spontaneous conversation. Let's also assume that the data we have collected includes information about the age of speakers. If we would like to factor out the potential influence of age on the use of questions and focus on the particular independent variables we've defined in our hypothesis, we can include the age of speakers as a \textbf{control variable}. A control variable will be added to the statistical analysis and documented in our report but it will not be included in the hypothesis nor interpreted in our results.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/04-approaching-analysis/inferential-variables} 

}

\caption{Variable roles in inferential analysis.}\label{fig:aa-inferential-variables}
\end{figure}

At this point let's look at the main characteristics that need to be taken into account to statistically interrogate the variables we have chosen to test our hypothesis. The type of statistical test that one chooses is based on (1) the informational value of the dependent variable and (2) the number of independent variables included in the analysis. Together these two characteristics go a long way in determining the appropriate class of statistical test, but other considerations about the distribution of particular variables (i.e.~normality), relationships between variables (i.e.~independence), and expected directionality of the predicted effect may condition the appropriate method to be applied.

As you can imagine, there are a host of combinations and statistical tests that apply in particular scenarios, too many to consider in given the scope of this coursebook (see \citet{Gries2013a} and \citet{Paquot2020a} for a more exhaustive description). Below I've summarized some common statistical scenarios and their associated tests which focus on the juxtaposition of informational values and the number of variables, leaving aside alternative tests which deal with non-normal distributions, ordinal variables, non-independent variables, etc.

In Table \ref{tab:ida-statistical-monofactorial-listing} we see \textbf{monofactorial tests}, tests with only one independent variable.

\begin{table}

\caption{\label{tab:ida-statistical-monofactorial-listing}Common monofactorial tests.}
\centering
\begin{tabular}[t]{lll}
\toprule
\multicolumn{2}{c}{Variable roles} & \multicolumn{1}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){1-2}
Dependent & Independent & Test\\
\midrule
Categorical & Categorical & Pearson's Chi-squared test\\
Continuous & Categorical & Student's t-Test\\
Continuous & Continuous & Pearson's correlation test\\
\bottomrule
\end{tabular}
\end{table}

Table \ref{tab:ida-statistical-multifactorial-listing} includes a listing of \textbf{multifactorial tests}, tests with more than one independent and/ or control variables.

\begin{table}

\caption{\label{tab:ida-statistical-multifactorial-listing}Common multifactorial tests.}
\centering
\begin{tabular}[t]{l>{}l>{}ll}
\toprule
\multicolumn{3}{c}{Variable roles} & \multicolumn{1}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){1-3}
Dependent & Independent & Control & Test\\
\midrule
Categorical & \em{varied} & \em{varied} & Logistic regression\\
Continuous & \em{varied} & \em{varied} & Linear regression\\
\bottomrule
\end{tabular}
\end{table}

One key point to make before we turn to how to interpret the statistical results is concerns the use of the data in IDA. In contrast to the other two analysis methods we will cover, the data in IDA is only used once. That is to say, that the entire dataset is used a single time to statistically interrogate the relationship(s) of interest. The resulting confidence metrics (p-values, etc.) are evaluated and the findings are interpreted. The practice of running multiple tests until a statistically significant result is found is called ``p-hacking'' \citep{Head2015} and like HARKing (described earlier) violates statistical hypothesis testing practice. For this reason it is vital to identify your statistical approach from the outset of your research project.

Now let's consider how to approach interpreting the results from a statistical test. As I have now made reference to multiple times, the results of statistical procedure in hypothesis testing will result in a confidence metric. The most standard and widely used of these confidence metrics is the p-value. The p-value provides a probability that the results of our statistical test could be explained by the null hypothesis. When this probability crosses below the threshold of .05, the result is considered statistically significant, otherwise we have a `null result' (i.e.~non-significant). However, this sets up a binary distinction that can be problematic. On the one hand what is one to do if a test returns a p-value of .051 or something `marginally significant'? According to standard practice these results would not be statistically significant. But it is important to note that a p-value is sensitive to the sample size. A small sample may return a non-significant result, but a larger sample size with the same underlying characteristics may very well return a significant result. On the other hand, if we get a statistically significant result, do we move on --case closed? As I just pointed out the sample size plays a role in finding statistically significant results, but that does not mean that the results are `important' for even small effects in large samples can return a significant p-value.

It is important to underscore that the purpose of IDA is to draw conclusions from a dataset which are generalizable to the population. These conclusions require that there are rigorous measures to ensure that the results of the analysis do not overgeneralize (suggest there is a relationship when there is not one) and balance that with the fact that we don't want to undergeneralize (miss the fact that there is an relationship in the population, but our analysis was not capable of detecting it). Overgeneralization is known as \textbf{Type I error} or false positive and undergeneralization is a \textbf{Type II error} or false negative.

For these reasons it is important to calculate the size and magnitude of the result to gauge the uncertainty of our result in standardized, sample size-independent way. This is performed by analyzing the \textbf{effect size} and reporting a \textbf{confidence interval (CI)} for the results. The wider the CI the more uncertainty surrounds our statistical result, and therefore the more likely that our significant p-value could be the result of Type I error. A non-significant p-value and large effect size could be the result of Type II error. In addition to vetting our p-value, the CI and effect size can help determine if a significant result is reliable and `important'. Together effect size and CIs aid in our ability to realistically interpret confidence metrics in statistical hypothesis testing.

\hypertarget{predictive-data-analysis}{%
\subsubsection{Predictive data analysis}\label{predictive-data-analysis}}

Predictive data analysis (PDA) is the first of the two types of statistical approaches we will cover that fall under \textbf{machine learning}. A branch of artificial intelligence (AI), machine learning aims to develop computer algorithms that can essentially learn patterns from data automatically. In the case of PDA, also known as \textbf{supervised learning}, the learning process is guided (supervised) by directing an algorithm to associate patterns in a variable or set of variables to single particular variable. The particular variable is analogous to some degree to a dependent variable in IDA, but in the machine learning literature this variable is known as the \textbf{target} variable. The other variable or (more often than not) variables are known as \textbf{features}. The goal of PDA is to develop a statistical generalization that can accurately predict the values of a target variable using the values of the feature variables. PDA can be seen as a mix of deductive (top-down) and inductive (bottom-up) methods in that the target variable is determined by a research goal but the feature variables and choice of statistical method (algorithm) are not fixed and can vary depending on their usefulness in effectively predicting the target variable. PDA is a versatile method that often employed to derive intelligent action from data, but it can also be used for hypothesis generation and even hypothesis testing, under certain conditions. If a researcher's aim is to create model that can perform a language related task, explore association strength between a target variable and various types and combinations of features, or to perform emerging alternative approaches to hypothesis testing \footnote{see \citet{Deshors2016} and \citet{Baayen2011}}, this is the analysis approach a researcher will take.

At this point let's consider some departures from the inferential data analysis (IDA) approach we covered in the last subsection that are important to highlight to orient our overview of PDA. First, while the cornerstone of IDA is the hypothesis, in PDA this is typically not the case. A research question which identifies a source of potential uncertainty in an area and outlines a strategy for addressing this uncertainty is sufficient groundwork to embark on an analysis. A second divergence, is the fact that the data is used in a very distinct way. In IDA the entire dataset is statistically interrogated once and only once. In PDA the dataset is (minimally) partitioned into a \textbf{training set} and a \textbf{test set}. The training set is used to train a statistical model and the test set is left to test the accuracy of the statistical model. The training set typically constitutes a larger portion of the data (typically around 75\%) and serves as the test bed for iteratively applying one or more algorithms and/ or feature combinations to produce the most successful learning model. The test set is reserved for a final evaluation of the model's performance. Depending on the application and the amount of available data, a third \emph{development set} is sometimes created as a pseudo test set to facilitate the testing of multiple approaches on data outside the training set before the final evaluation on the test set is performed. In this scenario the proportions of the partitions vary, but a good rule of thumb is to reserve 60\% of the data for training, 20\% for development, and 20\% for testing.

Let's now turn to the identification of variables, the statistical interrogation of these variables, and the interpretation of the statistical results. In IDA the variables (features) are pre-determined by the hypothesis and the informational values and number of these variables plays a significant role in selecting a statistical procedure (algorithm). Lacking a hypothesis, a PDA approach's main goal is to make accurate predictions on the target variable and is free to explore any number of features and feature combinations to that end. The target variable is the only variable which necessarily fixed and in this light pre-determined.

To give an example, let's consider a language task in which the goal is to take text messages (SMS) and develop a language model that predict if a message is spam or not. Minimally we would need data which includes individual text messages and each of these text message will need to be labeled as being either spam or legitimate messages (`ham' in this case). In Table \ref{tab:pda-sms-preview} we see the first ten of 5574 observations from the SMS Spam Collection (v.1) dataset collected by \citet{Almeida2011a}.

\begin{table}

\caption{\label{tab:pda-sms-preview}First ten observations from the SMS Spam Collection (v.1)}
\centering
\begin{tabular}[t]{ll}
\toprule
sms\_type & message\\
\midrule
ham & Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\
ham & Ok lar... Joking wif u oni...\\
spam & Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T\&C's apply 08452810075over18's\\
ham & U dun say so early hor... U c already then say...\\
ham & Nah I don't think he goes to usf, he lives around here though\\
\addlinespace
spam & FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\\
ham & Even my brother is not like to speak with me. They treat me like aids patent.\\
ham & As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\\
spam & WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\\
spam & Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\\
\bottomrule
\end{tabular}
\end{table}

As it stands we have two variables; \texttt{sms\_type} is clearly the target and \texttt{message} contain the full messages. The question is how best to transform the information in the \texttt{message} variable such that it will provide an algorithm useful information to predict each value of \texttt{sms\_type}. Since the informational value of \texttt{sms\_type} is categorical we will call the values \textbf{classes}. The process of deciding on how to transform the information in \texttt{message} into useful features is called \textbf{feature engineering} and it is a process which is much an art as a science. On the creative side of things it is often helpful to have a mixture of relevant domain knowledge and clever hacking skills to envision what features may work best. The more logistic side of things requires some knowledge about the strengths and weaknesses of various learning algorithms when dealing with certain number and informational value feature combinations.

Leaving the choice of algorithm aside, let's focus on feature engineering. Since each \texttt{message} value is a unique message, the chance that using \texttt{message} as it is, is not likely to help us make reliable predictions about the status of new message (`spam' or `ham'). A simple first-pass approach to decomposing \texttt{message} to draw out similarities and distinctions between the classes may be to break each message into words. Now SMS messages are not your average type of text --there are many non-standard forms. So our definition of word may simply be character groupings broken apart by whitespace. To avoid confusion between our common-sense understanding of word and the types of character strings, it is often the case that language feature values are called \textbf{terms}. Other term types may work better, n-grams, character sequences, stems/lemmas, or even combinations of these. Certain terms may be removed that are potentially uninformative either based on their class (stopwords, numerals, punctuation, etc.) or due to their distribution. The process of systematic isolation of terms which are more informative than others is called \textbf{dimensionality reduction} \citep{Kowsari2019}. With experience a research will become more adept a recognizing advantages and potential issues and alternative ways of approaching the creation of features but there is almost always some level of trial and error in the process. Feature engineering is very much an exploratory process. It is also iterative. You can try a set of features with an algorithm and produce a language model and test it on the training set --if is accurate, great. If not, you can brainstorm some more --you are free to try further engineer the features trying new features or feature measures (term weights) and/ or change the learning algorithm.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/04-approaching-analysis/predictive-variables} 

}

\caption{Variable roles in predictive analysis.}\label{fig:aa-predictive-variables}
\end{figure}

Let's now turn to some considerations to take into account when selecting a statistical algorithm. First, just as in IDA, variable informational value plays a role in algorithm selection, specifically the informational value of the target variable. If the target variable is categorical, then we are looking for a \textbf{classification} algorithm. If the target variable is continuous, we will employ a \textbf{regression} algorithm. \footnote{The name regression can be a bit confusing given a very common classification algorithm is ``Logistic Regression''.} Some common classification algorithms are listed in Table \ref{tab:pda-algorithms}.

\begin{table}

\caption{\label{tab:pda-algorithms}Some common supervised learning algorithms.}
\centering
\begin{tabular}[t]{ll}
\toprule
Classification & Regression\\
\midrule
Logistic Regression & Linear Regression\\
Support Vector Machine & Support Vector Regression\\
Naïve Bayes Classifier & Poisson Regression\\
Neural Network & \\
Decision Tree & \\
\bottomrule
\end{tabular}
\end{table}

Another consideration to take into account is the whether the researcher aims to go beyond simply using an algorithm to make accurate predictions, but also wants to understand how the algorithm made its predictions and what contribution features made in the process. There are algorithms that produce models that allow the researcher to peer into and understand its inner workings (e.g.~logistic regression, naïve bayes classifiers, inter alia) and those that do not (e.g.~neural networks, support vector machines, inter alia). Those that do not are called \textbf{`black-box' algorithms}. Neither type assures the best prediction accuracy. Important trade-offs need to be considered, however, if the best prediction comes from a black-box method, but the goal of the research is to understand the contribution of the features to the model's predictions.

Once we have identified our target variable, engineered a promising set of features, and selected an algorithm to employ that meets our research goals, it is now time to interrogate the dataset. The first step is to partition the dataset into a training and test set. The training set is the dataset we will use to try out different features and/ or algorithms with the aim of developing a model which can most accurately predict the target variable values in this training set. This is the second step and it's done by first training an algorithm to associate the features with the (actual) target values. Next, the resulting model is then applied to the same training data, yet with the target variable removed, or hidden, from the machine learner. The target values predicted by the model for each observation are compared to the actual target values. The more predicted and actual values for the target variable coincide, the more accurate the model. If the model shows high accuracy, then we are ready to move to evaluate this model on the test set (again removing the target variable). If the model accuracy is low, it's back to the drawing board either returning to feature engineering and/ or algorithm selection in hopes to improve model performance. In this way, the training data can be used multiple times, a clear divergence from standard IDA methods in which the data is interrogated and analyzed once and only once.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/04-approaching-analysis/predictive-phases} 

}

\caption{Phases in predictive analysis.}\label{fig:aa-predictive-phases}
\end{figure}

For all applications of PDA the interpretation of the prediction model includes some metric or metrics of accuracy comparing the extent to which the models predictions and the actual targets align. In cases in which the inner workings of the model are of interest, a researcher can dive into features and their contributions to the prediction model in an exploratory fashion according to the research goals. The exploration of features, then, varies, so at this time let's focus on the metrics of prediction accuracy.

The standard form for evaluating a model's performance differs between classification models (naive bayes) and regression models (linear regression). For classification models, a cross-tabulation of the predicted and actual classes results in a \textbf{contingency table} which can be used to calculate \textbf{accuracy} which is the sum of all the correctly predicted observations divided by the total number of observations in the test set. In addition to accuracy, there are various other measures which aim to assess a model's performance to gain more insight into the potential over- or under-generalization of the model (\emph{Precision} and \emph{Recall}). For regression models, differences between predicted and actual values can be assessed using a \textbf{coefficient of correlation} (typically \(R^2\)). Again, more fine-grained detail about the model's performance can be calculated (\emph{Root Mean Square Error}).

Another component worthy of consideration when evaluating a model's performance is how do we determine if the performance is actually good. One the one hand, accuracy rates into the 90+\% range on the test set is usually a good sign that the model is performing well. No model will perform with perfect accuracy, however, and depending on the goal of the research particular error patterns may be more important, and problematic, than the overall prediction accuracy. On the other hand, another eventuality is that the model performs very well on the training set but that on the test set (new data) the performance drops significantly. This is a sign that during the training phrase the machine learning algorithm learned nuances in the data (`noise') that obscure the signal pattern to be learned. This problem is called \textbf{overfitting} and to avoid it researchers iteratively run evaluations of the training data using resampling. The two most common resampling methods are \textbf{bootstrapping} (resampling with replacement) and \textbf{cross-validation} (resampling without replacement). The performance of these multiple models are summarized and the error between them is assessed. The goal is to minimize the performance differences between the models while maximizing the overall performance. These measures go a long way to avoiding overfitting and therefore maximizing the chance that the training phase will produce a model which is robust.

\hypertarget{exploratory-data-analysis}{%
\subsubsection{Exploratory data analysis}\label{exploratory-data-analysis}}

The last of the three analysis types, exploratory data analysis (EDA) includes a wide range of methods whose objective is to identify structure in datasets using only the data itself. In this way, EDA is an inductive, bottom-up approach to data analysis, which does not make any formal assumptions about the relationship(s) between variables. EDA can be roughly broken into two subgroups of analysis. \textbf{Unsupervised learning}, like supervised learning (PDA), is a subtype of machine learning. However, unlike prediction, unsupervised learning does not include a target variable to guide associations. The second subgroup of EDA methods can be seen as a (more robust) extension of the \textbf{descriptive analysis methods} covered earlier in this chapter. Either through unsupervised learning or descriptive methods, EDA employs quantitative methods to summarize, reduce, and sort complex datasets and statistically and visually interrogate a dataset in order to provide the researcher novel perspective to be qualitatively assessed. These qualitative assessments may prove useful to generate hypotheses or to generate groupings to be used in predictive analyses. So, if a researcher's aim is to probe a dataset in order to explore potential relationships in an area where predictions and/ or hypotheses cannot be clearly made, this is the analysis approach to choose.

In contrast to both IDA and even PDA in which there are assumptions made about what relationship(s) to explore, EDA makes no such assumptions. Furthermore, given the exploratory nature of the process, EDA is not an approach which can in itself be used to make conclusive generalizations about the populations from which the (corpus) sample in which it is drawn. For IDA the fidelity of the sample and the process of selection of the variables is of utmost importance to ensure that the statistical results are reliably generalizable. Even in the case of PDA, the sample and variables selected are key to building a robust predictive model. However, in contrast to IDA, but similar to PDA, EDA methods may reuse the data selecting different variables and/or methods as research goals dictate. If a machine learning approach to EDA is adopted, the dataset can be partitioned into training and test sets, in a similar fashion to PDA. And as with PDA, the training set is used for refining statistical measures and the test set is used to evaluate the refined measures. Although the evaluation results still cannot be used to generalize, the insight can be taken as stronger evidence that there is a potential relationship, or set of relationships, worthy of further study.

Another notable point of contrast concerns the interpretation of EDA results. Although quantitative in nature, exploratory methods involve a high level of human interpretation. Human interpretation is a part of each stage of data analysis, and each statistical approach, in general, but exploratory methods produce results that require associative thinking and pattern detection which is distinct from the other two analysis approaches, in particular, IDA.

Again, as we have done for the other two analysis approaches, let's turn to the process of variable identification, data interrogation, and interpretation methods. As in the case of PDA, EDA only requires a research goal. But in PDA, the research goal centered around predicting a target variable. In EDA, there is no such focus. The research goal may in fact be less defined and a researcher may consider various relationships in turn or simultaneously. The curation of the variables, however, does overlap in spirit to the process of \textbf{feature engineering} that we touched on for creating variables for predictive models. But in EDA the measure to gauge whether the engineered variables are good, is left to the qualitative evaluation of the researcher.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/04-approaching-analysis/exploratory-variables} 

}

\caption{Variable roles in exploratory analysis.}\label{fig:aa-exploratory-variables}
\end{figure}

For illustrative purposes let's consider the State of the Union Corpus (SOTU) \citep{R-quanteda.corpora}. The presidential addresses and a set of meta-data variables are included in the corpus. I've subsetted this corpus to only include U.S. presidents since 1946. A tabular preview of the first 10 addresses (truncated for display) can be found in Table \ref{tab:eda-sotu-corpus}.

\begin{table}

\caption{\label{tab:eda-sotu-corpus}First ten addresses from the SOTU Corpus.}
\centering
\begin{tabular}[t]{lllll}
\toprule
president & date & delivery & party & addresses\\
\midrule
Truman & 1946-01-21 & written & Democratic & To the Congress of the United States:

A quarte...\\
Truman & 1947-01-06 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
Truman & 1948-01-07 & spoken & Democratic & Mr. President, Mr. Speaker, and Members of the ...\\
Truman & 1949-01-05 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
Truman & 1950-01-04 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\addlinespace
Truman & 1951-01-08 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
Truman & 1952-01-09 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
Truman & 1953-01-07 & written & Democratic & To the Congress of the United States:

I have t...\\
Eisenhower & 1953-02-02 & spoken & Republican & Mr. President, Mr. Speaker, Members of the Eigh...\\
Eisenhower & 1954-01-07 & spoken & Republican & Mr. President, Mr. Speaker, Members of the Eigh...\\
\bottomrule
\end{tabular}
\end{table}

A dataset such as this one could be leveraged to explore many different types of research questions. Key to guiding the engineering of features, however, is to clarify from the outset of the research project what the entity of study is, or \textbf{unit of analysis}. In IDA and PDA approaches, the unit of analysis forms an explicit part of the research hypothesis or goal. In EDA the research question may have multiple fronts, which may be reflected in differing units of analysis. For example, based on the SOTU dataset, we could be interested in political rhetoric, language of particular presidents, party ideology, etc. Depending on the perspective we are interested in investigating, the choice of how to approach engineering features to gain insight will vary.

By the same token, approaches for interrogating the dataset can vary widely, between and within the same research project, but for instructive purposes we can draw a distinction between descriptive methods and unsupervised learning methods.

\begin{table}

\caption{\label{tab:eda-approaches-table}Some common EDA analysis methods.}
\centering
\begin{tabular}[t]{ll}
\toprule
Descriptive & Unsupervised learning\\
\midrule
Term frequency analysis & Cluster analysis\\
Term keyness analysis & Topic Modeling\\
Collocation analysis & Dimensionality reduction\\
\bottomrule
\end{tabular}
\end{table}

EDA leans heavily on visual representations of both descriptive and unsupervised learning methods. Visualizations enable humans to identify and extrapolate associative patterns. Visualizations range from standard barplots and scatterplots to network graphs and dendrograms and more. Some sample visualizations based on the SOTU Corpus are found in Figure \ref{fig:eda-visualizations-grid}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/eda-visualizations-grid-1} 

}

\caption{Sample visualizations from the SOTU Corpus (1946-2020).}\label{fig:eda-visualizations-grid}
\end{figure}

Just as feature selection and analysis method, the interpretation of the results in EDA are much more varied than in the other analysis methods. EDA methods provide information which requires much more human intervention and associative interpretation. In this way, EDA can be seen as a quantitatively informed qualitative assessment approach. The results from one approach can be used as the input to another. Findings can lead to further exploration and probing of nuances in the data. Speculative as they are the results from exploratory methods can be highly informative and lead to new insight and inspire further study in directions that may not have been expected.

\hypertarget{reporting}{%
\subsection{Reporting}\label{reporting}}

Much of the necessary reporting for an analysis features in prose as part of the write-up of a report or article. This will include descriptive summaries, a blueprint of the method(s) used, and the results. Descriptive summaries will often include assessments of individual variables and/ or relationships between variables (central tendency, dispersion, association strength, etc.). Any procedures applied to diagnose or to correct the data should also be included in the final report. This information is key to helping readers assess the results from the analysis. A blueprint of the methods used will describe the variable selection process, how the variables were used in the statistical analysis, and any other information that is relevant for a reader to understand what was done and why it was done. Reporting results from an analysis will depend on the type of analysis and the particular method(s) employed. For inferential analyses this will include the test statistic(s) (\(X^2\), \(R^2\), etc.) and some measure of confidence (\(p\)-value, confidence interval, effect size). In predictive analyses accuracy results and related information will need to be reported. For exploratory analyses, the reporting of results will vary and often include visualizations and metrics that require more human interpretation than the other analysis types.

While a good article write-up will include the most vital information to understand the procedures taken in an analysis, there are many more details which do not traditionally appear in prose. If a research project was conducted programmatically, however, the programming files (scripts) used to generate the analysis can (and should) be shared. While the scripts themselves are highly useful for other researchers to consult and understand in fine-grained detail the steps that were taken, it is important to also recognize that the research project should be well documented --through organized project directory and file structure as well as through code commenting. This description and instructions on how to run the analysis form a \textbf{research compendium} which ensure that the research conducted is easily understood and able to be reproduced and/ or enhanced by other researchers.

\hypertarget{summary-2}{%
\subsection*{Summary}\label{summary-2}}
\addcontentsline{toc}{subsection}{Summary}

In this chapter we have focused on description and analysis --the third component of DIKI Hierarchy. This process is visually summarized in Figure \ref{fig:approaching-analysis-visual-summary-graphic}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/04-approaching-analysis/approaching-analysis-visual-summary-paper} 

}

\caption{Approaching analysis: visual summary}\label{fig:approaching-analysis-visual-summary-graphic}
\end{figure}

Building on the strategies covered in \protect\hyperlink{understanding-data}{Chapter 2 ``Understanding data''} to derive a rich relational dataset, in this chapter we outlined key points in approaching analysis. The first key step in any analysis is to perform a descriptive assessment of the individual variables and relationships between variables. To select the appropriate descriptive measures we covered the various informational values that a variable can take. In addition to providing key information for reporting purposes, descriptive measures are important to explore so the researcher can get a better feel for the dataset before conducting an analysis.

We covered three data analysis types in this chapter: inferential, predictive, and exploratory. Each of these embodies very distinct approaches to deriving knowledge from data. Ultimately the choice of analysis type is highly dependent on the goals of the research. Inferential analysis is centered around the goal of testing a hypothesis, and for this reason it is the most highly structured approach to analysis. This structure is aimed at providing the mechanisms to draw conclusions from the results that can be generalized to the target population. Predictive analysis has a less-ambitious but at times more relevant goal of discovering the extent to which a given relationship can be extrapolated from the data to provide a model of language that can accurately predict an outcome using new data. While many times predictive analysis is used to perform language tasks, it can also be a highly effective methodology for applying different algorithmic approaches and exploring relationships a target variable and various configurations of variables. The ability to explore the data in multiple ways, is also a key strength of employing an exploratory analysis. The least structured and most variable of the analysis types, exploratory analyses are a powerful approach to deriving knowledge from data in an area where clear predictions cannot be made.

I rounded out this chapter with a short description of the importance of reporting the metrics, procedures, and results from analysis. Reporting, in its traditional form, is documented in prose in an article. This reporting aims to provide the key information that a reader will need to understand what was done, how it was done, and why it was done. This information also provides the necessary information for reader's with a critical eye to understand the analysis in more detail. Yet even the most detailed reporting in a write-up still leaves many practical, but key, points of the analysis obscured. A programming approach provides the procedural steps taken that when shared provide the exact methods applied. Together with the write-up a research compendium which provides the scripts to run the analysis and documentation on how to run the analysis forms an integral part of creating reproducible research.

\hypertarget{framing-research}{%
\section{Framing research}\label{framing-research}}

DRAFT

\begin{quote}
If we knew what it was we were doing, it would not be called research, would it?

―--Albert Einstein
\end{quote}

\begin{rmdkey}
The essential questions for this chapter are:

\begin{itemize}
\tightlist
\item
  What are the strategies for selecting a research area and identifying
  a research problem?
\item
  How does a research problem and research aim frame the development of
  a research statement?
\item
  What is a `research blueprint' and how do the conceptual and practical
  steps involved in developing it aid the researcher as well as the
  scientific community?
\end{itemize}
\end{rmdkey}

At this point in this part of the coursebook, we have covered Data, Information, and Knowledge from the Data to Insight Hierarchy. The goal has been to provide an orientation to the main building blocks of doing text analysis. Insight is the last component of the hierarchy. However, in practical terms, it is the first step to address in an research project as goals of a research project influence all subsequent steps. In this chapter we discuss how to frame research, that is how to position your research project's findings to contribute insight to understanding of the world. We will cover how to connect with the literature, selecting a research area and identifying a research problem, and how to design research best positioned to return relevant findings that will connect with this literature, establishing a research aim and research question. We will round out this chapter with a guide on developing a research blueprint --a working plan to organize the conceptual and practical steps to implement the research effectively and in a way that supports communicating the research findings and the process by which the findings were obtained.

Together a research area, problem, aim and question and the research blueprint that forms the conceptual and practical scaffolding of the project ensure from the outset that the project is solidly grounded in the main characteristics of good research. These characteristics, summarized by \citet{Cross2006}, are found in Table \ref{tab:fr-cross-research-char-table}.

\begin{table}

\caption{\label{tab:fr-cross-research-char-table}Characteristics of research (Cross, 2006).}
\centering
\begin{tabular}[t]{ll}
\toprule
Characteristic & Description\\
\midrule
Purposive & Based on identification of an issue or problem worthy and capable of investigation\\
Inquisitive & Seeking to acquire new knowledge\\
Informed & Conducted from an awareness of previous, related research\\
Methodical & Planned and carried out in a disciplined manner\\
Communicable & Generating and reporting results which are feasible and accessible by others\\
\bottomrule
\end{tabular}
\end{table}

With these characteristics in mind, let's get started with the first component to address --connecting with the literature.

\hypertarget{connect}{%
\subsection{Connect}\label{connect}}

\hypertarget{research-area}{%
\subsubsection{Research area}\label{research-area}}

The area of research is the first decision to make in terms of where to make a contribution to understanding. At this point, the aim is to identify a general area of interest where a researcher wants to derive insight. For those with an established research trajectory in language, the area of research to address through text analysis will likely be an extension of their prior work. For others, which include new researchers or researcher's that want to explore new areas of language research or approach an area through a language-based lens, the choice of area may be less obvious. In either case, the choice of a research area should be guided by a desire to contribute something relevant to a theoretical, social, and/ or practical matter of personal interest. Personal relevance goes a long way to developing and carrying out \textbf{purposive} and \textbf{inquisitive} research.

So how do we get started? The first step is to reflect on your own areas of interest and knowledge, be it academic, professional, or personal. Language is at the heart of the human experience and therefore found in some fashion anywhere one seeks to find it. But it is a big world and more often than not the general question about what area to explore language use is sometimes the most difficult. To get the ball rolling, it is helpful to peruse disciplinary encyclopedias or handbooks of linguistics and language-related an academic fields (e.g.~\href{https://www.sciencedirect.com/referencework/9780080448541/encyclopedia-of-language-and-linguistics}{Encyclopedia of Language and Linguistics} \citep{Brown2005}, \href{https://www.sciencedirect.com/book/9781843345978/a-practical-guide-to-electronic-resources-in-the-humanities}{A Practical Guide to Electronic Resources in the Humanities} \citep{Dubnjakovic2010}, \href{https://www.routledgehandbooks.com/doi/10.4324/9781315749129}{Routledge encyclopedia of translation technology} \citep{Chan2014})

A more personal, less academic, approach is to consult online forums, blogs, etc. that one already frequents or can be accessed via an online search. For example, \href{https://www.reddit.com/}{Reddit} has a wide variety of active subreddits (\href{https://www.reddit.com/r/LanguageTechnology/}{r/LanguageTechnology}, \href{https://www.reddit.com/r/linguistics/}{r/Linguistics}, \href{https://www.reddit.com/r/corpuslinguistics/}{r/corpuslinguistics}, \href{https://www.reddit.com/r/DigitalHumanities/}{r/DigitalHumanities}, etc.). Twitter and Facebook also have interesting posts on linguistics and language-related fields worth following. Through one of these social media site you may find particular people that maintain a blog worth browsing. For example, I follow \href{https://juliasilge.com/}{Julia Silge}, \href{http://www.rctatman.com/}{Rachel Tatman}, and \href{https://tedunderwood.com/}{Ted Underwood}, inter alia. Perusing these resources can help spark ideas and highlight the kinds of questions that interest you.

Regardless of whether your inquiry stems from academic, professional, or personal interest, try to connect these findings to academic areas of research. Academic research is highly structured and well-documented and making associations with this network will aid in subsequent steps in developing a research project.

\hypertarget{research-problem}{%
\subsubsection{Research problem}\label{research-problem}}

Once you've made a rough-cut decision about the area of research, it is now time to take a deeper dive into the subject area and jump into the literature. This is where the rich structure of disciplinary research will provide aid to traverse the vast world of academic knowledge and identify a research problem. A research problem highlights a particular topic of debate or uncertainty in existing knowledge which is worthy of study.

Surveying the relevant literature is key to ensuring that your research is \textbf{informed}, that is, connected to previous work. Identifying relevant research to consult can be a bit of a `chicken or the egg' problem --some knowledge of the area is necessary to find relevant topics, some knowledge of the topics is necessary to narrow the area of research. Many times the only way forward is to jump in conducting searches. These can be world-accessible resources (e.g.~\href{https://scholar.google.com/}{Google Scholar}) or limited-access resources that are provided through an academic institution (e.g.~\href{https://about.proquest.com/en/products-services/llba-set-c}{Linguistics and Language Behavior Abstracts}), \href{https://eric.ed.gov/}{ERIC}, \href{https://www.ebsco.com/products/research-databases/apa-psycinfo}{PsycINFO}, etc.). Some organizations and academic institutions provide \href{https://guides.zsr.wfu.edu/linguistics}{research guides} to help researcher's access the primary literature.

Another avenue to explore are journals dedicated to areas in which linguistics and language-related research is published. In the following tables I've listed a number of highly visable journals in linguistics, digital humanities, and computational linguistics.

\begin{table}

\caption{\label{tab:pinboard-journals-linguistics}A list of some linguistics journals.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="https://www.euppublishing.com/loi/cor">Corpora</a> & An international, peer-reviewed journal of corpus linguistics focusing on the many and varied uses of corpora both in linguistics and beyond.\\
\hline
<a href="https://www.degruyter.com/journal/key/CLLT/html">Corpus Linguistics and Linguistic Theory</a> & Corpus Linguistics and Linguistic Theory (CLLT) is a peer-reviewed journal publishing high-quality original corpus-based research focusing on theoretically relevant issues in all core areas of linguistic research, or other recognized topic areas.\\
\hline
<a href="https://benjamins.com/catalog/ijcl">International Journal of Corpus Linguistics</a> & The International Journal of Corpus Linguistics (IJCL) publishes original research covering methodological, applied and theoretical work in any area of corpus linguistics.\\
\hline
<a href="http://ijls.net/">International Journal of Language Studies</a> & It is a refereed international journal publishing articles and reports dealing with theoretical as well as practical issues focusing on language, communication, society and culture.\\
\hline
<a href="https://www.cambridge.org/core/journals/journal-of-child-language">Journal of Child Language</a> & A key publication in the field, Journal of Child Language publishes articles on all aspects of the scientific study of language behaviour in children, the principles which underlie it, and the theories which may account for it.\\
\hline
<a href="https://www.cambridge.org/core/journals/journal-of-linguistic-geography/all-issues">Journal of Linguistic Geography</a> & The Journal of Linguistic Geography focuses on dialect geography and the spatial distribution of language relative to questions of variation and change.\\
\hline
<a href="http://www.tandfonline.com/toc/njql20/current">Journal of Quantitative Linguistics</a> & Publishes research on the quantitative characteristics of language and text in mathematical form, introducing methods of advanced scientific disciplines.\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:pinboard-journals-humanities}A list of some humanities journals.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="http://www.digitalhumanities.org/dhq/">Digital Humanities Quarterly</a> & Digital Humanities Quarterly (DHQ), an open-access, peer-reviewed, digital journal covering all aspects of digital media in the humanities.\\
\hline
<a href="https://academic.oup.com/dsh">Digital Scholarship in the Humanities</a> & DSH or Digital Scholarship in the Humanities is an international, peer reviewed journal which publishes original contributions on all aspects of digital scholarship in the Humanities including, but not limited to, the field of what is currently called the Digital Humanities.\\
\hline
<a href="https://culturalanalytics.org/">Journal of Cultural Analytics</a> & Cultural Analytics is an open-access journal dedicated to the computational study of culture. Its aim is to promote high quality scholarship that applies computational and quantitative methods to the study of cultural objects (sound, image, text), cultural processes (reading, listening, searching, sorting, hierarchizing) and cultural agents (artists, editors, producers, composers).\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:pinboard-journals-cl}A list of some computational linguistics journals.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="https://direct.mit.edu/coli">Computational Linguistics</a> & Computational Linguistics is the longest-running publication devoted exclusively to the computational and mathematical properties of language and the design and analysis of natural language processing systems.\\
\hline
<a href="http://lrec-conf.org/">LREC Conferences</a> & The International Conference on Language Resources and Evaluation is organised by ELRA biennially with the support of institutions and organisations involved in HLT.\\
\hline
<a href="https://transacl.org/index.php/tacl/index">Transactions of the Association for Computational Linguistics</a> & Transactions of the Association for Computational Linguistics (TACL) is an ACL-sponsored journal published by MIT Press that publishes papers in all areas of computational linguistics and natural language processing.\\
\hline
\end{tabular}
\end{table}

To explore research related to text analysis it is helpful to start with the (sub)discipline name(s) you identified in when selecting your research area, more specific terms that occur to you or key terms from the literature, and terms such as `corpus study' or `corpus-based'. The results from first searches may not turn out to be sources that end up figuring explicitly in your research, but it is important to skim these results and the publications themselves to mine information that can be useful to formulate better and more targeted searches. Relevant information for honing your searches can be found throughout an academic publication (article or book). However, pay particular attention to the abstract, in articles, and the table of contents, in books, and the cited references. Abstracts and tables of contents often include discipline-specific jargon that is commonly used in the field. In some articles there is even a short list of key terms listed below the abstract which can be extremely useful to seed better and more precise search results. The references section will contain relevant and influential research. Scan these references for publications which appear to narrowing in on topic of interest and treat it like a search in its own right.

Once your searches begin to show promising results it is time to keep track and organize these references. Whether you plan to collect thousands of references over a lifetime of academic research or your aim is centered around one project, software such as \href{https://www.zotero.org/}{Zotero}\footnote{\href{https://guides.zsr.wfu.edu/zotero}{Zotero Guide}}, \href{https://www.mendeley.com/reference-management/reference-manager}{Mendeley}, or \href{https://bibdesk.sourceforge.io/}{BibDesk} provide powerful, flexible, and easy-to-use tools to collect, organize, annotate, search, and export references. Citation management software is indispensable for modern research --and often free!

As your list of relevant references grows, you will want to start the investigation process in earnest. Begin skimming (not reading) the contents of each of these publications, starting with the most relevant first\footnote{Or what appears to be most relevant. This may change as you start to take a closer look.}. Annotate these publications using highlighting features of the citation management software to identify: (1) the stated goal(s) of the research, (2) the data source(s) used, (3) the information drawn from the data source(s), (4) the analysis approach employed, and (5) the main finding(s) of the research as they pertain to the stated goal(s). Next, in your own words, summarize these five key areas in prose adding your summary to the notes feature of the citation management software. This process will allow you to efficiently gather and document references with the relevant information to guide the identification of a research problem, to guide the formation of your problem statement, and ultimately, to support the literature review that will figure in your project write-up.

From your preliminary annotated summaries you will undoubtedly start to recognize overlapping and contrasting aspects in the research literature. These aspects may be topical, theoretical, methodological, or appear along other lines. Note these aspects and continue to conduct more refine searches, annotate new references, and monitor for any emerging patterns of uncertainty or debate (gaps) which align with your research interest(s). When a promising pattern takes shape, it is time to engage with a more detailed reading of those references which appear most relevant highlighting the potential gap(s) in the literature. At this point you can focus energy on more nuanced aspects of a particular gap in the literature with the goal to formulate a problem statement. A problem statement directly acknowledges a gap in the literature and puts a finer point on the nature and relevance of this gap for understanding. This statement reflects your first deliberate attempt to establish a line of inquiry. It will be a targeted, but still somewhat general, statement framing the gap in the literature that will guide subsequent research design decisions.

\hypertarget{findings}{%
\subsection{Findings}\label{findings}}

\hypertarget{research-aim}{%
\subsubsection{Research aim}\label{research-aim}}

With a problem statement in hand, it is now time to consider the goal(s) of the research. A research aim frames the type of inquiry to be conducted. Will the research aim to explain, evaluate, or explore? In other words, will the research seek to test a particular relationship, assess the potential strength of a particular relationship, or uncover novel relationships? As you can appreciate, the research aim is directly related to the analysis methods we touched upon in \protect\hyperlink{approaching-analysis}{Chapter 3}.

To gauge how to frame your research aim, reflect on the literature that led you to your problem statement and the nature of the problem statement itself. If the gap at the center of the problem statement is a lack of knowledge, your research aim may be exploratory. If the gap concerns a conjecture about a relationship, then your research may take a predictive approach. When the gap points to the validation of a relationship, then your research will likely be inferential in nature. Before selecting your research aim it is also helpful to consult the research aims of the primary literature that led you to your research statement. Consider how your research statement relates the previous literature. Do you aim to test a hypothesis based on previous exploratory analyses? Are you looking to generate new knowledge in an (apparently) uncharted area?

In general, a problem statement which addresses a smaller, nuanced gap will tend to adopt similar research aims as the previous literature while a larger, more divergent gap will tend to adopt a distinct research aim. This is not a hard rule, but more of a heuristic, however, and it is important to be familiar with both the previous literature, the nature of different types of analysis, and the goals of the research to ensure that the research is best-positioned to generate findings that will contribute to the existing body of understanding in a principled way.

\hypertarget{research-question}{%
\subsubsection{Research question}\label{research-question}}

The next step in research design is to craft the research question. A research question is clearly defined statement which identifies an aspect of uncertainty and the particular relationships that this uncertainty concerns. The research question extends and narrows the line of inquiry established in the research statement and research aim. The research statement can be seen as the content and the research aim as the form.

The form of a research question will vary based on the analysis approach. For inferential-based research, the research question will actually be a statement, not a question. This statement makes a testable claim about the nature of a particular relationship --i.e.~asserts a hypothesis. For illustration, let's return to one of the hypotheses we previously sketched out in \protect\hyperlink{approaching-analysis}{Chapter 3}, leaving aside the implicit null hypothesis.

Women use more questions than men in spontaneous conversations.

For predictive- and exploratory-based research, the research question is in fact a question. A reframing of the example hypothesis for a predictive-based research question might looks something like this.

Can the number of questions used in spontaneous conversations predict if a speaker is male or female?

And a similar exploratory-based research question would take this form.

Do men and women differ in terms of the number of questions they use in spontaneous conversations?

The central research interest behind these hypothetical research questions is, admittedly, quite basic. But from these simplified examples, we are able to appreciate the similarities and differences between the forms of research statements that correspond to distinct research aims.

In terms of content, the research question will make reference to two key components. First, is the \textbf{unit of analysis}. The unit of analysis is the entity which the research aims to investigate. For our three example research aims, the unit of analysis is the same, namely men and women. Note, however, that the current unit of analysis is somewhat vague in the example research questions. A more precise unit of analysis would include more information about the population from which the men and women are drawn (e.g English speakers, American English speakers, American English speakers of the Southeast, etc.).

The second key component is the \textbf{unit of observation}. The unit of observation is the primary element on which the insight into the unit of analysis is derived and in this way constitutes the essential organization unit of the data to be collected. In our examples, the unit of observation, again, is unchanged and is spontaneous conversations. Note that while the unit of observation is key to identify as it forms the organizational backbone of the research, it is very common for the research to derive variables from this unit to provide evidence to investigate the research question. In the previous examples, we identified the number of conversations as part of the research question. But in other cases a researcher may seek to understand other aspects of questions in spontaneous conversations (i.e type of question, features of questions, etc.). The unit of observation, however, would remain the same.

\hypertarget{blueprint}{%
\subsection{Blueprint}\label{blueprint}}

Efforts to craft a research question are a very important aspect of developing purposive, inquisitive, and informed research (returning to Cross's characteristics of research). Moving beyond the research question in the project means developing and laying out the research design in a way such that the research is \textbf{Methodical} and \textbf{Communicable}. In this coursebook, the method to achieve these goals is through the development of a research blueprint. The blueprint includes two components: (1) the process of identifying the data, information, and methods to be used and (2) the creation of a plan to structure and document the project.

As \citet{Ignatow2017} point out:

\begin{quote}
Research design is essentially concerned with the basic architecture of research projects, with designing projects as systems that allow theory, data, and research methods to interface in such a way as to maximize a project's ability to achieve its goals \protect\hyperlink{section-1}{\ldots{}}. Research design involves a sequence of decisions that have to be taken in a project's early stages, when one oversight or poor decision can lead to results that are ultimately trivial or untrustworthy. Thus, it is critically important to think carefully and systematically about research design before committing time and resources to acquiring texts or mastering software packages or programming languages for your text mining project.
\end{quote}

\hypertarget{identify}{%
\subsubsection{Identify}\label{identify}}

Importance of identifying and documenting the key aspects required to conduct the research cannot be understated. On the one hand this process links concept to implementation. In doing so, a researcher is better-positioned to conduct research with a clear view of what will be entailed. On the other hand, a promising research question, on paper, may present challenges that may require modification or reevaluation of the viability of the project. It is not uncommon to encounter roadblocks or even dead-ends for moving a well-founded research question forward when considering the available data, a researcher's (current) technical and/ or research skills, and the given time frame for the project. In practice, the process of identifying the data, information, and methods of analysis are considered in tandem with the investigative work to develop a research aim and research question. In this subsection I will cover the main characteristics to consider when developing a research blueprint.

The first, and most important, part of establishing a research blueprint is to \textbf{identify a viable data source}. Regardless of how you find and access the data, it is essential to vet the corpus sample in light of the research question. In the case that research is inferential in nature, the sampling frame of the corpus is of primary importance as the goal is to generalize the findings to a target population. A corpus resource should align, to the extent feasible, with this target population. For predictive and exploratory research, the goal to generalize a claim is not central and for this reason the there is some freedom in terms of how representative a corpus sample is of a target population. Ideally a researcher will find and be able to model a language population of target interest. Since the goal, however, is not to test a hypothesis, but rather to explore particular or potential relationships, either in an predictive or exploratory fashion, the research can often continue with the stipulation that the results are interpreted in the light of the characteristic of the available corpus sample.

The second step is to \textbf{identify the key variables} need to conduct the research are and then ensure that this information can be derived from the corpus data. The research question will reference the unit of analysis and the unit of observation, but it is important at this point to then pinpoint what the key variables will be. If the unit of observation is spontaneous conversations. The question as to what aspects of these conversations will be used in the analysis. In the research questions presented in this chapter, we will want to envision what needs to be done to generate a variable which measures the number of questions in each of the conversations. In other research, their may be features that need to be extracted and recoded to address the research question. Other variables of importance may be non-linguistic in nature. Provided the corpus has the required meta-data for the research, variables can be normalized, recoded, and generated from the corpus itself to fit research needs. In cases where there the meta-data is incomplete for the goals of the research, it is sometimes possible to merge meta-data from other sources.

The third step is to \textbf{identify a method of analysis}. The selection of the analysis approach that was part of the research aim and then the research question goes a long way to narrowing the methods that a researcher must consider. But there are a number of factors which will make some methods more appropriate than others. In inferential research, the number and information values of the variables to be analyzed will be of key importance \citep{Gries2013a}. The informational value of the dependent variable will again narrow the search for the appropriate method. The number of independent variables also plays an important role. For example, a study with a categorical dependent variable with a single categorical independent variable will lead the researcher to the Chi-squared test. A study with a continuous dependent variable with multiple independent variables will lead to linear regression. Another aspect of note for inference studies is the consideration of the distribution of continuous variables --a normal distribution will use a parametric test where a non-normal distribution will use a non-parametric test. These details need not be nailed down at this point, but it is helpful to have them on your radar to ensure that when the time comes to analyze the data, the appropriate steps are taken to test for normality and then apply the correct test.

For predictive-based research, the informational value of the target variable is key to deciding whether the prediction will be a classification task or a numeric prediction task. This has downstream effects when it comes time to evaluate and interpret the results. Although the feature engineering process in predictive analyses means that the features do not need to be specified from the outset and can be tweaked and changed as needed during an analysis, it is a good idea to start with a basic sense of what features most likely will be helpful in developing a robust predictive model. Furthermore, while the number and informational values of the features (predictor variables) are not as important to selecting a prediction method (algorithm) as they are in inferential analysis methods, it is important to recognize that algorithms have strengths and shortcomings when working large numbers and/ or types of features \citep{Lantz2013}.

Exploratory research is the least restricted of the three types of analysis approaches. Although it may be the case that a research will not be able to specify from the outset of a project what the exact analysis methods will be, an attempt to consider what types of analysis methods will be most promising to provide results to address the research question goes a long way to steering a project in the right direction and grounding the research. As with the other analysis approaches, it is important to be aware of what the analysis methods available and what type of information they produce in light of the research question.

In sum, the identification of the data, information, and analysis methods that will be used in the proposed research are key to ensuring the research is viable. Be sure to document this process in prose and describe the strengths and potential shortcomings of (1) the corpus data selected, (2) the information to be extracted for analysis, and (3) the analysis method(s) that are appropriate for the research aim and what the evaluation method will be. Furthermore, not every eventuality can be foreseen. It is helpful to include a description of aspects of this process which may pose challenges and to include potential contingency plans as part of this prose description.

\hypertarget{plan}{%
\subsubsection{Plan}\label{plan}}

The next step in creating a research blueprint is to consider how to physically implement your project. This includes how to organize files and directories in a fashion that both provides the researcher a logical and predictable structure to work with but also ensures that the research is \textbf{Communicable}. On the one hand, communicable research includes a strong write-up of the research, but, on the other hand, it is also important that the research is reproducible. Reproducibility strategies are a benefit to the researcher (in the moment and in the future) as it leads to better work habits and to better teamwork and it makes changes to the project easier. Reproducibility is also of benefit to the scientific community as shared reproducible research enhances replicability and encourages cumulative knowledge development \citep{Gandrud2015}.

There are a set of guiding principles to accomplish these goals \citep{Gentleman2007, Marwick2018}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All files should be plain text which means they contain no formatting information other than whitespace.
\item
  There should be a clear separation between the data, method, and output of research. This should be apparent from the directory structure.
\item
  A separation between original data and derived data should be made. Original data should be treated as `read-only'. Any changes to the original data should be justified, generated by the code, and documented (see point 6).
\item
  Each analysis file (script) should represent a particular, well-defined step in the research process.
\item
  Each analysis script should be modular --that is, each file should correspond to a specific goal in the analysis procedure with input and output only corresponding to this step.
\item
  All analysis scripts should be tied together by a `master' script that is used to coordinate the execution of all the analysis steps.
\item
  Everything should be documented. This includes analysis steps, script code comments, data description in data dictionaries, information about the computing environment and packages used to conduct the analysis, and detailed instructions on how to reproduce the research.
\end{enumerate}

These seven principles can be physically implemented in countless ways. In recent years, there has been a growing number of efforts to create R packages and templates to quickly generate the scaffolding and tools to facilitate reproducible research. Some notable R packages include \href{https://jdblischak.github.io/workflowr/}{workflowr} and \href{http://projecttemplate.net/}{ProjectTemplate} but there are many other resources for R included on the \href{https://cran.r-project.org/web/views/ReproducibleResearch.html}{CRAN Task View for Reproducible Research}. There are many advantages to working with pre-existing frameworks for the savvy R programmer.

In this coursebook, however, I have developed a project template (\href{https://github.com/lin380/project_template}{available on GitHub}) which I believe simplifies and makes the process more transparent for beginning and intermediate R programmers, the directory structure is provided below.

\begin{verbatim}
#> ../project_template/
#> +-- README.md
#> +-- _pipeline.R
#> +-- analysis
#> |   +-- 1_acquire_data.Rmd
#> |   +-- 2_curate_dataset.Rmd
#> |   +-- 3_transform_dataset.Rmd
#> |   +-- 4_analyze_dataset.Rmd
#> |   +-- 5_generate_article.Rmd
#> |   +-- _session-info.Rmd
#> |   +-- _site.yml
#> |   +-- index.Rmd
#> |   \-- references.bib
#> +-- data
#> |   +-- derived
#> |   \-- original
#> \-- output
#>     +-- figures
#>     \-- results
\end{verbatim}

Let me now describe how this template structure aligns with the seven principles of quality reproducible research.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All files are plain text (e.g.~\texttt{.R}, \texttt{.Rmd}, \texttt{.csv}, \texttt{.txt}, etc.).
\item
  There are three main directories \texttt{analysis/}, \texttt{data/}, and \texttt{ouput/}.
\item
  The \texttt{data/} directory contains sub-directories for \texttt{original} (`read-only') data and \texttt{derived} data.
\item
  The \texttt{analysis/} directory contains five scripts which are numbered to correspond with their sequential role in the research process.
\item
  Each of these analysis scripts are designed to be modular; input and output must be explicit and no intermediate objects are carried over to other analysis scripts. Dataset output should be written to and read from the \texttt{data/derived/} directory. Figures and statistical results should be written to and read from \texttt{output/figures/} and \texttt{output/results} respectively.
\item
  All of the analysis scripts, and therefore the entire project, are tied to the \texttt{\_pipeline.R} script. To reproduce the entire project only this script need be run.
\item
  Documentation takes place at many levels. The \texttt{README.md} file is the first file that a researcher will consult. It contains a brief description of the project goals and how to reproduce the analysis. Analysis scripts use the Rmarkdown format (\texttt{.Rmd}). This format allows researchers to interleave prose description and executable code in the same script. This ensures that the rationale for the steps taken are described in prose, the code is made available to consult, and that code comments can be added to every line. The \texttt{\_sesssion-info.Rmd} script is merged with each analysis script to provide information about the computing environment and packages used to conduct each step analysis. As this is a template, no data or datasets appear. However, once data is acquired and that data is curated and transformed, documentation for these resources should be documented for each resource in a data dictionary along side the data(set) itself.
\end{enumerate}

The aspects of the project template described in points 1-7 together form the backbone for reproducible research. This template, however, includes additional functionality to enhance efficient and communicable research. The \texttt{\_pipeline.R} script executes the analysis scripts in the \texttt{analysis} directory, but as a side effect also produces \href{https://lin380.github.io/project_template_demo/}{a working website} and a journal-ready article for publishing your analysis, results, and findings to the web in \href{https://lin380.github.io/project_template_demo/5_generate_article.html}{HTML} and \href{https://lin380.github.io/project_template_demo/article.pdf}{PDF} format. The \texttt{index.Rmd} file is the splash page for the website and is a good place to house your pre-analysis investigative work including your research area, problem, aim, and question and to document your research blueprint including the identification of viable data resource(s), the key variables for the analysis, the analysis method, and the method of assessment. All Rmarkdown files provide functionality for citing and organizing references. The \texttt{references.bib} file is where references are stored and can be used to include citations that support your research throughout your project.

\hypertarget{prepare}{%
\subsubsection{Prepare}\label{prepare}}

This template will allow you to organize your research design and align it with implementation steps to conduct quality reproducible research. To prepare for your analysis, you will need to download or fork and clone this template from the GitHub repository and then make some adjustments to personalize this template for your research.

To create a local copy of this project template either:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Download and decompress the \href{https://github.com/lin380/project_template/archive/refs/heads/main.zip}{.zip file}
\item
  If you have \href{https://github.com/git-guides/install-git}{git} installed on your machine and a \href{https://github.com/signup?ref_cta=Sign+up\&ref_loc=header+logged+out\&ref_page=\%2F\&source=header-home}{GitHub account}, \href{https://docs.github.com/en/get-started/quickstart/fork-a-repo\#forking-a-repository}{fork the repository} to your own GitHub account. Then open a terminal in the desired location and \href{https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository-from-github/cloning-a-repository\#cloning-a-repository}{clone the repository}. If you are using RStudio, you can setup a new RStudio Project with the clone using the `New Project\ldots{}' dialog, choosing `Version Control', and following the steps.
\end{enumerate}

Before you begin configuring and adding your project-specific details to this template. Reproduce this project `as-is' to confirm that it builds on your local machine.

In RStudio or in R session in a Terminal application, open the console in the root directory of the project. Then run:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"\_pipeline.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

It will take some time to complete, when it does the prompt (\texttt{\textgreater{}}) in the console will return. Then navigate to and open \texttt{docs/index.html} in a browser.

Once you have confirmed that the project template builds, then you can begin to configure the template to reflect your project. There a few files to consider first. These files are places where the title of your project should appear.

\begin{itemize}
\tightlist
\item
  \texttt{README.md}
\item
  \texttt{\_pipeline.R}
\item
  \texttt{analysis/index.Rmd}
\end{itemize}

After updating these files, build the project again and make sure that the new changes appear as you would like them. You are now ready to start your research project!

\hypertarget{summary-3}{%
\subsection*{Summary}\label{summary-3}}
\addcontentsline{toc}{subsection}{Summary}

The aim of this chapter is to provide the key conceptual and practical points to guide the development of a viable research project. Good research is purposive, inquisitive, informed, methodological, and communicable. It is not, however, always a linear process. Exploring your area(s) of interest and connecting with existing work will help couch and refine your research. But practical considerations, such as the existence of viable data, technical skills, and/ or time constrains, sometimes pose challenges and require a researcher to rethink and/ or redirect the research in sometimes small and other times more significant ways. The process of formulating a research question and developing a viable research plan is key to supporting viable, successful, and insightful research. To ensure that the effort to derive insight from data is of most value to the researcher and the research community, the research should strive to be methodological and communicable adopting best practices for reproducible research.

This chapter concludes the Orientation section of this coursebook. At this point the fundamental characteristics of research are in place to move a project towards implementation. The next section, Preparation, aims to cover the acquisition, curation, and transformation of data in preparation for analysis. These are the first steps in putting a research blueprint into action and by no coincidence the first components in the Data to Insight Hierarchy. Following the Preparation section our attention will turn to the implementation of the three analysis approaches we have covered: inference, prediction, and exploration. Throughout these next sections we will maintain our aim to develop methodological and communicable research by connecting our implementation process to reproducible programming strategies.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/05-framing-research/framing-research-visual-summary} 

}

\caption{Framing research: visual summary}\label{fig:framing-research-visual-summary-graphic}
\end{figure}

\hypertarget{part-preparation}{%
\part{Preparation}\label{part-preparation}}

\hypertarget{preparation-overview}{%
\section*{Overview}\label{preparation-overview}}
\addcontentsline{toc}{section}{Overview}

\textbf{PREPARATION}

At this point we will turn our attention to implementing the specifics outlined in our research blueprint. This section will group the components which concern the acquisition, curation, and transformation of data into a dataset which is prepared to be submitted to analysis. In each of these three chapters I will outline some of the main characteristics to consider in each of these research steps and provide authentic examples of working with R to implement these steps. In \protect\hyperlink{acquire-data}{Chapter 5} this includes downloads, working with APIs, and webscraping. In \protect\hyperlink{curate-data}{Chapter 6} we turn to organize data into rectangular, or `tidy', format. Depending on the data or dataset acquired for the research project, the steps necessary to shape our data into a base dataset will vary, as we will see. In \protect\hyperlink{transform-data}{Chapter 7} we will work to manipulate curated datasets to create datasets which are aligned with the research aim and research question. This often includes normalizing values, recoding variables, and generating new variables as well as and sourcing and merging information from other datasets with the dataset to be submitted for analysis.

\hypertarget{acquire-data}{%
\section{Acquire data}\label{acquire-data}}

DRAFT

\begin{quote}
The scariest moment is always just before you start.

―--Stephen King
\end{quote}

\begin{rmdkey}
The essential questions for this chapter are:

\begin{itemize}
\tightlist
\item
  What are the most common strategies for acquiring corpus data?
\item
  What programmatic steps can we take to ensure the acquisition process
  is reproducible?
\item
  What is the importance of documenting data?
\end{itemize}
\end{rmdkey}

There are three main ways to acquire corpus data using R that I will introduce you to: \textbf{downloads}, \textbf{APIs}, and \textbf{web scraping}. In this chapter we will start by manual and programmatically downloading a corpus as it is the most straightforward process for the novice R programmer and typically incurs the least number of steps. Along the way I will introduce some key R coding concepts including control statements and custom functions. Next I will cover using R packages to interface with APIs, both open-access and authentication-based. APIs will require us to delve into more detail about R objects and custom functions. Finally acquiring data from the web via webscraping is the most idiosyncratic and involves both knowledge of the web, more sophisticated R skills, and often some clever hacking skills. I will start with a crash course on the structure of web documents (HTML) and then scale up to a real-world example. To round out the chapter we will cover the process of ensuring that our data is documented in such a way as to provide sufficient information to understand its key sampling characteristics and the source from which it was drawn.

\hypertarget{downloads}{%
\subsection{Downloads}\label{downloads}}

\hypertarget{manual}{%
\subsubsection{Manual}\label{manual}}

The first acquisition method I will cover here is inherently non-reproducible from the standpoint that the programming implementation cannot acquire the data based solely on running the project code itself. In other words, it requires manual intervention. Manual downloads are typical for data resources which are not openly accessible on the public facing web. These can be resources that require institutional or private licensing (\href{https://www.ldc.upenn.edu/}{Language Data Consortium}, \href{http://ice-corpora.net/ice/}{International Corpus of English}, \href{https://www.corpusdata.org/}{BYU Corpora}, etc.), require authorization/ registration (\href{https://archive.mpi.nl/tla/}{The Language Archive}, \href{https://www.webcorpora.org/}{COW Corpora}, etc.), and/ or are only accessible via resource search interfaces (\href{https://cesa.arizona.edu/}{Corpus of Spanish in Southern Arizona}, \href{http://cedel2.learnercorpora.com/}{Corpus Escrito del Español como L2 (CEDEL2)}, etc.).

Let's work with the CEDEL2 corpus \citep{Lozano2009} which provides a search interface and open access to the data through the search interface. The homepage can be seen in Figure \ref{fig:ad-show-page-cedel2-1}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/06-acquire-data/ad-cedel2-site} 

}

\caption{CEDEL2 Corpus homepage}\label{fig:ad-show-page-cedel2-1}
\end{figure}

Following the search/ download link you can find a search interface that allows the user to select the sub-corpus of interest. I've selected the subcorpus ``Learners of L2 Spanish'' and specified the L1 as English.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/06-acquire-data/ad-cedel2-search-download} 

}

\caption{Search and download interface for the CEDEL2 Corpus}\label{fig:ad-show-page-cedel2-2}
\end{figure}

The `Download' link now appears for this search criteria. Following this link will provide the user a form to fill out. This particular resource allows for access to different formats to download (Texts only, Texts with metadata, CSV (Excel), CSV (Others)). I will select the `CSV (Others)' option so that the data is structured for easier processing downstream when we work to curate the data in our next processing step. Then I will choose to save the CSV in the \texttt{data/original/} directory of my project and create a sub-directory called \texttt{cedel2/}.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{└──}\NormalTok{ cedel2}
       \ExtensionTok{└──}\NormalTok{ texts.csv}
\end{Highlighting}
\end{Shaded}

Other resources will inevitably include unique processes to obtaining the data, but in the end the data should be archived in the research structure in the \texttt{data/original/} directory and be treated as `read-only'.

\hypertarget{programmatic}{%
\subsubsection{Programmatic}\label{programmatic}}

There are many resources that provide corpus data is directly accessible for which programmatic approaches can be applied. Let's take a look at how this works starting with the a sample from the Switchboard Corpus, a corpus of 2,400 telephone conversations by 543 speakers. First we navigate to the site with a browser and download the file that we are looking for. In this case I found the Switchboard Corpus on the \href{http://www.nltk.org/nltk_data/}{NLTK data repository site}. More often than not this file will be some type of compressed archive file with an extension such as \texttt{.zip} or \texttt{.tz}, which is the case here. Archive files make downloading large single files or multiple files easy by grouping files and directories into one file. In R we can used the \texttt{download.file()} function from the base R library\footnote{Remember base R packages are installed by default with R and are loaded and accessible by default in each R session.}. There are a number of \textbf{arguments} that a function may require or provide optionally. The \texttt{download.file()} function minimally requires two: \texttt{url} and \texttt{destfile}. That is the file to download and the location where it is to be saved to disk.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Download .zip file and write to disk}
\FunctionTok{download.file}\NormalTok{(}\AttributeTok{url =} \StringTok{"https://raw.githubusercontent.com/nltk/nltk\_data/gh{-}pages/packages/corpora/switchboard.zip"}\NormalTok{,}
    \AttributeTok{destfile =} \StringTok{"../data/original/switchboard.zip"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As we can see looking at the directory structure for \texttt{data/} the \texttt{switchboard.zip} file has been downloaded.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{└──}\NormalTok{ switchboard.zip}
\end{Highlighting}
\end{Shaded}

Once an archive file is downloaded, however, the file needs to be `decompressed' to reveal the file structure. To decompress this file we use the \texttt{unzip()} function with the arguments \texttt{zipfile} pointing to the \texttt{.zip} file and \texttt{exdir} specifying the directory where we want the files to be extracted to.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Decompress .zip file and extract to our target directory}
\FunctionTok{unzip}\NormalTok{(}\AttributeTok{zipfile =} \StringTok{"../data/original/switchboard.zip"}\NormalTok{, }\AttributeTok{exdir =} \StringTok{"../data/original/"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The directory structure of \texttt{data/} now should look like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{├──}\NormalTok{ switchboard}
    \ExtensionTok{│}\NormalTok{   ├── README}
    \ExtensionTok{│}\NormalTok{   ├── discourse}
    \ExtensionTok{│}\NormalTok{   ├── disfluency}
    \ExtensionTok{│}\NormalTok{   ├── tagged}
    \ExtensionTok{│}\NormalTok{   ├── timed{-}transcript}
    \ExtensionTok{│}\NormalTok{   └── transcript}
    \ExtensionTok{└──}\NormalTok{ switchboard.zip}
\end{Highlighting}
\end{Shaded}

At this point we have acquired the data programmatically and with this code as part of our workflow anyone could run this code and reproduce the same results. The code as it is, however, is not ideally efficient. Firstly the \texttt{switchboard.zip} file is not strictly needed after we decompress it and it occupies disk space if we keep it. And second, each time we run this code the file will be downloaded from the remote serve leading to unnecessary data transfer and server traffic. Let's tackle each of these issues in turn.

To avoid writing the \texttt{switchboard.zip} file to disk (long-term) we can use the \texttt{tempfile()} function to open a temporary holding space for the file. This space can then be used to store the file, unzip it, and then the temporary file will be destroyed. We assign the temporary space to an R object we will name \texttt{temp} with the \texttt{tempfile()} function. This object can now be used as the value of the argument \texttt{destfile} in the \texttt{download.file()} function. Let's also assign the web address to another object \texttt{url} which we will use as the value of the \texttt{url} argument.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a temporary file space for our .zip file}
\NormalTok{temp }\OtherTok{\textless{}{-}} \FunctionTok{tempfile}\NormalTok{()}
\CommentTok{\# Assign our web address to \textasciigrave{}url\textasciigrave{}}
\NormalTok{url }\OtherTok{\textless{}{-}} \StringTok{"https://raw.githubusercontent.com/nltk/nltk\_data/gh{-}pages/packages/corpora/switchboard.zip"}
\CommentTok{\# Download .zip file and write to disk}
\FunctionTok{download.file}\NormalTok{(url, temp)}
\end{Highlighting}
\end{Shaded}

\begin{rmdtip}
In the previous code I've used the values stored in the objects
\texttt{url} and \texttt{temp} in the \texttt{download.file()} function
without specifying the argument names --only providing the names of the
objects. R will assume that values of a function map to the ordering of
the arguments. If your values do not map to ordering of the arguments
you are required to specify the argument name and the value. To view the
ordering of objects hit \texttt{TAB} after entering the function name or
consult the function documentation by prefixing the function name with
\texttt{?} and hitting \texttt{ENTER}.
\end{rmdtip}

At this point our downloaded file is stored temporarily on disk and can be accessed and decompressed to our target directory using \texttt{temp} as the value for the argument \texttt{zipfile} from the \texttt{unzip()} function. I've assigned our target directory path to \texttt{target\_dir} and used it as the value for the argument \texttt{exdir} to prepare us for the next tweak on our approach.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Assign our target directory to \textasciigrave{}target\_dir\textasciigrave{}}
\NormalTok{target\_dir }\OtherTok{\textless{}{-}} \StringTok{"../data/original/"}
\CommentTok{\# Decompress .zip file and extract to our target directory}
\FunctionTok{unzip}\NormalTok{(}\AttributeTok{zipfile =}\NormalTok{ temp, }\AttributeTok{exdir =}\NormalTok{ target\_dir)}
\end{Highlighting}
\end{Shaded}

Our directory structure now looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{└──}\NormalTok{ switchboard}
        \ExtensionTok{├──}\NormalTok{ README}
        \ExtensionTok{├──}\NormalTok{ discourse}
        \ExtensionTok{├──}\NormalTok{ disfluency}
        \ExtensionTok{├──}\NormalTok{ tagged}
        \ExtensionTok{├──}\NormalTok{ timed{-}transcript}
        \ExtensionTok{└──}\NormalTok{ transcript}
\end{Highlighting}
\end{Shaded}

The second issue I raised concerns the fact that running this code as part of our project will repeat the download each time. Since we would like to be good citizens and avoid unnecessary traffic on the web it would be nice if our code checked to see if we already have the data on disk and if it exists, then skip the download, if not then download it.

To achieve this we need to introduce two new functions \texttt{if()} and \texttt{dir.exists()}. \texttt{dir.exists()} takes a path to a directory as an argument and returns the logical value, \texttt{TRUE}, if that directory exists, and \texttt{FALSE} if it does not. \texttt{if()} evaluates logical statements and processes subsequent code based on the logical value it is passed as an argument. Let's look at a toy example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num }\OtherTok{\textless{}{-}} \DecValTok{1}
\ControlFlowTok{if}\NormalTok{ (num }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) \{}
    \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is 1"}\NormalTok{)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
    \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is not 1"}\NormalTok{)}
\NormalTok{\}}
\CommentTok{\#\textgreater{} 1 is 1}
\end{Highlighting}
\end{Shaded}

I assigned \texttt{num} to the value \texttt{1} and created a logical evaluation \texttt{num\ ==} whose result is passed as the argument to \texttt{if()}. If the statement returns \texttt{TRUE} then the code withing the first set of curly braces \texttt{\{...\}} is run. If \texttt{num\ ==\ 1} is false, like in the code below, the code withing the braces following the \texttt{else} will be run.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num }\OtherTok{\textless{}{-}} \DecValTok{2}
\ControlFlowTok{if}\NormalTok{ (num }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) \{}
    \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is 1"}\NormalTok{)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
    \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is not 1"}\NormalTok{)}
\NormalTok{\}}
\CommentTok{\#\textgreater{} 2 is not 1}
\end{Highlighting}
\end{Shaded}

The function \texttt{if()} is one of various functions that are called \textbf{control statements}. Theses functions provide a lot of power to make dynamic choices as code is run.

Before we get back to our key objective to avoid downloading resources that we already have on disk, let me introduce another strategy to making code more powerful and ultimately more efficient and as well as more legible --the \textbf{custom function}. Custom functions are functions that the user writes to create a set of procedures that can be run in similar contexts. I've created a custom function named \texttt{eval\_num()} below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eval\_num }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(num) \{}
    \ControlFlowTok{if}\NormalTok{ (num }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) \{}
        \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is 1"}\NormalTok{)}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
        \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is not 1"}\NormalTok{)}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Let's take a closer look at what's going on here. The function \texttt{function()} creates a function in which the user decides what arguments are necessary for the code to perform its task. In this case the only necessary argument is the object to store a numeric value to be evaluated. I've called it \texttt{num} because it reflects the name of the object in our toy example, but there is nothing special about this name. It's only important that the object names be consistently used. I've included our previous code (except for the hard-coded assignment of \texttt{num}) inside the curly braces and assigned the entire code chunk to \texttt{eval\_num}.

We can now use the function \texttt{eval\_num()} to perform the task of evaluating whether a value of \texttt{num} is or is not equal to \texttt{1}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{eval\_num}\NormalTok{(}\AttributeTok{num =} \DecValTok{1}\NormalTok{)}
\CommentTok{\#\textgreater{} 1 is 1}
\FunctionTok{eval\_num}\NormalTok{(}\AttributeTok{num =} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} 2 is not 1}
\FunctionTok{eval\_num}\NormalTok{(}\AttributeTok{num =} \DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{} 3 is not 1}
\end{Highlighting}
\end{Shaded}

I've put these coding strategies together with our previous code in a custom function I named \texttt{get\_zip\_data()}. There is a lot going on here. Take a look first and see if you can follow the logic involved given what you now know.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_zip\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(url, target\_dir) \{}
    \CommentTok{\# Function: to download and decompress a .zip file to a target directory}

    \CommentTok{\# Check to see if the data already exists if data does not exist, download/}
    \CommentTok{\# decompress}
    \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{dir.exists}\NormalTok{(target\_dir)) \{}
        \FunctionTok{cat}\NormalTok{(}\StringTok{"Creating target data directory }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)  }\CommentTok{\# print status message}
        \FunctionTok{dir.create}\NormalTok{(}\AttributeTok{path =}\NormalTok{ target\_dir, }\AttributeTok{recursive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{)  }\CommentTok{\# create target data directory}
        \FunctionTok{cat}\NormalTok{(}\StringTok{"Downloading data... }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)  }\CommentTok{\# print status message}
\NormalTok{        temp }\OtherTok{\textless{}{-}} \FunctionTok{tempfile}\NormalTok{()  }\CommentTok{\# create a temporary space for the file to be written to}
        \FunctionTok{download.file}\NormalTok{(}\AttributeTok{url =}\NormalTok{ url, }\AttributeTok{destfile =}\NormalTok{ temp)  }\CommentTok{\# download the data to the temp file}
        \FunctionTok{unzip}\NormalTok{(}\AttributeTok{zipfile =}\NormalTok{ temp, }\AttributeTok{exdir =}\NormalTok{ target\_dir, }\AttributeTok{junkpaths =} \ConstantTok{TRUE}\NormalTok{)  }\CommentTok{\# decompress the temp file in the target directory}
        \FunctionTok{cat}\NormalTok{(}\StringTok{"Data downloaded! }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)  }\CommentTok{\# print status message}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
        \CommentTok{\# if data exists, don\textquotesingle{}t download it again}
        \FunctionTok{cat}\NormalTok{(}\StringTok{"Data already exists }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)  }\CommentTok{\# print status message}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

OK. You should have recognized the general steps in this function: the argument \texttt{url} and \texttt{target\_dir} specify where to get the data and where to write the decompressed files, the \texttt{if()} statement evaluates whether the data already exists, if not (\texttt{!dir.exists(target\_dir)}) then the data is downloaded and decompressed, if it does exist (\texttt{else}) then it is not downloaded.

\begin{rmdtip}
The prefixed \texttt{!} in the logical expression
\texttt{dir.exists(target\_dir)} returns the opposite logical value.
This is needed in this case so when the target directory exists, the
expression will return \texttt{FALSE}, not \texttt{TRUE}, and therefore
not proceed in downloading the resource.
\end{rmdtip}

There are a couple key tweaks I've added that provide some additional functionality. For one I've included the function \texttt{dir.create()} to create the target directory where the data will be written. I've also added an additional argument to the \texttt{unzip()} function, \texttt{junkpaths\ =\ TRUE}. Together these additions allow the user to create an arbitrary directory path where the files, and only the files, will be extracted to on our disk. This will discard the containing directory of the \texttt{.zip} file which can be helpful when we want to add multiple \texttt{.zip} files to the same target directory.

A practical scenario where this applies is when we want to download data from a corpus that is contained in multiple \texttt{.zip} files but still maintain these files in a single primary data directory. Take for example the \href{http://www.linguistics.ucsb.edu/research/santa-barbara-corpus}{Santa Barbara Corpus}. This corpus resource includes a series of interviews in which there is one \texttt{.zip} file, \texttt{SBCorpus.zip} which contains the \href{http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip}{transcribed interviews} and another \texttt{.zip} file, \texttt{metadata.zip} which organizes the \href{http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip}{meta-data} associated with each speaker. Applying our initial strategy to download and decompress the data will lead to the following directory structure:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{├──}\NormalTok{ SBCorpus}
    \ExtensionTok{│  }\NormalTok{ ├── TRN}
    \ExtensionTok{│  }\NormalTok{ └── \_\_MACOSX}
    \ExtensionTok{│  }\NormalTok{     └── TRN}
    \ExtensionTok{└──}\NormalTok{ metadata}
        \ExtensionTok{└──}\NormalTok{ \_\_MACOSX}
\end{Highlighting}
\end{Shaded}

By applying our new custom function \texttt{get\_zip\_data()} to the transcriptions and then the meta-data we can better organize the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Download corpus transcriptions}
\FunctionTok{get\_zip\_data}\NormalTok{(}\AttributeTok{url =} \StringTok{"http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip"}\NormalTok{,}
    \AttributeTok{target\_dir =} \StringTok{"../data/original/sbc/transcriptions/"}\NormalTok{)}

\CommentTok{\# Download corpus meta{-}data}
\FunctionTok{get\_zip\_data}\NormalTok{(}\AttributeTok{url =} \StringTok{"http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip"}\NormalTok{,}
    \AttributeTok{target\_dir =} \StringTok{"../data/original/sbc/meta{-}data/"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{└──}\NormalTok{ sbc}
        \ExtensionTok{├──}\NormalTok{ meta{-}data}
        \ExtensionTok{└──}\NormalTok{ transcriptions}
\end{Highlighting}
\end{Shaded}

If we add data from other sources we can keep them logical separate and allow our data collection to scale without creating unnecessary complexity. Let's add the Switchboard Corpus sample using our \texttt{get\_zip\_data()} function to see this in action.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Download corpus}
\FunctionTok{get\_zip\_data}\NormalTok{(}\AttributeTok{url =} \StringTok{"https://raw.githubusercontent.com/nltk/nltk\_data/gh{-}pages/packages/corpora/switchboard.zip"}\NormalTok{,}
    \AttributeTok{target\_dir =} \StringTok{"../data/original/scs/"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{├──}\NormalTok{ sbc}
    \ExtensionTok{│}\NormalTok{   ├── meta{-}data}
    \ExtensionTok{│}\NormalTok{   └── transcriptions}
    \ExtensionTok{└──}\NormalTok{ scs}
        \ExtensionTok{├──}\NormalTok{ README}
        \ExtensionTok{├──}\NormalTok{ discourse}
        \ExtensionTok{├──}\NormalTok{ disfluency}
        \ExtensionTok{├──}\NormalTok{ tagged}
        \ExtensionTok{├──}\NormalTok{ timed{-}transcript}
        \ExtensionTok{└──}\NormalTok{ transcript}
\end{Highlighting}
\end{Shaded}

At this point we have what we need to continue to the next step in our data analysis project. But before we go, we should do some housekeeping to document and organize this process to make our work reproducible. We will take advantage of the \texttt{project-template} directory structure, seen below.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{├──}\NormalTok{ README.md}
\ExtensionTok{├──}\NormalTok{ \_pipeline.R}
\ExtensionTok{├──}\NormalTok{ analysis}
\ExtensionTok{│}\NormalTok{   ├── 1\_acquire\_data.Rmd}
\ExtensionTok{│}\NormalTok{   ├── 2\_curate\_dataset.Rmd}
\ExtensionTok{│}\NormalTok{   ├── 3\_transform\_dataset.Rmd}
\ExtensionTok{│}\NormalTok{   ├── 4\_analyze\_dataset.Rmd}
\ExtensionTok{│}\NormalTok{   ├── 5\_generate\_article.Rmd}
\ExtensionTok{│}\NormalTok{   ├── \_session{-}info.Rmd}
\ExtensionTok{│}\NormalTok{   ├── \_site.yml}
\ExtensionTok{│}\NormalTok{   ├── index.Rmd}
\ExtensionTok{│}\NormalTok{   └── references.bib}
\ExtensionTok{├──}\NormalTok{ data}
\ExtensionTok{│}\NormalTok{   ├── derived}
\ExtensionTok{│}\NormalTok{   └── original}
\ExtensionTok{│}\NormalTok{       ├── sbc}
\ExtensionTok{│}\NormalTok{       └── scs}
\ExtensionTok{├──}\NormalTok{ functions}
\ExtensionTok{└──}\NormalTok{ output}
    \ExtensionTok{├──}\NormalTok{ figures}
    \ExtensionTok{└──}\NormalTok{ results}
\end{Highlighting}
\end{Shaded}

First it is good practice to separate custom functions from our processing scripts. We can create a file in our \texttt{functions/} directory named \texttt{acquire\_functions.R} and add our custom function \texttt{get\_zip\_data()} there.

\begin{rmdtip}
Note that that the \texttt{acquire\_functions.R} file is an R script,
not an Rmarkdown document. Therefore code chunks that are used in
\texttt{.Rmd} files are not used, only the R code itself.
\end{rmdtip}

We then use the \texttt{source()} function to read that function into our current script to make it available to use as needed. It is good practice to source your functions in the SETUP section of your script.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load custom functions for this project}
\FunctionTok{source}\NormalTok{(}\AttributeTok{file =} \StringTok{"../functions/acquire\_functions.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this section, to sum up, we've covered how to access, download, and organize data contained in .zip files; the most common format for language data found on repositories and individual sites. This included an introduction to a few key R programming concepts and strategies including using functions, writing custom functions, and controlling program flow with control statements. Our approach was to gather data while also keeping in mind the reproducibility of the code. To this end I introduced programming strategies for avoiding unnecessary web traffic (downloads), scalable directory creation, and data documentation.

\begin{rmdnote}
The custom function \texttt{get\_zip\_data()} works with \texttt{.zip}
files. There are many other compressed file formats (e.g.~\texttt{.gz},
\texttt{.tar}, \texttt{.tgz}), however. In the R package \texttt{tadr}
that accompanies this coursebook, a modified version of the
\texttt{get\_zip\_data()} function, \texttt{get\_compressed\_data()},
extends the same logic to deal with a wider range of compressed file
formats, including \texttt{.zip} files.

Explore this function's documentation
(\texttt{?tadr::get\_compressed\_data()}) and/ or view the code
(\texttt{tadr::get\_compressed\_data}) to better understand this
function.
\end{rmdnote}

\hypertarget{apis}{%
\subsection{APIs}\label{apis}}

A convenient alternative method for acquiring data in R is through package interfaces to web services. These interfaces are built using R code to make connections with resources on the web through \textbf{Application Programming Interfaces} (APIs). Websites such as Project Gutenberg, Twitter, Facebook, and many others provide APIs to allow access to their data under certain conditions, some more limiting for data collection than others. Programmers (like you!) in the R community take up the task of wrapping calls to an API with R code to make accessing that data from R possible. For example, \href{https://CRAN.R-project.org/package=gutenbergr}{gutenbergr} provides access to Project Gutenberg, \href{https://CRAN.R-project.org/package=rtweet}{rtweet} to Twitter, and \href{https://CRAN.R-project.org/package=Rfacebook}{Rfacebook} to Facebook.\footnote{See Section \ref{sources} for a list of some other API packages.}

\hypertarget{open-access}{%
\subsubsection{Open access}\label{open-access}}

Using R package interfaces, however, often requires some more knowledge about R objects and functions. Let's take a look at how to access data from Project Gutenberg through the \texttt{gutenbergr} package. Along the way we will touch upon various functions and concepts that are key to working with the R data types vectors and data frames including filtering and writing tabular data to disk in plain-text format.

To get started let's install and/ or load the \texttt{gutenbergr} package. If a package is not part of the R base library, we cannot assume that the user will have the package in their library. The standard approach for installing and then loading a package is by using the \texttt{install.packages()} function and then calling \texttt{library()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"gutenbergr"}\NormalTok{)  }\CommentTok{\# install \textasciigrave{}gutenbergr\textasciigrave{} package}
\FunctionTok{library}\NormalTok{(gutenbergr)  }\CommentTok{\# load the \textasciigrave{}gutenbergr\textasciigrave{} package}
\end{Highlighting}
\end{Shaded}

This approach works just fine, but luck has it that there is an R package for installing and loading packages! The \href{https://CRAN.R-project.org/package=pacman}{pacman} package includes a set of functions for managing packages. A very useful one is \texttt{p\_load()} which will look for a package on a system, load it if it is found, and install and then load it if it is not found. This helps potentially avoid using unnecessary bandwidth to install packages that may already exist on a user's system. But, to use \texttt{pacman} we need to include the code to install and load it with the functions \texttt{install.packages()} and \texttt{library()}. I've included some code that will mimic the behavior of \texttt{p\_load()} for installing \texttt{pacman} itself, but as you can see it is not elegant, luckily it's only used once as we add it to the SETUP section of our master file, \texttt{\_pipeline.R}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load \textasciigrave{}pacman\textasciigrave{}. If not installed, install then load.}
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(}\StringTok{"pacman"}\NormalTok{, }\AttributeTok{character.only =} \ConstantTok{TRUE}\NormalTok{)) \{}
    \FunctionTok{install.packages}\NormalTok{(}\StringTok{"pacman"}\NormalTok{)}
    \FunctionTok{library}\NormalTok{(}\StringTok{"pacman"}\NormalTok{, }\AttributeTok{character.only =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now that we have \texttt{pacman} installed and loaded into our R session, let's use the \texttt{p\_load()} function to make sure to install/ load the two packages we will need for the upcoming tasks. If you are following along with the \texttt{project\_template}, add this code within the SETUP section of the \texttt{1\_acquire\_data.Rmd} file.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Script{-}specific options or packages}
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(tidyverse, gutenbergr)}
\end{Highlighting}
\end{Shaded}

\begin{rmdwarning}
Note that the arguments \texttt{tidyverse} and \texttt{gutenbergr} are
comma-separated but not quoted when using \texttt{p\_load()}. When using
\texttt{install.packages()} to install, package names need to be quoted
(character strings). \texttt{library()} can take quotes or no quotes,
but only one package at a time.
\end{rmdwarning}

Project Gutenberg provides access to thousands of texts in the public domain. The \texttt{gutenbergr} package contains a set of tables, or \textbf{data frames} in R speak, that index the meta-data for these texts broken down by text (\texttt{gutenberg\_metadata}), author (\texttt{gutenberg\_authors}), and subject (\texttt{gutenberg\_subjects}). I'll use the \texttt{glimpse()} function loaded in the \href{https://CRAN.R-project.org/package=tidyverse}{tidyverse} package \footnote{\texttt{tidyverse} is not a typical package. It is a set of packages: \texttt{ggplot2}, \texttt{dplyr}, \texttt{tidyr}, \texttt{readr}, \texttt{purrr}, and \texttt{tibble}. These packages are all installed/ loaded with \texttt{tidyverse} and form the backbone for the type of work you will typically do in most analyses.} to summarize the structure of these data frames.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(gutenberg\_metadata)  }\CommentTok{\# summarize text meta{-}data}
\CommentTok{\#\textgreater{} Rows: 51,997}
\CommentTok{\#\textgreater{} Columns: 8}
\CommentTok{\#\textgreater{} $ gutenberg\_id        \textless{}int\textgreater{} 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ title               \textless{}chr\textgreater{} NA, "The Declaration of Independence of the United\textasciitilde{}}
\CommentTok{\#\textgreater{} $ author              \textless{}chr\textgreater{} NA, "Jefferson, Thomas", "United States", "Kennedy\textasciitilde{}}
\CommentTok{\#\textgreater{} $ gutenberg\_author\_id \textless{}int\textgreater{} NA, 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7,\textasciitilde{}}
\CommentTok{\#\textgreater{} $ language            \textless{}chr\textgreater{} "en", "en", "en", "en", "en", "en", "en", "en", "e\textasciitilde{}}
\CommentTok{\#\textgreater{} $ gutenberg\_bookshelf \textless{}chr\textgreater{} NA, "United States Law/American Revolutionary War/\textasciitilde{}}
\CommentTok{\#\textgreater{} $ rights              \textless{}chr\textgreater{} "Public domain in the USA.", "Public domain in the\textasciitilde{}}
\CommentTok{\#\textgreater{} $ has\_text            \textless{}lgl\textgreater{} TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR\textasciitilde{}}
\FunctionTok{glimpse}\NormalTok{(gutenberg\_authors)  }\CommentTok{\# summarize authors meta{-}data}
\CommentTok{\#\textgreater{} Rows: 16,236}
\CommentTok{\#\textgreater{} Columns: 7}
\CommentTok{\#\textgreater{} $ gutenberg\_author\_id \textless{}int\textgreater{} 1, 3, 4, 5, 7, 8, 9, 10, 12, 14, 16, 17, 18, 20, 2\textasciitilde{}}
\CommentTok{\#\textgreater{} $ author              \textless{}chr\textgreater{} "United States", "Lincoln, Abraham", "Henry, Patri\textasciitilde{}}
\CommentTok{\#\textgreater{} $ alias               \textless{}chr\textgreater{} NA, NA, NA, NA, "Dodgson, Charles Lutwidge", NA, "\textasciitilde{}}
\CommentTok{\#\textgreater{} $ birthdate           \textless{}int\textgreater{} NA, 1809, 1736, NA, 1832, NA, 1819, 1860, 1805, 17\textasciitilde{}}
\CommentTok{\#\textgreater{} $ deathdate           \textless{}int\textgreater{} NA, 1865, 1799, NA, 1898, NA, 1891, 1937, 1844, 18\textasciitilde{}}
\CommentTok{\#\textgreater{} $ wikipedia           \textless{}chr\textgreater{} NA, "http://en.wikipedia.org/wiki/Abraham\_Lincoln"\textasciitilde{}}
\CommentTok{\#\textgreater{} $ aliases             \textless{}chr\textgreater{} NA, "United States President (1861{-}1865)/Lincoln, \textasciitilde{}}
\FunctionTok{glimpse}\NormalTok{(gutenberg\_subjects)  }\CommentTok{\# summarize subjects meta{-}data}
\CommentTok{\#\textgreater{} Rows: 140,173}
\CommentTok{\#\textgreater{} Columns: 3}
\CommentTok{\#\textgreater{} $ gutenberg\_id \textless{}int\textgreater{} 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ subject\_type \textless{}chr\textgreater{} "lcc", "lcsh", "lcsh", "lcc", "lcc", "lcsh", "lcsh", "lcc\textasciitilde{}}
\CommentTok{\#\textgreater{} $ subject      \textless{}chr\textgreater{} "E201", "United States. Declaration of Independence", "Un\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

\begin{rmdtip}
The \texttt{gutenberg\_metadata}, \texttt{gutenberg\_authors}, and \texttt{gutenberg\_subjects} are periodically updated. To check to see when each data frame was last updated run:

\texttt{attr(gutenberg\_metadata,\ "date\_updated")}
\end{rmdtip}

To download the text itself we use the \texttt{gutenberg\_download()} function which takes one required argument, \texttt{gutenberg\_id}. The \texttt{gutenberg\_download()} function is what is known as `vectorized', that is, it can take a single value or multiple values for the argument \texttt{gutenberg\_id}. Vectorization refers to the process of applying a function to each of the elements stored in a \textbf{vector} --a primary object type in R. A vector is a grouping of values of one of various types including character (\texttt{chr}), integer (\texttt{int}), double (\texttt{dbl}), and logical (\texttt{lgl}) and a data frame is a grouping of vectors. The \texttt{gutenberg\_download()} function takes an integer vector which can be manually added or selected from the \texttt{gutenberg\_metadata} or \texttt{gutenberg\_subjects} data frames using the \texttt{\$} operator (e.g.~\texttt{gutenberg\_metadata\$gutenberg\_id}).

Let's first add them manually here as a toy example by generating a vector of integers from 1 to 5 assigned to the variable name \texttt{ids}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ids }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}  \CommentTok{\# integer vector of values 1 to 5}
\NormalTok{ids}
\CommentTok{\#\textgreater{} [1] 1 2 3 4 5}
\end{Highlighting}
\end{Shaded}

To download the works from Project Gutenberg corresponding to the \texttt{gutenberg\_id}s 1 to 5, we pass the \texttt{ids} object to the \texttt{gutenberg\_download()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{works\_sample }\OtherTok{\textless{}{-}} \FunctionTok{gutenberg\_download}\NormalTok{(}\AttributeTok{gutenberg\_id =}\NormalTok{ ids)  }\CommentTok{\# download works with \textasciigrave{}gutenberg\_id\textasciigrave{} 1{-}5}
\FunctionTok{glimpse}\NormalTok{(works\_sample)  }\CommentTok{\# summarize \textasciigrave{}works\textasciigrave{} dataset}
\CommentTok{\#\textgreater{} Rows: 2,959}
\CommentTok{\#\textgreater{} Columns: 2}
\CommentTok{\#\textgreater{} $ gutenberg\_id \textless{}int\textgreater{} 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ text         \textless{}chr\textgreater{} "December, 1971  [Etext \#1]", "", "", "The Project Gutenb\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Two attributes are returned: \texttt{gutenberg\_id} and \texttt{text}. The \texttt{text} column contains values for each line of text (delimited by a carriage return) for each of the 5 works we downloaded. There are many more attributes available from the Project Gutenberg API that can be accessed by passing a character vector of the attribute names to the argument \texttt{meta\_fields}. The column names of the \texttt{gutenberg\_metadata} data frame contains the available attributes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(gutenberg\_metadata)  }\CommentTok{\# print the column names of the \textasciigrave{}gutenberg\_metadata\textasciigrave{} data frame}
\CommentTok{\#\textgreater{} [1] "gutenberg\_id"        "title"               "author"             }
\CommentTok{\#\textgreater{} [4] "gutenberg\_author\_id" "language"            "gutenberg\_bookshelf"}
\CommentTok{\#\textgreater{} [7] "rights"              "has\_text"}
\end{Highlighting}
\end{Shaded}

Let's augment our previous download with the title and author of each of the works. To create a character vector we use the \texttt{c()} function, then, quote and delimit the individual elements of the vector with a comma.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# download works with \textasciigrave{}gutenberg\_id\textasciigrave{} 1{-}5 including \textasciigrave{}title\textasciigrave{} and \textasciigrave{}author\textasciigrave{} as}
\CommentTok{\# attributes}
\NormalTok{works\_sample }\OtherTok{\textless{}{-}} \FunctionTok{gutenberg\_download}\NormalTok{(}\AttributeTok{gutenberg\_id =}\NormalTok{ ids, }\AttributeTok{meta\_fields =} \FunctionTok{c}\NormalTok{(}\StringTok{"title"}\NormalTok{, }\StringTok{"author"}\NormalTok{))}
\FunctionTok{glimpse}\NormalTok{(works\_sample)  }\CommentTok{\# summarize dataset}
\CommentTok{\#\textgreater{} Rows: 2,959}
\CommentTok{\#\textgreater{} Columns: 4}
\CommentTok{\#\textgreater{} $ gutenberg\_id \textless{}int\textgreater{} 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ text         \textless{}chr\textgreater{} "December, 1971  [Etext \#1]", "", "", "The Project Gutenb\textasciitilde{}}
\CommentTok{\#\textgreater{} $ title        \textless{}chr\textgreater{} "The Declaration of Independence of the United States of \textasciitilde{}}
\CommentTok{\#\textgreater{} $ author       \textless{}chr\textgreater{} "Jefferson, Thomas", "Jefferson, Thomas", "Jefferson, Tho\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Now, in a more practical scenario we would like to select the values of \texttt{gutenberg\_id} by some principled query such as works from a specific author, language, or subject. To do this we first query either the \texttt{gutenberg\_metadata} data frame or the \texttt{gutenberg\_subjects} data frame. Let's say we want to download a random sample of 10 works from English Literature (Library of Congress Classification, ``PR''). Using the \texttt{dplyr::filter()} function (\texttt{dplyr} is part of the \texttt{tidyverse} package set) we first extract all the Gutenberg ids from \texttt{gutenberg\_subjects} where \texttt{subject\_type\ ==\ "lcc"} and \texttt{subject\ ==\ "PR"} assigning the result to \texttt{ids}.\footnote{See \href{https://www.loc.gov/catdir/cpso/lcco/}{Library of Congress Classification} documentation for a complete list of subject codes.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# filter for only English literature}
\NormalTok{ids }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(gutenberg\_subjects, subject\_type }\SpecialCharTok{==} \StringTok{"lcc"}\NormalTok{, subject }\SpecialCharTok{==} \StringTok{"PR"}\NormalTok{)}
\FunctionTok{glimpse}\NormalTok{(ids)}
\CommentTok{\#\textgreater{} Rows: 7,100}
\CommentTok{\#\textgreater{} Columns: 3}
\CommentTok{\#\textgreater{} $ gutenberg\_id \textless{}int\textgreater{} 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43, 46, 58, 60, 8\textasciitilde{}}
\CommentTok{\#\textgreater{} $ subject\_type \textless{}chr\textgreater{} "lcc", "lcc", "lcc", "lcc", "lcc", "lcc", "lcc", "lcc", "\textasciitilde{}}
\CommentTok{\#\textgreater{} $ subject      \textless{}chr\textgreater{} "PR", "PR", "PR", "PR", "PR", "PR", "PR", "PR", "PR", "PR\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

\begin{rmdwarning}
The operators \texttt{=} and \texttt{==} are not equivalents.
\texttt{==} is used for logical evaluation and \texttt{=} is an
alternate notation for variable assignment (\texttt{\textless{}-}).
\end{rmdwarning}

The \texttt{gutenberg\_subjects} data frame does not contain information as to whether a \texttt{gutenberg\_id} is associated with a plain-text version. To limit our query to only those English Literature works with text, we filter the \texttt{gutenberg\_metadata} data frame by the ids we have selected in \texttt{ids} and the attribute \texttt{has\_text} in the \texttt{gutenberg\_metadata} data frame.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter for only those works that have text}
\NormalTok{ids\_has\_text }\OtherTok{\textless{}{-}} 
  \FunctionTok{filter}\NormalTok{(gutenberg\_metadata, }
\NormalTok{         gutenberg\_id }\SpecialCharTok{\%in\%}\NormalTok{ ids}\SpecialCharTok{$}\NormalTok{gutenberg\_id, }
\NormalTok{         has\_text }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{glimpse}\NormalTok{(ids\_has\_text)}
\CommentTok{\#\textgreater{} Rows: 6,724}
\CommentTok{\#\textgreater{} Columns: 8}
\CommentTok{\#\textgreater{} $ gutenberg\_id        \textless{}int\textgreater{} 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43, 46, 58\textasciitilde{}}
\CommentTok{\#\textgreater{} $ title               \textless{}chr\textgreater{} "Alice\textquotesingle{}s Adventures in Wonderland", "Through the L\textasciitilde{}}
\CommentTok{\#\textgreater{} $ author              \textless{}chr\textgreater{} "Carroll, Lewis", "Carroll, Lewis", "Carroll, Lewi\textasciitilde{}}
\CommentTok{\#\textgreater{} $ gutenberg\_author\_id \textless{}int\textgreater{} 7, 7, 7, 10, 17, 17, 23, 30, 30, 35, 35, 37, 17, 4\textasciitilde{}}
\CommentTok{\#\textgreater{} $ language            \textless{}chr\textgreater{} "en", "en", "en", "en", "en", "en", "en", "en", "e\textasciitilde{}}
\CommentTok{\#\textgreater{} $ gutenberg\_bookshelf \textless{}chr\textgreater{} "Children\textquotesingle{}s Literature", "Children\textquotesingle{}s Literature/Be\textasciitilde{}}
\CommentTok{\#\textgreater{} $ rights              \textless{}chr\textgreater{} "Public domain in the USA.", "Public domain in the\textasciitilde{}}
\CommentTok{\#\textgreater{} $ has\_text            \textless{}lgl\textgreater{} TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

\begin{rmdtip}
A couple R programming notes on the code phrase \texttt{gutenberg\_id\ \%in\%\ ids\$gutenberg\_id}. First, the \texttt{\$} symbol in \texttt{ids\$gutenberg\_id} is the programmatic way to target a particular column in an R data frame. In this example we select the \texttt{ids} data frame and the column \texttt{gutenberg\_id}, which is a integer vector. The \texttt{gutenberg\_id} variable that precedes the \texttt{\%in\%} operator does not need an explicit reference to a data frame because the primary argument of the \texttt{filter()} function is this data frame (\texttt{gutenberg\_metadata}). Second, the \texttt{\%in\%} operator logically evaluates whether the vector elements in \texttt{gutenberg\_metadata\$gutenberg\_ids} are also found in the vector \texttt{ids\$gutenberg\_id} returning \texttt{TRUE} and \texttt{FALSE} accordingly. This effectively filters those ids which are not in both vectors.
\end{rmdtip}

As we can see the number of works with text is fewer than the number of works listed, 7100 versus 6724. Now we can safely do our random selection of 10 works, with the function \texttt{slice\_sample()} and be confident that the ids we select will contain text when we take the next step by downloading the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)  }\CommentTok{\# make the sampling reproducible}
\NormalTok{ids\_sample }\OtherTok{\textless{}{-}} \FunctionTok{slice\_sample}\NormalTok{(ids\_has\_text, }\AttributeTok{n =} \DecValTok{10}\NormalTok{)  }\CommentTok{\# sample 10 works}
\FunctionTok{glimpse}\NormalTok{(ids\_sample)  }\CommentTok{\# summarize the dataset}
\CommentTok{\#\textgreater{} Rows: 10}
\CommentTok{\#\textgreater{} Columns: 8}
\CommentTok{\#\textgreater{} $ gutenberg\_id        \textless{}int\textgreater{} 10564, 10784, 9316, 1540, 24450, 13821, 7595, 3818\textasciitilde{}}
\CommentTok{\#\textgreater{} $ title               \textless{}chr\textgreater{} "Fairy Gold\textbackslash{}nShip\textquotesingle{}s Company, Part 4.", "Sentence D\textasciitilde{}}
\CommentTok{\#\textgreater{} $ author              \textless{}chr\textgreater{} "Jacobs, W. W. (William Wymark)", "Jacobs, W. W. (\textasciitilde{}}
\CommentTok{\#\textgreater{} $ gutenberg\_author\_id \textless{}int\textgreater{} 1865, 1865, 2364, 65, 999, 2685, 761, 1317, 3564, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ language            \textless{}chr\textgreater{} "en", "en", "en", "en", "en", "en", "en", "en", "e\textasciitilde{}}
\CommentTok{\#\textgreater{} $ gutenberg\_bookshelf \textless{}chr\textgreater{} NA, NA, NA, NA, "Adventure", "Fantasy", NA, NA, NA\textasciitilde{}}
\CommentTok{\#\textgreater{} $ rights              \textless{}chr\textgreater{} "Public domain in the USA.", "Public domain in the\textasciitilde{}}
\CommentTok{\#\textgreater{} $ has\_text            \textless{}lgl\textgreater{} TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{works\_pr }\OtherTok{\textless{}{-}} \FunctionTok{gutenberg\_download}\NormalTok{(}\AttributeTok{gutenberg\_id =}\NormalTok{ ids\_sample}\SpecialCharTok{$}\NormalTok{gutenberg\_id, }\AttributeTok{meta\_fields =} \FunctionTok{c}\NormalTok{(}\StringTok{"author"}\NormalTok{,}
    \StringTok{"title"}\NormalTok{))}
\FunctionTok{glimpse}\NormalTok{(works\_pr)  }\CommentTok{\# summarize the dataset}
\CommentTok{\#\textgreater{} Rows: 47,513}
\CommentTok{\#\textgreater{} Columns: 4}
\CommentTok{\#\textgreater{} $ gutenberg\_id \textless{}int\textgreater{} 1540, 1540, 1540, 1540, 1540, 1540, 1540, 1540, 1540, 154\textasciitilde{}}
\CommentTok{\#\textgreater{} $ text         \textless{}chr\textgreater{} "cover ", "", "", "", "", "THE TEMPEST", "", "by William \textasciitilde{}}
\CommentTok{\#\textgreater{} $ author       \textless{}chr\textgreater{} "Shakespeare, William", "Shakespeare, William", "Shakespe\textasciitilde{}}
\CommentTok{\#\textgreater{} $ title        \textless{}chr\textgreater{} "The Tempest", "The Tempest", "The Tempest", "The Tempest\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

At this point we have data and could move on to processing this dataset in preparation for analysis. However, we are aiming for a reproducible workflow and this code does not conform to our principle of modularity: each subsequent step in our analysis will depend on running this code first. Furthermore, running this code as it is creates issues with bandwidth, as in our previous examples from direct downloads. To address modularity we will write the dataset to disk in \textbf{plain-text format}. In this way each subsequent step in our analysis can access the dataset locally. To address bandwidth concerns, we will devise a method for checking to see if the dataset is already downloaded and skip the download, if possible, to avoid accessing the Project Gutenberg server unnecessarily.

To write our data frame to disk we will export it into a standard plain-text format for two-dimensional datasets: a CSV file (comma-separated value). The CSV structure for this dataset will look like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{works\_pr }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{head}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{format\_csv}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{cat}\NormalTok{()}
\CommentTok{\#\textgreater{} gutenberg\_id,text,author,title}
\CommentTok{\#\textgreater{} 1540,cover ,"Shakespeare, William",The Tempest}
\CommentTok{\#\textgreater{} 1540,,"Shakespeare, William",The Tempest}
\CommentTok{\#\textgreater{} 1540,,"Shakespeare, William",The Tempest}
\CommentTok{\#\textgreater{} 1540,,"Shakespeare, William",The Tempest}
\CommentTok{\#\textgreater{} 1540,,"Shakespeare, William",The Tempest}
\CommentTok{\#\textgreater{} 1540,THE TEMPEST,"Shakespeare, William",The Tempest}
\end{Highlighting}
\end{Shaded}

The first line contains the names of the columns and subsequent lines the observations. Data points that contain commas themselves (e.g.~``Shaw, Bernard'') are quoted to avoid misinterpreting these commas a deliminators in our data. To write this dataset to disk we will use the \texttt{reader::write\_csv()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(works\_pr, }\AttributeTok{file =} \StringTok{"../data/original/gutenberg\_works\_pr.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To avoid downloading dataset that already resides on disk, let's implement a similar strategy to the one used for direct downloads (\texttt{get\_zip\_data()}). I've incorporated the code for sampling and downloading data for a particular subject from Project Gutenberg with a control statement to check if the dataset file already exists into a function I named \texttt{get\_gutenberg\_subject()}. Take a look at this function below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_gutenberg\_subject }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(subject, target\_file, }\AttributeTok{sample\_size =} \DecValTok{10}\NormalTok{) \{}
  \CommentTok{\# Function: to download texts from Project Gutenberg with }
  \CommentTok{\# a specific LCC subject and write the data to disk.}
  
\NormalTok{  pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(tidyverse, gutenbergr) }\CommentTok{\# install/load necessary packages}
  
  \CommentTok{\# Check to see if the data already exists}
  \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(target\_file)) \{ }\CommentTok{\# if data does not exist, download and write}
\NormalTok{    target\_dir }\OtherTok{\textless{}{-}} \FunctionTok{dirname}\NormalTok{(target\_file) }\CommentTok{\# generate target directory for the .csv file}
    \FunctionTok{dir.create}\NormalTok{(}\AttributeTok{path =}\NormalTok{ target\_dir, }\AttributeTok{recursive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# create target data directory}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Downloading data... }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
    \CommentTok{\# Select all records with a particular LCC subject}
\NormalTok{    ids }\OtherTok{\textless{}{-}} 
      \FunctionTok{filter}\NormalTok{(gutenberg\_subjects, }
\NormalTok{             subject\_type }\SpecialCharTok{==} \StringTok{"lcc"}\NormalTok{, subject }\SpecialCharTok{==}\NormalTok{ subject) }\CommentTok{\# select subject}
    \CommentTok{\# Select only those records with plain text available}
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# make the sampling reproducible}
\NormalTok{    ids\_sample }\OtherTok{\textless{}{-}} 
      \FunctionTok{filter}\NormalTok{(gutenberg\_metadata, }
\NormalTok{             gutenberg\_id }\SpecialCharTok{\%in\%}\NormalTok{ ids}\SpecialCharTok{$}\NormalTok{gutenberg\_id, }\CommentTok{\# select ids in both data frames }
\NormalTok{             has\_text }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# select those ids that have text}
      \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =}\NormalTok{ sample\_size) }\CommentTok{\# sample N works }
    \CommentTok{\# Download sample with associated \textasciigrave{}author\textasciigrave{} and \textasciigrave{}title\textasciigrave{} metadata}
\NormalTok{    works\_sample }\OtherTok{\textless{}{-}} 
      \FunctionTok{gutenberg\_download}\NormalTok{(}\AttributeTok{gutenberg\_id =}\NormalTok{ ids\_sample}\SpecialCharTok{$}\NormalTok{gutenberg\_id, }
                         \AttributeTok{meta\_fields =} \FunctionTok{c}\NormalTok{(}\StringTok{"author"}\NormalTok{, }\StringTok{"title"}\NormalTok{))}
    \CommentTok{\# Write the dataset to disk in .csv format}
    \FunctionTok{write\_csv}\NormalTok{(works\_sample, }\AttributeTok{file =}\NormalTok{ target\_file)}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Data downloaded! }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{ }\CommentTok{\# if data exists, don\textquotesingle{}t download it again}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Data already exists }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Adding this function to our function script \texttt{functions/acquire\_functions.R}, we can now source this function in our \texttt{analysis/1\_acquire\_data.Rmd} script to download multiple subjects and store them in on disk in their own file.

Let's download American Literature now (LCC code ``PQ'').

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Download Project Gutenberg text for subject \textquotesingle{}PQ\textquotesingle{} (American Literature) and}
\CommentTok{\# then write this dataset to disk in .csv format}
\FunctionTok{get\_gutenberg\_subject}\NormalTok{(}\AttributeTok{subject =} \StringTok{"PQ"}\NormalTok{, }\AttributeTok{target\_file =} \StringTok{"../data/original/gutenberg/works\_pq.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Applying this function to both the English and American Literature datasets, our data directory structure now looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{├──}\NormalTok{ gutenberg}
    \ExtensionTok{│}\NormalTok{   ├── works\_pq.csv}
    \ExtensionTok{│}\NormalTok{   └── works\_pr.csv}
    \ExtensionTok{├──}\NormalTok{ sbc}
    \ExtensionTok{│}\NormalTok{   ├── meta{-}data}
    \ExtensionTok{│}\NormalTok{   └── transcriptions}
    \ExtensionTok{└──}\NormalTok{ scs}
        \ExtensionTok{├──}\NormalTok{ README}
        \ExtensionTok{├──}\NormalTok{ discourse}
        \ExtensionTok{├──}\NormalTok{ disfluency}
        \ExtensionTok{├──}\NormalTok{ documentation}
        \ExtensionTok{├──}\NormalTok{ tagged}
        \ExtensionTok{├──}\NormalTok{ timed{-}transcript}
        \ExtensionTok{└──}\NormalTok{ transcript}
\end{Highlighting}
\end{Shaded}

\hypertarget{authentication}{%
\subsubsection{Authentication}\label{authentication}}

Some APIs and the R interfaces that provide access to them require authentication. This may either be through an interactive process that is mediated between R and the web service and/ or by visiting the developer website of the particular API. In either case, there is an extra step that is necessary to make the connect to the API to access the data.

Let's take a look at the popular micro-blogging platform Twitter. The rtweet package \citep{R-rtweet} provides access to tweets in various ways. To get started install and/or load the rtweet package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(rtweet)  }\CommentTok{\# install/load rtweet package}
\end{Highlighting}
\end{Shaded}

Now before a researcher can access data from Twitter with rtweet, \href{https://docs.ropensci.org/rtweet/articles/auth.html}{an authentication token must be setup and made accessible}. After following the steps for setting up an authentication token and saving it, that token can be accessed with the \texttt{auth\_as()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{auth\_as}\NormalTok{(twitter\_auth)  }\CommentTok{\# load the saved \textasciigrave{}twitter\_auth\textasciigrave{} token}
\end{Highlighting}
\end{Shaded}

Now that we the R session is authenticated, we can explore a popular method for querying the Twitter API which searchs tweets (\texttt{search\_tweets}) posted in the recent past (6-9 days).

Let's look at a typical query using the \texttt{search\_tweets()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rt\_latinx }\OtherTok{\textless{}{-}} 
  \FunctionTok{search\_tweets}\NormalTok{(}\AttributeTok{q =} \StringTok{"latinx"}\NormalTok{, }\CommentTok{\# query term}
                \AttributeTok{n =} \DecValTok{100}\NormalTok{, }\CommentTok{\# number of tweets desired}
                \AttributeTok{type =} \StringTok{"mixed"}\NormalTok{, }\CommentTok{\# a mix of \textasciigrave{}recent\textasciigrave{} and \textasciigrave{}popular\textasciigrave{} tweets}
                \AttributeTok{include\_rts =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# do not include RTs}
\end{Highlighting}
\end{Shaded}

Looking at the arguments in this function, we see I've specified the query term to be `latinx'. This is a single word query but if the query included multiple words, the spaces between would be interpreted as the logical \texttt{AND} (only match tweets with all the individual terms). If one would like to include multi-word expressions, the expressions should be enclosed by single quotes (i.e.~\texttt{q\ =\ "\textquotesingle{}spanish\ speakers\textquotesingle{}\ AND\ latinx"}). Another approach would be to include the logical \texttt{OR} (match tweets with either of the terms). Multi-word expressions can be included as in the previous case. Of note, hashtags are acceptable terms, so \texttt{q\ =\ "\#latinx"} would match tweets with this hashtag.

The number of results has been set at `100', but this is the default, so I could have left it out. But you can increase the number of desired tweets. There are rate limits which cap the number of tweets you can access in a given 15-minute time period.

Another argument of importance is the \texttt{type} argument. This argument has three possible attributes \texttt{popular}, \texttt{recent}, and \texttt{mixed}. When the \texttt{popular} attribute he Twitter API will tend to return fewer tweets than specified by \texttt{n}. With \texttt{recent} or \texttt{mixed} you will most likely get the \texttt{n} you specified (note that \texttt{mixed} is a mix of \texttt{popular} and \texttt{recent}).

A final argument to note is the \texttt{include\_rts} whose attribute is logical. If \texttt{FALSE} no retweets will be included in the results. This is often what a language researcher will want.

Now, once the \texttt{search\_tweets} query has been run, there a a large number of variables that are included in the resulting data frame. Here's an overview of the names of the variables and the vector types for each variable.

\begin{table}

\caption{\label{tab:ad-rtweet-variables-table}Variables and variable types returned from Twitter API via rtweet's `search_tweets()` function.}
\centering
\begin{tabular}[t]{ll}
\toprule
created\_at & character\\
id & double\\
id\_str & character\\
full\_text & character\\
truncated & logical\\
\addlinespace
display\_text\_range & double\\
entities & list\\
metadata & list\\
source & character\\
in\_reply\_to\_status\_id & double\\
\addlinespace
in\_reply\_to\_status\_id\_str & character\\
in\_reply\_to\_user\_id & double\\
in\_reply\_to\_user\_id\_str & character\\
in\_reply\_to\_screen\_name & character\\
geo & logical\\
\addlinespace
coordinates & list\\
place & list\\
contributors & logical\\
is\_quote\_status & logical\\
retweet\_count & integer\\
\addlinespace
favorite\_count & integer\\
favorited & logical\\
retweeted & logical\\
lang & character\\
possibly\_sensitive & logical\\
\addlinespace
quoted\_status\_id & double\\
quoted\_status\_id\_str & character\\
quoted\_status & list\\
text & character\\
favorited\_by & logical\\
\addlinespace
display\_text\_width & logical\\
retweeted\_status & logical\\
quoted\_status\_permalink & logical\\
query & logical\\
possibly\_sensitive\_appealable & logical\\
\bottomrule
\end{tabular}
\end{table}

The \href{https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets}{Twitter API documentation for the standard Search Tweets call}, which is what \texttt{search\_tweets()} interfaces with has quite a few variables (35 to be exact). For many purposes it is not necessary to keep all the variables. Furthermore, since we will want to write a plain-text file to disk as part of our project, we will need to either convert or eliminate any of the variables that are marked as type \texttt{list}. The most common variable to convert is the \texttt{coordinates} variable, as it will contain the geolocation codes for those Twitter users' tweets captured in the query that have geolocation enabled on their device. It is of note, however, that using \texttt{search\_tweets()} without specifying that only tweets with geocodes should be captured (\texttt{geocode\ =}) will tend to return very few, if any, tweets with geolocation information as the majority of Twitter users do not have geolocation enabled.

Let's assume that we want to keep all the variables that are not of type \texttt{list}. One option is to use \texttt{select()} and name each variable we want to keep. On the other hand we can use a combination of \texttt{select()} and negated \texttt{!where()} to select all the variables that are not lists (\texttt{is\_list}). Let's do the later approach.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rt\_latinx\_subset }\OtherTok{\textless{}{-}} 
\NormalTok{  rt\_latinx }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{where}\NormalTok{(is\_list))  }\CommentTok{\# select all variables that are NOT lists}

\NormalTok{rt\_latinx\_subset }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# subsetted dataset}
  \FunctionTok{glimpse}\NormalTok{() }\CommentTok{\# overview}
\CommentTok{\#\textgreater{} Rows: 100}
\CommentTok{\#\textgreater{} Columns: 30}
\CommentTok{\#\textgreater{} $ created\_at                    \textless{}chr\textgreater{} "Sun Sep 26 17:38:06 +0000 2021", "Sun S\textasciitilde{}}
\CommentTok{\#\textgreater{} $ id                            \textless{}dbl\textgreater{} 1.44e+18, 1.44e+18, 1.44e+18, 1.44e+18, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ id\_str                        \textless{}chr\textgreater{} "1442181701967302659", "1442196629801488\textasciitilde{}}
\CommentTok{\#\textgreater{} $ full\_text                     \textless{}chr\textgreater{} "If we call it Latinx Mass they can\textquotesingle{}t ca\textasciitilde{}}
\CommentTok{\#\textgreater{} $ truncated                     \textless{}lgl\textgreater{} FALSE, FALSE, FALSE, FALSE, FALSE, FALSE\textasciitilde{}}
\CommentTok{\#\textgreater{} $ display\_text\_range            \textless{}dbl\textgreater{} 57, 177, 166, 23, 261, 153, 202, 211, 57\textasciitilde{}}
\CommentTok{\#\textgreater{} $ source                        \textless{}chr\textgreater{} "\textless{}a href=\textbackslash{}"https://mobile.twitter.com\textbackslash{}" \textasciitilde{}}
\CommentTok{\#\textgreater{} $ in\_reply\_to\_status\_id         \textless{}dbl\textgreater{} NA, NA, NA, 1.44e+18, NA, NA, NA, NA, 1.\textasciitilde{}}
\CommentTok{\#\textgreater{} $ in\_reply\_to\_status\_id\_str     \textless{}chr\textgreater{} NA, NA, NA, "1437436224042635269", NA, N\textasciitilde{}}
\CommentTok{\#\textgreater{} $ in\_reply\_to\_user\_id           \textless{}dbl\textgreater{} NA, NA, NA, 4.26e+08, NA, NA, NA, NA, 2.\textasciitilde{}}
\CommentTok{\#\textgreater{} $ in\_reply\_to\_user\_id\_str       \textless{}chr\textgreater{} NA, NA, NA, "426159377", NA, NA, NA, NA,\textasciitilde{}}
\CommentTok{\#\textgreater{} $ in\_reply\_to\_screen\_name       \textless{}chr\textgreater{} NA, NA, NA, "MorganStanley", NA, NA, NA,\textasciitilde{}}
\CommentTok{\#\textgreater{} $ geo                           \textless{}lgl\textgreater{} NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ contributors                  \textless{}lgl\textgreater{} NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ is\_quote\_status               \textless{}lgl\textgreater{} FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,\textasciitilde{}}
\CommentTok{\#\textgreater{} $ retweet\_count                 \textless{}int\textgreater{} 351, 124, 62, 0, 0, 0, 0, 0, 0, 0, 0, 1,\textasciitilde{}}
\CommentTok{\#\textgreater{} $ favorite\_count                \textless{}int\textgreater{} 3902, 898, 280, 0, 0, 0, 0, 0, 0, 7, 0, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ favorited                     \textless{}lgl\textgreater{} FALSE, FALSE, FALSE, FALSE, FALSE, FALSE\textasciitilde{}}
\CommentTok{\#\textgreater{} $ retweeted                     \textless{}lgl\textgreater{} FALSE, FALSE, FALSE, FALSE, FALSE, FALSE\textasciitilde{}}
\CommentTok{\#\textgreater{} $ lang                          \textless{}chr\textgreater{} "en", "en", "es", "en", "en", "en", "en"\textasciitilde{}}
\CommentTok{\#\textgreater{} $ possibly\_sensitive            \textless{}lgl\textgreater{} NA, FALSE, FALSE, FALSE, FALSE, FALSE, F\textasciitilde{}}
\CommentTok{\#\textgreater{} $ quoted\_status\_id              \textless{}dbl\textgreater{} NA, NA, NA, NA, 1.44e+18, NA, NA, NA, NA\textasciitilde{}}
\CommentTok{\#\textgreater{} $ quoted\_status\_id\_str          \textless{}chr\textgreater{} NA, NA, NA, NA, "1442475408058830856", N\textasciitilde{}}
\CommentTok{\#\textgreater{} $ text                          \textless{}chr\textgreater{} "If we call it Latinx Mass they can\textquotesingle{}t ca\textasciitilde{}}
\CommentTok{\#\textgreater{} $ favorited\_by                  \textless{}lgl\textgreater{} NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ display\_text\_width            \textless{}lgl\textgreater{} NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ retweeted\_status              \textless{}lgl\textgreater{} NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ quoted\_status\_permalink       \textless{}lgl\textgreater{} NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ query                         \textless{}lgl\textgreater{} NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ possibly\_sensitive\_appealable \textless{}lgl\textgreater{} NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Now we have the 30 variables which can be written to disk as a plain-text file. Let's go ahead a do this, but wrap it in a function that does all the work we've just laid out in one function. In addition we will check to see if the same query has been run, and skip running the query if the dataset is on disk.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{write\_search\_tweets }\OtherTok{\textless{}{-}} 
  \ControlFlowTok{function}\NormalTok{(query, path, }\AttributeTok{n =} \DecValTok{100}\NormalTok{, }\AttributeTok{type =} \StringTok{"mixed"}\NormalTok{, }\AttributeTok{include\_rts =} \ConstantTok{FALSE}\NormalTok{) \{}
    \CommentTok{\# Function}
    \CommentTok{\# Conduct a Twitter search query and write the results to a csv file}
    
    \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(path)) \{ }\CommentTok{\# check to see if the file already exists}
      \FunctionTok{cat}\NormalTok{(}\StringTok{"File does not exist }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# message}
      
      \FunctionTok{library}\NormalTok{(rtweet) }\CommentTok{\# to use Twitter API}
      \FunctionTok{library}\NormalTok{(tidyverse) }\CommentTok{\# to manipulate data}
      
      \FunctionTok{auth\_get}\NormalTok{() }\CommentTok{\# get authentication token}
      
\NormalTok{      results }\OtherTok{\textless{}{-}} \CommentTok{\# query results}
        \FunctionTok{search\_tweets}\NormalTok{(}\AttributeTok{q =}\NormalTok{ query, }\CommentTok{\# query term}
                      \AttributeTok{n =}\NormalTok{ n, }\CommentTok{\# number of tweets desired (default 100)}
                      \AttributeTok{type =}\NormalTok{ type, }\CommentTok{\# type of query}
                      \AttributeTok{include\_rts =}\NormalTok{ include\_rts) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# to include RTs}
        \FunctionTok{select}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{where}\NormalTok{(is\_list))  }\CommentTok{\# remove list variables}
      
      \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{dir.exists}\NormalTok{(}\FunctionTok{dirname}\NormalTok{(path))) \{ }\CommentTok{\# isolate directory and check if exists}
        \FunctionTok{cat}\NormalTok{(}\StringTok{"Creating directory }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# message}
        
        \FunctionTok{dir.create}\NormalTok{(}\AttributeTok{path =} \FunctionTok{dirname}\NormalTok{(path), }\CommentTok{\# isolate and create directory (remove file name)}
                   \AttributeTok{recursive =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# create embedded directories if necessary}
                   \AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# do not report warnings}
\NormalTok{      \}}
      
      \FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{x =}\NormalTok{ results, }\AttributeTok{file =}\NormalTok{ path) }\CommentTok{\# write results to csv file }
      \FunctionTok{cat}\NormalTok{(}\StringTok{"Twitter search results written to disk }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# message}
      
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
      \FunctionTok{cat}\NormalTok{(}\StringTok{"File already exists! }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# message}
\NormalTok{    \}}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

Let's run this function with the same query as above.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_search\_tweets}\NormalTok{(}\AttributeTok{query =} \StringTok{"latinx"}\NormalTok{, }\AttributeTok{path =} \StringTok{"../data/original/twitter/rt\_latinx.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And the appropriate directory structure and file have been written to disk.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/original/twitter/}
\ExtensionTok{└──}\NormalTok{ rt\_latinx.csv}
\end{Highlighting}
\end{Shaded}

In sum, this subsection provided an overview to acquiring data from web service APIs through R packages. We took at closer look at the \texttt{gutenbergr} package which provides programmatic access to works available on Project Gutenberg and the \texttt{rtweet} package which provides authenticated access to Twitter. Working with package interfaces requires more knowledge of R including loading/ installing packages, working with vectors and data frames, and exporting data from an R session. We touched on these programming concepts and also outlined a method to create a reproducible workflow.

\hypertarget{web-scraping}{%
\subsection{Web scraping}\label{web-scraping}}

There are many resources available through manula and direct downloads from repositories and individual sites and R package interfaces to web resources with APIs, but these resources are relatively limited to the amount of public-facing textual data recorded on the web. In the case that you want to acquire data from webpages, R can be used to access the web programmatically through a process known as web scraping. The complexity of web scrapes can vary but in general it requires more advanced knowledge of R as well as the structure of the language of the web: HTML (Hypertext Markup Language).

\hypertarget{a-toy-example}{%
\subsubsection{A toy example}\label{a-toy-example}}

HTML is a cousin of XML (eXtensible Markup Language) and as such organizes web documents in a hierarchical format that is read by your browser as you navigate the web. Take for example the toy webpage I created as a demonstration in Figure \ref{fig:ad-example-webpage}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/06-acquire-data/example-webpage} 

}

\caption{Example web page.}\label{fig:ad-example-webpage}
\end{figure}

The file accessed by my browser to render this webpage is \texttt{test.html} and in plain-text format looks like this:

\begin{verbatim}
<html>
  <head>
    <title>My website</title>
  </head>
  <body>
    <div class="intro">
      <p>Welcome!</p>
      <p>This is my first website. </p>
    </div>
    <table>
      <tr>
        <td>Contact me:</td>
        <td>
          <a href="mailto:francojc@wfu.edu">francojc@wfu.edu</a>
        </td>
      </tr>
    </table>
    <div class="conc">
      <p>Good-bye!</p>
    </div>
  </body>
</html>
\end{verbatim}

Each element in this file is delineated by an opening and closing tag, \texttt{\textless{}head\textgreater{}\textless{}/head\textgreater{}}. Tags are nested within other tags to create the structural hierarchy. Tags can take class and id labels to distinguish them from other tags and often contain other attributes that dictate how the tag is to behave when rendered visually by a browser. For example, there are two \texttt{\textless{}div\textgreater{}} tags in our toy example: one has the label \texttt{class\ =\ "intro"} and the other \texttt{class\ =\ "conc"}. \texttt{\textless{}div\textgreater{}} tags are often used to separate sections of a webpage that may require special visual formatting. The \texttt{\textless{}a\textgreater{}} tag, on the other hand, creates a web link. As part of this tag's function, it requires the attribute \texttt{href=} and a web protocol --in this case it is a link to an email address \texttt{mailto:francojc@wfu.edu}. More often than not, however, the \texttt{href=} contains a URL (Uniform Resource Locator). A working example might look like this: \texttt{\textless{}a\ href="https://francojc.github.io/"\textgreater{}My\ homepage\textless{}/a\textgreater{}}.

The aim of a web scrape is to download the HTML file, parse the document structure, and extract the elements containing the relevant information we wish to capture. Let's attempt to extract some information from our toy example. To do this we will need the \href{https://CRAN.R-project.org/package=rvest}{rvest}\citep{R-rvest} package. First, install/load the package, then, read and parse the HTML from the character vector named \texttt{web\_file} assigning the result to \texttt{html}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(rvest)  }\CommentTok{\# install/ load \textasciigrave{}rvest\textasciigrave{}}

\NormalTok{html }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(web\_file)  }\CommentTok{\# read raw html and parse to xml}
\NormalTok{html}
\CommentTok{\#\textgreater{} \{html\_document\}}
\CommentTok{\#\textgreater{} \textless{}html\textgreater{}}
\CommentTok{\#\textgreater{} [1] \textless{}head\textgreater{}\textbackslash{}n\textless{}meta http{-}equiv="Content{-}Type" content="text/html; charset=UTF{-}8 ...}
\CommentTok{\#\textgreater{} [2] \textless{}body\textgreater{}\textbackslash{}n    \textless{}div class="intro"\textgreater{}\textbackslash{}n      \textless{}p\textgreater{}Welcome!\textless{}/p\textgreater{}\textbackslash{}n      \textless{}p\textgreater{}This is  ...}
\end{Highlighting}
\end{Shaded}

\texttt{read\_html()} parses the raw HTML into an object of class \texttt{xml\_document}. The summary output above shows that tags the HTML structure have been parsed into `elements'. The tag elements can be accessed by using the \texttt{html\_elements()} function by specifying the tag to isolate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"div"}\NormalTok{)}
\CommentTok{\#\textgreater{} \{xml\_nodeset (2)\}}
\CommentTok{\#\textgreater{} [1] \textless{}div class="intro"\textgreater{}\textbackslash{}n      \textless{}p\textgreater{}Welcome!\textless{}/p\textgreater{}\textbackslash{}n      \textless{}p\textgreater{}This is my first web ...}
\CommentTok{\#\textgreater{} [2] \textless{}div class="conc"\textgreater{}\textbackslash{}n      \textless{}p\textgreater{}Good{-}bye!\textless{}/p\textgreater{}\textbackslash{}n    \textless{}/div\textgreater{}}
\end{Highlighting}
\end{Shaded}

Notice that \texttt{html\_elements("div")} has returned both \texttt{div} tags. To isolate one of tags by its class, we add the class name to the tag separating it with a \texttt{.}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"div.intro"}\NormalTok{)}
\CommentTok{\#\textgreater{} \{xml\_nodeset (1)\}}
\CommentTok{\#\textgreater{} [1] \textless{}div class="intro"\textgreater{}\textbackslash{}n      \textless{}p\textgreater{}Welcome!\textless{}/p\textgreater{}\textbackslash{}n      \textless{}p\textgreater{}This is my first web ...}
\end{Highlighting}
\end{Shaded}

Great. Now say we want to drill down and isolate the subordinate \texttt{\textless{}p\textgreater{}} nodes. We can add \texttt{p} to our node filter.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"div.intro p"}\NormalTok{)}
\CommentTok{\#\textgreater{} \{xml\_nodeset (2)\}}
\CommentTok{\#\textgreater{} [1] \textless{}p\textgreater{}Welcome!\textless{}/p\textgreater{}}
\CommentTok{\#\textgreater{} [2] \textless{}p\textgreater{}This is my first website. \textless{}/p\textgreater{}}
\end{Highlighting}
\end{Shaded}

To extract the text contained within a node we use the \texttt{html\_text()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"div.intro p"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{html\_text}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "Welcome!"                   "This is my first website. "}
\end{Highlighting}
\end{Shaded}

The result is a character vector with two elements corresponding to the text contained in each \texttt{\textless{}p\textgreater{}} tag. If you were paying close attention you might have noticed that the second element in our vector includes extra whitespace after the period. To trim leading and trailing whitespace from text we can add the \texttt{trim\ =\ TRUE} argument to \texttt{html\_text()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"div.intro p"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{html\_text}\NormalTok{(}\AttributeTok{trim =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "Welcome!"                  "This is my first website."}
\end{Highlighting}
\end{Shaded}

From here we would then work to organize the text into a format we want to store it in and write the results to disk. Let's leave writing data to disk for later in the chapter. For now keep our focus on working with \texttt{rvest} to acquire data from html documents working with a more practical example.

\hypertarget{a-practical-example}{%
\subsubsection{A practical example}\label{a-practical-example}}

With some basic understanding of HTML and how to use the \texttt{rvest} package, let's turn to a realistic example. Say we want to acquire lyrics from the online music website and database \href{https://www.last.fm/}{last.fm}. The first step in any web scrape is to investigate the site and page(s) we want to scrape to ascertain if there any licensing restrictions. Many, but not all websites, will include a plain text file \href{https://www.cloudflare.com/learning/bots/what-is-robots.txt/}{\texttt{robots.txt}} at the root of the main URL. This file is declares which webpages a `robot' (including web scraping scripts) can and cannot access. We can use the \texttt{robotstxt} package to find out which URLs are accessible \footnote{It is important to check the paths of sub-domains as some website allow access in some areas and not in others}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(robotstxt)  }\CommentTok{\# load/ install \textasciigrave{}robotstxt\textasciigrave{}}

\FunctionTok{paths\_allowed}\NormalTok{(}\AttributeTok{paths =} \StringTok{"https://www.last.fm/"}\NormalTok{)  }\CommentTok{\# check permissions}
\CommentTok{\#\textgreater{} [1] TRUE}
\end{Highlighting}
\end{Shaded}

The next step includes identifying the URL we want to target and exploring the structure of the HTML document. Take the following webpage I have identified, seen in Figure \ref{fig:ad-example-lyrics-page-lastfm}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/06-acquire-data/ad-lastfm-webpage-lyrics} 

}

\caption{Lyrics page from last.fm}\label{fig:ad-example-lyrics-page-lastfm}
\end{figure}

As in our toy example, first we want to feed the HTML web address to the \texttt{read\_html()} function to parse the tags into elements. We will then assign the result to \texttt{html}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read and parse html as an xml object}
\NormalTok{lyrics\_url }\OtherTok{\textless{}{-}} \StringTok{"https://www.last.fm/music/Radiohead/\_/Karma+Police/+lyrics"}
\NormalTok{html }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(lyrics\_url)  }\CommentTok{\# read raw html and parse to xml}
\NormalTok{html}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> {html_document}
#> <html lang="en" class="
#>         no-js
#>         playbar-masthead-release-shim
#>         youtube-provider-not-ready
#>     ">
#> [1] <head>\n<meta http-equiv="Content-Type" content="text/html; charset=UTF-8 ...
#> [2] <body>\n<div id="initial-tealium-data" data-require="tracking/tealium-uta ...
\end{verbatim}

At this point we have captured and parsed the raw HTML assigning it to the object named \texttt{html}. The next step is to identify the html elements that contain the information we want to extract from the page. To do this it is helpful to use a browser to inspect specific elements of the webpage. Your browser will be equipped with a command that you can enable by hovering your mouse over the element of the page you want to target and using a right click to select ``Inspect'' (Chrome) or ``Inspect Element'' (Safari, Brave). This will split your browser window vertical or horizontally showing you the raw HTML underlying the webpage.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/06-acquire-data/ad-lastfm-artist-inspect} 

}

\caption{Using the "Inspect Element" command to explore raw html.}\label{fig:ad-inspect-element-artist-lastfm}
\end{figure}

From Figure \ref{fig:ad-inspect-element-lyrics-lastfm} we see that the element we want to target is contained within an \texttt{\textless{}a\textgreater{}\textless{}/a\textgreater{}} tag. Now this tag is common and we don't want to extract every \texttt{a} so we use the class \texttt{header-new-crumb} to specify we only want the artist name. Using the convention described in our toy example, we can isolate the artist of the lyrics page.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{html\_element}\NormalTok{(}\StringTok{"a.header{-}new{-}crumb"}\NormalTok{)}
\CommentTok{\#\textgreater{} \{html\_node\}}
\CommentTok{\#\textgreater{} \textless{}a class="header{-}new{-}crumb" itemprop="url" href="/music/Radiohead"\textgreater{}}
\CommentTok{\#\textgreater{} [1] \textless{}span itemprop="name"\textgreater{}Radiohead\textless{}/span\textgreater{}}
\end{Highlighting}
\end{Shaded}

We can then extract the text with \texttt{html\_text()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{artist }\OtherTok{\textless{}{-}}\NormalTok{ html }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{html\_element}\NormalTok{(}\StringTok{"a.header{-}new{-}crumb"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{html\_text}\NormalTok{()}
\NormalTok{artist}
\CommentTok{\#\textgreater{} [1] "Radiohead"}
\end{Highlighting}
\end{Shaded}

Let's extract the song title in the same way.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{song }\OtherTok{\textless{}{-}}\NormalTok{ html }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{html\_element}\NormalTok{(}\StringTok{"h1.header{-}new{-}title"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{html\_text}\NormalTok{()}
\NormalTok{song}
\CommentTok{\#\textgreater{} [1] "Karma Police"}
\end{Highlighting}
\end{Shaded}

Now if we inspect the HTML of the lyrics page, we will notice that the lyrics are contained in \texttt{\textless{}p\textgreater{}\textless{}/p\textgreater{}} tags with the class \texttt{lyrics-paragraph}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/06-acquire-data/ad-lastfm-lyrics-inspect} 

}

\caption{Using the "Inspect Element" command to explore raw html.}\label{fig:ad-inspect-element-lyrics-lastfm}
\end{figure}

Since there are multiple elements that we want to extract, we will need to use the \texttt{html\_elements()} function instead of the \texttt{html\_element()} which only targets one element.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lyrics }\OtherTok{\textless{}{-}}\NormalTok{ html }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"p.lyrics{-}paragraph"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{html\_text}\NormalTok{()}
\NormalTok{lyrics}
\CommentTok{\#\textgreater{} [1] "Karma policeArrest this manHe talks in mathsHe buzzes like a fridgeHe\textquotesingle{}s like a detuned radio"      }
\CommentTok{\#\textgreater{} [2] "Karma policeArrest this girlHer Hitler hairdoIs making me feel illAnd we have crashed her party"   }
\CommentTok{\#\textgreater{} [3] "This is what you\textquotesingle{}ll getThis is what you\textquotesingle{}ll getThis is what you\textquotesingle{}ll getWhen you mess with us"        }
\CommentTok{\#\textgreater{} [4] "Karma policeI\textquotesingle{}ve given all I canIt\textquotesingle{}s not enoughI\textquotesingle{}ve given all I canBut we\textquotesingle{}re still on the payroll" }
\CommentTok{\#\textgreater{} [5] "This is what you\textquotesingle{}ll getThis is what you\textquotesingle{}ll getThis is what you\textquotesingle{}ll getWhen you mess with us"        }
\CommentTok{\#\textgreater{} [6] "For a minute thereI lost myself, I lost myselfPhew, for a minute thereI lost myself, I lost myself"}
\CommentTok{\#\textgreater{} [7] "For a minute thereI lost myself, I lost myselfPhew, for a minute thereI lost myself, I lost myself"}
\end{Highlighting}
\end{Shaded}

At this point, we have isolated and extracted the artist, song, and lyrics from the webpage. Each of these elements are stored in character vectors in our R session. To complete our task we need to write this data to disk as plain text. With an eye towards a tidy dataset, an ideal format to store the data is in a CSV file where each column corresponds to one of the elements from our scrape and each row an observation. A CSV file is a tabular format and so before we can write the data to disk let's coerce the data that we have into tabular format. We will use the \texttt{tibble()} function here to streamline our data frame creation. \footnote{\texttt{tibble} objects are \texttt{data.frame} objects with some added extra bells and whistles that we won't get into here.} Feeding each of the vectors \texttt{artist}, \texttt{song}, and \texttt{lyrics} as arguments to \texttt{tibble()} creates the tabular format we are looking for.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tibble}\NormalTok{(artist, song, lyrics) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{glimpse}\NormalTok{()}
\CommentTok{\#\textgreater{} Rows: 7}
\CommentTok{\#\textgreater{} Columns: 3}
\CommentTok{\#\textgreater{} $ artist \textless{}chr\textgreater{} "Radiohead", "Radiohead", "Radiohead", "Radiohead", "Radiohead"\textasciitilde{}}
\CommentTok{\#\textgreater{} $ song   \textless{}chr\textgreater{} "Karma Police", "Karma Police", "Karma Police", "Karma Police",\textasciitilde{}}
\CommentTok{\#\textgreater{} $ lyrics \textless{}chr\textgreater{} "Karma policeArrest this manHe talks in mathsHe buzzes like a f\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Notice that there are seven rows in this data frame, one corresponding to each paragraph in \texttt{lyrics}. R has a bias towards working with vectors of the same length. As such each of the other vectors (\texttt{artist}, and \texttt{song}) are replicated, or recycled, until they are the same length as the longest vector \texttt{lyrics}, which a length of seven.

For good documentation let's add our object \texttt{lyrics\_url} to the data frame, which contains the actual web link to this page, and assign the result to \texttt{song\_lyrics}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{song\_lyrics }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(artist, song, lyrics, lyrics\_url)}
\end{Highlighting}
\end{Shaded}

The final step is to write this data to disk. To do this we will use the \texttt{write\_csv()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{x =}\NormalTok{ song\_lyrics, }\AttributeTok{path =} \StringTok{"../data/original/lyrics.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{scaling-up}{%
\subsubsection{Scaling up}\label{scaling-up}}

At this point you may be think, `Great, I can download data from a single page, but what about downloading multiple pages?' Good question. That's really where the strength of a programming approach takes hold. Extracting information from multiple pages is not fundamentally different than working with a single page. However, it does require more sophisticated understanding of the web and R coding strategies, in particular \textbf{iteration}.

Before we get to iteration, let's first create a couple functions to make it possible to efficiently reuse the code we have developed so far:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the \texttt{get\_lyrics} function wraps the code for scraping a single lyrics webpage from last.fm.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_lyrics }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(lyrics\_url) \{}
    \CommentTok{\# Function: Scrape last.fm lyrics page for: artist, song, and lyrics from a}
    \CommentTok{\# provided content link.  Return as a tibble/data.frame}

    \FunctionTok{cat}\NormalTok{(}\StringTok{"Scraping song lyrics from:"}\NormalTok{, lyrics\_url, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}

\NormalTok{    pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(tidyverse, rvest)  }\CommentTok{\# install/ load package(s)}

\NormalTok{    url }\OtherTok{\textless{}{-}} \FunctionTok{url}\NormalTok{(lyrics\_url, }\StringTok{"rb"}\NormalTok{)  }\CommentTok{\# open url connection }
\NormalTok{    html }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(url)  }\CommentTok{\# read and parse html as an xml object}
    \FunctionTok{close}\NormalTok{(url)  }\CommentTok{\# close url connection}

\NormalTok{    artist }\OtherTok{\textless{}{-}}\NormalTok{ html }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{html\_element}\NormalTok{(}\StringTok{"a.header{-}new{-}crumb"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{html\_text}\NormalTok{()}

\NormalTok{    song }\OtherTok{\textless{}{-}}\NormalTok{ html }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{html\_element}\NormalTok{(}\StringTok{"h1.header{-}new{-}title"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{html\_text}\NormalTok{()}

\NormalTok{    lyrics }\OtherTok{\textless{}{-}}\NormalTok{ html }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"p.lyrics{-}paragraph"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{html\_text}\NormalTok{()}

    \FunctionTok{cat}\NormalTok{(}\StringTok{"...one moment "}\NormalTok{)}

    \FunctionTok{Sys.sleep}\NormalTok{(}\DecValTok{1}\NormalTok{)  }\CommentTok{\# sleep for 1 second to reduce server load}

\NormalTok{    song\_lyrics }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(artist, song, lyrics, lyrics\_url)}

    \FunctionTok{cat}\NormalTok{(}\StringTok{"... done! }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}

    \FunctionTok{return}\NormalTok{(song\_lyrics)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{rmdtip}
The \texttt{get\_lyrics} function includes all of the code developed previously, but also includes: (1) output messages (\texttt{cat()}), (2) a processing pause (\texttt{Sys.sleep()}), and (3) code to manage opening and closing web connections (\texttt{url()} and \texttt{close()}).

Points (1) and (2) will be useful when we iterate over this function to provide status messages and to reduce server load when processing multiple webpages from a web domain. (3) will be necessary to manage webpages that are non-existent. As we will see, we will generate url link to multiple song lyrics some of which will not be valid. To avoid errors that will stop the processing these steps have been incorporated here.
\end{rmdtip}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  the \texttt{write\_content} writes the webscraped data to our local machine, including functionality to create the necessary directory structure of the target file path we choose.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{write\_content }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(content, target\_file) \{}
    \CommentTok{\# Function: Write the tibble content to disk. Create the directory if it}
    \CommentTok{\# does not already exist.}

\NormalTok{    pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(tidyverse)  }\CommentTok{\# install/ load packages}

\NormalTok{    target\_dir }\OtherTok{\textless{}{-}} \FunctionTok{dirname}\NormalTok{(target\_file)  }\CommentTok{\# identify target file directory structure}
    \FunctionTok{dir.create}\NormalTok{(}\AttributeTok{path =}\NormalTok{ target\_dir, }\AttributeTok{recursive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{)  }\CommentTok{\# create directory}
    \FunctionTok{write\_csv}\NormalTok{(content, target\_file)  }\CommentTok{\# write csv file to target location}

    \FunctionTok{cat}\NormalTok{(}\StringTok{"Content written to disk!}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

With just these two functions, we can take a lyrics URL from last.fm and scrape and write the data to disk like this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lyrics\_url }\OtherTok{\textless{}{-}} \StringTok{"https://www.last.fm/music/Pixies/\_/Where+Is+My+Mind\%3F/+lyrics"}

\NormalTok{lyrics\_url }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{get\_lyrics}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{write\_content}\NormalTok{(}\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/lyrics.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/original/lastfm/}
\ExtensionTok{└──}\NormalTok{ lyrics.csv}
\end{Highlighting}
\end{Shaded}

Now we could manually search and copy URLs and run this function pipeline. This would be fine if we had just a few particular URLs that we wanted to scrape. But if we want to, say, scrape a set of lyrics grouped by genre. We would probably want a more programmatic approach. The good news is we can leverage our understanding of webscraping to scrape last.fm to harvest the information needed to create and store links to songs by genre. We can then pass these links to a pipeline, similar to the previous one, to scrape lyrics for many songs and store the results in files grouped by genre.

Last.fm provides a genres page where some of the top genres are listed and can be further explored.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/06-acquire-data/ad-lastfm-genres} 

}

\caption{Genre page on last.fm}\label{fig:ad-genre-page-lastfm}
\end{figure}

Diving into a a particular genre, `rock' for example, you will get a listing of the top tracks in that genre.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/06-acquire-data/ad-lastfm-genre-tracks-list} 

}

\caption{Tracks by genre list page on last.fm}\label{fig:ad-genre-tracks-list-lastfm}
\end{figure}

If we inspect the HTML elements for the track names in Figure \ref{fig:ad-genre-tracks-list-lastfm}, we can see that a relative URL is found for the track. In this case, I have `Smells Like Teen Spirit' by Nirvana highlighted in the inspector. If we follow this link to the track page and then to the lyrics for the track, you will notice that the relative URL on the track listings page has all the unique information. Only the web domain \texttt{https://www.last.fm} and the post-pended \texttt{/+lyrics} is missing.

So with this we can put together a function which gets the track listing for a last.fm genre, scrapes the relative URLs for each of the tracks, and creates a full absolute URL to the lyrics page.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_genre\_lyrics\_urls }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(last\_fm\_genre) \{}
  \CommentTok{\# Function: Scrapes a given last.fm genre title for top tracks in}
  \CommentTok{\# that genre and then creates links to the lyrics pages for these tracks}
  
  \FunctionTok{cat}\NormalTok{(}\StringTok{"Scraping top songs from:"}\NormalTok{, last\_fm\_genre, }\StringTok{"genre: }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
  
\NormalTok{  pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(tidyverse, rvest) }\CommentTok{\# install/ load packages}
  
  \CommentTok{\# create web url for the genre listing page}
\NormalTok{  genre\_listing\_url }\OtherTok{\textless{}{-}} 
    \FunctionTok{paste0}\NormalTok{(}\StringTok{"https://www.last.fm/tag/"}\NormalTok{, last\_fm\_genre, }\StringTok{"/tracks"}\NormalTok{) }
  
\NormalTok{  genre\_lyrics\_urls }\OtherTok{\textless{}{-}} 
    \FunctionTok{read\_html}\NormalTok{(genre\_listing\_url) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# read raw html and parse to xml}
    \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"td.chartlist{-}name a"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# isolate the track elements}
    \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"href"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract the href attribute}
    \FunctionTok{paste0}\NormalTok{(}\StringTok{"https://www.last.fm"}\NormalTok{, ., }\StringTok{"/+lyrics"}\NormalTok{) }\CommentTok{\# join the domain, relative artist path, and the post{-}pended /+lyrics to create an absolute URL}
  
  \FunctionTok{return}\NormalTok{(genre\_lyrics\_urls)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

With this function, all we need is to identify the verbatim way last.fm lists the genres. For Rock, it is \texttt{rock} but for Hip Hop, it is \texttt{hip+hop}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_genre\_lyrics\_urls}\NormalTok{(}\StringTok{"hip+hop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# get urls for top hip hop tracks}
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\CommentTok{\# only display 10 tracks}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Scraping top songs from: hip+hop genre:
#>  [1] "https://www.last.fm/music/Juzhin/_/Charlie+Conscience+(feat.+MMAIO)/+lyrics"
#>  [2] "https://www.last.fm/music/Juzhin/_/Railways/+lyrics"                        
#>  [3] "https://www.last.fm/music/Juzhin/_/Coming+Down/+lyrics"                     
#>  [4] "https://www.last.fm/music/Juzhin/_/Tupona/+lyrics"                          
#>  [5] "https://www.last.fm/music/Juzhin/_/Sakhalin/+lyrics"                        
#>  [6] "https://www.last.fm/music/Juzhin/_/3+Simple+Minutes/+lyrics"                
#>  [7] "https://www.last.fm/music/Juzhin/_/Lost+Sense/+lyrics"                      
#>  [8] "https://www.last.fm/music/Juzhin/_/Wonderful/+lyrics"                       
#>  [9] "https://www.last.fm/music/Gina+Moryson/_/Vanilla+Smoothy+(Live)/+lyrics"    
#> [10] "https://www.last.fm/music/Juzhin/_/Flunk-Down+(Juzhin+Remix)/+lyrics"
\end{verbatim}

So now we have a method to scrape URLs by genre and list them in a vector. Our approach, then, could be to pass these lyrics URLs to our existing pipeline which downloads the lyrics (\texttt{get\_lyrics()}) and then writes them to disk (\texttt{write\_content()}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note: will not run}
\FunctionTok{get\_genre\_lyrics\_urls}\NormalTok{(}\StringTok{"hip+hop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get lyrics urls for specific genre}
  \FunctionTok{get\_lyrics}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# scrape lyrics url}
  \FunctionTok{write\_content}\NormalTok{(}\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/hip\_hop.csv"}\NormalTok{) }\CommentTok{\# write to disk}
\end{Highlighting}
\end{Shaded}

This approach, however, has a couple problems. (1) our \texttt{get\_lyrics()} function only takes one URL at a time, but the result of \texttt{get\_genre\_lyrics\_urls()} will produce many URLs. We will be able to solve this with iteration using the \href{}{purrr} package, specifically the \texttt{map()} function which will iteratively map each URL output from \texttt{get\_genre\_lyrics\_urls()} to \texttt{get\_lyrics()} in turn. (2) the output from our iterative application of \texttt{get\_lyrics()} will produce a tibble for each URL, which then sets up a problem with writing the tibbles to disk with the \texttt{write\_content()} function. To avoid this we will want to combine the tibbles into one single tibble and then send it to be written to disk. The \texttt{bind\_rows()} function will do just this.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note: will run, but with occasional errors}
\FunctionTok{get\_genre\_lyrics\_urls}\NormalTok{(}\StringTok{"hip+hop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get lyrics urls for specific genre}
  \FunctionTok{map}\NormalTok{(get\_lyrics) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# scrape lyrics url}
  \FunctionTok{bind\_rows}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# combine tibbles into one}
  \FunctionTok{write\_content}\NormalTok{(}\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/hip\_hop.csv"}\NormalTok{) }\CommentTok{\# write to disk}
\end{Highlighting}
\end{Shaded}

This preceding pipeline conceptually will work. However, on my testing, it turns out that some of the URLs that are generated in the \texttt{get\_genre\_lyrics\_urls()} do not exist on the site. That is, the song is listed but no lyrics have been added to the song site. This will mean that when the URL is sent to the \texttt{get\_lyrics()} function, there will be an error when attempting to download and parse the page with \texttt{read\_html()} which will halt the entire process. To avoid this error, we can wrap the \texttt{get\_lyrics()} function in a function designed to attempt to download and parse the URL (\texttt{tryCatch()}), but if there is an error, it will skip it and move on to the next URL without stopping the processing. This approach is reflected in the \texttt{get\_lyrics\_catch()} function below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Wrap the \textasciigrave{}get\_lyrics()\textasciigrave{} function with \textasciigrave{}tryCatch()\textasciigrave{} to skip URLs that have no}
\CommentTok{\# lyrics}

\NormalTok{get\_lyrics\_catch }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(lyrics\_url) \{}
    \FunctionTok{tryCatch}\NormalTok{(}\FunctionTok{get\_lyrics}\NormalTok{(lyrics\_url), }\AttributeTok{error =} \ControlFlowTok{function}\NormalTok{(e) }\FunctionTok{return}\NormalTok{(}\ConstantTok{NULL}\NormalTok{))  }\CommentTok{\# no, URL, return(NULL)/ skip}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Updating the pipeline with the \texttt{get\_lyrics\_catch()} function would look like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note: will run, but we can do better}
\FunctionTok{get\_genre\_lyrics\_urls}\NormalTok{(}\StringTok{"hip+hop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get lyrics urls for specific genre}
  \FunctionTok{map}\NormalTok{(get\_lyrics\_catch) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# scrape lyrics url}
  \FunctionTok{bind\_rows}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# combine tibbles into one}
  \FunctionTok{write\_content}\NormalTok{(}\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/hip\_hop.csv"}\NormalTok{) }\CommentTok{\# write to disk}
\end{Highlighting}
\end{Shaded}

This will work, but as we have discussed before one of this goals we have we acquiring data for a reproducible research project is to make sure that we are developing efficient code that will not burden site's server we are scraping from. In this case, we would like to check to see if the data is already downloaded. If not, then the script should run. If so, then the script does not run. Of course this is a perfect use of a conditional statement. To make this a single function we can call, I've wrapped the functions we created for getting lyric URLs from last.fm, scraping the URLs, and writing the results to disk in the \texttt{download\_lastfm\_lyrics()} function below. I also added a line to add a \texttt{last\_fm\_genre} column to the combined tibble to store the name of the genre we scraped (i.e.~\texttt{mutate(genre\ =\ last\_fm\_genre)}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{download\_lastfm\_lyrics }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(last\_fm\_genre, target\_file) \{}
    \CommentTok{\# Function: get last.fm lyric urls by genre and write them to disk}

    \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(target\_file)) \{}

        \FunctionTok{cat}\NormalTok{(}\StringTok{"Downloading data.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}

        \FunctionTok{get\_genre\_lyrics\_urls}\NormalTok{(last\_fm\_genre) }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{map}\NormalTok{(get\_lyrics\_catch) }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{bind\_rows}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{mutate}\NormalTok{(}\AttributeTok{genre =}\NormalTok{ last\_fm\_genre) }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{write\_content}\NormalTok{(target\_file)}

\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
        \FunctionTok{cat}\NormalTok{(}\StringTok{"Data already downloaded!}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we can call this function on any genre on the last.fm site and download the top 50 song lyrics for that genre (provided they all have lyrics pages).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Scrape lyrics for \textquotesingle{}pop\textquotesingle{}}
\FunctionTok{download\_lastfm\_lyrics}\NormalTok{(}\AttributeTok{last\_fm\_genre =} \StringTok{"pop"}\NormalTok{, }\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/pop.csv"}\NormalTok{)}

\CommentTok{\# Scrape lyrics for \textquotesingle{}rock\textquotesingle{}}
\FunctionTok{download\_lastfm\_lyrics}\NormalTok{(}\AttributeTok{last\_fm\_genre =} \StringTok{"rock"}\NormalTok{, }\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/rock.csv"}\NormalTok{)}

\CommentTok{\# Scrape lyrics for \textquotesingle{}hip hop\textquotesingle{}}
\FunctionTok{download\_lastfm\_lyrics}\NormalTok{(}\AttributeTok{last\_fm\_genre =} \StringTok{"hip+hop"}\NormalTok{, }\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/hip\_hop.csv"}\NormalTok{)}

\CommentTok{\# Scrape lyrics for \textquotesingle{}metal\textquotesingle{}}
\FunctionTok{download\_lastfm\_lyrics}\NormalTok{(}\AttributeTok{last\_fm\_genre =} \StringTok{"metal"}\NormalTok{, }\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/metal.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we can see that our web scrape data is organized in a similar fashion to the other data we acquired in this chapter.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{├──}\NormalTok{ cedel2/}
    \ExtensionTok{│}\NormalTok{   └── texts.csv}
    \ExtensionTok{├──}\NormalTok{ gutenberg/}
    \ExtensionTok{│}\NormalTok{   ├── works\_pq.csv}
    \ExtensionTok{│}\NormalTok{   └── works\_pr.csv}
    \ExtensionTok{├──}\NormalTok{ lastfm/}
    \ExtensionTok{│}\NormalTok{   ├── country.csv}
    \ExtensionTok{│}\NormalTok{   ├── hip\_hop.csv}
    \ExtensionTok{│}\NormalTok{   ├── lyrics.csv}
    \ExtensionTok{│}\NormalTok{   ├── metal.csv}
    \ExtensionTok{│}\NormalTok{   ├── pop.csv}
    \ExtensionTok{│}\NormalTok{   └── rock.csv}
    \ExtensionTok{├──}\NormalTok{ sbc/}
    \ExtensionTok{│}\NormalTok{   ├── meta{-}data/}
    \ExtensionTok{│}\NormalTok{   └── transcriptions/}
    \ExtensionTok{├──}\NormalTok{ scs/}
    \ExtensionTok{│}\NormalTok{   ├── README}
    \ExtensionTok{│}\NormalTok{   ├── discourse}
    \ExtensionTok{│}\NormalTok{   ├── disfluency}
    \ExtensionTok{│}\NormalTok{   ├── documentation/}
    \ExtensionTok{│}\NormalTok{   ├── tagged}
    \ExtensionTok{│}\NormalTok{   ├── timed{-}transcript}
    \ExtensionTok{│}\NormalTok{   └── transcript}
    \ExtensionTok{└──}\NormalTok{ twitter/}
        \ExtensionTok{└──}\NormalTok{ rt\_latinx.csv}
\end{Highlighting}
\end{Shaded}

Again, it is important to add these custom functions to our \texttt{acquire\_functions.R} script in the \texttt{functions/} directory so we can access them in our scripts more efficiently and make our analysis steps more succinct and legible.

In this section we covered scraping language data from the web. The rvest package provides a host of functions for downloading and parsing HTML. We first looked at a toy example to get a basic understanding of how HTML works and then moved to applying this knowledge to a practical example. To maintain a reproducible workflow, the code developed in this example was grouped into task-oriented functions which were in turn joined and wrapped into a function that provided convenient access to our workflow and avoided unnecessary downloads (in the case the data already exists on disk).

Here we have built on previously introduced R coding concepts and demonstrated various others. Web scraping often requires more knowledge of and familiarity with R as well as other web technologies. Rest assured, however, practice will increase confidence in your abilities. I encourage you to practice on your own with other websites. You will encounter problems. Consult the R documentation in RStudio or online and lean on the R community on the web at sites such as \href{https://stackoverflow.com/}{Stack Overflow} inter alia.

\hypertarget{documentation-1}{%
\subsection{Documentation}\label{documentation-1}}

As part of the data acquisition process it is important include documentation that describes the data resource(s) that will serve as the base for a research project. For all resources the data should include as much information possible that outlines the sampling frame of the data \citep{Adel2020}. For a corpus sample acquired from a repository will often include documentation which will outline the sampling frame --this most likely will be the very information which leads a researcher to select this resource for the project at hand. It is important to include this documentation (HTML or PDF file) or reference to the documentation (article citation or link\footnote{Note that web links can change and it is often best to safeguard the documentation by downloading the HTML documentation page instead of linking}) within the reproducible project's directory structure.

In other cases where the data acquisition process is formulated and conducted by the researcher for the specific aims of the research (i.e.~API and web scraping approaches), the researcher should make an effort to document those aspects which are key for the study, but that may also be of interest to other researchers for similar research questions. This will may include language characteristics such as modality, register, genre, etc., speaker/ writer characteristics such as demographics, time period(s), context of the linguistic communication, etc. and process characteristics such as the source of the data, the process of acquisition, date of acquisition, etc. However, it is important to recognize that each language sample and the resource from which it is drawn is unique. As a general rule of thumb, a researcher should document the resource as if this were a resource \emph{they} were to encounter for the first time. To archive this information, it is standard practice to include a \texttt{README} file in the relevant directory where the data is stored.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{├──}\NormalTok{ cedel2/}
    \ExtensionTok{│  }\NormalTok{ ├── documentation/}
    \ExtensionTok{│  }\NormalTok{ └── texts.csv}
    \ExtensionTok{├──}\NormalTok{ gutenberg/}
    \ExtensionTok{│  }\NormalTok{ ├── README.md}
    \ExtensionTok{│  }\NormalTok{ ├── works\_pq.csv}
    \ExtensionTok{│  }\NormalTok{ └── works\_pr.csv}
    \ExtensionTok{├──}\NormalTok{ lastfm/}
    \ExtensionTok{│  }\NormalTok{ ├── README.md}
    \ExtensionTok{│  }\NormalTok{ ├── country.csv}
    \ExtensionTok{│  }\NormalTok{ ├── hip\_hop.csv}
    \ExtensionTok{│  }\NormalTok{ ├── lyrics.csv}
    \ExtensionTok{│  }\NormalTok{ ├── metal.csv}
    \ExtensionTok{│  }\NormalTok{ ├── pop.csv}
    \ExtensionTok{│  }\NormalTok{ └── rock.csv}
    \ExtensionTok{├──}\NormalTok{ sbc/}
    \ExtensionTok{│  }\NormalTok{ ├── meta{-}data/}
    \ExtensionTok{│  }\NormalTok{ └── transcriptions/}
    \ExtensionTok{├──}\NormalTok{ scs/}
    \ExtensionTok{│  }\NormalTok{ ├── README}
    \ExtensionTok{│  }\NormalTok{ ├── discourse}
    \ExtensionTok{│  }\NormalTok{ ├── disfluency}
    \ExtensionTok{│  }\NormalTok{ ├── documentation/}
    \ExtensionTok{│  }\NormalTok{ ├── tagged}
    \ExtensionTok{│  }\NormalTok{ ├── timed{-}transcript}
    \ExtensionTok{│  }\NormalTok{ └── transcript}
    \ExtensionTok{└──}\NormalTok{ twitter/}
        \ExtensionTok{├──}\NormalTok{ README.md}
        \ExtensionTok{└──}\NormalTok{ rt\_latinx.csv}
\end{Highlighting}
\end{Shaded}

For both existing corpora and data samples acquired by the researcher it is also important to signal if there are conditions and/ or licensing restrictions that one should heed when using and potentially sharing the data. In some cases existing corpus data come with restrictions on data sharing. These can be quite restrictive and ultimately require that the corpus data not be included in publically available reproducible project or data can only be shared in a derived format. If this the case, it is important to document the steps to legally acquire the data so that a researcher can acquire their own license and take full advantage of your reproducible project.

In the case of data from APIs or web scraping, there too may be stipulations on sharing data. A growing number of data sources apply one of \href{https://creativecommons.org/about/cclicenses/}{the available Creative Common Licenses}. Check the source of your data for more information and if you are a member of a research institution you will likely have a \href{https://zsr.wfu.edu/digital-scholarship/copyright/}{specialist} on \href{https://www.copyright.gov/fair-use/more-info.html}{Copyright and Fair Use}.

\hypertarget{summary-4}{%
\subsection*{Summary}\label{summary-4}}
\addcontentsline{toc}{subsection}{Summary}

In this chapter we have covered a lot of ground. On the surface we have discussed three methods for acquiring corpus data for use in text analysis. In the process we have delved into various aspects of the R programming language. Some key concepts include writing custom functions and working with those function in an iterative manner. We have also considered topics that are more general in nature and concern interacting with data found on the internet.

Each of these methods should be approached in a way that is transparent to the researcher and to would-be collaborators and the general research community. For this reason the documentation of the steps taken to acquire data are key both in the code and in human-facing documentation.

At this point you have both a bird's eye view of the data available on the web and strategies on how to access a great majority of it. It is now time to turn to the next step in our data analysis project: data curation. In the next posts I will cover how to wrangle your raw data into a tidy dataset. This will include working with and incorporating meta-data as well as augmenting a dataset with linguistic annotations.

\hypertarget{curate-data}{%
\section{Curate data}\label{curate-data}}

DRAFT

\begin{quote}
The hardest bit of information to extract is the first piece.

―--Robert Ferrigno
\end{quote}

\begin{rmdkey}
The essential questions for this chapter are:

\begin{itemize}
\tightlist
\item
  what are some of the formats that data can take?
\item
  what R programming strategies are used to read these formats into
  tabular, tidy dataset structures?
\item
  what is the importance of maintaining modularity between data and data
  processing in a reproducible research project?
\end{itemize}
\end{rmdkey}

In this chapter we will now look at the next step in a text analysis project: data curation. That is, the process of converting the original data we acquire to a tidy dataset. As Acquired data can come in a wide variety of formats that depend largely on the richness of the metadata that is included, but also can reflect individual preferences. In this chapter we will consider three general types of formats: (1) unstructured data, (2) structured data, and (3) semi-structured data. Regardless of the file type and the structure of the data, it will be necessary to consider how to curate a dataset that such that the structure reflects the basic the unit of analysis that we wish to investigate (see \protect\hyperlink{framing-research.htmlux5cux23research-question}{Chapter 4, section 4.2}. The resulting dataset will be the base from which we will work to further transform the dataset such that it aligns with the analysis method(s) that we will implement. And as in previous implementation steps, we will discuss the important role of documentation.

\hypertarget{unstructured}{%
\subsection{Unstructured}\label{unstructured}}

The bulk of text that is available in the wild is of the unstructured variety. Unstructured data is data that has not been organized to make the information contained within explicit. Explicit information that is included with data is called metadata. Metadata can be linguistic or non-linguistic in nature. So for unstructured data there is little to no metadata directly associated with the data. This information needs to be added or derived for the purposes of the research, either through manual inspection or (semi-)automatic processes. For now, however, our job is just to get the unstructured data into a structured format with a minimal set of metadata that we can derive from the resource.

As an example of an unstructured source of text data, let's take a look at the \href{https://www.statmt.org/europarl/}{Europarle Parallel Corpus}, as introduced in \protect\hyperlink{understanding-data}{Chapter 2 ``Understanding data''}. This data contains parallel texts (source and translated documents) from the European Parliamentary proceedings for some 21 European languages. Here we will focus in on the translation from Spanish to English (Spanish-English).

\hypertarget{orientation}{%
\subsubsection{Orientation}\label{orientation}}

With the data downloaded into the \texttt{data/original/europarle/} directory we see that there are two files. One corresponding to the source language (Spanish) and one for the target language (English).

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/original/europarle/}
\ExtensionTok{├──}\NormalTok{ europarl{-}v7.es{-}en.en}
\ExtensionTok{└──}\NormalTok{ europarl{-}v7.es{-}en.es}
\end{Highlighting}
\end{Shaded}

Looking at the first 10 lines of the first file, we can see that this is running text.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Resumption of the session}
\NormalTok{I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.}
\NormalTok{Although, as you will have seen, the dreaded \textquotesingle{}millennium bug\textquotesingle{} failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.}
\NormalTok{You have requested a debate on this subject in the course of the next few days, during this part{-}session.}
\NormalTok{In the meantime, I should like to observe a minute\textquotesingle{} s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.}
\NormalTok{Please rise, then, for this minute\textquotesingle{} s silence.}
\NormalTok{(The House rose and observed a minute\textquotesingle{} s silence)}
\NormalTok{Madam President, on a point of order.}
\NormalTok{You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.}
\NormalTok{One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.}
\end{Highlighting}
\end{Shaded}

The only meta information that we can surmise from these files is the fact that we know one is the source language and one is the target language and that each sentence is aligned (parallel) with the lines in the other file.

So with what we have we'd like to create a data frame that has the following structure.

\begin{table}

\caption{\label{tab:cd-unstructured-europarle-structure-example}Idealized structure for the Europarle Corpus dataset.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 1 & ...sentence from source language\\
Target & 1 & ...sentence from target language\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{tidy-the-data}{%
\subsubsection{Tidy the data}\label{tidy-the-data}}

To create this dataset structure lets's read the files with the \texttt{readtext()} function from readtext package and assign them to a meaningful variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read the Europarle files}
\NormalTok{europarle\_en }\OtherTok{\textless{}{-}}  \CommentTok{\# English target text}
\NormalTok{  readtext}\SpecialCharTok{::}\FunctionTok{readtext}\NormalTok{(}\StringTok{"../data/original/europarle/europarl{-}v7.es{-}en.en"}\NormalTok{, }\CommentTok{\# path to the data}
                     \AttributeTok{verbosity =} \DecValTok{0}\NormalTok{) }\CommentTok{\# don\textquotesingle{}t show warnings}

\NormalTok{europarle\_es }\OtherTok{\textless{}{-}} \CommentTok{\# Spanish source text}
\NormalTok{  readtext}\SpecialCharTok{::}\FunctionTok{readtext}\NormalTok{(}\StringTok{"../data/original/europarle/europarl{-}v7.es{-}en.es"}\NormalTok{, }\CommentTok{\# path to the data}
                     \AttributeTok{verbosity =} \DecValTok{0}\NormalTok{) }\CommentTok{\# don\textquotesingle{}t show warnings}
\end{Highlighting}
\end{Shaded}

\begin{rmdtip}
The \texttt{readtext()} function can read many different types of file
formats, from structured to unstructured. However, it depends in large
part on the extension of the file to recognize what algorithm to use
when reading a file. In this particular case the Europarle files do not
have a typical extension (they have \texttt{.en} and \texttt{.es}). The
\texttt{readtext()} function will treat them as plain text
(\texttt{.txt}), but it will throw a warning. To suppress the warning
you can add the \texttt{verbosity\ =\ 0} argument to the function.
\end{rmdtip}

Now there are a couple things to note about thbe \texttt{europarle\_en} and \texttt{europarle\_es} objects. If we inspect their structure, we will find that the dimensions of the data frame that is created is one row by two columns.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(europarle\_en)  }\CommentTok{\# inspect the structure of the object}
\CommentTok{\#\textgreater{} Classes \textquotesingle{}readtext\textquotesingle{} and \textquotesingle{}data.frame\textquotesingle{}: 1 obs. of  2 variables:}
\CommentTok{\#\textgreater{}  $ doc\_id: chr "europarl{-}v7.es{-}en.en"}
\CommentTok{\#\textgreater{}  $ text  : chr "Resumption of the session\textbackslash{}nI declare resumed the session of the European Parliament adjourned on Friday 17 Dece"| \_\_truncated\_\_}
\end{Highlighting}
\end{Shaded}

The columns are \texttt{doc\_id} and \texttt{text}. \texttt{doc\_id} is created by readtext to index each file that is read in. The \texttt{text} column is where the text appears. The fact that we only have one row means that all the text in the entire file is contained in one cell! We will want to break this cell up into rows for each sentence, but for now let's work with getting the columns to line up with our idealized dataset structure.

\begin{rmdtip}
Note that I've used the \texttt{str()} function instead of
\texttt{glimpse()} to preview the structure. This is because
\texttt{glimpse()} will try to provide a preview of each of the cells
and in this particular case the cell with the text is so large that it
will take a long time to render.
\end{rmdtip}

First let's change the type of data frame that we are working with to a tibble. This will make sure we don't accidentally print hundreds of lines to our R Markdown output and/ or the R Console. Then we will rename the \texttt{doc\_id} column to \texttt{type} and change the value of that column to ``Target'' (for English) and ``Source'' (for Spanish).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle\_target }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle\_en }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# readtext data frame}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# convert to tibble}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{type =}\NormalTok{ doc\_id) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# rename doc\_id to type}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"Target"}\NormalTok{) }\CommentTok{\# change type value to \textquotesingle{}Target\textquotesingle{}}

\NormalTok{europarle\_source }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle\_es }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# readtext data frame}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# convert to tibble}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{type =}\NormalTok{ doc\_id) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# rename doc\_id to type}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"Source"}\NormalTok{) }\CommentTok{\# change type value to \textquotesingle{}Source\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

We have two objects now, one corresponding to the `Source' and the other the `Target' parallel texts. Let's now join these two datasets, one on top of the other --that is, by rows. We wil use the \texttt{bind\_rows()} function for this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(europarle\_target, europarle\_source)}

\FunctionTok{str}\NormalTok{(europarle)  }\CommentTok{\# inspect the structure of the object}
\CommentTok{\#\textgreater{} tibble [2 x 2] (S3: tbl\_df/tbl/data.frame)}
\CommentTok{\#\textgreater{}  $ type: chr [1:2] "Target" "Source"}
\CommentTok{\#\textgreater{}  $ text: chr [1:2] "Resumption of the session\textbackslash{}nI declare resumed the session of the European Parliament adjourned on Friday 17 Dece"| \_\_truncated\_\_ "Reanudación del período de sesiones\textbackslash{}nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrump"| \_\_truncated\_\_}
\end{Highlighting}
\end{Shaded}

The \texttt{europarle} dataset now has 2 columns, as before, and 2 rows --each corresponding to the distinct language types (Source/ Target).

Remember our goal is to create a dataset structure with three columns \texttt{type}, \texttt{sentence\_id}, and \texttt{sentence}. At the moment we have \texttt{type} and \texttt{text} --where \texttt{text} has all of the sentences in for each type in a cell. So we are going to want to break up the \texttt{text} column into sentences, group the sentences that are created by \texttt{type}, and then number these sentences so that they are aligned between the distinct types.

To break up the text into sentences we are going to turn to the tidytext package. This package has a extremely useful function \texttt{unnest\_tokens()} which provides an effective way to break text into various units (see \texttt{?tidytext::unnest\_tokens} for a full list of token types). Since I know from looking at the raw text that each sentence is on its own line, the best strategy to break the text into sentence units is to find a way to break each line into a new row in our dataset. To do this we need to use the \texttt{token\ =\ "regex"} (for Regular Expression) and use the \texttt{pattern\ =\ "\textbackslash{}\textbackslash{}n"} which tells R to look for carriage returns to use as the breaking criterion.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle\_sentences }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tidytext}\SpecialCharTok{::}\FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{output =}\NormalTok{ sentence, }\CommentTok{\# new column}
                          \AttributeTok{input =}\NormalTok{ text, }\CommentTok{\# column to find text}
                          \AttributeTok{token =} \StringTok{"regex"}\NormalTok{, }\CommentTok{\# use a regular expression to break up the text}
                          \AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\CommentTok{\# break text by carriage returns (returns after lines)}
                          \AttributeTok{to\_lower =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# do not lowercase the text}

\FunctionTok{glimpse}\NormalTok{(europarle\_sentences) }\CommentTok{\# preview the structure}
\CommentTok{\#\textgreater{} Rows: 3,926,375}
\CommentTok{\#\textgreater{} Columns: 2}
\CommentTok{\#\textgreater{} $ type     \textless{}chr\textgreater{} "Target", "Target", "Target", "Target", "Target", "Target", "\textasciitilde{}}
\CommentTok{\#\textgreater{} $ sentence \textless{}chr\textgreater{} "Resumption of the session", "I declare resumed the session o\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

\begin{rmdtip}
Regular Expressions are a powerful pattern matching syntax. They are
used extensively in text manipulation and we will see them again and
again. A good website to practice Regular Expressions is
\href{https://regex101.com/}{RegEx101}. You can also install the
regexplain package in R to get access to a useful
\href{https://rstudio.github.io/rstudioaddins/}{RStudio Addin}.
\end{rmdtip}

Our new \texttt{europarle\_sentences} object is a data frame with almost 4 million rows! The final step to get to our envisioned dataset structure is to add the \texttt{sentence\_id} column which will be calculated by grouping the data by \texttt{type} and then assigning a row number to each of the sentences in each group.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle\_sentences\_id }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle\_sentences }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{group\_by}\NormalTok{(type) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# group by type}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{sentence\_id =} \FunctionTok{row\_number}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# add a row number for each sentence for each level of type}
  \FunctionTok{select}\NormalTok{(type, sentence\_id, sentence) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# select the relevant columns to keep}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# ungroup by type}
  \FunctionTok{arrange}\NormalTok{(sentence\_id, type) }\CommentTok{\# arrange the dataset}

\NormalTok{europarle\_sentences\_id }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{caption =} \StringTok{"First ten sentences in the Europarle Corpus curated dataset."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:cd-unstructured-europarle-add-sentence-id}First ten sentences in the Europarle Corpus curated dataset.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 1 & Reanudación del período de sesiones\\
Target & 1 & Resumption of the session\\
Source & 2 & Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\\
Target & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Source & 3 & Como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\
\addlinespace
Target & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Source & 4 & Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\
Target & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
Source & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\\
Target & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{write-dataset}{%
\subsubsection{Write dataset}\label{write-dataset}}

At this point we have the curated dataset (\texttt{europarle\_sentences\_id}) in a tidy format. This dataset, however, is only in the current R session. We will want to write this dataset to disk so that in the next step of the text analysis workflow (transform data) we will be able to start work on this dataset and make changes as needed to fit our analysis needs.

We will leverage the project directory structure which has distinct directories for \texttt{original/} and \texttt{derived/} data(sets).

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
\end{Highlighting}
\end{Shaded}

Since this dataset is derived by our work, we will added it to the \texttt{derived/} directory. I'll create a \texttt{europarle/} directory just to keep things organized.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Write the curated dataset to disk}
\NormalTok{fs}\SpecialCharTok{::}\FunctionTok{dir\_create}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/derived/europarle/"}\NormalTok{) }\CommentTok{\# create the europarle directory}
\FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{x =}\NormalTok{ europarle\_sentences\_id, }\CommentTok{\# object to write}
          \AttributeTok{file =} \StringTok{"../data/derived/europarle/europarle\_curated.csv"}\NormalTok{) }\CommentTok{\# target file location/ name}
\end{Highlighting}
\end{Shaded}

This is how the directory structure under the \texttt{derived/} directory looks now.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{│}\NormalTok{   └── europarle}
\ExtensionTok{│}\NormalTok{       └── europarle\_curated.csv}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{└──europarle}
        \ExtensionTok{├──}\NormalTok{ europarl{-}v7.es{-}en.en}
        \ExtensionTok{└──}\NormalTok{ europarl{-}v7.es{-}en.es}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-5}{%
\subsubsection{Summary}\label{summary-5}}

In this section we worked with unstructured data and looked at how to read the data into an R session and manipulate the data to form a tidy dataset with a few columns that we could derive based on the information we have about the corpus.

In our discussion we worked step by step to curate the Europarle Corpus, adding in intermediate steps for illustration purposes. However, in a more realistic case the code would most likely make more extensive use of piping (\texttt{\%\textgreater{}\%}) to reduce the number of intermediate objects and make the code more legible. Below I've included a sample of what that code might look like.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read data and set up \textasciigrave{}type\textasciigrave{} column}
\NormalTok{europarle\_en }\OtherTok{\textless{}{-}}  
\NormalTok{  readtext}\SpecialCharTok{::}\FunctionTok{readtext}\NormalTok{(}\StringTok{"../data/original/europarle/europarl{-}v7.es{-}en.en"}\NormalTok{, }\CommentTok{\# path to the data}
                     \AttributeTok{verbosity =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# don\textquotesingle{}t show warnings}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# covert to tibble}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{type =}\NormalTok{ doc\_id) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# rename doc\_id to type}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"Target"}\NormalTok{) }\CommentTok{\# change type value to \textquotesingle{}Target\textquotesingle{}}

\NormalTok{europarle\_es }\OtherTok{\textless{}{-}} 
\NormalTok{  readtext}\SpecialCharTok{::}\FunctionTok{readtext}\NormalTok{(}\StringTok{"../data/original/europarle/europarl{-}v7.es{-}en.en"}\NormalTok{, }\CommentTok{\# path to the data}
                     \AttributeTok{verbosity =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# don\textquotesingle{}t show warnings}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# covert to tibble}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{type =}\NormalTok{ doc\_id) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# rename doc\_id to type}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"Source"}\NormalTok{) }\CommentTok{\# change type value to \textquotesingle{}Source\textquotesingle{}}

\CommentTok{\# Join the datasets by rows}
\NormalTok{europarle }\OtherTok{\textless{}{-}} 
  \FunctionTok{bind\_rows}\NormalTok{(europarle\_en, europarle\_es)}

\CommentTok{\# Segment the \textasciigrave{}text\textasciigrave{} column into \textasciigrave{}sentence\textasciigrave{} units}
\NormalTok{europarle }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
\NormalTok{  tidytext}\SpecialCharTok{::}\FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{output =}\NormalTok{ sentence, }\CommentTok{\# new column}
                          \AttributeTok{input =}\NormalTok{ text, }\CommentTok{\# column to find text}
                          \AttributeTok{token =} \StringTok{"regex"}\NormalTok{, }\CommentTok{\# use a regular expression to break up the text}
                          \AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\CommentTok{\# break text by carriage returns (returns after lines)}
                          \AttributeTok{to\_lower =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# do not lowercase the text}

\CommentTok{\# Add \textasciigrave{}sentence\_id\textasciigrave{} to each \textasciigrave{}type\textasciigrave{}}
\NormalTok{europarle }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{group\_by}\NormalTok{(type) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# group by type}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{sentence\_id =} \FunctionTok{row\_number}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# add a row number for each sentence for each level of type}
  \FunctionTok{select}\NormalTok{(type, sentence\_id, sentence) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# select the relevant columns to keep}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# ungroup by type}
  \FunctionTok{arrange}\NormalTok{(sentence\_id, type) }\CommentTok{\# arrange the dataset}

\CommentTok{\# Write the curated dataset to disk}
\NormalTok{fs}\SpecialCharTok{::}\FunctionTok{dir\_create}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/derived/europarle/"}\NormalTok{) }\CommentTok{\# create the europarle directory}
\FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{x =}\NormalTok{ europarle\_sentences\_id, }\CommentTok{\# object to write}
          \AttributeTok{file =} \StringTok{"../data/derived/europarle/europarle\_curated.csv"}\NormalTok{) }\CommentTok{\# target file location/ name}
\end{Highlighting}
\end{Shaded}

\hypertarget{structured}{%
\subsection{Structured}\label{structured}}

On the opposite side of the spectrum from unstructured data, structured data includes more metadata information --often much more. The association of metadata with the language to be analyzed means that the data has already be curated to some degree, therefore it is more apt to discuss structured data as a dataset. There are two questions, however, that need to be taken into account. One, logistical question, is what file format the dataset is in and how to we read it into R. And the second, more research-based, is whether the data is curated in a fashion that makes sense for the current research. Let's look at each of these questions briefly and then get to a practical example.

There are file formats which are purposely designed for storing structured datasets. Some very common file types are .csv, .xml, .json, etc. The data within these files is explicitly organized. For example, in a .csv file, the dataset structure is represented by delimiting the columns and rows by commas.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{column\_1,column\_2,column\_3}
\NormalTok{row 1 value 1,row 1 value 2,row 1 value 3}
\NormalTok{row 2 value 1,row 2 value 2,row 2 value 3}
\end{Highlighting}
\end{Shaded}

When read into R, the .csv file format is converted to a data frame with the appropriate structure.

\begin{table}

\caption{\label{tab:cd-structured-example-table-csv}Example .csv file in R}
\centering
\begin{tabular}[t]{lll}
\toprule
column\_1 & column\_2 & column\_3\\
\midrule
row 1 value 1 & row 1 value 2 & row 1 value 3\\
row 2 value 1 & row 2 value 2 & row 2 value 3\\
\bottomrule
\end{tabular}
\end{table}

With an understanding of how the information is encoding into a file, we can now turn to considerations about how the original dataset is structure and how that structure is to be used for a given research project. The curation process that is reflected in a structured dataset may or may not initially align with the goals of our research either in terms of the type(s) of information or the unit of analysis of the structured dataset. The aim, then, is to take advantage of the information and curate it such that it does align.

As an example case of curating structured datasets, we will look at the song lyric datasets acquired from Last.fm in the \protect\hyperlink{acquire-data}{previous chapter}.

\hypertarget{orientation-1}{%
\subsubsection{Orientation}\label{orientation-1}}

The individual datasets from the Last.fm webscrape are found inside the \texttt{data/original/lastfm/} directory, and includes the \texttt{README.md} documentation file.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{└──}\NormalTok{ lastfm/}
        \ExtensionTok{├──}\NormalTok{ README.md}
        \ExtensionTok{├──}\NormalTok{ country.csv}
        \ExtensionTok{├──}\NormalTok{ hip\_hop.csv}
        \ExtensionTok{├──}\NormalTok{ lyrics.csv}
        \ExtensionTok{├──}\NormalTok{ metal.csv}
        \ExtensionTok{├──}\NormalTok{ pop.csv}
        \ExtensionTok{└──}\NormalTok{ rock.csv}
\end{Highlighting}
\end{Shaded}

Let's take a look at the structure of one of genres from these set of lyrics to familiarize ourselves with the structure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lf\_country }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/original/lastfm/country.csv"}\NormalTok{) }\CommentTok{\# read the csv file}
\NormalTok{lf\_country }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# first 10 observations }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# print pretty table}
             \AttributeTok{caption =} \StringTok{"Example file fromt the Last.fm dataset of song lyrics."}\NormalTok{) }\CommentTok{\# add caption}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:cd-structured-read-single-run}Example file fromt the Last.fm dataset of song lyrics.}
\centering
\begin{tabular}[t]{lllll}
\toprule
artist & song & lyrics & lyrics\_url & genre\\
\midrule
Johnny Cash & Hurt & I hurt myself todayTo see if I still feelI focus on the painThe only thing that's realThe needle tears a holeThe old familiar stingTry to kill it all awayBut I remember everything & https://www.last.fm/music/Johnny+Cash/\_/Hurt/+lyrics & country\\
Johnny Cash & Hurt & What have I becomeMy sweetest friendEveryone I know goes awayIn the endAnd you could have it allMy empire of dirtI will let you downI will make you hurt & https://www.last.fm/music/Johnny+Cash/\_/Hurt/+lyrics & country\\
Johnny Cash & Hurt & I wear this crown of thornsUpon my liar's chairFull of broken thoughtsI cannot repairBeneath the stains of timeThe feelings disappearYou are someone elseI am still right here & https://www.last.fm/music/Johnny+Cash/\_/Hurt/+lyrics & country\\
Johnny Cash & Hurt & What have I becomeMy sweetest friendEveryone I know goes awayIn the endAnd you could have it allMy empire of dirtI will let you downI will make you hurt & https://www.last.fm/music/Johnny+Cash/\_/Hurt/+lyrics & country\\
Johnny Cash & Hurt & If I could start againA million miles awayI would keep myselfI would find a way & https://www.last.fm/music/Johnny+Cash/\_/Hurt/+lyrics & country\\
\addlinespace
Johnny Cash & Ring of Fire & Love is a burning thingAnd it makes a fiery ringBound by wild desireI fell into a ring of fire & https://www.last.fm/music/Johnny+Cash/\_/Ring+of+Fire/+lyrics & country\\
Johnny Cash & Ring of Fire & I fell into a burning ring of fireI went down, down, downAnd the flames went higherAnd it burns, burns, burnsThe ring of fire, the ring of fire & https://www.last.fm/music/Johnny+Cash/\_/Ring+of+Fire/+lyrics & country\\
Johnny Cash & Ring of Fire & I fell into a burning ring of fireI went down, down, downAnd the flames went higherAnd it burns, burns, burnsThe ring of fire, the ring of fire & https://www.last.fm/music/Johnny+Cash/\_/Ring+of+Fire/+lyrics & country\\
Johnny Cash & Ring of Fire & The taste of love is sweetWhen hearts like ours meetI fell for you like a child & https://www.last.fm/music/Johnny+Cash/\_/Ring+of+Fire/+lyrics & country\\
Johnny Cash & Ring of Fire & Oh, but the fire went wild & https://www.last.fm/music/Johnny+Cash/\_/Ring+of+Fire/+lyrics & country\\
\bottomrule
\end{tabular}
\end{table}

We can see a couple important characteristics from this preview of the dataset. First, we see the columns include \texttt{artist}, \texttt{song}, \texttt{lyrics}, \texttt{lyrics\_url}, and \texttt{genre}. Second, we see that for each son the lyrics are segmented across multiple rows.

\begin{rmdtip}
You may notice that in addition to the lyrics being separated by line,
there appears to be an artifact from the original webscrape of this data
which has individual lyric lines run in to the next. An example is the
the lyrics ``\ldots{} hurt myself toda\textbf{yT}osee if I still
fee\textbf{lI} focus\ldots{}''. We will address this issue when it comes
time to normalize the dataset in the transform process.
\end{rmdtip}

Given the fact that each of these files will include a \texttt{genre} label, that means that we will be able to read in each of these files in one operation and the distinction between genres will be recoverable. The next thing to think about is how we want to curate the dataset for our purposes. That is, what should the base structure of our curated dataset look like?

Let's make the assumption that we want to have the columns \texttt{artist}, \texttt{song}, \texttt{lyrics}, and \texttt{genre}. The \texttt{lyrics\_url} could be useful for documentation purposes, but for our text analysis it does not appear to be very relevant --so we will drop it. The second aspect concerns the observations. As it stands, the dataset the observations reflect the formatting of the website from which the lyrics were drawn. A potentially better organization would to have each observation correspond to all the lyrics for a single song. In this case we will want to collapse the current \texttt{lyrics} column's values into lyrics for the entire song --maintaining the other measure for each of the other columns.

With this structure in mind, we are shooting for an idealized structure such as the one below.

\begin{table}

\caption{\label{tab:cd-structured-idealized-dataset}Idealized structure for the Last.fm dataset.}
\centering
\begin{tabular}[t]{llll}
\toprule
artist & song & lyrics & genre\\
\midrule
Johnny Cash & Hurt & ...lyrics for the song... & country\\
Johnny Cash & Ring of Fire & ...lyrics for the song... & country\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{tidy-the-datasets}{%
\subsubsection{Tidy the datasets}\label{tidy-the-datasets}}

So our objectives are set, let's first read in all the files. To do this we will again use the \texttt{readtext()} function. But instead of reading one file at a time we will read all the files of interest (those with the .csv extension) in one go. The \texttt{readtext()} function allows for the use of `wildcard' notation (\texttt{*}) in the file(s) path to enable pattern matching.

So the files in the \texttt{data/original/lastfm/} directory look like this.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{../data/original/lastfm/README.md}
\ExtensionTok{../data/original/lastfm/country.csv}
\ExtensionTok{../data/original/lastfm/hip\_hop.csv}
\ExtensionTok{../data/original/lastfm/lyrics.csv}
\ExtensionTok{../data/original/lastfm/metal.csv}
\ExtensionTok{../data/original/lastfm/pop.csv}
\ExtensionTok{../data/original/lastfm/rock.csv}
\end{Highlighting}
\end{Shaded}

We want all the files, except the \texttt{REAME.md} file. To do this we want our path to look like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{../data/original/lastfm/*.csv}
\end{Highlighting}
\end{Shaded}

The wildcard \texttt{*} replaces the genre names and this effectively only matches files ending in \texttt{.csv}.

Great, that will capture the files we are looking for but when working with \texttt{readtext()} we will need to set the \texttt{text\_field} argument to the column that corresponds to the text in our dataset. That is the \texttt{lyrics} column. Let's go ahead and do this and convert the result to a tibble.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\OtherTok{\textless{}{-}} 
  \FunctionTok{readtext}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/original/lastfm/*.csv"}\NormalTok{, }\CommentTok{\# files to match using *.csv}
           \AttributeTok{text\_field =} \StringTok{"lyrics"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# text column from the datasets}
  \FunctionTok{as\_tibble}\NormalTok{() }\CommentTok{\# convert to a tibble}

\FunctionTok{glimpse}\NormalTok{(lastfm) }\CommentTok{\# preview}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 2,172
#> Columns: 6
#> $ doc_id     <chr> "country.csv.1", "country.csv.2", "country.csv.3", "country~
#> $ text       <chr> "I hurt myself todayTo see if I still feelI focus on the pa~
#> $ artist     <chr> "Johnny Cash", "Johnny Cash", "Johnny Cash", "Johnny Cash",~
#> $ song       <chr> "Hurt", "Hurt", "Hurt", "Hurt", "Hurt", "Ring of Fire", "Ri~
#> $ lyrics_url <chr> "https://www.last.fm/music/Johnny+Cash/_/Hurt/+lyrics", "ht~
#> $ genre      <chr> "country", "country", "country", "country", "country", "cou~
\end{verbatim}

Looking at the preview of the data frame we now have in \texttt{lastfm} there are a couple things to note. First, we see that a column \texttt{doc\_id} has been added. This column is used by \texttt{readtext()} to index the file from which the data was read. In our case since we already have sufficient information to index our dataset, we can drop this column. Next we see that the \texttt{lyrics} column has been renamed to \texttt{text}. This is because we set this as the \texttt{text\_field} when we read in the files. We can easily rename this column, but we'll leave that for later.

Let's go ahead and drop the columns that we have decided will not figure in our curated dataset. We can use the \texttt{select()} function to either select those columns we want to keep or by using the \texttt{-} operator, identify the columns we want to drop. The decision of `selecting' or `deselecting' is usually one of personal choice and code succinctness. In this case, we are dropping two columns and keeping four, so let's deselect. I will assign the result to the same name as our current dataset, effectively overwritting that dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\OtherTok{\textless{}{-}} \CommentTok{\# new dataset}
  \FunctionTok{select}\NormalTok{(lastfm, }\CommentTok{\# original dataset}
         \SpecialCharTok{{-}}\NormalTok{doc\_id, }\SpecialCharTok{{-}}\NormalTok{lyrics\_url) }\CommentTok{\# drop these columns}

\FunctionTok{glimpse}\NormalTok{(lastfm) }\CommentTok{\# preview}
\CommentTok{\#\textgreater{} Rows: 2,172}
\CommentTok{\#\textgreater{} Columns: 4}
\CommentTok{\#\textgreater{} $ text   \textless{}chr\textgreater{} "I hurt myself todayTo see if I still feelI focus on the painTh\textasciitilde{}}
\CommentTok{\#\textgreater{} $ artist \textless{}chr\textgreater{} "Johnny Cash", "Johnny Cash", "Johnny Cash", "Johnny Cash", "Jo\textasciitilde{}}
\CommentTok{\#\textgreater{} $ song   \textless{}chr\textgreater{} "Hurt", "Hurt", "Hurt", "Hurt", "Hurt", "Ring of Fire", "Ring o\textasciitilde{}}
\CommentTok{\#\textgreater{} $ genre  \textless{}chr\textgreater{} "country", "country", "country", "country", "country", "country\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Now let's work to collapse the lyrics in the \texttt{text} column by each distinct \texttt{artist}, \texttt{song}, and \texttt{genre} combination. We will use the \texttt{group\_by()} function to create \texttt{artist} \texttt{song} \texttt{genre} groupings and then use \texttt{summarize()} to create a new column in which the \texttt{text} field is collapsed into all the song lyrics for this grouping. Inside the \texttt{summarize()} function we use \texttt{str\_flatten()} with the argument \texttt{collapse\ =\ "\ "} to collapse each observation in \texttt{text} leaving a single whitespace between the observations (otherwise each line would then be joined contiguously to the next).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{group\_by}\NormalTok{(artist, song, genre) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# grouping}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{lyrics =} \FunctionTok{str\_flatten}\NormalTok{(text, }\AttributeTok{collapse =} \StringTok{" "}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# collapse text into the new column \textasciigrave{}lyrics\textasciigrave{} (dropping \textasciigrave{}text\textasciigrave{})}
  \FunctionTok{ungroup}\NormalTok{() }\CommentTok{\# unset the groupings}

\FunctionTok{glimpse}\NormalTok{(lastfm) }\CommentTok{\# preview}
\CommentTok{\#\textgreater{} Rows: 199}
\CommentTok{\#\textgreater{} Columns: 4}
\CommentTok{\#\textgreater{} $ artist \textless{}chr\textgreater{} "3 Doors Down", "3 Doors Down", "50 Cent", "a{-}ha", "ABBA", "Aer\textasciitilde{}}
\CommentTok{\#\textgreater{} $ song   \textless{}chr\textgreater{} "Here Without You", "Kryptonite", "In Da Club", "Take On Me", "\textasciitilde{}}
\CommentTok{\#\textgreater{} $ genre  \textless{}chr\textgreater{} "rock", "rock", "hip{-}hop", "pop", "pop", "rock", "country", "co\textasciitilde{}}
\CommentTok{\#\textgreater{} $ lyrics \textless{}chr\textgreater{} "A hundred days have made me olderSince the last time that I sa\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Let's take a look at the first 5 observations from this collapsed dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# first 5 observations}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# print pretty table}
               \AttributeTok{caption =} \StringTok{"Sample lyrics from Last.fm dataset collapsed by artist, song, and genre."}\NormalTok{) }\CommentTok{\# add caption to the table}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:cd-structure-lastfm-collapse-table}Sample lyrics from Last.fm dataset collapsed by artist, song, and genre.}
\centering
\begin{tabular}[t]{llll}
\toprule
artist & song & genre & lyrics\\
\midrule
3 Doors Down & Here Without You & rock & A hundred days have made me olderSince the last time that I saw your pretty faceA thousand lies have made me colderAnd I don't think I can look at this the same But all the miles that separateDisappear now when I'm dreaming of your face I'm here without you, babyBut you're still on my lonely mindI think about you, babyAnd I dream about you all the time I'm here without you, babyBut you're still with me in my dreamsAnd tonight it's only you and me, yeah The miles just keep rollin'As the people leave their way to say helloI've heard this life is overratedBut I hope that it gets better as we go, oh yeah, yeah I'm here without you, babyBut you're still on my lonely mindI think about you, babyAnd I dream about you all the time I'm here without you, babyBut you're still with me in my dreamsAnd tonight, girl, it's only you and me Everything I know, and anywhere I go (Oh whoa)It gets hard but it won't take away my love (Oh whoa)And when the last one fallsWhen it's all said and doneIt gets hard but it won't take away my love, whoa, oh, oh I'm here without you, babyBut you're still on my lonely mindI think about you, babyAnd I dream about you all the time I'm here without you, babyBut you're still with me in my dreamsAnd tonight, girl, it's only you and me, yeahOh girl, oh oh\\
3 Doors Down & Kryptonite & rock & Well I took a walk around the world to ease my troubled mindI left my body lying somewhere in the sands of timeWell I watched the world float to the dark side of the moonI feel there's nothing I can do,yeah I watched the world float to the dark side of the moonAfter all I knew it had to be something to do with youI really don't mind what happens now and thenAs long as you'll be my friend at the end If I go crazy then will you still call me Superman?If I'm alive and well, will you be there a-holding my hand?I'll keep you by my side with my superhuman mightKryptonite You called me strong, you called me weakBut still your secrets I will keepYou took for granted all the times I never let you downYou stumbled in and bumped your headIf not for me then you'd be deadI picked you up and put you back on solid ground If I go crazy then will you still call me Superman?If I'm alive and well, will you be there a-holding my hand?I'll keep you by my side with my superhuman mightKryptonite If I go crazy then will you still call me Superman?If I'm alive and well, will you be there holding my hand?I'll keep you by my side with my superhuman mightKryptoniteYeah! If I go crazy then will you still call me Superman?If I'm alive and well, will you be there a-holding my hand?I'll keep you by my side with my superhuman mightKryptonite Oh, whoa, whoaOh, whoa, whoaOh, whoa, whoa\\
50 Cent & In Da Club & hip-hop & Go, go, go, go, go, goGo, shortyIt's your birthdayWe gon' party like it's your birthdayWe gon' sip Bacardi like it's your birthdayAnd you know we don't give a fuck it's not your birthday You can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin' drugsI'm into havin' sex, I ain't into makin' loveSo come give me a hug, if you into getting rubbed You can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin' drugsI'm into havin' sex, I ain't into makin' loveSo come give me a hug, if you into getting rubbed When I pull out up front, you see the Benz on dubsWhen I roll 20 deep, it's 20 knives in the clubNiggas heard I fuck with Dre, now they wanna show me loveWhen you sell like Eminem, and the hoes they wanna fuckBut, homie, ain't nothing change hoes down, G's upI see Xzibit in the Cut, that nigga roll that weed upIf you watch how I move, you'll mistake me for a playa or pimpBeen hit wit' a few shells, but I don't walk wit' a limp (I'm ight)In the hood, in L.A, they saying ""50 you hot""They like me, I want them to love me like they love 'PacBut holla, in New York them niggas'll tell ya I'm locoAnd the plan is to put the rap game in a choke holdI'm full of focused man, my money on my mindI got a mill out the deal and I'm still on the grindNow shorty said she feeling my style, she feeling my flowHer girlfriend wanna get bi and they ready to go You can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin' drugsI'm into havin' sex, I ain't into makin' loveSo come give me a hug, if you into getting rubbed You can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin' drugsI'm into havin' sex I, ain't into makin' loveSo come give me a hug, if you into getting rubbed My flow, my show brought me the doeThat bought me all my fancy thingsMy crib, my cars, my clothes, my jewelsLook, nigga, I done came up and I ain't changeAnd you should love it, way more then you hate itNigga, you mad? I thought that you'd be happy I made itI'm that cat by the bar toasting to the good lifeYou that faggot ass nigga trying to pull me back right?When my jaws get to bumpin' in the club it's onI wink my eye at you, bitch, if she smiles she goneIf the roof on fire, let the motherfucker burnIf you talking 'bout money, homie, I ain't concernedI'm a tell you what Banks told me 'cuz go 'head switch the style upIf the niggas hate then let 'em hate and watch the money pile upOr we go upside they head wit' a bottle of bubThey know where we fuckin' be You can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin' drugsI'm into havin' sex, I ain't into makin' love So come give me a hug, if you into getting rubbedYou can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin' drugsI'm into havin' sex, I ain't into makin' loveSo come give me a hug, if you into getting rubbed Don't try to act like you ain't know where we been either, niggaWe in the club all the time, nigga, it's about to pop off, niggaG-Unit\\
a-ha & Take On Me & pop & Talking awayI don't know whatWhat to sayI'll say it anywayToday is another day to find youShying awayOh, I'll be coming for your love, okay? Take On Me (Take On Me)Take me on (Take On Me)I'll be goneIn a day or two So needless to sayI'm odds and endsBut I'll beStumbling awaySlowly learning that life is okaySay after meOh, it's no better to be safe than sorry Take On Me (Take On Me)Take me on (Take On Me)I'll be goneIn a day or two Oh, the things that you sayIs it life or just to playMy worries away?You're all the things I've got to rememberYou're shying awayI'll be coming for you anyway Take On Me (Take On Me)Take me on (Take On Me)I'll be goneIn a dayTake On Me (Take On Me)Take me on (Take On Me)I'll be gone (Take On Me)In a day (Take me on, Take On Me)Take On Me (Take On Me)Take me on (Take On Me)\\
ABBA & Dancing Queen & pop & You can dance, you can jiveHaving the time of your life, oohSee that girl, watch that sceneDig in the Dancing Queen Friday night and the lights are lowLooking out for a place to goWhere they play the right music, getting in the swingYou come to look for a king Anybody could be that guyNight is young and the music's highWith a bit of rock music, everything is fineYou're in the mood for a dance And when you get the chance You are the Dancing QueenYoung and sweet, only seventeenDancing QueenFeel the beat from the tambourine, oh yeah You can dance, you can jiveHaving the time of your life, oohSee that girl, watch that sceneDig in the Dancing Queen You're a teaser, you turn 'em onLeave them burning and then you're goneLooking out for another, anyone will doYou're in the mood for a dance And when you get the chance You are the Dancing QueenYoung and sweet, only seventeenDancing QueenFeel the beat from the tambourine, oh yeah You can dance, you can jiveHaving the time of your life, ohSee that girl, watch that sceneDig in the Dancing Queen Dig in the Dancing Queen\\
\bottomrule
\end{tabular}
\end{table}

At this point, the only thing left to do to get this dataset to align with our idealized dataset structure is to organize the column ordering (using \texttt{select()}). I will also arrange the dataset alphabetically by \texttt{genre} and \texttt{artist} (using \texttt{arrange()}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# original dataset}
  \FunctionTok{select}\NormalTok{(artist, song, lyrics, genre) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# order columns (and rename \textasciigrave{}text\textasciigrave{} to \textasciigrave{}lyrics\textasciigrave{})}
  \FunctionTok{arrange}\NormalTok{(genre, artist) }\CommentTok{\# arrange rows by \textasciigrave{}genre\textasciigrave{} and \textasciigrave{}artist\textasciigrave{}}

\NormalTok{lastfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# curated dataset}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# first 5 observations}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# print pretty table}
               \AttributeTok{caption =} \StringTok{"Sample lyrics from curated Last.fm dataset."}\NormalTok{) }\CommentTok{\# add caption to the table}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:cd-structure-lastfm-order-arrange}Sample lyrics from curated Last.fm dataset.}
\centering
\begin{tabular}[t]{llll}
\toprule
artist & song & lyrics & genre\\
\midrule
Alan Jackson & Little Bitty & Have a little love on a little honeymoonYou got a little dish and you got a little spoonA little bitty house and a little bitty yardA little bitty dog and a little bitty car Well, it's alright to be little bittyA little hometown or a big old cityMight as well share, might as well smileLife goes on for a little bitty while A little bitty baby in a little bitty gownIt'll grow up in a little bitty townA big yellow bus and little bitty booksIt all started with a little bitty look Well, it's alright to be little bittyA little hometown or a big old cityMight as well share, might as well smileLife goes on for a little bitty while Heey You know you got a job and a little bitty checkA six pack of beer and a television setLittle bitty world goes around and aroundLittle bit of silence and a little bit of sound A good ol' boy and a pretty little girlStart all over in a little bitty worldLittle bitty plan and a little bitty dreamIt's all part of a little bitty schemeIt's alright to be little bitty A little hometown or a big old cityMight as well share, might as well smileLife goes on for a little bitty whileIt's alright to be little bittyA little hometown or a big old cityMight as well share, might as well smileLife goes on for a little bitty while Whoo & country\\
Alan Jackson & Remember When & Remember when I was young and so were youAnd time stood still and love was all we knewYou were the first so was IWe made love and then you criedRemember when Remember when we vowed the vowsAnd walk the walkGave our hearts, made the start and it was hardWe lived and learned, life threw curvesThere was joy, there was hurtRemember when Remember when old ones died and new were bornAnd life was changed, dissassembled, rearrangedWe came together, fell apartAnd broke each other's heartsRemember when Remember when the sound of little feetWas the music we danced to week to weekBrought back the love, we found trustVowed we'd never give it upRemember when Remember when thirty seemed so oldNow,lookin' back, it's just a steppin' stoneTo where we are, where we've beenSaid we'd do it all againRemember when Remember when we said when we turned grayWhen the children grow up and move awayWe won't be sad, we'll be gladFor all the life we've hadAnd we'll remember when & country\\
Brad Paisley & Mud on the Tires & I've got some big newsThe bank finally came throughAnd I'm holdin' the keys to a brand new ChevroletHave you been outside, it sure is a nice nightHow about a little test driveDown by the lake? There's a place I know about where the dirt road runs outAnd we can try out the four-wheel driveCome on now what do you sayGirl, I can hardly wait to get a little mud on the tires 'Cause it's a good nightTo be out there soakin' up the moonlightStake out a little piece of shore lineI've got the perfect place in mindIt's in the middle of nowhere only one way to get thereYou got to get a little mud on the tires Moonlight on a duck blindCatfish on a trout lineSun sets about nine this time of yearWe can throw a blanket downCrickets singin' in the backgroundAnd more stars that you can count on a night this clear I tell you what we need to doIs grab a sleepin' bag or twoAnd build us a little campfireAnd then with a little luck we might just get stuckLet's get a little mud on the tires 'Cause it's a good nightTo be out there soakin' up the moonlightStake out a little piece of shore lineI've got the perfect place in mind It's in the middle of nowhere only one way to get thereYou got to get a little mud on the tiresAnd then with a little luck we might just get stuckLet's get a little mud on the tires & country\\
Carrie Underwood & Before He Cheats & Right now, he's probably slow dancin'With a bleached-blond tramp and she's probably gettin' friskyRight now, he's probably buyin' her some fruity little drink'Cause she can't shoot whiskeyRight now, he's probably up behind her with a pool-stickShowin' her how to shoot a comboAnd he don't know I dug my key into the side of his pretty little souped-up four-wheel driveCarved my name into his leather seatsI took a Louisville slugger to both headlightsI slashed a hole in all four tiresMaybe next time he'll think before he cheats Right now, she's probably up singing someWhite-trash version of Shania karaokeRight now, she's probably sayin' ""I'm drunk""And he's a-thinkin' that he's gonna get luckyRight now, he's probably dabbin' onThree dollars worth of that bathroom PoloOh, and he don't know That I dug my key into the side of his pretty little souped-up four-wheel driveCarved my name into his leather seatsI took a Louisville slugger to both headlightsI slashed a hole in all four tiresMaybe next time he'll think before he cheats I might have saved a little trouble for the next girlA-'cause the next time that he cheatsOh, you know it won't be on meNo, not on me 'Cause I dug my key into the side of his pretty little souped-up four-wheel driveCarved my name into his leather seatsI took a Louisville slugger to both headlightsI slashed a hole in all four tiresMaybe next time he'll think before he cheats Oh, maybe next time he'll think before he cheatsOh, before he cheatsOh-oh & country\\
Dierks Bentley & What Was I Thinkin' & Becky was a beauty from south AlabamaHer daddy had a heart like a nine pound hammerThink he even did a little time in the slammerWhat was I thinkin'? She snuck out one night and met me by the front gateHer daddy came out wavin' that 12-gaugeWe tore out the drive, he peppered my tailgateWhat was I thinkin'? Oh, I knew there'd be hell to payBut that crossed my mind a little too late 'Cause I was thinkin' 'bout a little white tank topSittin' right there in the middle by meI was thinkin' 'bout a long kissMan, just gotta get goin' where the night might lead I know what I was feelin'But what was I thinkin'?What was I thinkin'? By the county line, the cops were nippin' on our heelsPulled off the road and kicked it in four-wheelShut off the lights and tore through a cornfieldWhat was I thinkin'? Out the other side, she was hollerin', ""Faster""Took a dirt road, had the radio blastin'Hit the honky-tonk for a little close dancin'What was I thinkin'? Oh, I knew there'd be hell to payBut that crossed my mind a little too late 'Cause I was thinkin' 'bout a little white tank topSittin' right there in the middle by meI was thinkin' 'bout a long kissMan, just gotta get goin' where the night might lead I know what I was feelin'But what was I thinkin'? When a mountain of a man with a ""Born to kill"" tattooTried to cut in, I knocked out his front toothWe ran outside, hood slidin' like Bo DukeWhat was I thinkin'? I finally got her home at a half past two, laterDaddy's in a lawn chair sittin' on the drivewayPut it in park as he started my wayWhat was I thinkin'? Oh, what was I thinkin'?Oh, what was I thinkin'? Then she gave a ""Come and get me"" grinAnd like a bullet, we were gone again 'Cause I was thinkin' 'bout a little white tank topSittin' right there in the middle by meI was thinkin' 'bout a long kissMan, just gotta get goin' where the night might leadI know what I was feelin'Yeah, I know what was I feelin'But what was I thinkin'? What was I thinkin'?I know what I was feelin'What was I thinkin'?Guess I was thinkin' 'bout that tank topThose cutoffs & country\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{write-dataset-1}{%
\subsubsection{Write dataset}\label{write-dataset-1}}

We now have a curated dataset that we can write to disk. Again, as with the Europarle Corpus dataset we curated before, we will write this dataset to the \texttt{data/derived/} directory --effectively ensuring that it is clear that this dataset was created by our project work.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fs}\SpecialCharTok{::}\FunctionTok{dir\_create}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/derived/lastfm/"}\NormalTok{)  }\CommentTok{\# create lastfm subdirectory}
\FunctionTok{write\_csv}\NormalTok{(lastfm, }\AttributeTok{file =} \StringTok{"../data/derived/lastfm/lastfm\_curated.csv"}\NormalTok{)  }\CommentTok{\# write lastfm to disk and label as the curated dataset}
\end{Highlighting}
\end{Shaded}

And here's an overview of our new directory structure.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{│}\NormalTok{   └── lastfm/}
\ExtensionTok{│}\NormalTok{       └── lastfm\_curated.csv}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{└──}\NormalTok{ lastfm/}
        \ExtensionTok{├──}\NormalTok{ README.md}
        \ExtensionTok{├──}\NormalTok{ country.csv}
        \ExtensionTok{├──}\NormalTok{ hip\_hop.csv}
        \ExtensionTok{├──}\NormalTok{ lyrics.csv}
        \ExtensionTok{├──}\NormalTok{ metal.csv}
        \ExtensionTok{├──}\NormalTok{ pop.csv}
        \ExtensionTok{└──}\NormalTok{ rock.csv}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-6}{%
\subsubsection{Summary}\label{summary-6}}

Again, to summarize, here is the code that will accomplish the steps we covered in this section on curating structured datasets.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read Last.fm lyrics and subset relevant columns}
\NormalTok{lastfm }\OtherTok{\textless{}{-}} 
  \FunctionTok{readtext}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/original/lastfm/*.csv"}\NormalTok{, }\CommentTok{\# files to match using *.csv}
           \AttributeTok{text\_field =} \StringTok{"lyrics"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# text column from the datasets}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# convert to a tibble}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{doc\_id, }\SpecialCharTok{{-}}\NormalTok{lyrics\_url) }\CommentTok{\# drop these columns}

\CommentTok{\# Collapse text by artist, song, and genre grouping}
\NormalTok{lastfm }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{group\_by}\NormalTok{(artist, song, genre) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# grouping}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{lyrics =} \FunctionTok{str\_flatten}\NormalTok{(text, }\AttributeTok{collapse =} \StringTok{" "}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# collapse text into the new column \textasciigrave{}lyrics\textasciigrave{} (dropping \textasciigrave{}text\textasciigrave{})}
  \FunctionTok{ungroup}\NormalTok{() }\CommentTok{\# unset the groupings}

\CommentTok{\# Order columns and arrange rows}
\NormalTok{lastfm }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# original dataset}
  \FunctionTok{select}\NormalTok{(artist, song, lyrics, genre) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# order columns (and rename \textasciigrave{}text\textasciigrave{} to \textasciigrave{}lyrics\textasciigrave{})}
  \FunctionTok{arrange}\NormalTok{(genre, artist) }\CommentTok{\# arrange rows by \textasciigrave{}genre\textasciigrave{} and \textasciigrave{}artist\textasciigrave{}}

\CommentTok{\# Write curated dataset to disk}
\NormalTok{fs}\SpecialCharTok{::}\FunctionTok{dir\_create}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/derived/lastfm/"}\NormalTok{) }\CommentTok{\# create lastfm subdirectory}
\FunctionTok{write\_csv}\NormalTok{(lastfm, }
          \AttributeTok{file =} \StringTok{"../data/derived/lastfm/lastfm\_curated.csv"}\NormalTok{) }\CommentTok{\# write lastfm to disk and label as the curated dataset}
\end{Highlighting}
\end{Shaded}

\hypertarget{semi-structured}{%
\subsection{Semi-structured}\label{semi-structured}}

At this point we have discussed curating unstructured data and structured datasets. Between these two extremes falls semi-structured data. And as the name suggests, it is a hybrid between unstructured and structured data. This means that there will be important structured metadata included with unstructured elements. The file formats and approaches to encoding the structured aspects of the data vary widely from resource to resource and therefore often requires more detailed attention to the structure of the data and often includes more sophisticated programming strategies to curate the data to produce a tidy dataset.

As an example we will work with the The Switchboard Dialog Act Corpus (SDAC) which extends the \href{https://catalog.ldc.upenn.edu/LDC97S62}{Switchboard Corpus} with speech act annotation. \textbf{(ADD CITATION)}

\begin{rmdtip}
The SDAC dialogues (\texttt{swb1\_dialogact\_annot.tar.gz}) are
available as a \href{https://catalog.ldc.upenn.edu/docs/LDC97S62/}{free
download from the LDC}. To download, decompress, and organize this
resource, follow the strategies discussed in
\protect\hyperlink{acquire-data}{``Acquire data''} for Direct Downloads.
The tadr package provides the \texttt{tadr::get\_compressed\_data()}
function to accomplish this step.
\end{rmdtip}

\hypertarget{orientation-2}{%
\subsubsection{Orientation}\label{orientation-2}}

The main directory structure of the SDAC data looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{└──}\NormalTok{ sdac/}
        \ExtensionTok{├──}\NormalTok{ README}
        \ExtensionTok{├──}\NormalTok{ doc/}
        \ExtensionTok{├──}\NormalTok{ sw00utt/}
        \ExtensionTok{├──}\NormalTok{ sw01utt/}
        \ExtensionTok{├──}\NormalTok{ sw02utt/}
        \ExtensionTok{├──}\NormalTok{ sw03utt/}
        \ExtensionTok{├──}\NormalTok{ sw04utt/}
        \ExtensionTok{├──}\NormalTok{ sw05utt/}
        \ExtensionTok{├──}\NormalTok{ sw06utt/}
        \ExtensionTok{├──}\NormalTok{ sw07utt/}
        \ExtensionTok{├──}\NormalTok{ sw08utt/}
        \ExtensionTok{├──}\NormalTok{ sw09utt/}
        \ExtensionTok{├──}\NormalTok{ sw10utt/}
        \ExtensionTok{├──}\NormalTok{ sw11utt/}
        \ExtensionTok{├──}\NormalTok{ sw12utt/}
        \ExtensionTok{└──}\NormalTok{ sw13utt/}
\end{Highlighting}
\end{Shaded}

The \texttt{README} file contains basic information about the resource, the \texttt{doc/} directory contains more detailed information about the dialog annotations, and each of the following directories prefixed with \texttt{sw...} contain individual conversation files. Here's a peek at internal structure of the first couple directories.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{├──}\NormalTok{ README}
\ExtensionTok{├──}\NormalTok{ doc}
\ExtensionTok{│}\NormalTok{   └── manual.august1.html}
\ExtensionTok{├──}\NormalTok{ sw00utt}
\ExtensionTok{│}\NormalTok{   ├── sw\_0001\_4325.utt}
\ExtensionTok{│}\NormalTok{   ├── sw\_0002\_4330.utt}
\ExtensionTok{│}\NormalTok{   ├── sw\_0003\_4103.utt}
\ExtensionTok{│}\NormalTok{   ├── sw\_0004\_4327.utt}
\ExtensionTok{│}\NormalTok{   ├── sw\_0005\_4646.utt}
\end{Highlighting}
\end{Shaded}

Let's take a look at the first conversation file (\texttt{sw\_0001\_4325.utt}) to see how it is structured.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*}
\NormalTok{*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*}
\NormalTok{*x*                                                                     *x*}
\NormalTok{*x*            Copyright (C) 1995 University of Pennsylvania            *x*}
\NormalTok{*x*                                                                     *x*}
\NormalTok{*x*    The data in this file are part of a preliminary version of the   *x*}
\NormalTok{*x*    Penn Treebank Corpus and should not be redistributed.  Any       *x*}
\NormalTok{*x*    research using this corpus or based on it should acknowledge     *x*}
\NormalTok{*x*    that fact, as well as the preliminary nature of the corpus.      *x*}
\NormalTok{*x*                                                                     *x*}
\NormalTok{*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*}
\NormalTok{*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*}


\NormalTok{FILENAME:   4325\_1632\_1519}
\NormalTok{TOPIC\#:     323}
\NormalTok{DATE:       920323}
\NormalTok{TRANSCRIBER:    glp}
\NormalTok{UTT\_CODER:  tc}
\NormalTok{DIFFICULTY: 1}
\NormalTok{TOPICALITY: 3}
\NormalTok{NATURALNESS:    2}
\NormalTok{ECHO\_FROM\_B:    1}
\NormalTok{ECHO\_FROM\_A:    4}
\NormalTok{STATIC\_ON\_A:    1}
\NormalTok{STATIC\_ON\_B:    1}
\NormalTok{BACKGROUND\_A:   1}
\NormalTok{BACKGROUND\_B:   2}
\NormalTok{REMARKS:        None.}

\NormalTok{=========================================================================}
  

\NormalTok{o          A.1 utt1: Okay.  /}
\NormalTok{qw          A.1 utt2: \{D So, \}   }

\NormalTok{qy\^{}d          B.2 utt1: [ [ I guess, +   }

\NormalTok{+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /}
\end{Highlighting}
\end{Shaded}

There are few things to take note of here. First we see that the conversation files have a meta-data header offset from the conversation text by a line of \texttt{=} characters. Second the header contains meta-information of various types. Third, the text is interleaved with an annotation scheme.

Some of the information may be readily understandable, such as the various pieces of meta-data in the header, but to get a better understanding of what information is encoded here let's take a look at the \texttt{README} file. In this file we get a birds eye view of what is going on. In short, the data includes 1155 telephone conversations between two people annotated with 42 `DAMSL' dialog act labels. The \texttt{README} file refers us to the \texttt{doc/manual.august1.html} file for more information on this scheme.

At this point we open the the \texttt{doc/manual.august1.html} file in a browser and do some investigation. We find out that `DAMSL' stands for `Discourse Annotation and Markup System of Labeling' and that the first characters of each line of the conversation text correspond to one or a combination of labels for each utterance. So for our first utterances we have:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{o = "Other"}
\NormalTok{qw = "Wh{-}Question"}
\NormalTok{qy\^{}d = "Declarative Yes{-}No{-}Question"}
\NormalTok{+ = "Segment (multi{-}utterance)"}
\end{Highlighting}
\end{Shaded}

Each utterance is also labeled for speaker (`A' or `B'), speaker turn (`1', `2', `3', etc.), and each utterance within that turn (`utt1', `utt2', etc.). There is other annotation provided withing each utterance, but this should be enough to get us started on the conversations.

Now let's turn to the meta-data in the header. We see here that there is information about the creation of the file: `FILENAME', `TOPIC', `DATE', etc. The \texttt{doc/manual.august1.html} file doesn't have much to say about this information so I returned to the \href{https://catalog.ldc.upenn.edu/docs/LDC97S62/}{LDC Documentation} and found more information in the \href{https://catalog.ldc.upenn.edu/docs/LDC97S62/}{Online Documentation} section. After some poking around in this documentation I discovered that that meta-data for each speaker in the corpus is found in the \texttt{caller\_tab.csv} file. This tabular file does not contain column names, but the \texttt{caller\_doc.txt} does. After inspecting these files manually and comparing them with the information in the conversation file I noticed that the `FILENAME' information contained three pieces of useful information delimited by underscores \texttt{\_}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*}


\NormalTok{FILENAME:   4325\_1632\_1519}
\NormalTok{TOPIC\#:     323}
\NormalTok{DATE:       920323}
\NormalTok{TRANSCRIBER:    glp}
\end{Highlighting}
\end{Shaded}

The first information is the document id (\texttt{4325}), the second and third correspond to the speaker number: the first being speaker A (\texttt{1632}) and the second speaker B (\texttt{1519}).

In sum, we have 1155 conversation files. Each file has two parts, a header and text section, separated by a line of \texttt{=} characters. The header section contains a `FILENAME' line which has the document id, and ids for speaker A and speaker B. The text section is annotated with DAMSL tags beginning each line, followed by speaker, turn number, utterance number, and the utterance text. With this knowledge in hand, let's set out to create a tidy dataset with the following column structure:

\begin{table}

\caption{\label{tab:cd-semi-sdac-idealized-dataset}Idealized structure for the SDAC dataset.}
\centering
\begin{tabular}[t]{lllllll}
\toprule
doc\_id & damsl\_tag & speaker & turn\_num & utterance\_num & utterance\_text & speaker\_id\\
\midrule
4325 & o & A & 1 & 1 & Okay.  / & 1632\\
4325 & qw & A & 1 & 2 & \{D So, \} & 1632\\
4325 & qy\textasciicircum{}d & B & 2 & 1 & {}[ [ I guess, + & 1519\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{tidy-the-data-1}{%
\subsubsection{Tidy the data}\label{tidy-the-data-1}}

Let's begin by reading one of the conversation files into R as a character vector using the \texttt{read\_lines()} function from the readr package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc }\OtherTok{\textless{}{-}} \FunctionTok{read\_lines}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/original/sdac/sw00utt/sw\_0001\_4325.utt"}\NormalTok{)  }\CommentTok{\# read a single file as character vector}
\end{Highlighting}
\end{Shaded}

To isolate the vector element that contains the document and speaker ids, we use \texttt{str\_detect()} from the stringr package. This function takes two arguments, a string and a pattern, and returns a logical value, \texttt{TRUE} if the pattern is matched or \texttt{FALSE} if not. We can use the output of this function, then, to subset the \texttt{doc} character vector and only return the vector element (line) that contains \texttt{digits\_digits\_digits} with a regular expression. The expression combines the digit matching operator \texttt{\textbackslash{}\textbackslash{}d} with the \texttt{+} operator to match 1 or more contiguous digits. We then separate three groups of \texttt{\textbackslash{}\textbackslash{}d+} with underscores \texttt{\_}. The result is \texttt{\textbackslash{}\textbackslash{}d+\_\textbackslash{}\textbackslash{}d+\_\textbackslash{}\textbackslash{}d+}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc[}\FunctionTok{str\_detect}\NormalTok{(doc, }\AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)]  }\CommentTok{\# isolate pattern}
\CommentTok{\#\textgreater{} [1] "FILENAME:\textbackslash{}t4325\_1632\_1519"}
\end{Highlighting}
\end{Shaded}

\begin{rmdtip}
The stringr package has a handy function \texttt{str\_view()} and
\texttt{str\_view\_all()} which allow for interactive pattern matching.
There is also an RStudio Addin with the regexplain package which also
can be very helpful for developing regular expression syntax.
\end{rmdtip}

The next step is to extract the three digit sequences that correspond to the \texttt{doc\_id}, \texttt{speaker\_a\_id}, and \texttt{speaker\_b\_id}. First we extract the pattern that we have identified with \texttt{str\_extract()} and then we can break up the single character vector into multiple parts based on the underscore \texttt{\_}. The \texttt{str\_split()} function takes a string and then a pattern to use to split a character vector. It will return a list of character vectors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc[}\FunctionTok{str\_detect}\NormalTok{(doc, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)] }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# isolate pattern}
  \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract the pattern}
  \FunctionTok{str\_split}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"\_"}\NormalTok{) }\CommentTok{\# split the character vector}
\CommentTok{\#\textgreater{} [[1]]}
\CommentTok{\#\textgreater{} [1] "4325" "1632" "1519"}
\end{Highlighting}
\end{Shaded}

A \textbf{list} is a special object type in R. It is an unordered collection of objects whose lengths can differ (contrast this with a data frame which is a collection of objects whose lengths are the same --hence the tabular format). In this case we have a list of length 1, whose sole element is a character vector of length 3 --one element per segment returned from our split. This is a desired result in most cases as if we were to pass multiple character vectors to our \texttt{str\_split()} function we don't want the results to be conflated as a single character vector blurring the distinction between the individual character vectors. If we \emph{would} like to conflate, or \emph{flatten} a list, we can use the \texttt{unlist()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc[}\FunctionTok{str\_detect}\NormalTok{(doc, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)] }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# isolate pattern}
  \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract the pattern}
  \FunctionTok{str\_split}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"\_"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# split the character vector}
  \FunctionTok{unlist}\NormalTok{() }\CommentTok{\# flatten the list to a character vector}
\CommentTok{\#\textgreater{} [1] "4325" "1632" "1519"}
\end{Highlighting}
\end{Shaded}

Let's flatten the list in this case, as we have a single character vector, and assign this result to \texttt{doc\_speaker\_info}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc\_speaker\_info }\OtherTok{\textless{}{-}} 
\NormalTok{  doc[}\FunctionTok{str\_detect}\NormalTok{(doc, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)] }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# isolate pattern}
  \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract the pattern}
  \FunctionTok{str\_split}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"\_"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# split the character vector}
  \FunctionTok{unlist}\NormalTok{() }\CommentTok{\# flatten the list to a character vector}
\end{Highlighting}
\end{Shaded}

\texttt{doc\_speaker\_info} is now a character vector of length three. Let's subset each of the elements and assign them to meaningful variable names so we can conveniently use them later on in the tidying process.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc\_id }\OtherTok{\textless{}{-}}\NormalTok{ doc\_speaker\_info[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# extract by index}
\NormalTok{speaker\_a\_id }\OtherTok{\textless{}{-}}\NormalTok{ doc\_speaker\_info[}\DecValTok{2}\NormalTok{]  }\CommentTok{\# extract by index}
\NormalTok{speaker\_b\_id }\OtherTok{\textless{}{-}}\NormalTok{ doc\_speaker\_info[}\DecValTok{3}\NormalTok{]  }\CommentTok{\# extract by index}
\end{Highlighting}
\end{Shaded}

The next step is to isolate the text section extracting it from rest of the document. As noted previously, a sequence of \texttt{=} separates the header section from the text section. What we need to do is to index the point in our character vector \texttt{doc} where that line occurs and then subset the \texttt{doc} from that point until the end of the character vector. Let's first find the point where the \texttt{=} sequence occurs. We will again use the \texttt{str\_detect()} function to find the pattern we are looking for (a contiguous sequence of \texttt{=}), but then we will pass the logical result to the \texttt{which()} function which will return the element index number of this match.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{str\_detect}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"=+"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# match 1 or more \textasciigrave{}=\textasciigrave{}}
  \FunctionTok{which}\NormalTok{() }\CommentTok{\# find vector index}
\CommentTok{\#\textgreater{} [1] 31}
\end{Highlighting}
\end{Shaded}

So for this file \texttt{31} is the index in \texttt{doc} where the \texttt{=} sequence occurs. Now it is important to keep in mind that we are working with a single file from the \texttt{sdac/} data. We need to be cautious to not create a pattern that may be matched multiple times in another document in the corpus. As the \texttt{=+} pattern will match \texttt{=}, or \texttt{==}, or \texttt{===}, etc. it is not implausible to believe that there might be a \texttt{=} character on some other line in one of the other files. Let's update our regular expression to avoid this potential scenario by only matching sequences of three or more \texttt{=}. In this case we will make use of the curly bracket operators \texttt{\{\}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{str\_detect}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"=\{3,\}"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# match 3 or more \textasciigrave{}=\textasciigrave{}}
  \FunctionTok{which}\NormalTok{() }\CommentTok{\# find vector index}
\CommentTok{\#\textgreater{} [1] 31}
\end{Highlighting}
\end{Shaded}

We will get the same result for this file, but will safeguard ourselves a bit as it is unlikely we will find multiple matches for \texttt{===}, \texttt{====}, etc.

\texttt{31} is the index for the \texttt{=} sequence, but we want the next line to be where we start reading the text section. To do this we increment the index by 1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text\_start\_index }\OtherTok{\textless{}{-}} 
\NormalTok{  doc }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{str\_detect}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"=\{3,\}"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# match 3 or more \textasciigrave{}=\textasciigrave{} }
  \FunctionTok{which}\NormalTok{() }\CommentTok{\# find vector index}
\NormalTok{text\_start\_index }\OtherTok{\textless{}{-}}\NormalTok{ text\_start\_index }\SpecialCharTok{+} \DecValTok{1} \CommentTok{\# increment index by 1}
\end{Highlighting}
\end{Shaded}

The index for the end of the text is simply the length of the \texttt{doc} vector. We can use the \texttt{length()} function to get this index.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text\_end\_index }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(doc)}
\end{Highlighting}
\end{Shaded}

We now have the bookends, so to speak, for our text section. To extract the text we subset the \texttt{doc} vector by these indices.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text }\OtherTok{\textless{}{-}}\NormalTok{ doc[text\_start\_index}\SpecialCharTok{:}\NormalTok{text\_end\_index]  }\CommentTok{\# extract text}
\FunctionTok{head}\NormalTok{(text)  }\CommentTok{\# preview first lines of \textasciigrave{}text\textasciigrave{}}
\CommentTok{\#\textgreater{} [1] "  "                                       }
\CommentTok{\#\textgreater{} [2] ""                                         }
\CommentTok{\#\textgreater{} [3] "o          A.1 utt1: Okay.  /"            }
\CommentTok{\#\textgreater{} [4] "qw          A.1 utt2: \{D So, \}   "        }
\CommentTok{\#\textgreater{} [5] ""                                         }
\CommentTok{\#\textgreater{} [6] "qy\^{}d          B.2 utt1: [ [ I guess, +   "}
\end{Highlighting}
\end{Shaded}

The text has some extra whitespace on some lines and there are blank lines as well. We should do some cleaning up before moving forward to organize the data. To get rid of the whitespace we use the \texttt{str\_trim()} function which by default will remove leading and trailing whitespace from each line.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{str\_trim}\NormalTok{(text)  }\CommentTok{\# remove leading and trailing whitespace}
\FunctionTok{head}\NormalTok{(text)  }\CommentTok{\# preview first lines of \textasciigrave{}text\textasciigrave{}}
\CommentTok{\#\textgreater{} [1] ""                                      }
\CommentTok{\#\textgreater{} [2] ""                                      }
\CommentTok{\#\textgreater{} [3] "o          A.1 utt1: Okay.  /"         }
\CommentTok{\#\textgreater{} [4] "qw          A.1 utt2: \{D So, \}"        }
\CommentTok{\#\textgreater{} [5] ""                                      }
\CommentTok{\#\textgreater{} [6] "qy\^{}d          B.2 utt1: [ [ I guess, +"}
\end{Highlighting}
\end{Shaded}

To remove blank lines we will use the a logical expression to subset the \texttt{text} vector. \texttt{text\ !=\ ""} means return TRUE where lines are not blank, and FALSE where they are.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text }\OtherTok{\textless{}{-}}\NormalTok{ text[text }\SpecialCharTok{!=} \StringTok{""}\NormalTok{]  }\CommentTok{\# remove blank lines}
\FunctionTok{head}\NormalTok{(text)  }\CommentTok{\# preview first lines of \textasciigrave{}text\textasciigrave{}}
\CommentTok{\#\textgreater{} [1] "o          A.1 utt1: Okay.  /"                                                                  }
\CommentTok{\#\textgreater{} [2] "qw          A.1 utt2: \{D So, \}"                                                                 }
\CommentTok{\#\textgreater{} [3] "qy\^{}d          B.2 utt1: [ [ I guess, +"                                                         }
\CommentTok{\#\textgreater{} [4] "+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /"}
\CommentTok{\#\textgreater{} [5] "+          B.4 utt1: I think, ] + \{F uh, \} I wonder ] if that worked. /"                        }
\CommentTok{\#\textgreater{} [6] "qy          A.5 utt1: Does it say something? /"}
\end{Highlighting}
\end{Shaded}

Our first step towards a tidy dataset is to now combine the \texttt{doc\_id} and each element of \texttt{text} in a data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(doc\_id, text) }\CommentTok{\# tidy format \textasciigrave{}doc\_id\textasciigrave{} and \textasciigrave{}text\textasciigrave{}}
\FunctionTok{slice\_head}\NormalTok{(data, }\AttributeTok{n =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# preview first lines of \textasciigrave{}text\textasciigrave{}}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
               \AttributeTok{caption =} \StringTok{"First 5 observations of prelim data curation of the SDAC data."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:cd-semi-sdac-text-8}First 5 observations of prelim data curation of the SDAC data.}
\centering
\begin{tabular}[t]{ll}
\toprule
doc\_id & text\\
\midrule
4325 & o          A.1 utt1: Okay.  /\\
4325 & qw          A.1 utt2: \{D So, \}\\
4325 & qy\textasciicircum{}d          B.2 utt1: [ [ I guess, +\\
4325 & +          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\\
4325 & +          B.4 utt1: I think, ] + \{F uh, \} I wonder ] if that worked. /\\
\bottomrule
\end{tabular}
\end{table}

With our data now in a data frame, its time to parse the \texttt{text} column and extract the damsl tags, speaker, speaker turn, utterance number, and the utterance text itself into separate columns. To do this we will make extensive use of regular expressions. Our aim is to find a consistent pattern that distinguishes each piece of information from other other text in a given row of \texttt{data\$text} and extract it.

The best way to learn regular expressions is to use them. To this end I've included a link to the interactive regular expression practice website \href{https://regex101.com}{regex101}.

Open this site and copy the text below into the `TEST STRING' field.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{o          A.1 utt1: Okay.  /}
\NormalTok{qw          A.1 utt2: \{D So, \}}
\NormalTok{qy\^{}d          B.2 utt1: [ [ I guess, +}
\NormalTok{+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /}
\NormalTok{+          B.4 utt1: I think, ] + \{F uh, \} I wonder ] if that worked. /}
\NormalTok{qy          A.5 utt1: Does it say something? /}
\NormalTok{sd          B.6 utt1: I think it usually does.  /}
\NormalTok{ad          B.6 utt2: You might try, \{F uh, \}  /}
\NormalTok{h          B.6 utt3: I don\textquotesingle{}t know,  /}
\NormalTok{ad          B.6 utt4: hold it down a little longer,  /}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/07-curate-data/cd-regex-101} 

}

\caption{RegEx101.}\label{fig:cd-regex-101-image}
\end{figure}

Now manually type the following regular expressions into the `REGULAR EXPRESSION' field one-by-one (each is on a separate line). Notice what is matched as you type and when you've finished typing. You can find out exactly what the component parts of each expression are doing by toggling the top right icon in the window or hovering your mouse over the relevant parts of the expression.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\^{}.+?\textbackslash{}s}
\NormalTok{[AB]\textbackslash{}.\textbackslash{}d+}
\NormalTok{utt\textbackslash{}d+}
\NormalTok{:.+$}
\end{Highlighting}
\end{Shaded}

As you can now see, we have regular expressions that will match the damsl tags, speaker and speaker turn, utterance number, and the utterance text. To apply these expressions to our data and extract this information into separate columns we will make use of the \texttt{mutate()} and \texttt{str\_extract()} functions. \texttt{mutate()} will take our data frame and create new columns with values we match and extract from each row in the data frame with \texttt{str\_extract()}. Notice that \texttt{str\_extract()} is different than \texttt{str\_extract\_all()}. When we work with \texttt{mutate()} each row will be evaluated in turn, therefore we only need to make one match per row in \texttt{data\$text}.

I've chained each of these steps in the code below, dropping the original \texttt{text} column with \texttt{select(-text)}, and overwriting \texttt{data} with the results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract column information from \textasciigrave{}text\textasciigrave{}}
\NormalTok{data }\OtherTok{\textless{}{-}} 
\NormalTok{  data }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# current dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{damsl\_tag =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{"\^{}.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# extract damsl tags}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{speaker\_turn =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{"[AB]}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract speaker\_turn pairs}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_num =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{"utt}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract utterance number}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_text =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{":.+$"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# extract utterance text}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{text) }\CommentTok{\# drop the \textasciigrave{}text\textasciigrave{} column}

\FunctionTok{glimpse}\NormalTok{(data) }\CommentTok{\# preview the data set}
\CommentTok{\#\textgreater{} Rows: 159}
\CommentTok{\#\textgreater{} Columns: 5}
\CommentTok{\#\textgreater{} $ doc\_id         \textless{}chr\textgreater{} "4325", "4325", "4325", "4325", "4325", "4325", "4325",\textasciitilde{}}
\CommentTok{\#\textgreater{} $ damsl\_tag      \textless{}chr\textgreater{} "o ", "qw ", "qy\^{}d ", "+ ", "+ ", "qy ", "sd ", "ad ", \textasciitilde{}}
\CommentTok{\#\textgreater{} $ speaker\_turn   \textless{}chr\textgreater{} "A.1", "A.1", "B.2", "A.3", "B.4", "A.5", "B.6", "B.6",\textasciitilde{}}
\CommentTok{\#\textgreater{} $ utterance\_num  \textless{}chr\textgreater{} "utt1", "utt2", "utt1", "utt1", "utt1", "utt1", "utt1",\textasciitilde{}}
\CommentTok{\#\textgreater{} $ utterance\_text \textless{}chr\textgreater{} ": Okay.  /", ": \{D So, \}", ": [ [ I guess, +", ": What\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

\begin{rmdwarning}
One twist you will notice is that regular expressions in R require
double backslashes
(\texttt{\textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}})
where other programming environments use a single backslash
(\texttt{\textbackslash{}\textbackslash{}}).
\end{rmdwarning}

There are a couple things left to do to the columns we extracted from the text before we move on to finishing up our tidy dataset. First, we need to separate the \texttt{speaker\_turn} column into \texttt{speaker} and \texttt{turn\_num} columns and second we need to remove unwanted characters from the \texttt{damsl\_tag}, \texttt{utterance\_num}, and \texttt{utterance\_text} columns.

To separate the values of a column into two columns we use the \texttt{separate()} function. It takes a column to separate and character vector of the names of the new columns to create. By default the values of the input column will be separated by non-alphanumeric characters. In our case this means the \texttt{.} will be our separator.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}}
\NormalTok{  data }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# current dataset}
  \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ speaker\_turn, }\CommentTok{\# source column}
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"speaker"}\NormalTok{, }\StringTok{"turn\_num"}\NormalTok{)) }\CommentTok{\# separate speaker\_turn into distinct columns: speaker and turn\_num}

\FunctionTok{glimpse}\NormalTok{(data) }\CommentTok{\# preview the data set}
\CommentTok{\#\textgreater{} Rows: 159}
\CommentTok{\#\textgreater{} Columns: 6}
\CommentTok{\#\textgreater{} $ doc\_id         \textless{}chr\textgreater{} "4325", "4325", "4325", "4325", "4325", "4325", "4325",\textasciitilde{}}
\CommentTok{\#\textgreater{} $ damsl\_tag      \textless{}chr\textgreater{} "o ", "qw ", "qy\^{}d ", "+ ", "+ ", "qy ", "sd ", "ad ", \textasciitilde{}}
\CommentTok{\#\textgreater{} $ speaker        \textless{}chr\textgreater{} "A", "A", "B", "A", "B", "A", "B", "B", "B", "B", "B", \textasciitilde{}}
\CommentTok{\#\textgreater{} $ turn\_num       \textless{}chr\textgreater{} "1", "1", "2", "3", "4", "5", "6", "6", "6", "6", "6", \textasciitilde{}}
\CommentTok{\#\textgreater{} $ utterance\_num  \textless{}chr\textgreater{} "utt1", "utt2", "utt1", "utt1", "utt1", "utt1", "utt1",\textasciitilde{}}
\CommentTok{\#\textgreater{} $ utterance\_text \textless{}chr\textgreater{} ": Okay.  /", ": \{D So, \}", ": [ [ I guess, +", ": What\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

To remove unwanted leading or trailing whitespace we apply the \texttt{str\_trim()} function. For removing other characters we matching the character(s) and replace them with an empty string (\texttt{""}) with the \texttt{str\_replace()} function. Again, I've chained these functions together and overwritten \texttt{data} with the results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Clean up column information}
\NormalTok{data }\OtherTok{\textless{}{-}} 
\NormalTok{  data }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# current dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{damsl\_tag =} \FunctionTok{str\_trim}\NormalTok{(damsl\_tag)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# remove leading/ trailing whitespace}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_num =} \FunctionTok{str\_replace}\NormalTok{(}\AttributeTok{string =}\NormalTok{ utterance\_num, }\AttributeTok{pattern =} \StringTok{"utt"}\NormalTok{, }\AttributeTok{replacement =} \StringTok{""}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# remove \textquotesingle{}utt\textquotesingle{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_text =} \FunctionTok{str\_replace}\NormalTok{(}\AttributeTok{string =}\NormalTok{ utterance\_text, }\AttributeTok{pattern =} \StringTok{":}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{, }\AttributeTok{replacement =} \StringTok{""}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# remove \textquotesingle{}: \textquotesingle{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_text =} \FunctionTok{str\_trim}\NormalTok{(utterance\_text)) }\CommentTok{\# trim leading/ trailing whitespace}

\FunctionTok{glimpse}\NormalTok{(data) }\CommentTok{\# preview the data set}
\CommentTok{\#\textgreater{} Rows: 159}
\CommentTok{\#\textgreater{} Columns: 6}
\CommentTok{\#\textgreater{} $ doc\_id         \textless{}chr\textgreater{} "4325", "4325", "4325", "4325", "4325", "4325", "4325",\textasciitilde{}}
\CommentTok{\#\textgreater{} $ damsl\_tag      \textless{}chr\textgreater{} "o", "qw", "qy\^{}d", "+", "+", "qy", "sd", "ad", "h", "ad\textasciitilde{}}
\CommentTok{\#\textgreater{} $ speaker        \textless{}chr\textgreater{} "A", "A", "B", "A", "B", "A", "B", "B", "B", "B", "B", \textasciitilde{}}
\CommentTok{\#\textgreater{} $ turn\_num       \textless{}chr\textgreater{} "1", "1", "2", "3", "4", "5", "6", "6", "6", "6", "6", \textasciitilde{}}
\CommentTok{\#\textgreater{} $ utterance\_num  \textless{}chr\textgreater{} "1", "2", "1", "1", "1", "1", "1", "2", "3", "4", "5", \textasciitilde{}}
\CommentTok{\#\textgreater{} $ utterance\_text \textless{}chr\textgreater{} "Okay.  /", "\{D So, \}", "[ [ I guess, +", "What kind of\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

To round out our tidy dataset for this single conversation file we will connect the \texttt{speaker\_a\_id} and \texttt{speaker\_b\_id} with speaker A and B in our current dataset adding a new column \texttt{speaker\_id}. The \texttt{case\_when()} function does exactly this: allows us to map rows of \texttt{speaker} with the value ``A'' to \texttt{speaker\_a\_id} and rows with value ``B'' to \texttt{speaker\_b\_id}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Link speaker with speaker\_id}
\NormalTok{data }\OtherTok{\textless{}{-}} 
\NormalTok{  data }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# current dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{speaker\_id =} \FunctionTok{case\_when}\NormalTok{( }\CommentTok{\# create speaker\_id}
\NormalTok{    speaker }\SpecialCharTok{==} \StringTok{"A"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ speaker\_a\_id, }\CommentTok{\# speaker\_a\_id value when A}
\NormalTok{    speaker }\SpecialCharTok{==} \StringTok{"B"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ speaker\_b\_id }\CommentTok{\# speaker\_b\_id value when B}
\NormalTok{  ))}

\FunctionTok{glimpse}\NormalTok{(data) }\CommentTok{\# preview the data set}
\CommentTok{\#\textgreater{} Rows: 159}
\CommentTok{\#\textgreater{} Columns: 7}
\CommentTok{\#\textgreater{} $ doc\_id         \textless{}chr\textgreater{} "4325", "4325", "4325", "4325", "4325", "4325", "4325",\textasciitilde{}}
\CommentTok{\#\textgreater{} $ damsl\_tag      \textless{}chr\textgreater{} "o", "qw", "qy\^{}d", "+", "+", "qy", "sd", "ad", "h", "ad\textasciitilde{}}
\CommentTok{\#\textgreater{} $ speaker        \textless{}chr\textgreater{} "A", "A", "B", "A", "B", "A", "B", "B", "B", "B", "B", \textasciitilde{}}
\CommentTok{\#\textgreater{} $ turn\_num       \textless{}chr\textgreater{} "1", "1", "2", "3", "4", "5", "6", "6", "6", "6", "6", \textasciitilde{}}
\CommentTok{\#\textgreater{} $ utterance\_num  \textless{}chr\textgreater{} "1", "2", "1", "1", "1", "1", "1", "2", "3", "4", "5", \textasciitilde{}}
\CommentTok{\#\textgreater{} $ utterance\_text \textless{}chr\textgreater{} "Okay.  /", "\{D So, \}", "[ [ I guess, +", "What kind of\textasciitilde{}}
\CommentTok{\#\textgreater{} $ speaker\_id     \textless{}chr\textgreater{} "1632", "1632", "1519", "1632", "1519", "1632", "1519",\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

We now have the tidy dataset we set out to create. But this dataset only includes one conversation file! We want to apply this code to all 1155 conversation files in the \texttt{sdac/} corpus. The approach will be to create a custom function which groups the code we've done for this single file and then iterative send each file from the corpus through this function and combine the results into one data frame.

Here's the custom function with some extra code to print a progress message for each file when it runs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extract\_sdac\_metadata }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(file) \{}
  \CommentTok{\# Function: to read a Switchboard Corpus Dialogue file and extract meta{-}data}
  \FunctionTok{cat}\NormalTok{(}\StringTok{"Reading"}\NormalTok{, }\FunctionTok{basename}\NormalTok{(file), }\StringTok{"..."}\NormalTok{)}
  
  \CommentTok{\# Read \textasciigrave{}file\textasciigrave{} by lines}
\NormalTok{  doc }\OtherTok{\textless{}{-}} \FunctionTok{read\_lines}\NormalTok{(file) }
  
  \CommentTok{\# Extract \textasciigrave{}doc\_id\textasciigrave{}, \textasciigrave{}speaker\_a\_id\textasciigrave{}, and \textasciigrave{}speaker\_b\_id\textasciigrave{}}
\NormalTok{  doc\_speaker\_info }\OtherTok{\textless{}{-}} 
\NormalTok{    doc[}\FunctionTok{str\_detect}\NormalTok{(doc, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)] }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# isolate pattern}
    \FunctionTok{str\_extract}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract the pattern}
    \FunctionTok{str\_split}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"\_"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# split the character vector}
    \FunctionTok{unlist}\NormalTok{() }\CommentTok{\# flatten the list to a character vector}
\NormalTok{  doc\_id }\OtherTok{\textless{}{-}}\NormalTok{ doc\_speaker\_info[}\DecValTok{1}\NormalTok{] }\CommentTok{\# extract \textasciigrave{}doc\_id\textasciigrave{}}
\NormalTok{  speaker\_a\_id }\OtherTok{\textless{}{-}}\NormalTok{ doc\_speaker\_info[}\DecValTok{2}\NormalTok{] }\CommentTok{\# extract \textasciigrave{}speaker\_a\_id\textasciigrave{}}
\NormalTok{  speaker\_b\_id }\OtherTok{\textless{}{-}}\NormalTok{ doc\_speaker\_info[}\DecValTok{3}\NormalTok{] }\CommentTok{\# extract \textasciigrave{}speaker\_b\_id\textasciigrave{}}
  
  \CommentTok{\# Extract \textasciigrave{}text\textasciigrave{}}
\NormalTok{  text\_start\_index }\OtherTok{\textless{}{-}} \CommentTok{\# find where header info stops}
\NormalTok{    doc }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{str\_detect}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"=\{3,\}"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# match 3 or more \textasciigrave{}=\textasciigrave{}}
    \FunctionTok{which}\NormalTok{() }\CommentTok{\# find vector index}
  
\NormalTok{  text\_start\_index }\OtherTok{\textless{}{-}}\NormalTok{ text\_start\_index }\SpecialCharTok{+} \DecValTok{1} \CommentTok{\# increment index by 1}
\NormalTok{  text\_end\_index }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(doc) }\CommentTok{\# get the end of the text section}
  
\NormalTok{  text }\OtherTok{\textless{}{-}}\NormalTok{ doc[text\_start\_index}\SpecialCharTok{:}\NormalTok{text\_end\_index] }\CommentTok{\# extract text}
\NormalTok{  text }\OtherTok{\textless{}{-}} \FunctionTok{str\_trim}\NormalTok{(text) }\CommentTok{\# remove leading and trailing whitespace}
\NormalTok{  text }\OtherTok{\textless{}{-}}\NormalTok{ text[text }\SpecialCharTok{!=} \StringTok{""}\NormalTok{] }\CommentTok{\# remove blank lines}
  
\NormalTok{  data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(doc\_id, text) }\CommentTok{\# tidy format \textasciigrave{}doc\_id\textasciigrave{} and \textasciigrave{}text\textasciigrave{}}
  
  \CommentTok{\# Extract column information from \textasciigrave{}text\textasciigrave{}}
\NormalTok{  data }\OtherTok{\textless{}{-}} 
\NormalTok{    data }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{damsl\_tag =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{"\^{}.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# extract damsl tags}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{speaker\_turn =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{"[AB]}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract speaker\_turn pairs}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_num =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{"utt}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract utterance number}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_text =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{":.+$"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# extract utterance text}
    \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{text)}
  
  \CommentTok{\# Separate speaker\_turn into distinct columns}
\NormalTok{  data }\OtherTok{\textless{}{-}}
\NormalTok{    data }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# current dataset}
    \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ speaker\_turn, }\CommentTok{\# source column}
             \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"speaker"}\NormalTok{, }\StringTok{"turn\_num"}\NormalTok{)) }\CommentTok{\# separate speaker\_turn into distinct columns: speaker and turn\_num}
  
  \CommentTok{\# Clean up column information}
\NormalTok{  data }\OtherTok{\textless{}{-}} 
\NormalTok{    data }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{damsl\_tag =} \FunctionTok{str\_trim}\NormalTok{(damsl\_tag)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# remove leading/ trailing whitespace}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_num =} \FunctionTok{str\_replace}\NormalTok{(}\AttributeTok{string =}\NormalTok{ utterance\_num, }\AttributeTok{pattern =} \StringTok{"utt"}\NormalTok{, }\AttributeTok{replacement =} \StringTok{""}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# remove \textquotesingle{}utt\textquotesingle{}}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_text =} \FunctionTok{str\_replace}\NormalTok{(}\AttributeTok{string =}\NormalTok{ utterance\_text, }\AttributeTok{pattern =} \StringTok{":}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{, }\AttributeTok{replacement =} \StringTok{""}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# remove \textquotesingle{}: \textquotesingle{}}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_text =} \FunctionTok{str\_trim}\NormalTok{(utterance\_text)) }\CommentTok{\# trim leading/ trailing whitespace}
  
  \CommentTok{\# Link speaker with speaker\_id}
\NormalTok{  data }\OtherTok{\textless{}{-}} 
\NormalTok{    data }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# current dataset}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{speaker\_id =} \FunctionTok{case\_when}\NormalTok{( }\CommentTok{\# create speaker\_id}
\NormalTok{      speaker }\SpecialCharTok{==} \StringTok{"A"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ speaker\_a\_id, }\CommentTok{\# speaker\_a\_id value when A}
\NormalTok{      speaker }\SpecialCharTok{==} \StringTok{"B"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ speaker\_b\_id }\CommentTok{\# speaker\_b\_id value when B}
\NormalTok{    ))}
  \FunctionTok{cat}\NormalTok{(}\StringTok{" done.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(data) }\CommentTok{\# return the data frame object}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

As a sanity check we will run the \texttt{extract\_sdac\_metadata()} function on a the conversation file we were just working on to make sure it works as expected.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{extract\_sdac\_metadata}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/original/sdac/sw00utt/sw\_0001\_4325.utt"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Reading sw_0001_4325.utt ... done.
#> Rows: 159
#> Columns: 7
#> $ doc_id         <chr> "4325", "4325", "4325", "4325", "4325", "4325", "4325",~
#> $ damsl_tag      <chr> "o", "qw", "qy^d", "+", "+", "qy", "sd", "ad", "h", "ad~
#> $ speaker        <chr> "A", "A", "B", "A", "B", "A", "B", "B", "B", "B", "B", ~
#> $ turn_num       <chr> "1", "1", "2", "3", "4", "5", "6", "6", "6", "6", "6", ~
#> $ utterance_num  <chr> "1", "2", "1", "1", "1", "1", "1", "2", "3", "4", "5", ~
#> $ utterance_text <chr> "Okay.  /", "{D So, }", "[ [ I guess, +", "What kind of~
#> $ speaker_id     <chr> "1632", "1632", "1519", "1632", "1519", "1632", "1519",~
\end{verbatim}

Looks good!

So now it's time to create a vector with the paths to all of the conversation files. \texttt{fs::dir\_ls()} interfaces with our OS file system and will return the paths to the files in the specified directory. We also add a pattern to match conversation files (\texttt{regexp\ =\ \textbackslash{}\textbackslash{}.utt\$}) so we don't accidentally include other files in the corpus. \texttt{recurse} set to \texttt{TRUE} means we will get the full path to each file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_files }\OtherTok{\textless{}{-}} 
\NormalTok{  fs}\SpecialCharTok{::}\FunctionTok{dir\_ls}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/original/sdac/"}\NormalTok{, }\CommentTok{\# source directory}
             \AttributeTok{recurse =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# traverse all sub{-}directories}
             \AttributeTok{type =} \StringTok{"file"}\NormalTok{, }\CommentTok{\# only return files}
             \AttributeTok{regexp =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.utt$"}\NormalTok{) }\CommentTok{\# only return files ending in .utt}
\FunctionTok{head}\NormalTok{(sdac\_files) }\CommentTok{\# preview file paths}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{../data/original/sdac/sw00utt/sw\_0001\_4325.utt}
\NormalTok{../data/original/sdac/sw00utt/sw\_0002\_4330.utt}
\NormalTok{../data/original/sdac/sw00utt/sw\_0003\_4103.utt}
\NormalTok{../data/original/sdac/sw00utt/sw\_0004\_4327.utt}
\NormalTok{../data/original/sdac/sw00utt/sw\_0005\_4646.utt}
\NormalTok{../data/original/sdac/sw00utt/sw\_0006\_4108.utt}
\end{Highlighting}
\end{Shaded}

To pass each conversation file in the vector of paths to our conversation files iteratively to the \texttt{extract\_sdac\_metadata()} function we use \texttt{map()}. This will apply the function to each conversation file and return a data frame for each. \texttt{bind\_rows()} will then join the resulting data frames by rows to give us a single tidy dataset for all 1155 conversations. Note there is a lot of processing going on here we have to be patient.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read files and return a tidy dataset}
\NormalTok{sdac }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_files }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# pass file names}
  \FunctionTok{map}\NormalTok{(extract\_sdac\_metadata) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# read and tidy iteratively }
  \FunctionTok{bind\_rows}\NormalTok{() }\CommentTok{\# bind the results into a single data frame}
\end{Highlighting}
\end{Shaded}

We now see that we have 223606 observations (individual utterances in this dataset).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(sdac)  }\CommentTok{\# preview complete curated dataset}
\CommentTok{\#\textgreater{} Rows: 223,606}
\CommentTok{\#\textgreater{} Columns: 7}
\CommentTok{\#\textgreater{} $ doc\_id         \textless{}chr\textgreater{} "4325", "4325", "4325", "4325", "4325", "4325", "4325",\textasciitilde{}}
\CommentTok{\#\textgreater{} $ damsl\_tag      \textless{}chr\textgreater{} "o", "qw", "qy\^{}d", "+", "+", "qy", "sd", "ad", "h", "ad\textasciitilde{}}
\CommentTok{\#\textgreater{} $ speaker        \textless{}chr\textgreater{} "A", "A", "B", "A", "B", "A", "B", "B", "B", "B", "B", \textasciitilde{}}
\CommentTok{\#\textgreater{} $ turn\_num       \textless{}chr\textgreater{} "1", "1", "2", "3", "4", "5", "6", "6", "6", "6", "6", \textasciitilde{}}
\CommentTok{\#\textgreater{} $ utterance\_num  \textless{}chr\textgreater{} "1", "2", "1", "1", "1", "1", "1", "2", "3", "4", "5", \textasciitilde{}}
\CommentTok{\#\textgreater{} $ utterance\_text \textless{}chr\textgreater{} "Okay.  /", "\{D So, \}", "[ [ I guess, +", "What kind of\textasciitilde{}}
\CommentTok{\#\textgreater{} $ speaker\_id     \textless{}chr\textgreater{} "1632", "1632", "1519", "1632", "1519", "1632", "1519",\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{write-datasets}{%
\subsubsection{Write datasets}\label{write-datasets}}

Again as in the previous cases, we will write this dataset to disk to prepare for the next step in our text analysis project.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fs}\SpecialCharTok{::}\FunctionTok{dir\_create}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/derived/sdac/"}\NormalTok{)  }\CommentTok{\# create sdac subdirectory}
\FunctionTok{write\_csv}\NormalTok{(sdac, }\AttributeTok{file =} \StringTok{"../data/derived/sdac/sdac\_curated.csv"}\NormalTok{)  }\CommentTok{\# write sdac to disk and label as the curated dataset}
\end{Highlighting}
\end{Shaded}

The directory structure now looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{│}\NormalTok{   └── sdac/}
\ExtensionTok{│}\NormalTok{       └── sdac\_curated.csv}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{└──}\NormalTok{ sdac/}
        \ExtensionTok{├──}\NormalTok{ README}
        \ExtensionTok{├──}\NormalTok{ doc/}
        \ExtensionTok{├──}\NormalTok{ sw00utt/}
        \ExtensionTok{├──}\NormalTok{ sw01utt/}
        \ExtensionTok{├──}\NormalTok{ sw02utt/}
        \ExtensionTok{├──}\NormalTok{ sw03utt/}
        \ExtensionTok{├──}\NormalTok{ sw04utt/}
        \ExtensionTok{├──}\NormalTok{ sw05utt/}
        \ExtensionTok{├──}\NormalTok{ sw06utt/}
        \ExtensionTok{├──}\NormalTok{ sw07utt/}
        \ExtensionTok{├──}\NormalTok{ sw08utt/}
        \ExtensionTok{├──}\NormalTok{ sw09utt/}
        \ExtensionTok{├──}\NormalTok{ sw10utt/}
        \ExtensionTok{├──}\NormalTok{ sw11utt/}
        \ExtensionTok{├──}\NormalTok{ sw12utt/}
        \ExtensionTok{└──}\NormalTok{ sw13utt/}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-7}{%
\subsubsection{Summary}\label{summary-7}}

In this section we looked at semi-structured data. This type of data often requires the most work to organize into a tidy dataset. We continued to work with many of the R programming strategies introduced to this point in the coursebook. We also made more extensive use of regular expressions to pick out information from a semi-structured document format.

To round out this section I've provided a code summary of the steps involved to conduct the curation of the Switchboard Dialogue Act Corpus files. Note that I've added the \texttt{extract\_sdac\_metadata()} custom function to a file called \texttt{curate\_functions.R} and sourced this file. This will make the code more succinct and legible here, as well in your own research projects.

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Source the \textasciigrave{}extract\_sdac\_metadata()\textasciigrave{} function}
\FunctionTok{source}\NormalTok{(}\StringTok{"../functions/curate\_functions.R"}\NormalTok{) }

\CommentTok{\# Get list of the corpus files (.utt)}
\NormalTok{sdac\_files }\OtherTok{\textless{}{-}} 
\NormalTok{  fs}\SpecialCharTok{::}\FunctionTok{dir\_ls}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/original/sdac/"}\NormalTok{, }\CommentTok{\# source directory}
             \AttributeTok{recurse =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# traverse all sub{-}directories}
             \AttributeTok{type =} \StringTok{"file"}\NormalTok{, }\CommentTok{\# only return files}
             \AttributeTok{regexp =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.utt$"}\NormalTok{) }\CommentTok{\# only return files ending in .utt}

\CommentTok{\# Read files and return a tidy dataset}
\NormalTok{sdac }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_files }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# pass file names}
  \FunctionTok{map}\NormalTok{(extract\_sdac\_metadata) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# read and tidy iteratively }
  \FunctionTok{bind\_rows}\NormalTok{() }\CommentTok{\# bind the results into a single data frame}
  
\CommentTok{\# Write curated dataset to disk}
\NormalTok{fs}\SpecialCharTok{::}\FunctionTok{dir\_create}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/derived/sdac/"}\NormalTok{) }\CommentTok{\# create sdac subdirectory}
\FunctionTok{write\_csv}\NormalTok{(sdac, }
          \AttributeTok{file =} \StringTok{"../data/derived/sdac/sdac\_curated.csv"}\NormalTok{) }\CommentTok{\# write sdac to disk and label as the curated dataset}
\end{Highlighting}
\end{Shaded}

\hypertarget{documentation-2}{%
\subsection{Documentation}\label{documentation-2}}

At this stage we again want to ensure that the data that we have derived is well-documented. Where in the data acquisition process the documentation was focused on the sampling frame, curated datasets require documentation that describes the structure of the now rectangular dataset and its attributes. This documentation is known as a \textbf{data dictionary}. At the curation stage this documentation often contains the following information \citep{HowMakeData10-2021}:

\begin{itemize}
\tightlist
\item
  names of the variables (as they appear in the dataset)
\item
  human-readable names for the variables
\item
  short prose descriptions of the variables, including units of measurement (where applicable)
\end{itemize}

A data dictionary will take the format of a table and can be stored in a tabular-oriented file format (such as .csv). It is often easier to work with a spreadsheet to create this documentation. I suggest creating a .csv file with the basic structure of the documentation. You can do this however you choose, but I suggest using something along these lines as seen in the following custom function, \texttt{data\_dic\_starter()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_dic\_starter }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, file\_path) \{}
  \CommentTok{\# Function:}
  \CommentTok{\# Creates a .csv file with the basic information}
  \CommentTok{\# to document a curated dataset}
  
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{variable\_name =} \FunctionTok{names}\NormalTok{(data), }\CommentTok{\# column with existing variable names }
       \AttributeTok{name =} \StringTok{""}\NormalTok{, }\CommentTok{\# column for human{-}readable names}
       \AttributeTok{description =} \StringTok{""}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# column for prose description}
  \FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{file =}\NormalTok{ file\_path) }\CommentTok{\# write to disk}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Running this function in the R Console on the curated dataset (in this case the \texttt{sdac} dataset), will provide this structure.

\begin{table}

\caption{\label{tab:cd-documentation-dic-starter-structure}Data dictionary starter structure for the SDAC curated dataset.}
\centering
\begin{tabular}[t]{lll}
\toprule
variable\_name & name & description\\
\midrule
doc\_id &  & \\
damsl\_tag &  & \\
speaker &  & \\
turn\_num &  & \\
utterance\_num &  & \\
\addlinespace
utterance\_text &  & \\
speaker\_id &  & \\
\bottomrule
\end{tabular}
\end{table}

The resulting .csv file can then be opened with spreadsheet software (such as MS Excel, Google Sheets, etc.) and edited.\footnote{Note on RStudio Cloud you will need to download the .csv file and, after editing, upload the complete data dictionary file. Make sure to save the edited file as a .csv file.}

\begin{center}\includegraphics[width=0.9\linewidth]{images/07-curate-data/cd-data-dictionary} \end{center}

Save this file as a .csv file and replace the original starter file. Note that it is important to use a plain-text file format for the official documentation file and avoid proprietary formats to ensure open accessibility and future compatibility.\footnote{Although based on spreadsheets, \citet{Broman2018} outlines many of the best for good data organization regardless of the technology.}

Our \texttt{data/derived/} directory now looks like this.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{└──}\NormalTok{ derived/}
    \ExtensionTok{└──}\NormalTok{ sdac/}
        \ExtensionTok{├──}\NormalTok{ sdac\_curated.csv}
        \ExtensionTok{└──}\NormalTok{ data\_dictionary\_sdac.csv}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-8}{%
\subsection{Summary}\label{summary-8}}

In this chapter we looked at the process of structuring data into a dataset. This included a discussion on three main types of data --unstructured, structured, and semi-structured. The level of structure of the original data(set) will vary from resource to resource and by the same token so will the file format used to support the level of meta-information included. The results from our data curation resulted in a curated dataset that is saved separate from the original data to maintain modularity between what the data(set) looked like before we intervene and afterwards. In addition to the code we use to derived the curated dataset's structure, we also include a data dictionary which documents the names of the variables and provides sufficient description of these variables so that it is clear what our dataset contains.

It is important to recognized that this curated dataset will form the base for the next step in our text analysis project and the last step in data preparation for analysis: dataset transformation. This last step in preparing data for analysis is to convert this curated dataset into a dataset that is directly aligned with the research aims (i.e.~analysis method(s)) of the project. Since there can be multiple analysis approaches applied the original data in a research project, this curated dataset serves as the point of departure for each of the subsequent datasets derived from the transformational steps.

\hypertarget{transform-data}{%
\section{Transform data}\label{transform-data}}

DRAFT

\begin{quote}
Nothing is lost. Everything is transformed.

--- Michael Ende, The Neverending Story
\end{quote}

\begin{rmdkey}
The essential questions for this chapter are:

\begin{itemize}
\tightlist
\item
  What is the role of data transformation in a text analysis project?
\item
  What are the general processes for preparing datasets for analysis?
\item
  How do each of these general processes transform datasets?
\end{itemize}
\end{rmdkey}

In this chapter we turn out attention to the process of moving a curated dataset one step closer to analysis. Where in the process of curating data into a dataset the goal was to derived a tidy dataset that contained the main relational characteristics of the data for our text analysis project, the transformation step refines and potential expands these characteristics such that they are more in line with our analysis aims. In this chapter I have grouped various transformation steps into four categories: normalization, recoding, generation, and merging. It is of note that the these categories have been ordered and are covered separately for descriptive reasons. In practice the ordering of which transformation to apply before another is highly idiosyncratic and requires that the researcher evaluate the characteristics of the dataset and the desired results.

Furthermore, since in any given project there may be more than one analysis that may be performed on the data, there may be distinct transformation steps which correspond to each analysis approach. Therefore it is possible that there are more than one transformed dataset created from the curated dataset. This is one of the reasons that we create a curated dataset instead of derived a transformed dataset from the original data. The curated dataset serves as a point of departure from which multiple transformational methods can derive distinct formats for distinct analyses.

Let's now turn to demonstrations of some common transformational steps using datasets with which we are now familiar

\hypertarget{normalize}{%
\subsection{Normalize}\label{normalize}}

The process of normalizing datasets in essence is to santize the values of variable or set of variables such that there are no artifacts that will contaminate subsequent processing. It may be the case that non-linguistic metadata may require normalization but more often than not linguistic information is the most common target for normalization as text often includes artifacts from the acquisition process which will not be desired in the analysis.

\textbf{Europarle Corpus}

Consider the curated Europarle Corpus dataset. I will read in the dataset. Since the dataset is quite large, I have also subsetted the dataset keeping only the first 1,000 observations for each of value of \texttt{type} for demonstration purposes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/derived/europarle/europarle\_curated.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# read curated dataset}
  \FunctionTok{filter}\NormalTok{(sentence\_id }\SpecialCharTok{\textless{}} \DecValTok{1001}\NormalTok{) }\CommentTok{\# keep first 1000 observations for each type}

\FunctionTok{glimpse}\NormalTok{(europarle)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 2,000
#> Columns: 3
#> $ type        <chr> "Source", "Target", "Source", "Target", "Source", "Target"~
#> $ sentence_id <dbl> 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, ~
#> $ sentence    <chr> "Reanudación del período de sesiones", "Resumption of the ~
\end{verbatim}

Simply looking at the first 14 lines of this dataset, we can see that if our goal is to work with the transcribed (`Source') and translated (`Target') language, there are lines which do not appear to be of interest.

\begin{table}

\caption{\label{tab:td-europarle-preview-1}Europarle Corpus curated dataset preview.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 1 & Reanudación del período de sesiones\\
Target & 1 & Resumption of the session\\
Source & 2 & Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\\
Target & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Source & 3 & Como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\
\addlinespace
Target & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Source & 4 & Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\
Target & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
Source & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\\
Target & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
\addlinespace
Source & 6 & Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
Target & 6 & Please rise, then, for this minute' s silence.\\
Source & 7 & (El Parlamento, de pie, guarda un minuto de silencio)\\
Target & 7 & (The House rose and observed a minute' s silence)\\
\bottomrule
\end{tabular}
\end{table}

\texttt{sentence\_id} 1 appears to be title and \texttt{sentence\_id} 7 reflects description of the parliamentary session. Both of these are artifacts that we would like to remove from the dataset.

To remove these lines we can turn to the programming strategies we've previously worked with. Namely we will use \texttt{filter()} to filter observations in combination with \texttt{str\_detect()} to detect matches for some pattern that is indicative of these lines that we want to remove and not of the other lines that we want to keep.

Before we remove any lines, let's try craft a search pattern to identify these lines, and exclude the lines we will want to keep. Condition one is lines which start with an opening parenthesis \texttt{(}. Condition two is lines that do not end in standard sentence punctuation (\texttt{.}, \texttt{!}, or \texttt{?}). I've added both conditions to one \texttt{filter()} using the logical \emph{OR} operator (\texttt{\textbar{}}) to ensure that either condition is matched in the output.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Identify non{-}speech lines}
\NormalTok{europarle }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(sentence, }\StringTok{"\^{}}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{("}\NormalTok{) }\SpecialCharTok{|} \FunctionTok{str\_detect}\NormalTok{(sentence, }\StringTok{"[\^{}.!?]$"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# filter lines that detect a match for either condition 1 or 2}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# random sample of 10 observations}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{caption =} \StringTok{\textquotesingle{}Non{-}speech lines in the Europarle dataset.\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:td-europarle-search-non-speech}Non-speech lines in the Europarle dataset.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 7 & (El Parlamento, de pie, guarda un minuto de silencio)\\
Target & 220 & Transport of dangerous goods by road\\
Target & 499 & Social and economic situation and development of the regions of the Union\\
Source & 93 & (El Parlamento rechaza la petición) El Presidente.\\
Target & 670 & (The Minutes were approved)\\
\addlinespace
Target & 7 & (The House rose and observed a minute' s silence)\\
Source & 69 & Miércoles :\\
Target & 638 & (The sitting was closed at 8.25 p.m.)\\
Target & 85 & (Applause from the PSE Group)\\
Target & 109 & (Parliament rejected the request, with 164 votes for, 166 votes against and 7 abstentions)\\
\bottomrule
\end{tabular}
\end{table}

Since this search appears to match lines that we do not want to preserve, let's move now to eliminate these lines from the dataset. To do this we will use the same regular expression patterns, but now each condition will have it's own \texttt{filter()} call and the \texttt{str\_detect()} will be negated with a prefixed \texttt{!}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{str\_detect}\NormalTok{(sentence, }\AttributeTok{pattern =} \StringTok{"\^{}}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{("}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# remove lines starting with (}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{str\_detect}\NormalTok{(sentence, }\AttributeTok{pattern =} \StringTok{"[\^{}.!?]$"}\NormalTok{)) }\CommentTok{\# remove lines not ending in ., !, or ?}
\end{Highlighting}
\end{Shaded}

Let's look at the first 14 lines again, now that we have eliminated these artifacts.

\begin{table}

\caption{\label{tab:td-europarle-preview-2}Europarle Corpus non-speech lines removed.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 2 & Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\\
Target & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Source & 3 & Como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\
Target & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Source & 4 & Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\
\addlinespace
Target & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
Source & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\\
Target & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Source & 6 & Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
Target & 6 & Please rise, then, for this minute' s silence.\\
\addlinespace
Source & 8 & Señora Presidenta, una cuestión de procedimiento.\\
Target & 8 & Madam President, on a point of order.\\
Source & 9 & Sabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en Sri Lanka.\\
Target & 9 & You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\\
\bottomrule
\end{tabular}
\end{table}

One further issue that we may want to resolve concerns the fact that there are whitespaces between possessive forms (i.e.~``minute' s silence''). In this case we can employ \texttt{str\_replace\_all()} inside the \texttt{mutate()} function to overwrite the \texttt{sentence} values that match an apostrophe \texttt{\textquotesingle{}} with whitespace (\texttt{\textbackslash{}\textbackslash{}s}) before \texttt{s}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{sentence =} \FunctionTok{str\_replace\_all}\NormalTok{(}\AttributeTok{string =}\NormalTok{ sentence, }
                                    \AttributeTok{pattern =} \StringTok{"\textquotesingle{}}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{ss"}\NormalTok{, }
                                    \AttributeTok{replacement =} \StringTok{"\textquotesingle{}s"}\NormalTok{)) }\CommentTok{\# replace \textquotesingle{} s with \textasciigrave{}s}
\end{Highlighting}
\end{Shaded}

Now we have normalized text in the \texttt{sentence} column in the Europarle dataset.

\textbf{Last FM Lyrics}

Let's look at another dataset we have worked with during this coursebook: the Lastfm lyrics. Reading in the \texttt{lastfm\_curated} dataset from the \texttt{data/derived/} directory we can see the structure for the curated structure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/derived/lastfm/lastfm\_curated.csv"}\NormalTok{)  }\CommentTok{\# read in lastfm\_curated dataset}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:td-lastfm-read-preview}Last fm lyrics dataset preview with one artist/ song per genre and the `lyrics` text truncated  at 200 characters for display purposes.}
\centering
\begin{tabular}[t]{llll}
\toprule
artist & song & lyrics & genre\\
\midrule
Alan Jackson & Little Bitty & Have a little love on a little honeymoonYou got a little dish and you got a little spoonA little bitty house and a little bitty yardA little bitty dog and a little bitty car Well, it's alright to b... & country\\
50 Cent & In Da Club & Go, go, go, go, go, goGo, shortyIt's your birthdayWe gon' party like it's your birthdayWe gon' sip Bacardi like it's your birthdayAnd you know we don't give a fuck it's not your birthday You can fi... & hip-hop\\
Black Sabbath & Paranoid & Finished with my woman'Cause she couldn't help me with my mindPeople think I'm insaneBecause I am frowning all the time All day long, I think of thingsBut nothing seems to satisfyThink I'll lose my... & metal\\
a-ha & Take On Me & Talking awayI don't know whatWhat to sayI'll say it anywayToday is another day to find youShying awayOh, I'll be coming for your love, okay? Take On Me (Take On Me)Take me on (Take On Me)I'll be go... & pop\\
3 Doors Down & Here Without You & A hundred days have made me olderSince the last time that I saw your pretty faceA thousand lies have made me colderAnd I don't think I can look at this the same But all the miles that separateDisap... & rock\\
\bottomrule
\end{tabular}
\end{table}

There are a few things that we might want to clean out of the \texttt{lyrics} column's values. First, there are lines from the original webscrape where the end of one stanza runs into the next without whitespace between them (i.e.~``honeymoonYou''). These reflect contiguous end-new line segments where stanzas were joined in the curation process. Second, we see that there are what appear to be backing vocals which appear between parentheses (i.e.~``(Take On Me)'').

In both cases we will use \texttt{mutate()}. With contiguous end-new line segments we will use \texttt{str\_replace\_all()} inside and for backing vocals in parentheses we will use \texttt{str\_remove\_all()}.

The pattern to match for end-new lines from the stanzas will use some regular expression magic. The base pattern includes finding a pair of lowercase-uppercase letters (i.e.~``nY'', in ``honeymoo\textbf{nY}ou''). For this we can use the pattern \texttt{{[}a-z{]}{[}A-Z{]}}. To replace this pattern using the lowercase letter then a space and then the uppercase letter we take advantage of the grouping syntax in regular expressions \texttt{(...)}. So we add parentheses around the two groups to capture like this \texttt{({[}a-z{]})({[}A-Z{]})}. In the replacement argument of the \texttt{str\_replace\_all()} function we then specify to use the captured groups in the order they appear \texttt{\textbackslash{}\textbackslash{}1} for the lowercase letter match and \texttt{\textbackslash{}\textbackslash{}2} for the uppercase letter match.

Now, I've looked more extensively at the \texttt{lyrics} column and found that there are other combinations that are joined between stanzas. Namely that \texttt{\textquotesingle{}}, \texttt{!}, \texttt{,}, \texttt{)}, \texttt{?}, and \texttt{I} also may precede the uppercase letter. To make sure we capture these possibilities as well I've updated the regular expression to \texttt{({[}a-z\textquotesingle{}!,.)?I{]})({[}A-Z{]})}.

Now to remove the backing vocals, the regex pattern is \texttt{\textbackslash{}\textbackslash{}(.+?\textbackslash{}\textbackslash{})} --match the parentheses and everything within the parentheses. The added \texttt{?} after the \texttt{+} operator is what is known as a `lazy' operator. This specifies that the \texttt{.+} will match the minimal string that is enclosed by the trailing \texttt{)}. If we did not include this then we would get matches that span from the first parenthesis \texttt{(} all the way to the last, which would match real lyrics, not just the backing vocals.

Putting this to work let's clean the \texttt{lyrics} column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lyrics =} 
           \FunctionTok{str\_replace\_all}\NormalTok{(}\AttributeTok{string =}\NormalTok{ lyrics, }
                           \AttributeTok{pattern =} \StringTok{"([a{-}z\textquotesingle{}!,.)?I])([A{-}Z])"}\NormalTok{, }\CommentTok{\# find contiguous end/ new line segments}
                           \AttributeTok{replacement =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{1 }\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{2"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# replace with whitespace between}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lyrics =} \FunctionTok{str\_remove\_all}\NormalTok{(lyrics, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{(.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{)) }\CommentTok{\# remove backing vocals (Take On Me)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:td-lastfm-clean-end-lines-preview}Last fm lyrics with cleaned lyrics...}
\centering
\begin{tabular}[t]{llll}
\toprule
artist & song & lyrics & genre\\
\midrule
Alan Jackson & Little Bitty & Have a little love on a little honeymoon You got a little dish and you got a little spoon A little bitty house and a little bitty yard A little bitty dog and a little bitty car Well, it's alright t... & country\\
50 Cent & In Da Club & Go, go, go, go, go, go Go, shorty It's your birthday We gon' party like it's your birthday We gon' sip Bacardi like it's your birthday And you know we don't give a fuck it's not your birthday You c... & hip-hop\\
Black Sabbath & Paranoid & Finished with my woman' Cause she couldn't help me with my mind People think I'm insane Because I am frowning all the time All day long, I think of things But nothing seems to satisfy Think I'll lo... & metal\\
a-ha & Take On Me & Talking away I don't know what What to say I'll say it anyway Today is another day to find you Shying away Oh, I'll be coming for your love, okay? Take On Me  Take me on  I'll be gone In a day or t... & pop\\
3 Doors Down & Here Without You & A hundred days have made me older Since the last time that I saw your pretty face A thousand lies have made me colder And I don't think I can look at this the same But all the miles that separate D... & rock\\
\bottomrule
\end{tabular}
\end{table}

Now given the fact that songs are poems, there are many lines that are not complete sentences so there is no practical way to try to segment these into grammatical sentence units. So in this case, this seems like a good stopping point for normalizing the lastfm dataset.

\hypertarget{recode}{%
\subsection{Recode}\label{recode}}

Normalizing text can be seen as an extension of dataset curation to some extent in that the structure of the dataset is maintained. In both the Europarle and Lastfm cases we saw this to be true. In the case of recoding, and other transformational steps, the aim will be to modify the dataset structure either by rows, columns, or both. Recoding processes can be characterized by the creation of structural changes which are derived from values in variables effectively recasting values as new variables to enable more direct access in our analyses.

\textbf{Switchboard Dialogue Act Corpus}

The Switchboard Dialogue Act Corpus dataset that was curated in the previous chapter contains a number of variables describing conversations between speakers of American English.

Let's read in this dataset and take a closer look.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/derived/sdac/sdac\_curated.csv"}\NormalTok{)  }\CommentTok{\# read curated dataset}
\end{Highlighting}
\end{Shaded}

Among a number of metadata variables, curated dataset includes the \texttt{utterance\_text} column which contains dialogue from the conversations interleaved with a \href{https://staff.fnwi.uva.nl/r.fernandezrovira/teaching/DM-materials/DFL-book.pdf}{disfluency annotation scheme}.

\begin{table}

\caption{\label{tab:td-sdac-preview-curated-dataset}20 randomly sampled lines of the SDAC curated dataset.}
\centering
\begin{tabular}[t]{rllrrlr}
\toprule
doc\_id & damsl\_tag & speaker & turn\_num & utterance\_num & utterance\_text & speaker\_id\\
\midrule
2650 & sd & B & 108 & 2 & cats are really almost more territorial [ than, + than ]  dogs  are.  / & 1251\\
3073 & b & B & 18 & 1 & Uh-huh. / & 1293\\
2770 & b & B & 3 & 1 & Yeah, / & 1059\\
2131 & sd & B & 177 & 1 & {}[ I've, + I've  ] never seen a Gene Autry movie, / & 1126\\
2235 & qo & B & 62 & 1 & \{D Well, \} what about, \{F uh, \} \{F uh, \} sending  all this money overseas supporting other governments, third world governments.  / & 1052\\
\addlinespace
4799 & sd & A & 77 & 5 & \{C and \} [ sh-, + she's ] doing really well.  / & 1423\\
2349 & + & B & 5 & 1 & for, \{F uh, \} young people  [ to be, + to go ] into public service.  / & 1027\\
3526 & sd & B & 93 & 3 & I was amazed I was able to get this four eighty-six for six hundred even with a trade in,  / & 1325\\
2999 & sd & B & 122 & 3 & you have to pay for it right away anyway. / & 1235\\
2342 & sv & B & 19 & 5 & they want to keep something over there, so that they don't have to, \{F um, \} - / & 1028\\
\addlinespace
3051 & b\textasciicircum{}m & A & 58 & 1 & Sixty, eighty. / & 1229\\
2275 & sd & A & 71 & 1 & -- \{C but \} [ they, + they ] \{D like \} started it,  / & 1115\\
3495 & sd & A & 2 & 1 & \{D Well, \} [ I'm in a, +  I'm in an ]  urban area.  / & 1345\\
2131 & bh & B & 111 & 1 & Was it? / & 1126\\
4339 & sv & A & 27 & 3 & \{C and \} [ it, + that ] seems to be just fine.  / & 1539\\
\addlinespace
2830 & nn & B & 198 & 1 & Huh-uh. / & 1238\\
3952 & + & A & 65 & 1 & -- \{D you know, \}  / & 1481\\
3054 & sd & B & 50 & 1 & {}[ It's not, + it's not  ] very deep. / & 1258\\
2913 & b & A & 91 & 2 & yeah,  / & 1236\\
3331 & b & B & 32 & 1 & Uh-huh. / & 1004\\
\bottomrule
\end{tabular}
\end{table}

Let's drop a few variables from our dataset to rein in our focus. I will keep the \texttt{doc\_id}, \texttt{speaker\_id}, and \texttt{utterance\_text}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_simplified }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(doc\_id, speaker\_id, utterance\_text) }\CommentTok{\# columns to retain}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:td-sdac-simple-preview}First 10 lines of the simplified SDAC curated dataset.}
\centering
\begin{tabular}[t]{rrl}
\toprule
doc\_id & speaker\_id & utterance\_text\\
\midrule
4325 & 1632 & Okay.  /\\
4325 & 1632 & \{D So, \}\\
4325 & 1519 & {}[ [ I guess, +\\
4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? /\\
4325 & 1519 & I think, ] + \{F uh, \} I wonder ] if that worked. /\\
\addlinespace
4325 & 1632 & Does it say something? /\\
4325 & 1519 & I think it usually does.  /\\
4325 & 1519 & You might try, \{F uh, \}  /\\
4325 & 1519 & I don't know,  /\\
4325 & 1519 & hold it down a little longer,  /\\
\bottomrule
\end{tabular}
\end{table}

In this disfluency annotation system, there are various conventions used for non-sentence elements. If say, for example, a researcher were to be interested in understanding the use of filled pauses (`uh' or `uh'), the aim would be to identify those lines where the \texttt{\{F\ ...\}} annotation is used around the utterances `uh' and `um'.

To do this we turn to the \texttt{str\_count()} function. This function will count the number of matches found for a pattern. We can use a regular expression to identify the pattern of interest which is all the instances of \texttt{\{F} followed by either \texttt{uh} or \texttt{um}. Since the disfluencies may start an utterance, and therefore be capitalized we need to formulate a regular expression which allows for either \texttt{U} or \texttt{u} for each disfluency type. The result from each disfluency match will be added to a new column. To create a new column we will wrap each \texttt{str\_count()} with \texttt{mutate()} and give the new column a meaningful name. In this case I've opted for \texttt{uh} and \texttt{um}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_simplified }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{uh =} \FunctionTok{str\_count}\NormalTok{(utterance\_text, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\{F [Uu]h"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# match \{F Uh or \{F uh\}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{um =} \FunctionTok{str\_count}\NormalTok{(utterance\_text, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\{F [Uu]m"}\NormalTok{)) }\CommentTok{\# match \{F Um or \{F um\}}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:td-sdac-count-disfluencies-show}First 20 lines of SDAC dataset with counts for the disfluencies 'uh' and 'um'.}
\centering
\begin{tabular}[t]{rrlrr}
\toprule
doc\_id & speaker\_id & utterance\_text & uh & um\\
\midrule
4325 & 1632 & Okay.  / & 0 & 0\\
4325 & 1632 & \{D So, \} & 0 & 0\\
4325 & 1519 & {}[ [ I guess, + & 0 & 0\\
4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? / & 0 & 0\\
4325 & 1519 & I think, ] + \{F uh, \} I wonder ] if that worked. / & 1 & 0\\
\addlinespace
4325 & 1632 & Does it say something? / & 0 & 0\\
4325 & 1519 & I think it usually does.  / & 0 & 0\\
4325 & 1519 & You might try, \{F uh, \}  / & 1 & 0\\
4325 & 1519 & I don't know,  / & 0 & 0\\
4325 & 1519 & hold it down a little longer,  / & 0 & 0\\
\addlinespace
4325 & 1519 & \{C and \} see if it, \{F uh, \} -/ & 1 & 0\\
4325 & 1632 & Okay <beep>.  / & 0 & 0\\
4325 & 1632 & <<long pause>> \{D Well, \} & 0 & 0\\
4325 & 1519 & Okay  / & 0 & 0\\
4325 & 1519 & {}[ I, + & 0 & 0\\
\addlinespace
4325 & 1632 & Does it usually make a recording or s-, / & 0 & 0\\
4325 & 1519 & \{D Well, \} I ] don't remember.  / & 0 & 0\\
4325 & 1519 & It seemed like it did,  / & 0 & 0\\
4325 & 1519 & \{C but \} <laughter> it might not.  / & 0 & 0\\
4325 & 1519 & {}[ I guess + -- & 0 & 0\\
\bottomrule
\end{tabular}
\end{table}

Now we have two new columns, \texttt{uh} and \texttt{um} which indicate how many times the relevant pattern was matched for a given utterance. By choosing to focus on disfluencies, however, we have made a decision to change the unit of observation from the utterance to the use of filled pauses (\texttt{uh} and \texttt{um}). This means that as the dataset stands, it is not in tidy format --where each observation corresponds to the observational unit. When datasets are misaligned in this particular way, there are in what is known as `wide' format. What we want to do, then, is to restructure our dataset such that each row corresponds to the unit of observation --in this case each filled pause type.

To convert our current (wide) dataset to one where each filler type is listed and the counts are measured for each utterance we turn to the \texttt{pivot\_longer()} function. This function creates two new columns, one in which the column names are listed and one for the values for each of the column names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"uh"}\NormalTok{, }\StringTok{"um"}\NormalTok{), }\CommentTok{\# columns to convert}
               \AttributeTok{names\_to =} \StringTok{"filler"}\NormalTok{, }\CommentTok{\# column for the column names (i.e. filler types)}
               \AttributeTok{values\_to =} \StringTok{"count"}\NormalTok{) }\CommentTok{\# column for the column values (i.e. counts)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:td-sdac-count-disfluencies-longer-show}First 20 lines of SDAC dataset with tidy format for `fillers` as the unit of observation.}
\centering
\begin{tabular}[t]{rrllr}
\toprule
doc\_id & speaker\_id & utterance\_text & filler & count\\
\midrule
4325 & 1632 & Okay.  / & uh & 0\\
4325 & 1632 & Okay.  / & um & 0\\
4325 & 1632 & \{D So, \} & uh & 0\\
4325 & 1632 & \{D So, \} & um & 0\\
4325 & 1519 & {}[ [ I guess, + & uh & 0\\
\addlinespace
4325 & 1519 & {}[ [ I guess, + & um & 0\\
4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? / & uh & 0\\
4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? / & um & 0\\
4325 & 1519 & I think, ] + \{F uh, \} I wonder ] if that worked. / & uh & 1\\
4325 & 1519 & I think, ] + \{F uh, \} I wonder ] if that worked. / & um & 0\\
\addlinespace
4325 & 1632 & Does it say something? / & uh & 0\\
4325 & 1632 & Does it say something? / & um & 0\\
4325 & 1519 & I think it usually does.  / & uh & 0\\
4325 & 1519 & I think it usually does.  / & um & 0\\
4325 & 1519 & You might try, \{F uh, \}  / & uh & 1\\
\addlinespace
4325 & 1519 & You might try, \{F uh, \}  / & um & 0\\
4325 & 1519 & I don't know,  / & uh & 0\\
4325 & 1519 & I don't know,  / & um & 0\\
4325 & 1519 & hold it down a little longer,  / & uh & 0\\
4325 & 1519 & hold it down a little longer,  / & um & 0\\
\bottomrule
\end{tabular}
\end{table}

\textbf{Last fm}

In the previous example, we used a matching approach to extract information embedded in one column of the dataset and recoded the dataset to maintain the fidelity between the particular unit of observation and the other metadata.

Another common approach for recoding datasets in text analysis projects involves recoding linguistic units as smaller units; a process known as tokenization.

Let's return to the \texttt{lastfm} object we normalized earlier in the chapter to see the various ways one can choose to tokenize linguistic information.

\begin{table}

\caption{\label{tab:td-lastfm-clean-end-lines-preview-2}Last fm dataset with normalized lyrics.}
\centering
\begin{tabular}[t]{llll}
\toprule
artist & song & lyrics & genre\\
\midrule
Alan Jackson & Little Bitty & Have a little love on a little honeymoon You got a little dish and you got a little spoon A little bitty house and a little bitty yard A little bitty dog and a little bitty car Well, it's alright t... & country\\
50 Cent & In Da Club & Go, go, go, go, go, go Go, shorty It's your birthday We gon' party like it's your birthday We gon' sip Bacardi like it's your birthday And you know we don't give a fuck it's not your birthday You c... & hip-hop\\
Black Sabbath & Paranoid & Finished with my woman' Cause she couldn't help me with my mind People think I'm insane Because I am frowning all the time All day long, I think of things But nothing seems to satisfy Think I'll lo... & metal\\
a-ha & Take On Me & Talking away I don't know what What to say I'll say it anyway Today is another day to find you Shying away Oh, I'll be coming for your love, okay? Take On Me  Take me on  I'll be gone In a day or t... & pop\\
3 Doors Down & Here Without You & A hundred days have made me older Since the last time that I saw your pretty face A thousand lies have made me colder And I don't think I can look at this the same But all the miles that separate D... & rock\\
\bottomrule
\end{tabular}
\end{table}

In the current \texttt{lastfm} dataset, the unit of observation is the lyrics for the entire artist, song, and genre combination. If, however, we would like to change the unit to say words, we would like each word used to appear on its own row, while still maintaining the other relevant attributes associated with each word.

The tidytext package includes a very useful function \texttt{unnest\_tokens()} which allows us to tokenize some textual input into smaller linguistic units. The `unnest' part of the the function name refers to the process of extracting the unit of interest while maintaining the other relevant attributes. Let's see this in action.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{output =}\NormalTok{ word, }\CommentTok{\# column for tokenized output}
                \AttributeTok{input =}\NormalTok{ lyrics, }\CommentTok{\# input column}
                \AttributeTok{token =} \StringTok{"words"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# tokenize unit type}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# preview first 10 lines}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{caption =} \StringTok{"First 10 observations for lastfm dataset tokenized by words."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:td-lastfm-tokenize-words}First 10 observations for lastfm dataset tokenized by words.}
\centering
\begin{tabular}[t]{llll}
\toprule
artist & song & genre & word\\
\midrule
Alan Jackson & Little Bitty & country & have\\
Alan Jackson & Little Bitty & country & a\\
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & love\\
Alan Jackson & Little Bitty & country & on\\
\addlinespace
Alan Jackson & Little Bitty & country & a\\
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & honeymoon\\
Alan Jackson & Little Bitty & country & you\\
Alan Jackson & Little Bitty & country & got\\
\bottomrule
\end{tabular}
\end{table}

We can see from the output, each word appears on a separate line in the order of appearance in the input text (\texttt{lyrics}). Furthermore, the output is in tidy format as each of the words is still associated with the relevant attribute values (\texttt{artist}, \texttt{song}, and \texttt{genre}). By default the tokenized text output is lowercased and the original text input column is dropped. These can be overridden, however, if desired.

In addition to `words', the \texttt{unnest\_tokens()} function provides easy access to a number of common tokenized units including `characters', `sentences', and `paragraphs'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{output =}\NormalTok{ character, }\CommentTok{\# column for tokenized output}
                \AttributeTok{input =}\NormalTok{ lyrics, }\CommentTok{\# input column}
                \AttributeTok{token =} \StringTok{"characters"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# tokenize unit type}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# preview first 10 lines}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{caption =} \StringTok{"First 10 observations for lastfm dataset tokenized by characters."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:td-lastfm-tokenize-characters}First 10 observations for lastfm dataset tokenized by characters.}
\centering
\begin{tabular}[t]{llll}
\toprule
artist & song & genre & character\\
\midrule
Alan Jackson & Little Bitty & country & h\\
Alan Jackson & Little Bitty & country & a\\
Alan Jackson & Little Bitty & country & v\\
Alan Jackson & Little Bitty & country & e\\
Alan Jackson & Little Bitty & country & a\\
\addlinespace
Alan Jackson & Little Bitty & country & l\\
Alan Jackson & Little Bitty & country & i\\
Alan Jackson & Little Bitty & country & t\\
Alan Jackson & Little Bitty & country & t\\
Alan Jackson & Little Bitty & country & l\\
\bottomrule
\end{tabular}
\end{table}

The other two built-in options `sentences' and `paragraphs' depend on punctuation and/ or line breaks to function, so in this particular dataset, these options will not work given the particular characteristics of the \texttt{lyrics} variable.

There are even other options which allow for the creation of sequences of linguistic units. Say we want to tokenize our lyrics into two-word sequences, we can specify the \texttt{token} as `ngrams' and then add the argument \texttt{n\ =\ 2} to reflect we want two-word sequences.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{output =}\NormalTok{ bigram, }\CommentTok{\# column for tokenized output}
                \AttributeTok{input =}\NormalTok{ lyrics, }\CommentTok{\# input column}
                \AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{, }\CommentTok{\# tokenize unit type}
                \AttributeTok{n =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# size of word sequences }
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# preview first 10 lines}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{caption =} \StringTok{"First 10 observations for lastfm dataset tokenized by bigrams"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:td-lastfm-tokenize-bigrams}First 10 observations for lastfm dataset tokenized by bigrams}
\centering
\begin{tabular}[t]{llll}
\toprule
artist & song & genre & bigram\\
\midrule
Alan Jackson & Little Bitty & country & have a\\
Alan Jackson & Little Bitty & country & a little\\
Alan Jackson & Little Bitty & country & little love\\
Alan Jackson & Little Bitty & country & love on\\
Alan Jackson & Little Bitty & country & on a\\
\addlinespace
Alan Jackson & Little Bitty & country & a little\\
Alan Jackson & Little Bitty & country & little honeymoon\\
Alan Jackson & Little Bitty & country & honeymoon you\\
Alan Jackson & Little Bitty & country & you got\\
Alan Jackson & Little Bitty & country & got a\\
\bottomrule
\end{tabular}
\end{table}

The `n' in `ngram' refers to the number of word-sequence units we want to tokenize. Two-word sequences are known as `bigrams', three-word sequences `trigrams', and so on.

\hypertarget{generate}{%
\subsection{Generate}\label{generate}}

In the process of recoding a dataset the transformation of the dataset works with information that is already explicit. The process of generation, however, aims to make implicit information explicit. The most common type of operation involved in the generation process is the addition of linguistic annotation. This process can be accomplished manually by a researcher or research team or automatically through the use of pre-trained linguistic resources and/ or software. Ideally the annotation of linguistic information can be conducted automatically.

There are important considerations, however, that need to be taken into account when choosing whether linguistic annotation can be conducted automatically. First and foremost has to do with the type of annotation desired. Information such as part of speech (grammatical category) and morpho-syntactic information are the the most common types of linguistic annotation that can be conducted automatically. Second the degree to which the resource that will be used to annotate the linguistic information is aligned with the language variety and/or register is also a key consideration. As noted, automatic linguistic annotation methods are contingent on pre-trained resources. The language and language variety used to develop these resources may not be available for the language under investigation, or if it does, the language variety and/ or register may not align. The degree to which a resource does not align with the linguistic information targeted for annotation is directly related to the quality of the final annotations. To be clear, no annotation method, whether manual or automatic is guaranteed to be perfectly accurate.

Let's take a look at annotation some of the language from the Europarle dataset we normalized.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(type }\SpecialCharTok{==} \StringTok{"Target"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{caption =} \StringTok{"First 10 lines in English from the normalized SDAC dataset."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-3}First 10 lines in English from the normalized SDAC dataset.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Target & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Target & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Target & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
Target & 5 & In the meantime, I should like to observe a minute's silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Target & 6 & Please rise, then, for this minute's silence.\\
\addlinespace
Target & 8 & Madam President, on a point of order.\\
Target & 9 & You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\\
Target & 10 & One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\\
Target & 11 & Would it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament's regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\\
Target & 12 & Yes, Mr Evans, I feel an initiative of the type you have just suggested would be entirely appropriate.\\
\bottomrule
\end{tabular}
\end{table}

We will use the cleanNLP package to do our linguistic annotation. The annotation process depends on the pre-trained language models. There is \href{https://github.com/bnosac/udpipe\#pre-trained-models}{a list of the models available to access}. The \texttt{load\_model\_udpipe()} custom function below downloads the specified language model and initialized the \texttt{udpipe} engine (\texttt{cnlp\_init\_udpipe()}) for conducting annotations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{load\_model\_udpipe }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(model\_lang) \{}
  \CommentTok{\# Function}
  \CommentTok{\# Download and load the specified udpipe language model}
  
  \FunctionTok{cnlp\_init\_udpipe}\NormalTok{(model\_lang) }\CommentTok{\# to download the model, if not downloaded}
\NormalTok{base\_path }\OtherTok{\textless{}{-}} \FunctionTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{, }\AttributeTok{package =} \StringTok{"cleanNLP"}\NormalTok{) }\CommentTok{\# get the base path}
\NormalTok{  model\_name }\OtherTok{\textless{}{-}} \CommentTok{\# extract the model\_name}
\NormalTok{    base\_path }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract the base path}
    \FunctionTok{dir}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get the directory}
\NormalTok{    stringr}\SpecialCharTok{::}\FunctionTok{str\_subset}\NormalTok{(}\AttributeTok{pattern =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"\^{}"}\NormalTok{, model\_lang)) }\CommentTok{\# extract the name of the model}
  
\NormalTok{  udpipe}\SpecialCharTok{::}\FunctionTok{udpipe\_load\_model}\NormalTok{(}\AttributeTok{file =} \FunctionTok{file.path}\NormalTok{(base\_path, model\_name, }\AttributeTok{fsep =} \StringTok{"/"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# create the path to the downloaded model stored on disk}
    \FunctionTok{return}\NormalTok{()}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

In a test case, let's load the `english' model to annotate a sentence line from the Europarle dataset to illustrate the basic workflow.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eng\_model }\OtherTok{\textless{}{-}} \FunctionTok{load\_model\_udpipe}\NormalTok{(}\StringTok{"english"}\NormalTok{) }\CommentTok{\# load and initialize the language model, \textquotesingle{}english\textquotesingle{} in this case.}

\NormalTok{eng\_annotation }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset }
  \FunctionTok{filter}\NormalTok{(type }\SpecialCharTok{==} \StringTok{"Target"} \SpecialCharTok{\&}\NormalTok{ sentence\_id }\SpecialCharTok{==} \DecValTok{6}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# select English and sentence\_id 6}
  \FunctionTok{cnlp\_annotate}\NormalTok{(}\AttributeTok{text\_name =} \StringTok{"sentence"}\NormalTok{, }\CommentTok{\# input text (sentence)}
                \AttributeTok{doc\_name =} \StringTok{"sentence\_id"}\NormalTok{) }\CommentTok{\# specify the grouping column (sentence\_id)}

\FunctionTok{glimpse}\NormalTok{(eng\_annotation) }\CommentTok{\# preview structure}
\CommentTok{\#\textgreater{} List of 2}
\CommentTok{\#\textgreater{}  $ token   : tibble [11 x 11] (S3: tbl\_df/tbl/data.frame)}
\CommentTok{\#\textgreater{}   ..$ doc\_id       : num [1:11] 6 6 6 6 6 6 6 6 6 6 ...}
\CommentTok{\#\textgreater{}   ..$ sid          : int [1:11] 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{\#\textgreater{}   ..$ tid          : chr [1:11] "1" "2" "3" "4" ...}
\CommentTok{\#\textgreater{}   ..$ token        : chr [1:11] "Please" "rise" "," "then" ...}
\CommentTok{\#\textgreater{}   ..$ token\_with\_ws: chr [1:11] "Please " "rise" ", " "then" ...}
\CommentTok{\#\textgreater{}   ..$ lemma        : chr [1:11] "please" "rise" "," "then" ...}
\CommentTok{\#\textgreater{}   ..$ upos         : chr [1:11] "INTJ" "VERB" "PUNCT" "ADV" ...}
\CommentTok{\#\textgreater{}   ..$ xpos         : chr [1:11] "UH" "VB" "," "RB" ...}
\CommentTok{\#\textgreater{}   ..$ feats        : chr [1:11] NA "Mood=Imp|VerbForm=Fin" NA "PronType=Dem" ...}
\CommentTok{\#\textgreater{}   ..$ tid\_source   : chr [1:11] "2" "0" "2" "10" ...}
\CommentTok{\#\textgreater{}   ..$ relation     : chr [1:11] "discourse" "root" "punct" "advmod" ...}
\CommentTok{\#\textgreater{}  $ document: tibble [1 x 2] (S3: tbl\_df/tbl/data.frame)}
\CommentTok{\#\textgreater{}   ..$ type  : chr "Target"}
\CommentTok{\#\textgreater{}   ..$ doc\_id: num 6}
\CommentTok{\#\textgreater{}  {-} attr(*, "class")= chr [1:2] "cnlp\_annotation" "list"}
\end{Highlighting}
\end{Shaded}

We see that the structure returned by the \texttt{cnlp\_annotate()} function is a list. This list contains two data frames (tibbles). One for the tokens (and there annotation information) and the document (the metadata information). We can inspect the annotation characteristics for this one sentence by targetting the \texttt{\$tokens} data frame. Let's take a look at the linguistic annotation information returned.

\begin{table}

\caption{\label{tab:td-generation-test-annotation-english}Annotation information for a single English sentence from the Europarle dataset.}
\centering
\begin{tabular}[t]{rrlllllllll}
\toprule
doc\_id & sid & tid & token & token\_with\_ws & lemma & upos & xpos & feats & tid\_source & relation\\
\midrule
6 & 1 & 1 & Please & Please & please & INTJ & UH & NA & 2 & discourse\\
6 & 1 & 2 & rise & rise & rise & VERB & VB & Mood=Imp|VerbForm=Fin & 0 & root\\
6 & 1 & 3 & , & , & , & PUNCT & , & NA & 2 & punct\\
6 & 1 & 4 & then & then & then & ADV & RB & PronType=Dem & 10 & advmod\\
6 & 1 & 5 & , & , & , & PUNCT & , & NA & 10 & punct\\
\addlinespace
6 & 1 & 6 & for & for & for & ADP & IN & NA & 10 & case\\
6 & 1 & 7 & this & this & this & DET & DT & Number=Sing|PronType=Dem & 8 & det\\
6 & 1 & 8 & minute & minute & minute & NOUN & NN & Number=Sing & 10 & nmod:poss\\
6 & 1 & 9 & 's & 's & 's & PART & POS & NA & 8 & case\\
6 & 1 & 10 & silence & silence & silence & NOUN & NN & Number=Sing & 2 & conj\\
\addlinespace
6 & 1 & 11 & . & . & . & PUNCT & . & NA & 2 & punct\\
\bottomrule
\end{tabular}
\end{table}

There is quite a bit of information which is returned from \texttt{cnlp\_annotate()}. First note that the input sentence has been tokenized by word. Each token includes the token, lemma, part of speech (\texttt{upos} and \texttt{xpos}), morphological features (\texttt{feats}), and syntactic relationships (\texttt{tid\_source} and \texttt{relation}). It is also key to note that the \texttt{doc\_id}, \texttt{sid} and \texttt{tid} maintain the relational attributes from the original dataset --and therefore maintains our annotated dataset in tidy format.

Let's now annotate the same sentence from the Europarle corpus for the Source (`Spanish') and note the similarities and differences.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spa\_model }\OtherTok{\textless{}{-}} \FunctionTok{load\_model\_udpipe}\NormalTok{(}\StringTok{"spanish"}\NormalTok{) }\CommentTok{\# load and initialize the language model, \textquotesingle{}spanish\textquotesingle{} in this case.}

\NormalTok{spa\_annotation }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset }
  \FunctionTok{filter}\NormalTok{(type }\SpecialCharTok{==} \StringTok{"Source"} \SpecialCharTok{\&}\NormalTok{ sentence\_id }\SpecialCharTok{==} \DecValTok{6}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# select Spanish and sentence\_id 6}
  \FunctionTok{cnlp\_annotate}\NormalTok{(}\AttributeTok{text\_name =} \StringTok{"sentence"}\NormalTok{, }\CommentTok{\# input text (sentence)}
                \AttributeTok{doc\_name =} \StringTok{"sentence\_id"}\NormalTok{) }\CommentTok{\# specify the grouping column (sentence\_id)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:td-generation-test-annotation-spanish}Annotation information for a single Spanish sentence from the Europarle dataset.}
\centering
\begin{tabular}[t]{rrlllllllll}
\toprule
doc\_id & sid & tid & token & token\_with\_ws & lemma & upos & xpos & feats & tid\_source & relation\\
\midrule
6 & 1 & 1 & Invito & Invito & Invito & VERB & NA & Gender=Masc|Number=Sing|VerbForm=Fin & 0 & root\\
6 & 1 & 2 & a & a & a & ADP & NA & NA & 3 & case\\
6 & 1 & 3 & todos & todos & todo & PRON & NA & Gender=Masc|Number=Plur|PronType=Tot & 1 & obj\\
6 & 1 & 4 & a & a & a & ADP & NA & NA & 7 & mark\\
6 & 1 & 5 & que & que & que & SCONJ & NA & NA & 4 & fixed\\
\addlinespace
6 & 1 & 6 & nos & nos & yo & PRON & NA & Case=Acc,Dat|Number=Plur|Person=1|PrepCase=Npr|PronType=Prs|Reflex=Yes & 7 & iobj\\
6 & 1 & 7 & pongamos & pongamos & pongar & VERB & NA & Mood=Ind|Number=Plur|Person=1|Tense=Pres|VerbForm=Fin & 1 & advcl\\
6 & 1 & 8 & de & de & de & ADP & NA & NA & 9 & case\\
6 & 1 & 9 & pie & pie & pie & NOUN & NA & Gender=Masc|Number=Sing & 7 & obl\\
6 & 1 & 10 & para & para & para & ADP & NA & NA & 11 & mark\\
\addlinespace
6 & 1 & 11 & guardar & guardar & guardar & VERB & NA & VerbForm=Inf & 1 & advcl\\
6 & 1 & 12 & un & un & uno & DET & NA & Definite=Ind|Gender=Masc|Number=Sing|PronType=Art & 13 & det\\
6 & 1 & 13 & minuto & minuto & minuto & NOUN & NA & Gender=Masc|Number=Sing & 11 & obj\\
6 & 1 & 14 & de & de & de & ADP & NA & NA & 15 & case\\
6 & 1 & 15 & silencio & silencio & silencio & NOUN & NA & Gender=Masc|Number=Sing & 13 & nmod\\
\addlinespace
6 & 1 & 16 & . & . & . & PUNCT & NA & NA & 1 & punct\\
\bottomrule
\end{tabular}
\end{table}

For the Spanish version of this sentence, we see the same variables. However, the \texttt{feats} variable has morphological information which is specific to Spanish --notably gender and mood.

\begin{rmdtip}
The rsyntax package \citep{R-rsyntax} can be used to recode and extract patterns from the output from automatic linguistic annotations using cleanNLP. \href{https://github.com/vanatteveldt/rsyntax}{See the documentation for more information}.
\end{rmdtip}

\hypertarget{merge}{%
\subsection{Merge}\label{merge}}

One final class of transformations that can be applied to curated datasets to enhance their informativeness for a research project is the process of merging two or more datasets. To merge datasets it is required that the datasets share one or more attributes. With a common attribute two datasets can be joined to coordinate the attributes of one dataset with the other effectively adding attributes and one dataset with extended information. Another approach is to join datasets with the goal of filtering one of the datasets given the matching attribute.

Let's see this in practice. Take the \texttt{lastfm} dataset. Let's tokenize the dataset into words, using \texttt{unnest\_tokens()} such that our unit of observation is words.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_words }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{output =} \StringTok{"word"}\NormalTok{, }\CommentTok{\# output column}
                \AttributeTok{input =} \StringTok{"lyrics"}\NormalTok{, }\CommentTok{\# input column}
                \AttributeTok{token =} \StringTok{"words"}\NormalTok{) }\CommentTok{\# tokenized unit (words)}

\NormalTok{lastfm\_words }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# first 10 observations}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{caption =} \StringTok{"First 10 observations for \textasciigrave{}lastfm\_words\textasciigrave{} dataset."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-5}First 10 observations for `lastfm_words` dataset.}
\centering
\begin{tabular}[t]{llll}
\toprule
artist & song & genre & word\\
\midrule
Alan Jackson & Little Bitty & country & have\\
Alan Jackson & Little Bitty & country & a\\
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & love\\
Alan Jackson & Little Bitty & country & on\\
\addlinespace
Alan Jackson & Little Bitty & country & a\\
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & honeymoon\\
Alan Jackson & Little Bitty & country & you\\
Alan Jackson & Little Bitty & country & got\\
\bottomrule
\end{tabular}
\end{table}

Consider the \texttt{get\_sentiments()} function which returns words which have been classified as `positive'- or `negative'-biased, if the lexicon is set to `bing' \citep{Hu2004}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sentiments\_bing }\OtherTok{\textless{}{-}} \FunctionTok{get\_sentiments}\NormalTok{(}\AttributeTok{lexicon =} \StringTok{"bing"}\NormalTok{)  }\CommentTok{\# get \textquotesingle{}bing\textquotesingle{} lexicon}

\NormalTok{sentiments\_bing }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)  }\CommentTok{\# preview first 10 observations}
\CommentTok{\#\textgreater{} \# A tibble: 10 x 2}
\CommentTok{\#\textgreater{}    word        sentiment}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}       \textless{}chr\textgreater{}    }
\CommentTok{\#\textgreater{}  1 2{-}faces     negative }
\CommentTok{\#\textgreater{}  2 abnormal    negative }
\CommentTok{\#\textgreater{}  3 abolish     negative }
\CommentTok{\#\textgreater{}  4 abominable  negative }
\CommentTok{\#\textgreater{}  5 abominably  negative }
\CommentTok{\#\textgreater{}  6 abominate   negative }
\CommentTok{\#\textgreater{}  7 abomination negative }
\CommentTok{\#\textgreater{}  8 abort       negative }
\CommentTok{\#\textgreater{}  9 aborted     negative }
\CommentTok{\#\textgreater{} 10 aborts      negative}
\end{Highlighting}
\end{Shaded}

Since the \texttt{sentiments\_bing} dataset and the \texttt{lastfm\_words} dataset both share a column \texttt{word} (which has the same type of values) we can join these two datasets. The \texttt{sentiments\_bing} dataset has 6786 unique words. Let's check how many distinct words our \texttt{lastfm\_words} dataset has.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_words }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{distinct}\NormalTok{(word) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# find unique words}
  \FunctionTok{nrow}\NormalTok{() }\CommentTok{\# count distinct rows/ words}
\CommentTok{\#\textgreater{} [1] 4614}
\end{Highlighting}
\end{Shaded}

One thing to note is that the \texttt{sentiments\_bing} dataset does not include function words, that is words that are associated with closed-class categories (pronouns, determiners, prepositions, etc.) as these words do not have semantic content along the lines of positive and negative. So many of the words that appear in the \texttt{lastfm\_words} will not be matched. Other thing to note is that the \texttt{sentiments\_bing} lexicon will undoubtly have words that do not appear in the \texttt{lastfm\_words} and vice versa.

If we want to keep all the words in the \texttt{lastfm\_words} and add the sentiment information for those words that do match in both datasets, we can use the \texttt{left\_join()} function. \texttt{lastfm\_words} will be the dataset on the `left' and therefore all rows in this dataset will be retained.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{left\_join}\NormalTok{(lastfm\_words, sentiments\_bing) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# first 10 observations}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{caption =} \StringTok{"First 10 observations for the \textasciigrave{}lastfm\_words\textasciigrave{} sentiments\_bing\textasciigrave{} left join."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:td-lastfm-words-bing-left-joing}First 10 observations for the `lastfm_words` sentiments_bing` left join.}
\centering
\begin{tabular}[t]{lllll}
\toprule
artist & song & genre & word & sentiment\\
\midrule
Alan Jackson & Little Bitty & country & have & NA\\
Alan Jackson & Little Bitty & country & a & NA\\
Alan Jackson & Little Bitty & country & little & NA\\
Alan Jackson & Little Bitty & country & love & positive\\
Alan Jackson & Little Bitty & country & on & NA\\
\addlinespace
Alan Jackson & Little Bitty & country & a & NA\\
Alan Jackson & Little Bitty & country & little & NA\\
Alan Jackson & Little Bitty & country & honeymoon & NA\\
Alan Jackson & Little Bitty & country & you & NA\\
Alan Jackson & Little Bitty & country & got & NA\\
\bottomrule
\end{tabular}
\end{table}

So we see that quite a few of the words from \texttt{lastfm\_words} are not matched. To focus in on those words in \texttt{lastfm\_words} that do match, we'll run the same join operation and filter for rows where \texttt{sentiment} is not empty (i.e.~that there is a match in the \texttt{sentiments\_bing} lexicon).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{left\_join}\NormalTok{(lastfm\_words, sentiments\_bing) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(sentiment }\SpecialCharTok{!=} \StringTok{""}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# return matched sentiments}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# first 10 observations}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{caption =} \StringTok{"First 10 observations for the \textasciigrave{}lastfm\_words\textasciigrave{} sentiments\_bing\textasciigrave{} left join."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:td-lastfm-words-bing-left-joing-filter}First 10 observations for the `lastfm_words` sentiments_bing` left join.}
\centering
\begin{tabular}[t]{lllll}
\toprule
artist & song & genre & word & sentiment\\
\midrule
Alan Jackson & Little Bitty & country & love & positive\\
Alan Jackson & Little Bitty & country & well & positive\\
Alan Jackson & Little Bitty & country & well & positive\\
Alan Jackson & Little Bitty & country & well & positive\\
Alan Jackson & Little Bitty & country & smile & positive\\
\addlinespace
Alan Jackson & Little Bitty & country & well & positive\\
Alan Jackson & Little Bitty & country & well & positive\\
Alan Jackson & Little Bitty & country & well & positive\\
Alan Jackson & Little Bitty & country & smile & positive\\
Alan Jackson & Little Bitty & country & good & positive\\
\bottomrule
\end{tabular}
\end{table}

Let's turn to another type of join: an anti-join. The purpose of an anti-join is to eliminate matches. This makes sense for a quick and dirty approach to removing function words (i.e.~those grammatical words with little semantic content). In this case we use the \texttt{get\_stopwords()} function to get the dataset. We'll specify English as the language and we'll use the default lexicon (`Snowball').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{english\_stopwords }\OtherTok{\textless{}{-}} \FunctionTok{get\_stopwords}\NormalTok{(}\AttributeTok{language =} \StringTok{"en"}\NormalTok{)  }\CommentTok{\# get English stopwords from the Snowball lexicon}

\NormalTok{english\_stopwords }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)  }\CommentTok{\# preview first 10 observations}
\CommentTok{\#\textgreater{} \# A tibble: 10 x 2}
\CommentTok{\#\textgreater{}    word      lexicon }
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}     \textless{}chr\textgreater{}   }
\CommentTok{\#\textgreater{}  1 i         snowball}
\CommentTok{\#\textgreater{}  2 me        snowball}
\CommentTok{\#\textgreater{}  3 my        snowball}
\CommentTok{\#\textgreater{}  4 myself    snowball}
\CommentTok{\#\textgreater{}  5 we        snowball}
\CommentTok{\#\textgreater{}  6 our       snowball}
\CommentTok{\#\textgreater{}  7 ours      snowball}
\CommentTok{\#\textgreater{}  8 ourselves snowball}
\CommentTok{\#\textgreater{}  9 you       snowball}
\CommentTok{\#\textgreater{} 10 your      snowball}
\end{Highlighting}
\end{Shaded}

Now if we want to eliminate stopwords from our \texttt{lastfm\_words} dataset we use \texttt{anti\_join()}. All the observations in the \texttt{lastfm\_words} where there is not a match in \texttt{english\_stopwords} will be returned.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anti\_join}\NormalTok{(lastfm\_words, english\_stopwords) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{caption =} \StringTok{"First 10 observations in \textasciigrave{}lastfm\_words\textasciigrave{} after filtering for English stopwords."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:td-lastfm-words-stopwords-anti-join}First 10 observations in `lastfm_words` after filtering for English stopwords.}
\centering
\begin{tabular}[t]{llll}
\toprule
artist & song & genre & word\\
\midrule
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & love\\
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & honeymoon\\
Alan Jackson & Little Bitty & country & got\\
\addlinespace
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & dish\\
Alan Jackson & Little Bitty & country & got\\
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & spoon\\
\bottomrule
\end{tabular}
\end{table}

We can also merge datasets that we generate in our analysis or that we import from other sources. This can be useful when there are cases in which a corpus has associated metadata that is contained in files separate from the corpus itself. This is the case for the Switchboard Dialogue Act Corpus.

Our existing, disfluency recoded, version includes the following variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\CommentTok{\# preview first 10 observations}
\CommentTok{\#\textgreater{} \# A tibble: 10 x 5}
\CommentTok{\#\textgreater{}    doc\_id speaker\_id utterance\_text                                 filler count}
\CommentTok{\#\textgreater{}     \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{} \textless{}chr\textgreater{}                                          \textless{}chr\textgreater{}  \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{}  1   4325       1632 Okay.  /                                       uh         0}
\CommentTok{\#\textgreater{}  2   4325       1632 Okay.  /                                       um         0}
\CommentTok{\#\textgreater{}  3   4325       1632 \{D So, \}                                       uh         0}
\CommentTok{\#\textgreater{}  4   4325       1632 \{D So, \}                                       um         0}
\CommentTok{\#\textgreater{}  5   4325       1519 [ [ I guess, +                                 uh         0}
\CommentTok{\#\textgreater{}  6   4325       1519 [ [ I guess, +                                 um         0}
\CommentTok{\#\textgreater{}  7   4325       1632 What kind of experience [ do you, + do you ] \textasciitilde{} uh         0}
\CommentTok{\#\textgreater{}  8   4325       1632 What kind of experience [ do you, + do you ] \textasciitilde{} um         0}
\CommentTok{\#\textgreater{}  9   4325       1519 I think, ] + \{F uh, \} I wonder ] if that work\textasciitilde{} uh         1}
\CommentTok{\#\textgreater{} 10   4325       1519 I think, ] + \{F uh, \} I wonder ] if that work\textasciitilde{} um         0}
\end{Highlighting}
\end{Shaded}

The \href{https://catalog.ldc.upenn.edu/docs/LDC97S62/}{online documentation page} provides a key file \texttt{caller\_tab.csv} which contains speaker metadata information. Included in this \texttt{.csv} file is a column \texttt{caller\_no} which contains the \texttt{speaker\_id} we currently have in the \texttt{sdac\_disfluencies} dataset. Let's read this file into our R session renaming \texttt{caller\_no} to \texttt{speaker\_id} to prepare to join these datasets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_speaker\_meta }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"https://catalog.ldc.upenn.edu/docs/LDC97S62/caller\_tab.csv"}\NormalTok{, }
           \AttributeTok{col\_names =} \FunctionTok{c}\NormalTok{(}\StringTok{"speaker\_id"}\NormalTok{, }\CommentTok{\# changed from \textasciigrave{}caller\_no\textasciigrave{}}
                         \StringTok{"pin"}\NormalTok{,}
                         \StringTok{"target"}\NormalTok{,}
                         \StringTok{"sex"}\NormalTok{,}
                         \StringTok{"birth\_year"}\NormalTok{,}
                         \StringTok{"dialect\_area"}\NormalTok{,}
                         \StringTok{"education"}\NormalTok{,}
                         \StringTok{"ti"}\NormalTok{,}
                         \StringTok{"payment\_type"}\NormalTok{,}
                         \StringTok{"amt\_pd"}\NormalTok{,}
                         \StringTok{"con"}\NormalTok{,}
                         \StringTok{"remarks"}\NormalTok{,}
                         \StringTok{"calls\_deleted"}\NormalTok{,}
                         \StringTok{"speaker\_partition"}\NormalTok{))}

\FunctionTok{glimpse}\NormalTok{(sdac\_speaker\_meta)}
\CommentTok{\#\textgreater{} Rows: 543}
\CommentTok{\#\textgreater{} Columns: 14}
\CommentTok{\#\textgreater{} $ speaker\_id        \textless{}dbl\textgreater{} 1000, 1001, 1002, 1003, 1004, 1005, 1007, 1008, 1010\textasciitilde{}}
\CommentTok{\#\textgreater{} $ pin               \textless{}dbl\textgreater{} 32, 102, 104, 5656, 123, 166, 274, 322, 445, 461, 57\textasciitilde{}}
\CommentTok{\#\textgreater{} $ target            \textless{}chr\textgreater{} "N", "N", "N", "N", "N", "Y", "N", "N", "N", "N", "Y\textasciitilde{}}
\CommentTok{\#\textgreater{} $ sex               \textless{}chr\textgreater{} "FEMALE", "MALE", "FEMALE", "MALE", "FEMALE", "FEMAL\textasciitilde{}}
\CommentTok{\#\textgreater{} $ birth\_year        \textless{}dbl\textgreater{} 1954, 1940, 1963, 1947, 1958, 1956, 1965, 1939, 1932\textasciitilde{}}
\CommentTok{\#\textgreater{} $ dialect\_area      \textless{}chr\textgreater{} "SOUTH MIDLAND", "WESTERN", "SOUTHERN", "NORTH MIDLA\textasciitilde{}}
\CommentTok{\#\textgreater{} $ education         \textless{}dbl\textgreater{} 1, 3, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 3, 3, 2, 3\textasciitilde{}}
\CommentTok{\#\textgreater{} $ ti                \textless{}dbl\textgreater{} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\textasciitilde{}}
\CommentTok{\#\textgreater{} $ payment\_type      \textless{}chr\textgreater{} "CASH", "GIFT", "GIFT", "NONE", "GIFT", "GIFT", "CAS\textasciitilde{}}
\CommentTok{\#\textgreater{} $ amt\_pd            \textless{}dbl\textgreater{} 15, 10, 11, 7, 11, 22, 20, 3, 11, 9, 25, 9, 1, 16, 1\textasciitilde{}}
\CommentTok{\#\textgreater{} $ con               \textless{}chr\textgreater{} "N", "N", "N", "Y", "N", "Y", "N", "Y", "N", "N", "N\textasciitilde{}}
\CommentTok{\#\textgreater{} $ remarks           \textless{}dbl\textgreater{} NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ calls\_deleted     \textless{}dbl\textgreater{} 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0\textasciitilde{}}
\CommentTok{\#\textgreater{} $ speaker\_partition \textless{}chr\textgreater{} "DN2", "XP", "XP", "DN2", "XP", "ET", "DN1", "DN1", \textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Now to join the \texttt{sdac\_disfluencies} and \texttt{sdac\_speaker\_meta}. Let's turn to \texttt{left\_join()} again as we want to retain all the observations (rows) from \texttt{sdac\_disfluencies} and add the columns for \texttt{sdac\_speaker\_meta} where the \texttt{speaker\_id} column values match.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} \FunctionTok{left\_join}\NormalTok{(sdac\_disfluencies, sdac\_speaker\_meta)  }\CommentTok{\# join by \textasciigrave{}\textasciigrave{}speaker\_id\textasciigrave{}}

\FunctionTok{glimpse}\NormalTok{(sdac\_disfluencies)}
\CommentTok{\#\textgreater{} Rows: 447,212}
\CommentTok{\#\textgreater{} Columns: 18}
\CommentTok{\#\textgreater{} $ doc\_id            \textless{}dbl\textgreater{} 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325\textasciitilde{}}
\CommentTok{\#\textgreater{} $ speaker\_id        \textless{}dbl\textgreater{} 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519\textasciitilde{}}
\CommentTok{\#\textgreater{} $ utterance\_text    \textless{}chr\textgreater{} "Okay.  /", "Okay.  /", "\{D So, \}", "\{D So, \}", "[ [\textasciitilde{}}
\CommentTok{\#\textgreater{} $ filler            \textless{}chr\textgreater{} "uh", "um", "uh", "um", "uh", "um", "uh", "um", "uh"\textasciitilde{}}
\CommentTok{\#\textgreater{} $ count             \textless{}int\textgreater{} 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0\textasciitilde{}}
\CommentTok{\#\textgreater{} $ pin               \textless{}dbl\textgreater{} 7713, 7713, 7713, 7713, 775, 775, 7713, 7713, 775, 7\textasciitilde{}}
\CommentTok{\#\textgreater{} $ target            \textless{}chr\textgreater{} "N", "N", "N", "N", "N", "N", "N", "N", "N", "N", "N\textasciitilde{}}
\CommentTok{\#\textgreater{} $ sex               \textless{}chr\textgreater{} "FEMALE", "FEMALE", "FEMALE", "FEMALE", "FEMALE", "F\textasciitilde{}}
\CommentTok{\#\textgreater{} $ birth\_year        \textless{}dbl\textgreater{} 1962, 1962, 1962, 1962, 1971, 1971, 1962, 1962, 1971\textasciitilde{}}
\CommentTok{\#\textgreater{} $ dialect\_area      \textless{}chr\textgreater{} "WESTERN", "WESTERN", "WESTERN", "WESTERN", "SOUTH M\textasciitilde{}}
\CommentTok{\#\textgreater{} $ education         \textless{}dbl\textgreater{} 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1\textasciitilde{}}
\CommentTok{\#\textgreater{} $ ti                \textless{}dbl\textgreater{} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\textasciitilde{}}
\CommentTok{\#\textgreater{} $ payment\_type      \textless{}chr\textgreater{} "CASH", "CASH", "CASH", "CASH", "CASH", "CASH", "CAS\textasciitilde{}}
\CommentTok{\#\textgreater{} $ amt\_pd            \textless{}dbl\textgreater{} 10, 10, 10, 10, 4, 4, 10, 10, 4, 4, 10, 10, 4, 4, 4,\textasciitilde{}}
\CommentTok{\#\textgreater{} $ con               \textless{}chr\textgreater{} "Y", "Y", "Y", "Y", "Y", "Y", "Y", "Y", "Y", "Y", "Y\textasciitilde{}}
\CommentTok{\#\textgreater{} $ remarks           \textless{}dbl\textgreater{} NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ calls\_deleted     \textless{}dbl\textgreater{} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\textasciitilde{}}
\CommentTok{\#\textgreater{} $ speaker\_partition \textless{}chr\textgreater{} "UNC", "UNC", "UNC", "UNC", "UNC", "UNC", "UNC", "UN\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Now there are some metadata columns we may want to keep and others we may want to drop as they may not be of importance for our analysis. I'm going to assume that we want to keep \texttt{sex}, \texttt{birth\_year}, \texttt{dialect\_area}, and \texttt{education} and drop the rest.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(doc\_id}\SpecialCharTok{:}\NormalTok{count, sex}\SpecialCharTok{:}\NormalTok{education) }\CommentTok{\# subset key columns}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:td-sdac-disfluencies-meta-preview}First 10 observations for the `sdac_disfluencies` dataset with speaker metadata.}
\centering
\begin{tabular}[t]{rrllrlrlr}
\toprule
doc\_id & speaker\_id & utterance\_text & filler & count & sex & birth\_year & dialect\_area & education\\
\midrule
4325 & 1632 & Okay.  / & uh & 0 & FEMALE & 1962 & WESTERN & 2\\
4325 & 1632 & Okay.  / & um & 0 & FEMALE & 1962 & WESTERN & 2\\
4325 & 1632 & \{D So, \} & uh & 0 & FEMALE & 1962 & WESTERN & 2\\
4325 & 1632 & \{D So, \} & um & 0 & FEMALE & 1962 & WESTERN & 2\\
4325 & 1519 & {}[ [ I guess, + & uh & 0 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
\addlinespace
4325 & 1519 & {}[ [ I guess, + & um & 0 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? / & uh & 0 & FEMALE & 1962 & WESTERN & 2\\
4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? / & um & 0 & FEMALE & 1962 & WESTERN & 2\\
4325 & 1519 & I think, ] + \{F uh, \} I wonder ] if that worked. / & uh & 1 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
4325 & 1519 & I think, ] + \{F uh, \} I wonder ] if that worked. / & um & 0 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{documentation-3}{%
\subsection{Documentation}\label{documentation-3}}

Documentation of the transformed dataset is just as important as the curated dataset. Therefore we use the same process as covered in the previous chapter. First we write the transformed dataset to disk and then we work to provide a data dictionary for this dataset. I've included the \texttt{data\_dic\_starter()} custom function to apply to our dataset(s).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_dic\_starter }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, file\_path) \{}
  \CommentTok{\# Function:}
  \CommentTok{\# Creates a .csv file with the basic information}
  \CommentTok{\# to document a curated dataset}
  
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{variable\_name =} \FunctionTok{names}\NormalTok{(data), }\CommentTok{\# column with existing variable names }
       \AttributeTok{name =} \StringTok{""}\NormalTok{, }\CommentTok{\# column for human{-}readable names}
       \AttributeTok{description =} \StringTok{""}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# column for prose description}
  \FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{file =}\NormalTok{ file\_path) }\CommentTok{\# write to disk}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Let's apply our function to the \texttt{sdac\_disfluencies} dataset using the R console (not part of our project script to avoid overwriting our documentation!).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data\_dic\_starter}\NormalTok{(}\AttributeTok{data =}\NormalTok{ sdac\_disfluencies, }\AttributeTok{file\_path =} \StringTok{"../data/derived/sdac/sdac\_disfluencies\_data\_dictionary.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/derived/}
\ExtensionTok{└──}\NormalTok{ sdac/}
    \ExtensionTok{├──}\NormalTok{ data\_dictionary\_sdac.csv}
    \ExtensionTok{├──}\NormalTok{ sdac\_curated.csv}
    \ExtensionTok{├──}\NormalTok{ sdac\_disfluencies.csv}
    \ExtensionTok{└──}\NormalTok{ sdac\_disfluencies\_data\_dictionary.csv}
\end{Highlighting}
\end{Shaded}

Open the \texttt{data\_dictionary\_sdac\_disfluencies.csv} file in spreadsheet software and add the relevant description of the dataset.

\hypertarget{summary-9}{%
\subsection*{Summary}\label{summary-9}}
\addcontentsline{toc}{subsection}{Summary}

In this chapter we covered the process of transforming datasets. The goal is to manipulate the curated dataset to make it align better for analysis.
There are four general types of transformation steps: normalization, recoding, generation, and merging. In any given research project some or all of these steps will be employed --but not necessarily in the order presented in this chapter. Furthermore there may also be various datasets generated in at this stage each with a distinct analysis focus in mind. In any case it is important to write these datasets to disk and to document them according to the principles that we have established in the previous chapter.

This chapter concludes the section on data/ dataset preparation. The next section we turn to analyzing datasets. This is the stage where we interrogate the datasets to derive knowledge and insight either through inference, prediction, and/ or exploratory methods.

\hypertarget{part-analysis}{%
\part{Analysis}\label{part-analysis}}

\hypertarget{analysis-overview}{%
\section*{Overview}\label{analysis-overview}}
\addcontentsline{toc}{section}{Overview}

\ldots{} UNDER DEVELOPMENT \ldots{}

\hypertarget{inference}{%
\section{Inference}\label{inference}}

DRAFT

\begin{quote}
People generally see what they look for, and hear what they listen for.

-- Harper Lee, To Kill a Mockingbird
\end{quote}

\begin{rmdkey}
The essential questions for this chapter are:

\begin{itemize}
\tightlist
\item
  what are the three main types of inferential analysis approaches?
\item
  how does the informational value of the dependent variable relate to
  the statistical approach adopted?
\item
  how to descriptive, statistical, and evaluative steps work together to
  produce the reliable results?
\end{itemize}
\end{rmdkey}

In this chapter we consider approaches to deriving knowledge from information which can be generalized to the population from which the data is sampled. This process is known as statistical inference. The discussion here builds on concepts developed in \protect\hyperlink{approaching-analysis}{Chapter 3 ``Approaching analysis''} and implements descriptive assessments, statistical tests, and evaluation procedures for a series of contexts which are common in the analysis of corpus-based data. The chapter is structured into three main sections which correspond to the number of variables included in the statistical procedure. Each of these sections includes a subsection dedicated to the informational value of the dependent variable; the variable whose variation is to be explained.

For this discussion two datasets will be used as the base to pose various questions to submit for interrogation. It is of note that the questions in the subsequent sections are posited to highlight various descriptive, statistic, and evaluation procedures and do not reflect the standard approach to hypothesis testing which assumes that the null and alternative hypotheses are developed at the outset of the research project.

The process for each inferential data analysis in this section will include three steps: (1) descriptive assessment, (2) statistical interrogation, and (3) evaluation of the results.

\hypertarget{preparation}{%
\subsection{Preparation}\label{preparation}}

At this point let's now get familiar with the datasets and prepare them for analysis. The first dataset to consider is the \texttt{dative} dataset. This dataset can be loaded from the languageR package \citep{R-languageR}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dative }\OtherTok{\textless{}{-}} 
\NormalTok{  languageR}\SpecialCharTok{::}\NormalTok{dative }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# load the \textasciigrave{}dative\textasciigrave{} dataset  }
  \FunctionTok{as\_tibble}\NormalTok{() }\CommentTok{\# convert the data frame to a tibble object}
  
\FunctionTok{glimpse}\NormalTok{(dative) }\CommentTok{\# preview structure }
\CommentTok{\#\textgreater{} Rows: 3,263}
\CommentTok{\#\textgreater{} Columns: 15}
\CommentTok{\#\textgreater{} $ Speaker                \textless{}fct\textgreater{} NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,\textasciitilde{}}
\CommentTok{\#\textgreater{} $ Modality               \textless{}fct\textgreater{} written, written, written, written, written, wr\textasciitilde{}}
\CommentTok{\#\textgreater{} $ Verb                   \textless{}fct\textgreater{} feed, give, give, give, offer, give, pay, bring\textasciitilde{}}
\CommentTok{\#\textgreater{} $ SemanticClass          \textless{}fct\textgreater{} t, a, a, a, c, a, t, a, a, a, a, a, t, a, c, a,\textasciitilde{}}
\CommentTok{\#\textgreater{} $ LengthOfRecipient      \textless{}int\textgreater{} 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1,\textasciitilde{}}
\CommentTok{\#\textgreater{} $ AnimacyOfRec           \textless{}fct\textgreater{} animate, animate, animate, animate, animate, an\textasciitilde{}}
\CommentTok{\#\textgreater{} $ DefinOfRec             \textless{}fct\textgreater{} definite, definite, definite, definite, definit\textasciitilde{}}
\CommentTok{\#\textgreater{} $ PronomOfRec            \textless{}fct\textgreater{} pronominal, nonpronominal, nonpronominal, prono\textasciitilde{}}
\CommentTok{\#\textgreater{} $ LengthOfTheme          \textless{}int\textgreater{} 14, 3, 13, 5, 3, 4, 4, 1, 11, 2, 3, 3, 5, 2, 8,\textasciitilde{}}
\CommentTok{\#\textgreater{} $ AnimacyOfTheme         \textless{}fct\textgreater{} inanimate, inanimate, inanimate, inanimate, ina\textasciitilde{}}
\CommentTok{\#\textgreater{} $ DefinOfTheme           \textless{}fct\textgreater{} indefinite, indefinite, definite, indefinite, d\textasciitilde{}}
\CommentTok{\#\textgreater{} $ PronomOfTheme          \textless{}fct\textgreater{} nonpronominal, nonpronominal, nonpronominal, no\textasciitilde{}}
\CommentTok{\#\textgreater{} $ RealizationOfRecipient \textless{}fct\textgreater{} NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP,\textasciitilde{}}
\CommentTok{\#\textgreater{} $ AccessOfRec            \textless{}fct\textgreater{} given, given, given, given, given, given, given\textasciitilde{}}
\CommentTok{\#\textgreater{} $ AccessOfTheme          \textless{}fct\textgreater{} new, new, new, new, new, new, new, new, accessi\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

From \texttt{glimpse()} we can see that this dataset contains 3,263 observations and 15 columns.

The R Documentation can be consulted using \texttt{?dative} in the R Console. The description states:

\begin{quote}
Data describing the realization of the dative as NP or PP in the Switchboard corpus and the Treebank Wall Street Journal collection.
\end{quote}

For a bit more context, a dative is the phrase which reflects the entity that takes the recipient role in a ditransitive clause. In English, the recipient (dative) can be realized as either a noun phrase (NP) as seen in (1) or as a prepositional phrase (PP) as seen in (2) below.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  They give {[}you \textsubscript{NP}{]} a drug test.
\item
  They give a drug test {[}to you \textsubscript{PP}{]}.
\end{enumerate}

Together these two syntactic options are known as the Dative Alternation.

The observational unit for this dataset is \texttt{RealizationOfRecipient} variable which is either `NP' or `PP'. For the purposes of this chapter I will select a subset of the key variables we will use in the upcoming analyses and drop the others.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dative }\OtherTok{\textless{}{-}} 
\NormalTok{  dative }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(RealizationOfRecipient, Modality, LengthOfRecipient, LengthOfTheme) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# select key variables}
\NormalTok{  janitor}\SpecialCharTok{::}\FunctionTok{clean\_names}\NormalTok{() }\CommentTok{\# normalize variable names}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:i-dative-preview}First 10 observations of simplified `dative` dataset.}
\centering
\begin{tabular}[t]{llrr}
\toprule
realization\_of\_recipient & modality & length\_of\_recipient & length\_of\_theme\\
\midrule
NP & written & 1 & 14\\
NP & written & 2 & 3\\
NP & written & 1 & 13\\
NP & written & 1 & 5\\
NP & written & 2 & 3\\
\addlinespace
NP & written & 2 & 4\\
NP & written & 2 & 4\\
NP & written & 1 & 1\\
NP & written & 1 & 11\\
NP & written & 1 & 2\\
\bottomrule
\end{tabular}
\end{table}

In Table \ref{tab:i-dative-dictionary} I've created a data dictionary describing the variables in our new \texttt{dative} dataset based on the variable descriptions in the \texttt{languageR::dative} documentation.

\begin{table}

\caption{\label{tab:i-dative-dictionary}Data dictionary for the `dative` dataset.}
\centering
\begin{tabular}[t]{lll}
\toprule
variable\_name & name & description\\
\midrule
realization\_of\_recipient & Realization of Recipient & A factor with levels NP and PP coding the realization of the dative.\\
modality & Language Modality & A factor with levels *spoken*, *written*.\\
length\_of\_recipient & Length of Recipient & A numeric vector coding the number of words comprising the recipient.\\
length\_of\_theme & Length of Theme & A numeric vector coding the number of words comprising the theme.\\
\bottomrule
\end{tabular}
\end{table}

The second dataset that we will use in this chapter is the \texttt{sdac\_disfluencies} dataset that we worked to derived in the previous chapter. Let's read in the dataset and preview the structure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/derived/sdac/sdac\_disfluencies.csv"}\NormalTok{)  }\CommentTok{\# read transformed dataset}

\FunctionTok{glimpse}\NormalTok{(sdac\_disfluencies)  }\CommentTok{\# preview structure}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 447,212
#> Columns: 9
#> $ doc_id         <dbl> 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4~
#> $ speaker_id     <dbl> 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519, 1~
#> $ utterance_text <chr> "Okay.  /", "Okay.  /", "{D So, }", "{D So, }", "[ [ I ~
#> $ filler         <chr> "uh", "um", "uh", "um", "uh", "um", "uh", "um", "uh", "~
#> $ count          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0~
#> $ sex            <chr> "FEMALE", "FEMALE", "FEMALE", "FEMALE", "FEMALE", "FEMA~
#> $ birth_year     <dbl> 1962, 1962, 1962, 1962, 1971, 1971, 1962, 1962, 1971, 1~
#> $ dialect_area   <chr> "WESTERN", "WESTERN", "WESTERN", "WESTERN", "SOUTH MIDL~
#> $ education      <dbl> 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1~
\end{verbatim}

We prepared a data dictionary that reflects this transformed dataset. Let's read that file and then view it Table \ref{tab:i-sdac-disfluencies-dictionary}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies\_dictionary }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/derived/sdac/sdac\_disfluencies\_data\_dictionary.csv"}\NormalTok{)  }\CommentTok{\# read data dictionary}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:i-sdac-disfluencies-dictionary}Data dictionary for the `sdac_disfluencies` dataset.}
\centering
\begin{tabular}[t]{lll}
\toprule
variable\_name & name & description\\
\midrule
doc\_id & Document ID & Unique identifier for each conversation file.\\
speaker\_id & Speaker ID & Unique identifier for each speaker in the corpus.\\
utterance\_text & Utterance Text & Transcribed utterances for each conversation. Includes disfluency annotation tags.\\
filler & Filler & Filler type either uh or um.\\
count & Count & Number of fillers for each utterance.\\
\addlinespace
sex & Sex & Sex for each speaker either male or female.\\
birth\_year & Birth Year & The year each speaker was born.\\
dialect\_area & Dialect Area & Region from the US where the speaker spent first 10 years.\\
education & Education & Highest educational level attained: values 0, 1, 2, 3, and 9.\\
\bottomrule
\end{tabular}
\end{table}

For our analysis purposes we will reduce this dataset, as we did for the \texttt{dative} dataset, retaining only the variables of interest for the upcoming analyses.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(speaker\_id, filler, count, sex, birth\_year, education) }\CommentTok{\# select key variables}
\end{Highlighting}
\end{Shaded}

Let's preview this simplified \texttt{sdac\_disfluencies} dataset.

\begin{table}

\caption{\label{tab:i-sdac-disfluencies-preview}First 10 observations of simplified `sdac_disfluencies` dataset.}
\centering
\begin{tabular}[t]{rlrlrr}
\toprule
speaker\_id & filler & count & sex & birth\_year & education\\
\midrule
1632 & uh & 0 & FEMALE & 1962 & 2\\
1632 & um & 0 & FEMALE & 1962 & 2\\
1632 & uh & 0 & FEMALE & 1962 & 2\\
1632 & um & 0 & FEMALE & 1962 & 2\\
1519 & uh & 0 & FEMALE & 1971 & 1\\
\addlinespace
1519 & um & 0 & FEMALE & 1971 & 1\\
1632 & uh & 0 & FEMALE & 1962 & 2\\
1632 & um & 0 & FEMALE & 1962 & 2\\
1519 & uh & 1 & FEMALE & 1971 & 1\\
1519 & um & 0 & FEMALE & 1971 & 1\\
\bottomrule
\end{tabular}
\end{table}

Now the \texttt{sdac\_disfluencies} dataset needs some extra transformation to better prepare it for statistical interrogation. On the one hand the variables \texttt{birth\_year} and \texttt{education} are not maximally informative. First it would be more ideal if \texttt{birth\_year} would reflect the age of the speaker at the time of the conversation(s) and furthermore the coded values of \texttt{education} are not explicit as far what the numeric values refer to.

The second issue has to do with preparing the \texttt{sdac\_disfluencies} dataset for statistical analysis. This involves converting our column types to the correct vector types for statistical methods. Specifically we need to convert our categorical variables to the R type `factor' (fct). This includes of our current variables which are character vectors, but also the \texttt{speaker\_id} and \texttt{education} which appear as numeric but do not reflect a continuous variables; one is merely a code which uniquely labels each speaker and the other is an ordinal list of educational levels.

This will be a three step process, first we will normalize the \texttt{birth\_year} to reflect the age of the speaker, second we will convert all the relevant categorical variables to factors, and third we will convert the \texttt{education} variable to a factor adding meaningful labels for the levels of this factor.

Consulting the \href{https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1_manual.txt}{online manual for this corpus}, we see that the recording date for these conversations took place in 1992, so we can simply subtract the \texttt{birth\_year} from 1992 to get each participant's age. We'll rename this new column \texttt{age} and drop the \texttt{birth\_year} column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age =}\NormalTok{ (}\DecValTok{1992} \SpecialCharTok{{-}}\NormalTok{ birth\_year)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# calculate age}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{birth\_year) }\CommentTok{\# drop \textasciigrave{}birth\_year\textasciigrave{} column}
\end{Highlighting}
\end{Shaded}

Now let's convert all the variables which are character vectors. We can do this using the the \texttt{factor()} function; first on \texttt{speaker\_id} and then, with the help of \texttt{mutate\_if()}, to all the other variables which are character vectors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{speaker\_id =} \FunctionTok{factor}\NormalTok{(speaker\_id)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# convert numeric to factor}
  \FunctionTok{mutate\_if}\NormalTok{(is.character, factor) }\CommentTok{\# convert all character to factor}
\end{Highlighting}
\end{Shaded}

We know from the data dictionary that the \texttt{education} column contains four values (0, 1, 2, 3, and 9). Again, consulting the corpus manual we can see what these values mean.

\begin{verbatim}
EDUCATION    COUNT
--------------------

0            14      less than high school
1            39      less than college
2            309     college
3            176     more than college
9            4       unknown
\end{verbatim}

So let's convert \texttt{education} to a factor adding these descriptions as factor level labels. The function \texttt{factor()} can take an argument \texttt{labels\ =} which we can manually assign the label names for the factor levels in the order of the factor levels. Since the original values were numeric, the factor level ordering defaults to ascending order.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{education =} \FunctionTok{factor}\NormalTok{(education, }
                            \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"less than high school"}\NormalTok{, }\CommentTok{\# value 0}
                                       \StringTok{"less than college"}\NormalTok{, }\CommentTok{\# value 1}
                                       \StringTok{"college"}\NormalTok{, }\CommentTok{\# value 2}
                                       \StringTok{"more than college"}\NormalTok{, }\CommentTok{\# value 3 }
                                       \StringTok{"unknown"}\NormalTok{))) }\CommentTok{\# value 9}
\end{Highlighting}
\end{Shaded}

So let's take a look at the \texttt{sdac\_disfluencies} dataset we've prepared for analysis.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(sdac\_disfluencies)}
\CommentTok{\#\textgreater{} Rows: 447,212}
\CommentTok{\#\textgreater{} Columns: 6}
\CommentTok{\#\textgreater{} $ speaker\_id \textless{}fct\textgreater{} 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519, 1519,\textasciitilde{}}
\CommentTok{\#\textgreater{} $ filler     \textless{}fct\textgreater{} uh, um, uh, um, uh, um, uh, um, uh, um, uh, um, uh, um, uh,\textasciitilde{}}
\CommentTok{\#\textgreater{} $ count      \textless{}dbl\textgreater{} 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\textasciitilde{}}
\CommentTok{\#\textgreater{} $ sex        \textless{}fct\textgreater{} FEMALE, FEMALE, FEMALE, FEMALE, FEMALE, FEMALE, FEMALE, FEM\textasciitilde{}}
\CommentTok{\#\textgreater{} $ education  \textless{}fct\textgreater{} college, college, college, college, less than college, less\textasciitilde{}}
\CommentTok{\#\textgreater{} $ age        \textless{}dbl\textgreater{} 30, 30, 30, 30, 21, 21, 30, 30, 21, 21, 30, 30, 21, 21, 21,\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Now the datasets \texttt{dative} and \texttt{sdac\_disfluencies} are ready to be statistically interrogated.

\hypertarget{univariate-analysis}{%
\subsection{Univariate analysis}\label{univariate-analysis}}

In what follows I will provide a description of inferential data analysis when only one variable is to be interrogated. This is known as a univariate analysis, or one-variable analysis. We will consider a case when the variable is categorical and the other continuous.

\hypertarget{categorical}{%
\subsubsection{Categorical}\label{categorical}}

As an example of a univariate analysis where the variable used in the analysis is categorical we will look at the \texttt{dative} dataset. In this analysis we may be interested in knowing whether the recipient role in a ditransitive construction is realized more as an `NP' or `PP'.

\textbf{Descriptive assessment}

The \texttt{realization\_of\_recipient} variable contains the relevant information. Let's take a first look using the skimr package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dative }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(realization\_of\_recipient) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# select the variable}
  \FunctionTok{skim}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get data summary}
  \FunctionTok{yank}\NormalTok{(}\StringTok{"factor"}\NormalTok{) }\CommentTok{\# only show factor{-}oriented information}
\end{Highlighting}
\end{Shaded}

\textbf{Variable type: factor}

\begin{tabular}{l|r|r|l|r|l}
\hline
skim\_variable & n\_missing & complete\_rate & ordered & n\_unique & top\_counts\\
\hline
realization\_of\_recipient & 0 & 1 & FALSE & 2 & NP: 2414, PP: 849\\
\hline
\end{tabular}

The output from \texttt{skim()} produces various pieces of information that can be helpful. On the one hand we get diagnostics that tell us if there are missing cases (\texttt{NA} values), what the proportion of complete cases is, if the the factor is ordered, how many distinct levels the factor has, as well as the level counts.

Looking at the \texttt{top\_counts} we can see that of the 3,263 observations, in 2,414 the dative is expressed as an `NP' and 849 as `PP'. Numerically we can see that there is a difference between the use of the alternation types. A visualization is often helpful for descriptive purposes in statistical analysis. In this particular case, however, we are considering a single categorical variable with only two levels (values) so a visualization is not likely to be more informative than the numeric values we have already obtained. But for demonstration purposes and to get more familiar with building plots, let's create a visualization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dative }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ realization\_of\_recipient)) }\SpecialCharTok{+} \CommentTok{\# mapping}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Realization of recipient"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{) }\CommentTok{\# labels}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-inference_files/figure-latex/i-uni-cat-visual-dative-1} \end{center}

The question we want to address, however, is whether this numerical difference is in fact a statistical difference.

\textbf{Statistical interrogation}

To statistical assess the distribution for a categorical variable, we will turn to the Chi-squared test. This test aims to gauge whether the numerical differences between `NP' and `PP' counts observed in the data is greater than what would be expected by chance. Chance in the case where there are only two possible outcome levels is 50/50. For our particular data where there are 3,263 observations half would be `NP' and the other half `PP' --specifically 1631.5 for each.

To run this test we first will need to create a cross-tabulation of the variable. We will use the \texttt{xtabs()} function to create the table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ror\_table }\OtherTok{\textless{}{-}} 
  \FunctionTok{xtabs}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ realization\_of\_recipient, }\CommentTok{\# formula selecting the variable}
        \AttributeTok{data =}\NormalTok{ dative) }\CommentTok{\# dataset}

\NormalTok{ror\_table }\CommentTok{\# preview}
\CommentTok{\#\textgreater{} realization\_of\_recipient}
\CommentTok{\#\textgreater{}   NP   PP }
\CommentTok{\#\textgreater{} 2414  849}
\end{Highlighting}
\end{Shaded}

No new information here, but the format (i.e.~an object of class `table') is what is important for the input argument for the \texttt{chisq.test()} function we will use to run the test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c1 }\OtherTok{\textless{}{-}} \FunctionTok{chisq.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ ror\_table)  }\CommentTok{\# apply the chi{-}squared test to \textasciigrave{}ror\_table\textasciigrave{}}

\NormalTok{c1  }\CommentTok{\# preview the test results}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Chi{-}squared test for given probabilities}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  ror\_table}
\CommentTok{\#\textgreater{} X{-}squared = 751, df = 1, p{-}value \textless{}2e{-}16}
\end{Highlighting}
\end{Shaded}

The preview of the \texttt{c1} object reveals the main information of interest including the Chi-squared statistic, the degrees of freedom, and the \(p\)-value (albeit in scientific notation). However, the \texttt{c1} is an `htest' object an includes a number of other pieces information about the test.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(c1)  }\CommentTok{\# preview column names}
\CommentTok{\#\textgreater{} [1] "statistic" "parameter" "p.value"   "method"    "data.name" "observed" }
\CommentTok{\#\textgreater{} [7] "expected"  "residuals" "stdres"}
\end{Highlighting}
\end{Shaded}

For our purposes let's simply confirm that the \(p\)-value is lower than the standard .05 threshold for statistical significance.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c1}\SpecialCharTok{$}\NormalTok{p.value }\SpecialCharTok{\textless{}} \FloatTok{0.05}  \CommentTok{\# confirm p{-}value below .05}
\CommentTok{\#\textgreater{} [1] TRUE}
\end{Highlighting}
\end{Shaded}

Other information can be organized in a more readable format using the broom package's \texttt{augment()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c1 }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# statistical result}
  \FunctionTok{augment}\NormalTok{() }\CommentTok{\# view detailed statistical test information}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 6}
\CommentTok{\#\textgreater{}   realization\_of\_recipient .observed .prop .expected .resid .std.resid}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}                        \textless{}int\textgreater{} \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 NP                            2414 0.740     1632.   19.4       27.4}
\CommentTok{\#\textgreater{} 2 PP                             849 0.260     1632.  {-}19.4      {-}27.4}
\end{Highlighting}
\end{Shaded}

Here we can see the observed and expected counts and the proportions for each level of \texttt{realization\_of\_recipient}. We also get additional information concerning residuals, but we will leave these aside.

\textbf{Evaluation}

At this point we may think we are done. We have statistically interrogated the \texttt{realization\_of\_recipient} variable and found that the difference between `NP' and `PP' realization in the datives in this dataset is statistically significant. However, we need to evaluate the size (`effect size') and the reliability of the effect (`confidence interval'). The effectsize package provides a function \texttt{effectsize()} that can provide us both the effect size and confidence interval.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effects }\OtherTok{\textless{}{-}} \FunctionTok{effectsize}\NormalTok{(c1, }\AttributeTok{type =} \StringTok{"phi"}\NormalTok{)  }\CommentTok{\# evaluate effect size and generate a confidence interval (phi type given 2x1 contingency table)}

\NormalTok{effects  }\CommentTok{\# preview effect size and confidence interval}
\CommentTok{\#\textgreater{} Phi  |           95\% CI}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} 0.48 | [0.45,      Inf]}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-} One{-}sided CIs: upper bound fixed at (Inf).}
\end{Highlighting}
\end{Shaded}

\texttt{effectsize()} recognizes the type of test results in \texttt{c1} and calculates the appropriate effect size measure and generates a confidence interval. Since the effect statistic (``Phi'') falls between the 95\% confidence interval this suggests the results are reliably interpreted (chances of Type I (false positive) or Type II (false negative) are low).

Now, the remaining question is to evaluate whether the significant result here is a strong effect or not. To do this we can pass the effect size measure to the \texttt{interpret\_r()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{interpret\_r}\NormalTok{(effects}\SpecialCharTok{$}\NormalTok{phi)  }\CommentTok{\# interpret the effect size }
\CommentTok{\#\textgreater{} [1] "very large"}
\CommentTok{\#\textgreater{} (Rules: funder2019)}
\end{Highlighting}
\end{Shaded}

Turns out we have a strong effect; the realization of dative alternation heavily favors the `NP' form in our data. The potential reasons why are not considered in this univariate analysis, but we will return to this question later as we add independent variables to the statistical analysis.

\hypertarget{continuous}{%
\subsubsection{Continuous}\label{continuous}}

Now let's turn to a case when the variable we aim to interrogate is non-categorical. For this case we will turn to the \texttt{sdac\_disfluencies} dataset. Specifically we will aim to test whether the use of fillers is normally distributed across speakers.

\begin{rmdtip}
This is an important step when working with numeric dependent variables
as the type of distribution will dictate decisions about whether we will
use parametric or non-parametric tests if we consider the extent to
which an independent variable (or variables) can explain the variation
of the dependent variable.
\end{rmdtip}

Since the dataset is currently organized around fillers as the observational unit, I will first transform this dataset to sum the use of fillers for each speaker in the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_speaker\_fillers }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{group\_by}\NormalTok{(speaker\_id) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# group by each speaker}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{sum =} \FunctionTok{sum}\NormalTok{(count)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# add up all fillers used}
  \FunctionTok{ungroup}\NormalTok{() }\CommentTok{\# remove grouping parameter}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:i-uni-cont-sdac-transform-preview}First 10 observations of `sdac_speaker_fillers` dataset.}
\centering
\begin{tabular}[t]{lr}
\toprule
speaker\_id & sum\\
\midrule
155 & 28\\
1000 & 45\\
1001 & 264\\
1002 & 54\\
1004 & 45\\
\addlinespace
1005 & 129\\
1007 & 0\\
1008 & 27\\
1010 & 2\\
1011 & 54\\
\bottomrule
\end{tabular}
\end{table}

\textbf{Descriptive assessment}

Let's perform some descriptive assessement of the variable of interest \texttt{sum}. First let's apply the \texttt{skim()} function and retrieve just the relevant numeric descriptors with \texttt{yank()}. One twist here, however, is that I've customized the \texttt{skim()} function using the \texttt{skim\_with()} to remove the default histogram and add the Interquartile Range (IQR) to the output. This new skim function \texttt{num\_skim()} will take the place of \texttt{skim()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num\_skim }\OtherTok{\textless{}{-}} 
  \FunctionTok{skim\_with}\NormalTok{(}\AttributeTok{numeric =} \FunctionTok{sfl}\NormalTok{(}\AttributeTok{hist =} \ConstantTok{NULL}\NormalTok{, }\CommentTok{\# remove hist skim}
                                   \AttributeTok{iqr =}\NormalTok{ IQR)) }\CommentTok{\# add IQR to skim}

\NormalTok{sdac\_speaker\_fillers }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(sum) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# variable of interest}
  \FunctionTok{num\_skim}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get custom data summary}
  \FunctionTok{yank}\NormalTok{(}\StringTok{"numeric"}\NormalTok{) }\CommentTok{\# only show numeric{-}oriented information}
\end{Highlighting}
\end{Shaded}

\textbf{Variable type: numeric}

\begin{tabular}{l|r|r|r|r|r|r|r|r|r|r}
\hline
skim\_variable & n\_missing & complete\_rate & mean & sd & p0 & p25 & p50 & p75 & p100 & iqr\\
\hline
sum & 0 & 1 & 87.1 & 108 & 0 & 16 & 45 & 114 & 668 & 98\\
\hline
\end{tabular}

We see here that the mean use of fillers is 87.1 across speakers. However, the standard deviation and IQR are large relative to this mean which indicates that the dispersion is quite large, in other words this suggests that there are large differences between speakers. Furthermore, since the median (p50) is smaller than the mean, the distribution is right skewed.

Let's look a couple visualizations of this distribution to appreciate these descriptives. A histogram will provide us a view of the distribution using the counts of the values of \texttt{sum} and a density plot will provide a smooth curve which represents the scaled distribution of the observed data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_speaker\_fillers }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(sum)) }\SpecialCharTok{+} \CommentTok{\# mapping}
  \FunctionTok{geom\_histogram}\NormalTok{() }\SpecialCharTok{+}  \CommentTok{\# geometry}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Fillers"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}

\NormalTok{p2 }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_speaker\_fillers }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(sum)) }\SpecialCharTok{+} \CommentTok{\# mapping}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{geom\_rug}\NormalTok{() }\SpecialCharTok{+}  \CommentTok{\# visualize individual observations}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Fillers"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Density"}\NormalTok{)}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2 }\SpecialCharTok{+} \FunctionTok{plot\_annotation}\NormalTok{(}\StringTok{"Filler count distributions."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-inference_files/figure-latex/i-uni-cont-sdac-visual-1} \end{center}

From these plots that our initial intuitions about the distribution of \texttt{sum} are correct. There is large dispersion between speakers and the data distribution is right skewed.

\begin{rmdtip}
Note that I've used the patchwork package for organizing the display of
plots and including a plot annotation label.
\end{rmdtip}

Since our aim is to test for normality, we can generate a Quantile-Quantile plots (QQ Plot).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_speaker\_fillers }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ sum)) }\SpecialCharTok{+} \CommentTok{\# mapping}
  \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# calculate expected quantile{-}quantile distribution}
  \FunctionTok{stat\_qq\_line}\NormalTok{() }\CommentTok{\# plot the qq{-}line}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-inference_files/figure-latex/i-uni-cont-sdac-qq-plot-1} \end{center}

Since many points do not fall on the expected normal distribution line we have even more evidence to support the notion that the distribution of \texttt{sum} is non-normal.

\textbf{Statistical interrogation}

Although the descriptives and visualizations strongly suggest that we do not have normally distributed data let's run a normality test. For this we turn to the \texttt{shapiro.test()} function which performs the Shapiro-Wilk test of normality. We pass the \texttt{sum} variable to this function to run the test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s1 }\OtherTok{\textless{}{-}} \FunctionTok{shapiro.test}\NormalTok{(sdac\_speaker\_fillers}\SpecialCharTok{$}\NormalTok{sum)  }\CommentTok{\# apply the normality test to \textasciigrave{}sum\textasciigrave{}}

\NormalTok{s1  }\CommentTok{\# preview the test results}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Shapiro{-}Wilk normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  sdac\_speaker\_fillers$sum}
\CommentTok{\#\textgreater{} W = 0.8, p{-}value \textless{}2e{-}16}
\end{Highlighting}
\end{Shaded}

As we saw with the results from the \texttt{chisq.test()} function, the \texttt{shapiro.test()} function produces an object with information about the test including the \(p\)-value. Let's run our logical test to see if the test is statistically significant.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s1}\SpecialCharTok{$}\NormalTok{p.value }\SpecialCharTok{\textless{}} \FloatTok{0.05}  \CommentTok{\# }
\CommentTok{\#\textgreater{} [1] TRUE}
\end{Highlighting}
\end{Shaded}

\textbf{Evaluation}

The results from the Shapiro-Wilk Normality Test tell us that the distribution of \texttt{sum} is statistically found to differ from the normal distribution. So in this case, statistical significance suggests that \texttt{sum} cannot be used as a parametric dependent variable. For our aims this is all the evaluation required. Effect size and confidence intervals are not applicable.

It is of note, however, that the expectation that the variable \texttt{sum} would conform to the normal distribution was low from the outset as we are working with count data. Count data, or frequencies, are in a strict sense not continuous, but rather discrete --meaning that they are real numbers (whole numbers which are always positive). This is a common informational type to encounter in text analysis.

\hypertarget{bivariate-analysis}{%
\subsection{Bivariate analysis}\label{bivariate-analysis}}

A more common scenario in statistical analysis is the consideration of the relationship between two-variables, known as bivariate analysis.

\hypertarget{categorical-1}{%
\subsubsection{Categorical}\label{categorical-1}}

Let's build on our univariate analysis of \texttt{realization\_of\_recipient} and include an explanatory, or independent variable which we will explore to test whether it can explain our earlier finding that `NP' datives are more common that `PP' datives. The question to test, then, is whether modality explains the distribution of the \texttt{realization\_of\_recipient}.

\textbf{Descriptive assessment}

Both the \texttt{realization\_of\_recipient} and \texttt{modality} variables are categorical, specifically nominal as we can see by using \texttt{skim()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dative }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(realization\_of\_recipient, modality) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# select key variables}
  \FunctionTok{skim}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get custom data summary}
  \FunctionTok{yank}\NormalTok{(}\StringTok{"factor"}\NormalTok{) }\CommentTok{\# only show factor{-}oriented information}
\end{Highlighting}
\end{Shaded}

\textbf{Variable type: factor}

\begin{tabular}{l|r|r|l|r|l}
\hline
skim\_variable & n\_missing & complete\_rate & ordered & n\_unique & top\_counts\\
\hline
realization\_of\_recipient & 0 & 1 & FALSE & 2 & NP: 2414, PP: 849\\
\hline
modality & 0 & 1 & FALSE & 2 & spo: 2360, wri: 903\\
\hline
\end{tabular}

For this reason measures of central tendency are not applicable and we will turn to a contingency table to summarize the relationship. The janitor package has a set of functions, the primary function being \texttt{tabyl()}. Other functions used here are to adorn the contingency table with totals, percentages, and to format the output for readability.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dative }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tabyl}\NormalTok{(realization\_of\_recipient, modality) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# cross{-}tabulate}
  \FunctionTok{adorn\_totals}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"row"}\NormalTok{, }\StringTok{"col"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# provide row and column totals}
  \FunctionTok{adorn\_percentages}\NormalTok{(}\StringTok{"col"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# add percentages to the columns}
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{(}\AttributeTok{rounding =} \StringTok{"half up"}\NormalTok{, }\AttributeTok{digits =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# round the digits}
  \FunctionTok{adorn\_ns}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# add observation number}
  \FunctionTok{adorn\_title}\NormalTok{(}\StringTok{"combined"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# add a header title}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# pretty table}
        \AttributeTok{caption =} \StringTok{"Contingency table for \textasciigrave{}realization\_of\_recipient\textasciigrave{} and \textasciigrave{}modality\textasciigrave{}."}\NormalTok{) }\CommentTok{\# caption}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:i-bi-cat-contingency-table}Contingency table for `realization_of_recipient` and `modality`.}
\centering
\begin{tabular}[t]{llll}
\toprule
realization\_of\_recipient/modality & spoken & written & Total\\
\midrule
NP & 79\% (1859) & 61\% (555) & 74\% (2414)\\
PP & 21\%  (501) & 39\% (348) & 26\%  (849)\\
Total & 100\% (2360) & 100\% (903) & 100\% (3263)\\
\bottomrule
\end{tabular}
\end{table}

To gain a better appreciation for this relationship let's generate a couple plots one which shows cross-tabulated counts and the other calculated proportions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} 
\NormalTok{  dative }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ realization\_of\_recipient, }\AttributeTok{fill =}\NormalTok{ modality)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Count"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Realization of recipient"}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p2 }\OtherTok{\textless{}{-}} 
\NormalTok{  dative }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ realization\_of\_recipient, }\AttributeTok{fill =}\NormalTok{ modality)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# geometry, with fill for proportion plot}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Proportion"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Realization of recipient"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"Modality"}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p1 }\OtherTok{\textless{}{-}}\NormalTok{ p1 }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\CommentTok{\# remove legend from left plot}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2 }\SpecialCharTok{+} \FunctionTok{plot\_annotation}\NormalTok{(}\StringTok{"Relationship between Realization of recipient and Modality."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-inference_files/figure-latex/i-bi-cat-visual-1} \end{center}

Looking at the count plot (in the left pane) we see that large difference between the realization of the dative as an `NP' or `PP' obscures to some degree our ability to see to what degree modality is related to the realization of the dative. So, a proportion plot (in the right pane) standardizes each level of \texttt{realization\_of\_recipient} to provide a more comparable view. From the proportion plot we see that there appears to be a trend towards more use of `PP' than `NP' in the written modality.

\textbf{Statistical interrogation}

Although the proportion plot is visually helpful, we use the raw counts to statistically analyze this relationship. Again, as we are working with categorical variables, now for a dependent and independent variable, we use the Chi-squared test. And as before we need to create the cross-tabulation table to pass to the \texttt{chisq.test()} to perform the test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ror\_mod\_table }\OtherTok{\textless{}{-}} 
  \FunctionTok{xtabs}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ realization\_of\_recipient }\SpecialCharTok{+}\NormalTok{ modality, }\CommentTok{\# formula }
        \AttributeTok{data =}\NormalTok{ dative) }\CommentTok{\# dataset}

\NormalTok{c2 }\OtherTok{\textless{}{-}} \FunctionTok{chisq.test}\NormalTok{(ror\_mod\_table) }\CommentTok{\# apply the chi{-}squared test to \textasciigrave{}ror\_mod\_table\textasciigrave{}}

\NormalTok{c2 }\CommentTok{\# \# preview the test results}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pearson\textquotesingle{}s Chi{-}squared test with Yates\textquotesingle{} continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  ror\_mod\_table}
\CommentTok{\#\textgreater{} X{-}squared = 101, df = 1, p{-}value \textless{}2e{-}16}

\NormalTok{c2}\SpecialCharTok{$}\NormalTok{p.value }\SpecialCharTok{\textless{}}\NormalTok{ .}\DecValTok{05} \CommentTok{\# confirm p{-}value below .05}
\CommentTok{\#\textgreater{} [1] TRUE}
\end{Highlighting}
\end{Shaded}

We can preview the result and provide a confirmation of the \(p\)-value. This evidence suggests that there is a difference between the distribution of dative realization according to modality.

We can also see more details about the test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c2 }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# statistical result}
  \FunctionTok{augment}\NormalTok{() }\CommentTok{\# view detailed statistical test information}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 9}
\CommentTok{\#\textgreater{}   realization\_of\_\textasciitilde{} modality .observed .prop .row.prop .col.prop .expected .resid}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}            \textless{}fct\textgreater{}        \textless{}int\textgreater{} \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 NP               spoken        1859 0.570     0.770     0.788     1746.   2.71}
\CommentTok{\#\textgreater{} 2 PP               spoken         501 0.154     0.590     0.212      614.  {-}4.56}
\CommentTok{\#\textgreater{} 3 NP               written        555 0.170     0.230     0.615      668.  {-}4.37}
\CommentTok{\#\textgreater{} 4 PP               written        348 0.107     0.410     0.385      235.   7.38}
\CommentTok{\#\textgreater{} \# ... with 1 more variable: .std.resid \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

\textbf{Evaluation}

Now we want to calculate the effect size and the confidence interval to provide measures of assurance that our finding is robust.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effects }\OtherTok{\textless{}{-}} \FunctionTok{effectsize}\NormalTok{(c2)  }\CommentTok{\# evaluate effect size and generate a confidence interval}

\NormalTok{effects  }\CommentTok{\# preview effect size and confidence interval}
\CommentTok{\#\textgreater{} Cramer\textquotesingle{}s V |       95\% CI}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} 0.18       | [0.15, 1.00]}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-} One{-}sided CIs: upper bound fixed at (1).}

\FunctionTok{interpret\_r}\NormalTok{(effects}\SpecialCharTok{$}\NormalTok{Cramers\_v)  }\CommentTok{\# interpret the effect size}
\CommentTok{\#\textgreater{} [1] "small"}
\CommentTok{\#\textgreater{} (Rules: funder2019)}
\end{Highlighting}
\end{Shaded}

We get effect size and confidence interval information. Note that the effect size, reflected by Cramer's V, for this relationship is weak. This points out an important aspect to evaluation of statistical tests. The fact that a test is significant does not mean that it is meaningful. A small effect size suggests that we should be cautious about the extent to which this significant finding is robust in the population from which the data is sampled.

\hypertarget{continuous-1}{%
\subsubsection{Continuous}\label{continuous-1}}

For a bivariate analysis in which the dependent variable is not categorical, we will turn to the \texttt{sdac\_disfluencies} dataset. The question we will pose to test is whether the use of fillers is related to the type of filler (`uh' or `um').

\textbf{Descriptive assessment}

The key variables to assess in this case are the variables \texttt{count} and \texttt{filler}. But before we start to explore this relationship we will need to transform the dataset such that each speaker's use of the levels of \texttt{filler} is summed. We will use \texttt{group\_by()} to group \texttt{speaker\_id} and \texttt{filler} combinations and then use \texttt{summarize()} to then sum the counts for each filler type for each speaker

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_fillers }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{group\_by}\NormalTok{(speaker\_id, filler) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# grouping parameters}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{sum =} \FunctionTok{sum}\NormalTok{(count)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# summed counts for each speaker{-}filler combination}
  \FunctionTok{ungroup}\NormalTok{() }\CommentTok{\# remove the grouping parameters}
\end{Highlighting}
\end{Shaded}

Let's preview this transformation.

\begin{table}

\caption{\label{tab:i-bi-cont-sdac-fillers-preview}First 10 observations from `sdac_fillers` dataset.}
\centering
\begin{tabular}[t]{llr}
\toprule
speaker\_id & filler & sum\\
\midrule
155 & uh & 28\\
155 & um & 0\\
1000 & uh & 37\\
1000 & um & 8\\
1001 & uh & 262\\
\addlinespace
1001 & um & 2\\
1002 & uh & 34\\
1002 & um & 20\\
1004 & uh & 30\\
1004 & um & 15\\
\bottomrule
\end{tabular}
\end{table}

Let's take a look at them together by grouping the dataset by \texttt{filler} and then using the custom skim function \texttt{num\_skim()} for the numeric variable\texttt{count}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_fillers }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{group\_by}\NormalTok{(filler) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# grouping parameter}
  \FunctionTok{num\_skim}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get custom data summary}
  \FunctionTok{yank}\NormalTok{(}\StringTok{"numeric"}\NormalTok{) }\CommentTok{\# only show numeric{-}oriented information}
\end{Highlighting}
\end{Shaded}

\textbf{Variable type: numeric}

\begin{tabular}{l|l|r|r|r|r|r|r|r|r|r|r}
\hline
skim\_variable & filler & n\_missing & complete\_rate & mean & sd & p0 & p25 & p50 & p75 & p100 & iqr\\
\hline
sum & uh & 0 & 1 & 71.4 & 91.5 & 0 & 14 & 39 & 91 & 661 & 77\\
\hline
sum & um & 0 & 1 & 15.7 & 31.0 & 0 & 0 & 4 & 16 & 265 & 16\\
\hline
\end{tabular}

We see here that the standard deviation and IQR for both `uh' and `um' are relatively large for the respective means (71.4 and 15.7) suggesting the distribution is quite dispersed. Let's take a look at a boxplot to visualize the counts in \texttt{sum} for each level of \texttt{filler}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_fillers }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ filler, }\AttributeTok{y =}\NormalTok{ sum)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Filler"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Counts"}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p2 }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_fillers }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ filler, }\AttributeTok{y =}\NormalTok{ sum)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# scale the y axis to trim outliers}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Filler"}\NormalTok{, }\AttributeTok{y =} \StringTok{""}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-inference_files/figure-latex/i-bi-cont-visual-1} \end{center}

In the plot in the left pane we see a couple things. First, it appears that there is in fact quite a bit of dispersion as there are quite a few outliers (dots) above the lines extending from the boxes. Recall that the boxes represent the first and third quantile, that is the IQR and that the notches represent the confidence interval. Second, when we compare the boxes and their notches we see that there is little overlap (looking horizontally). In the right pane I've zoomed in a bit trimming some outliers to get a better view of the relationship between the boxes. Since the overlap is minimal and in particular the notches do not overlap at all, this is a good indication that there is a significant trend.

From the descriptive statistics and the visual summary it appears that the filler `uh' is more common than `um'. It's now time to submit this to statistical interrogation.

\textbf{Statistical interrogation}

In a bivariate (and multivariate) analysis where the dependent variable is non-categorical we apply Linear Regression Modeling (LM). The default assumption of linear models, however, is that the dependent variable is normally distributed. As we have seen our variable \texttt{sum} does not conform to the normal distribution. We know this because of our tests in the univariate case, but as mentioned at the end of that section, we are working with count data which by nature is understood as discrete and not continuous in a strict technical sense. So instead of using the linear model for our regression analysis we will use the Generalized Linear Model (GLM) \citep{Baayen2008a, Gries2013a}.

The function \texttt{glm()} implements generalized linear models. In addition to the formula (\texttt{sum\ \textasciitilde{}\ filler}) and the dataset to use, we also include an appropriate distribution family for the dependent variable. For count and frequency data the appropriate family is the ``Poisson'' distribution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 }\OtherTok{\textless{}{-}} 
  \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ sum }\SpecialCharTok{\textasciitilde{}}\NormalTok{ filler, }\CommentTok{\# formula}
      \AttributeTok{data =}\NormalTok{ sdac\_fillers, }\CommentTok{\# dataset}
      \AttributeTok{family =} \StringTok{"poisson"}\NormalTok{) }\CommentTok{\# distribution family}

\FunctionTok{summary}\NormalTok{(m1) }\CommentTok{\# preview the test results}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = sum \textasciitilde{} filler, family = "poisson", data = sdac\_fillers)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}    Min      1Q  Median      3Q     Max  }
\CommentTok{\#\textgreater{} {-}11.95   {-}5.61   {-}3.94    0.80   41.99  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)  4.26794    0.00564     757   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} fillerum    {-}1.51308    0.01327    {-}114   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for poisson family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 72049  on 881  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 55071  on 880  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 58524}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 6}
\end{Highlighting}
\end{Shaded}

Let's focus on the coefficients, specifically for the `fillerum' line. Since our factor \texttt{filler} has two levels one level is used as the reference to contrast with the other level. In this case by default the first level is used as the reference. Therefore the coefficients we see in `fillerum' are `um' in contrast to `uh'. Without digging into the details of the other parameter statistics, let's focus on the last column which contains the \(p\)-value. A convenient aspect of the \texttt{summary()} function when applied to regression model results is that it provides statistical significance codes. In this case we can see that the contrast between `uh' and `um' is signficant at \(p < .001\) which of course is lower than our standard threshold of \(.05\).

Therefore we can say with some confidence that the filler `uh' is more frequent than `um'.

\textbf{Evaluation}

Given we have found a significant effect for \texttt{filler}, let's look at evaluating the effect size and the confidence interval. Again, we use the \texttt{effectsize()} function. We then can preview the \texttt{effects} object. Note that effect size of interest is in the second row of the coefficient (\texttt{Std\_Coefficient}) so we subset this column to extract only the effect coefficient for the \texttt{filler} contrast.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effects }\OtherTok{\textless{}{-}} \FunctionTok{effectsize}\NormalTok{(m1)  }\CommentTok{\# evaluate effect size and generate a confidence interval}

\NormalTok{effects  }\CommentTok{\# preview effect size and confidence interval}
\CommentTok{\#\textgreater{} \# Standardization method: refit}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameter   | Coefficient (std.) |         95\% CI}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} (Intercept) |               4.27 | [ 4.26,  4.28]}
\CommentTok{\#\textgreater{} fillerum    |              {-}1.51 | [{-}1.54, {-}1.49]}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Response is unstandardized)}

\FunctionTok{interpret\_r}\NormalTok{(effects}\SpecialCharTok{$}\NormalTok{Std\_Coefficient[}\DecValTok{2}\NormalTok{])  }\CommentTok{\# interpret the effect size}
\CommentTok{\#\textgreater{} [1] "very large"}
\CommentTok{\#\textgreater{} (Rules: funder2019)}
\end{Highlighting}
\end{Shaded}

The coefficient statistic falls within the confidence interval and the effect size is strong so we can be confident that our findings are reliable given this data.

\hypertarget{multivariate-analysis}{%
\subsection{Multivariate analysis}\label{multivariate-analysis}}

The last case to consider is when we have more than one independent variable we want to use to assess their potential relationship to the dependent variable. Again we will consider a categorical and non-categorical dependent variable. But, in this case the implementation methods are quite similar, as we will see.

\hypertarget{categorical-2}{%
\subsubsection{Categorical}\label{categorical-2}}

For the categorical multivariate case we will again consider the \texttt{dative} dataset and build on the previous analyses. The question to be posed is whether modality in combination with the length of the recipient (\texttt{length\_of\_recipient}) together explain the distribution of the realization of the recipient (\texttt{realization\_of\_recipient}).

\textbf{Descriptive assessment}

Now that we have three variables, there is more to summarize to get our descriptive information. Luckily, however, the same process can be applied to three (or more) variables using the \texttt{group\_by()} function and then passed to \texttt{skim()}. In this case we have two categorical variables and one numeric variable. So we will group by both the categorical variables and then pass the numeric variable to the custom \texttt{num\_skim()} function --pulling out only the relevant descriptive information for numeric variables with \texttt{yank()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dative }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(realization\_of\_recipient, modality, length\_of\_recipient) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# select key variables}
  \FunctionTok{group\_by}\NormalTok{(realization\_of\_recipient, modality) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# grouping parameters}
  \FunctionTok{num\_skim}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get custom data summary}
  \FunctionTok{yank}\NormalTok{(}\StringTok{"numeric"}\NormalTok{) }\CommentTok{\# only show numeric{-}oriented information}
\end{Highlighting}
\end{Shaded}

\textbf{Variable type: numeric}

\begin{tabular}{l|l|l|r|r|r|r|r|r|r|r|r|r}
\hline
skim\_variable & realization\_of\_recipient & modality & n\_missing & complete\_rate & mean & sd & p0 & p25 & p50 & p75 & p100 & iqr\\
\hline
length\_of\_recipient & NP & spoken & 0 & 1 & 1.14 & 0.60 & 1 & 1 & 1 & 1 & 12 & 0\\
\hline
length\_of\_recipient & NP & written & 0 & 1 & 1.95 & 1.59 & 1 & 1 & 2 & 2 & 17 & 1\\
\hline
length\_of\_recipient & PP & spoken & 0 & 1 & 2.30 & 2.04 & 1 & 1 & 2 & 3 & 15 & 2\\
\hline
length\_of\_recipient & PP & written & 0 & 1 & 4.75 & 4.10 & 1 & 2 & 4 & 6 & 31 & 4\\
\hline
\end{tabular}

There is much more information now that we are considering multiple independent variables, but if we look over the measures of dispersion we can see that the median and the IQR are relatively similar to their respective means suggesting that there are fewer outliers and relativley little skew.

Let's take a look at a visualization of this information. Since we are working with a categorical dependent variable and there is one non-categorical variable we can use a boxplot. The addition here is to include a \texttt{color} mapping which will provide a distinct box for each level of modality (`written' and `spoken').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} 
\NormalTok{  dative }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ realization\_of\_recipient, }\AttributeTok{y =}\NormalTok{ length\_of\_recipient, }\AttributeTok{color =}\NormalTok{ modality)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Realization of recipient"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Length of recipient (in words)"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Modality"}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p2 }\OtherTok{\textless{}{-}} 
\NormalTok{  dative }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ realization\_of\_recipient, }\AttributeTok{y =}\NormalTok{ length\_of\_recipient, }\AttributeTok{color =}\NormalTok{ modality)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# scale the y axis to trim outliers}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Realization of recipient"}\NormalTok{, }\AttributeTok{y =} \StringTok{""}\NormalTok{, }\AttributeTok{color =} \StringTok{"Modality"}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p1 }\OtherTok{\textless{}{-}}\NormalTok{ p1 }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\CommentTok{\# remove the legend from the left pane plot}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-inference_files/figure-latex/i-multi-cat-visual-1} \end{center}

In the left pane we see the entire visualization including all outliers. From this view it appears that there is a potential trend that the length of the recipient is larger when the realization of the recipient is `PP'. There is also a potential trend for modality with written language showing longer recipient lengths overall. The pane on the right is scaled to get a better view of the boxes by scaling the y-axis down and as such trimming the outliers. This plot shows more clearly that the length of the recipient is longer when the recipient is realized as a `PP'. Again, the contrast in modality is also a potential trend, but the boxes (of the same color), particularly for the spoken modality overlap to some degree.

So we have some trends in mind which will help us interpret the statistical interrogation so let's move there next.

\textbf{Statistical interrogation}

Once we involve more than two variables, the choice of statistical method turns towards regression. In the case that the dependent variable is categorical, however, we will use Logistic Regression. The workhorse function \texttt{glm()} can be used for a series of regression models, including logistic regression. The requirement, however, is that we specify the family of the distribution. For logistic regression the family is ``binomial''. The formula includes the dependent variable as a function of our other two variables, each are separated by the \texttt{+} operator.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ realization\_of\_recipient }\SpecialCharTok{\textasciitilde{}}\NormalTok{ modality }\SpecialCharTok{+}\NormalTok{ length\_of\_recipient, }\CommentTok{\# formula}
          \AttributeTok{data =}\NormalTok{ dative, }\CommentTok{\# dataset}
          \AttributeTok{family =} \StringTok{"binomial"}\NormalTok{) }\CommentTok{\# distribution family}

\FunctionTok{summary}\NormalTok{(m1) }\CommentTok{\# preview the test results}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = realization\_of\_recipient \textasciitilde{} modality + length\_of\_recipient, }
\CommentTok{\#\textgreater{}     family = "binomial", data = dative)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}    Min      1Q  Median      3Q     Max  }
\CommentTok{\#\textgreater{} {-}4.393  {-}0.598  {-}0.598   0.132   1.924  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                     Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)          {-}2.3392     0.0797  {-}29.35   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} modalitywritten      {-}0.0483     0.1069   {-}0.45     0.65    }
\CommentTok{\#\textgreater{} length\_of\_recipient   0.7081     0.0420   16.86   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 3741.1  on 3262  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 3104.7  on 3260  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 3111}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 5}
\end{Highlighting}
\end{Shaded}

The results from the model again provide a wealth of information. But the key information to focus on is the coefficients. In particular the coefficients for the independent variables \texttt{modality} and \texttt{length\_of\_recipient}. What we notice, is that the \(p\)-value for \texttt{length\_of\_recipient} is significant, but the contrast between `written' and `spoken' for \texttt{modality} is not. If you recall, we used this same dataset to explore \texttt{modality} as a single indpendent variable earlier --and it was found to be significant. So why now is it not? The answer is that when multiple variables are used to explain the distribution of a measure (dependent variable) each variable now adds more information to explain the dependent variable --each has it's own contribution. Since \texttt{length\_of\_recipient} is significant, this suggests that the explanatory power of \texttt{modality} is weak, especially when compared to \texttt{length\_of\_recipient}. This make sense as we saw in the earlier model the fact that the effect size for \texttt{modality} was not strong and that is now more evident that the \texttt{length\_of\_recipient} is included in the model.

\textbf{Evaluation}

Now let's move on and gauge the effect size and calculate the confidence interval for \texttt{length\_of\_recipient} in our model. We apply the \texttt{effectsize()} function to the model and then use \texttt{interpret\_r()} on the coefficient of interest (which is in the fourth row of the \texttt{Std\_Coefficients} column).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effects }\OtherTok{\textless{}{-}} \FunctionTok{effectsize}\NormalTok{(m1)  }\CommentTok{\# evaluate effect size and generate a confidence interval}

\NormalTok{effects  }\CommentTok{\# preview effect size and confidence interval}
\CommentTok{\#\textgreater{} \# Standardization method: refit}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameter           | Coefficient (std.) |         95\% CI}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} (Intercept)         |              {-}1.03 | [{-}1.15, {-}0.92]}
\CommentTok{\#\textgreater{} modalitywritten     |              {-}0.05 | [{-}0.26,  0.16]}
\CommentTok{\#\textgreater{} length\_of\_recipient |               1.46 | [ 1.30,  1.64]}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Response is unstandardized)}

\FunctionTok{interpret\_r}\NormalTok{(effects}\SpecialCharTok{$}\NormalTok{Std\_Coefficient[}\DecValTok{4}\NormalTok{])  }\CommentTok{\# interpret the effect size}
\CommentTok{\#\textgreater{} [1] NA}
\CommentTok{\#\textgreater{} (Rules: funder2019)}
\end{Highlighting}
\end{Shaded}

We see we have a coefficient that falls within the confidence interval and the effect size is large. So we can saw with some confidence that the length of the recipient is a significant predictor of the use of `PP' as the realization of the recipient in the dative alternation.

\hypertarget{continuous-2}{%
\subsubsection{Continuous}\label{continuous-2}}

The last case we will consider here is when the dependent variable is non-categorical and we have more than one independent variable. The question we will pose is whether the type of filler and the sex of the speaker can explain the use of fillers in conversational speech.

We will need to prepare the data before we get started as our current data frame \texttt{sdac\_fillers} has filler and the sum count for each filler grouped by speaker --but it does not include the \texttt{sex} of each speaker. The \texttt{sdac\_disfluencies} data frame does have the \texttt{sex} column, but it has not been grouped by speaker. So let's transform the \texttt{sdac\_disfluencies} summarizing it to only get the \texttt{speaker\_id} and \texttt{sex} combinations. This should result in a data frame with 441 observations, one observation for each speaker in the corpus.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_speakers\_sex }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{distinct}\NormalTok{(speaker\_id, sex) }\CommentTok{\# summarize for distinct \textasciigrave{}speaker\_id\textasciigrave{} and \textasciigrave{}sex\textasciigrave{} values}
\end{Highlighting}
\end{Shaded}

Let's preview the first 10 observations form this transformation.

\begin{table}

\caption{\label{tab:i-multi-cont-transform-sdac-preview}First 10 observations of the `sdac_speakers_sex` data frame.}
\centering
\begin{tabular}[t]{ll}
\toprule
speaker\_id & sex\\
\midrule
155 & NA\\
1000 & FEMALE\\
1001 & MALE\\
1002 & FEMALE\\
1004 & FEMALE\\
\addlinespace
1005 & FEMALE\\
1007 & FEMALE\\
1008 & FEMALE\\
1010 & MALE\\
1011 & FEMALE\\
\bottomrule
\end{tabular}
\end{table}

Great, now we have each \texttt{speaker\_id} and \texttt{sex} for all 441 speakers. One thing to note, however, is that speaker `155' does not have a value for \texttt{sex} --this seems to be an error in the metadata that we will need to deal with before we proceed in our analysis. Let's move on to join our new \texttt{sdac\_speakers\_sex} data frame and the \texttt{sdac\_fillers} data frame.

Now that we have a complete dataset with \texttt{speaker\_id} and \texttt{sex} we will now join this dataset with our \texttt{sdac\_fillers} dataset effectively adding the column \texttt{sex}. We want to keep all the observations in \texttt{sdac\_fillers} and add the column \texttt{sex} for observations that correspond between each data frame for the column \texttt{speaker\_id} so we will use a left join with the function \texttt{left\_join()} with the \texttt{sdac\_fillers} dataset on the left.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_fillers\_sex }\OtherTok{\textless{}{-}} \FunctionTok{left\_join}\NormalTok{(sdac\_fillers, sdac\_speakers\_sex)  }\CommentTok{\# join}
\end{Highlighting}
\end{Shaded}

Now let's preview the first observations in this new \texttt{sdac\_fillers\_sex} data frame.

\begin{table}

\caption{\label{tab:i-multi-cont-sdac-fillers-sex-preview}First 10 observations of the `sdac_fillers_sex` data frame.}
\centering
\begin{tabular}[t]{llrl}
\toprule
speaker\_id & filler & sum & sex\\
\midrule
155 & uh & 28 & NA\\
155 & um & 0 & NA\\
1000 & uh & 37 & FEMALE\\
1000 & um & 8 & FEMALE\\
1001 & uh & 262 & MALE\\
\addlinespace
1001 & um & 2 & MALE\\
1002 & uh & 34 & FEMALE\\
1002 & um & 20 & FEMALE\\
1004 & uh & 30 & FEMALE\\
1004 & um & 15 & FEMALE\\
\bottomrule
\end{tabular}
\end{table}

At this point let's drop this speaker from the \texttt{sdac\_speakers\_sex} data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_fillers\_sex }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_fillers\_sex }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{filter}\NormalTok{(speaker\_id }\SpecialCharTok{!=} \StringTok{"155"}\NormalTok{) }\CommentTok{\# drop speaker\_id 155}
\end{Highlighting}
\end{Shaded}

We are now ready to proceed in our analysis.

\textbf{Descriptive assessment}

The process by now should be quite routine for getting our descriptive statistics: select the key variables, group by the categorical variables, and finally pull the descriptives for the numeric variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_fillers\_sex }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(sum, filler, sex) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# select key variables}
  \FunctionTok{group\_by}\NormalTok{(filler, sex) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# grouping parameters}
  \FunctionTok{num\_skim}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get custom data summary}
  \FunctionTok{yank}\NormalTok{(}\StringTok{"numeric"}\NormalTok{) }\CommentTok{\# only show numeric{-}oriented information}
\end{Highlighting}
\end{Shaded}

\textbf{Variable type: numeric}

\begin{tabular}{l|l|l|r|r|r|r|r|r|r|r|r|r}
\hline
skim\_variable & filler & sex & n\_missing & complete\_rate & mean & sd & p0 & p25 & p50 & p75 & p100 & iqr\\
\hline
sum & uh & FEMALE & 0 & 1 & 63.22 & 76.5 & 0 & 12.0 & 39.0 & 81.8 & 509 & 69.8\\
\hline
sum & uh & MALE & 0 & 1 & 78.74 & 102.6 & 0 & 15.2 & 37.5 & 101.5 & 661 & 86.2\\
\hline
sum & um & FEMALE & 0 & 1 & 22.38 & 36.3 & 0 & 1.0 & 9.0 & 28.0 & 265 & 27.0\\
\hline
sum & um & MALE & 0 & 1 & 9.92 & 24.2 & 0 & 0.0 & 1.0 & 8.0 & 217 & 8.0\\
\hline
\end{tabular}

Looking at these descriptives, it seems like there is quite a bit of variability for some combinations and not others. In short, it's a mixed bag. Let's try to make sense of these numbers with a boxplot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_fillers\_sex }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ filler, }\AttributeTok{y =}\NormalTok{ sum, }\AttributeTok{color =}\NormalTok{ sex)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Filler"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Counts"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Sex"}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p2 }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_fillers\_sex }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ filler, }\AttributeTok{y =}\NormalTok{ sum, }\AttributeTok{color =}\NormalTok{ sex)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{200}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# scale the y axis to trim outliers}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Filler"}\NormalTok{, }\AttributeTok{y =} \StringTok{""}\NormalTok{, }\AttributeTok{color =} \StringTok{"Sex"}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p1 }\OtherTok{\textless{}{-}}\NormalTok{ p1 }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\CommentTok{\# drop the legend from the left pane plot}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-inference_files/figure-latex/i-mulit-cont-visual-1} \end{center}

We can see that `uh' is used more than `um' overall. But that whereas men and women use `uh' in similar ways, women use more `um' than men. This is known as an interaction. So we will approach our statistical analysis with this in mind.

\textbf{Statistical interrogation}

We will again use a generalized linear model with the \texttt{glm()} function to conduct our test. The distribution family will be the same has we are again using the \texttt{sum} as our dependent variable which contains discrete count values. The formula we will use, however, is new. Instead of adding a new variable to our independent variables, we will test the possible interaction between \texttt{filler} and \texttt{sex} that we noted in the descriptive assessment. To encode an interaction the \texttt{*} operator is used. So our formula will take the form \texttt{sum\ \textasciitilde{}\ filler\ *\ sex}. Let's generate the model and view the summary of the test results as we have done before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 }\OtherTok{\textless{}{-}} 
  \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ sum }\SpecialCharTok{\textasciitilde{}}\NormalTok{ filler }\SpecialCharTok{*}\NormalTok{ sex, }\CommentTok{\# formula}
      \AttributeTok{data =}\NormalTok{ sdac\_fillers\_sex, }\CommentTok{\# dataset}
      \AttributeTok{family =} \StringTok{"poisson"}\NormalTok{) }\CommentTok{\# distribution family}

\FunctionTok{summary}\NormalTok{(m1) }\CommentTok{\# preview the test results}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = sum \textasciitilde{} filler * sex, family = "poisson", data = sdac\_fillers\_sex)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}    Min      1Q  Median      3Q     Max  }
\CommentTok{\#\textgreater{} {-}12.55   {-}6.21   {-}3.64    1.08   40.60  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)       4.14660    0.00876   473.2   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} fillerum         {-}1.03827    0.01714   {-}60.6   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} sexMALE           0.21955    0.01145    19.2   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} fillerum:sexMALE {-}1.03344    0.02791   {-}37.0   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for poisson family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 71956  on 879  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 53543  on 876  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 56994}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 6}
\end{Highlighting}
\end{Shaded}

Again looking at the coefficients we something new. First we see that there is a row for the \texttt{filler} contrast and the \texttt{sex} contrast but also the interaction between \texttt{filler} and \texttt{sex} (`fillerum:sexMALE'). All rows show significant effects. It is important to note that when an interaction is explored and it is found to be significant, the other simple effects, known as main effects (`fillerum' and `sexMALE'), are ignored. Only the higer-order effect is considered significant.

Now what does the `fillerum:sexMALE' row mean. It means that there is an interaction between \texttt{filler} and \texttt{sex}. the directionality of that interaction should be interpreted using our descriptive assessment, in particular the visual boxplots we generated. In sum, women use more `um' than men or stated another way men use `um' less than women.

\textbf{Evaluation}

We finalize our analysis by looking at the effect size and confidence intervals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effects }\OtherTok{\textless{}{-}} \FunctionTok{effectsize}\NormalTok{(m1)  }\CommentTok{\# evaluate effect size and generate a confidence interval}

\NormalTok{effects  }\CommentTok{\# preview effect size and confidence interval}
\CommentTok{\#\textgreater{} \# Standardization method: refit}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameter        | Coefficient (std.) |         95\% CI}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} (Intercept)      |               4.15 | [ 4.13,  4.16]}
\CommentTok{\#\textgreater{} fillerum         |              {-}1.04 | [{-}1.07, {-}1.00]}
\CommentTok{\#\textgreater{} sexMALE          |               0.22 | [ 0.20,  0.24]}
\CommentTok{\#\textgreater{} fillerum:sexMALE |              {-}1.03 | [{-}1.09, {-}0.98]}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Response is unstandardized)}

\FunctionTok{interpret\_r}\NormalTok{(effects}\SpecialCharTok{$}\NormalTok{Std\_Coefficient[}\DecValTok{4}\NormalTok{])  }\CommentTok{\# interpret the effect size}
\CommentTok{\#\textgreater{} [1] "very large"}
\CommentTok{\#\textgreater{} (Rules: funder2019)}
\end{Highlighting}
\end{Shaded}

We can conclude, then, that there is a strong interaction effect for \texttt{filler} and \texttt{sex} and that women use more `um' than men.

\hypertarget{summary-10}{%
\subsection{Summary}\label{summary-10}}

In this chapter we have discussed various approaches to conducting inferential data analysis. Each configuration, however, always includes a descriptive assessment, statistical interrogation, and an evaluation of the results. We considered univariate, bivariate, and multivariate analyses using both categorical and non-categorical dependent variables to explore the similarities and differences between these approaches.

\hypertarget{prediction}{%
\section{Prediction}\label{prediction}}

INCOMPLETE DRAFT

\begin{quote}
\ldots{}
\end{quote}

\begin{rmdkey}
The essential questions for this chapter are:

\begin{itemize}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\item
  \ldots{}
\end{itemize}
\end{rmdkey}

In this chapter we will

Orientation to the question(s) and dataset(s) to be explored \ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wricle\_df }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"resources/10{-}prediction/data/derived/wricle\_formal\_curated.csv"}\NormalTok{)}

\NormalTok{locness\_df }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"resources/10{-}prediction/data/derived/locness\_curated.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{learners\_df }\OtherTok{\textless{}{-}}\NormalTok{ wricle\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(native\_language }\SpecialCharTok{==} \StringTok{"Spanish"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{student =} \StringTok{"Spanish"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"Learner"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(}\AttributeTok{essay\_id =}\NormalTok{ id, type, student, essay)}

\FunctionTok{glimpse}\NormalTok{(learners\_df)}
\CommentTok{\#\textgreater{} Rows: 689}
\CommentTok{\#\textgreater{} Columns: 4}
\CommentTok{\#\textgreater{} $ essay\_id \textless{}chr\textgreater{} "A1{-}1", "A1{-}2", "A10{-}1", "A10{-}2", "A101{-}1", "A101{-}2", "A101{-}3\textasciitilde{}}
\CommentTok{\#\textgreater{} $ type     \textless{}chr\textgreater{} "Learner", "Learner", "Learner", "Learner", "Learner", "Learn\textasciitilde{}}
\CommentTok{\#\textgreater{} $ student  \textless{}chr\textgreater{} "Spanish", "Spanish", "Spanish", "Spanish", "Spanish", "Spani\textasciitilde{}}
\CommentTok{\#\textgreater{} $ essay    \textless{}chr\textgreater{} "In our present society gay people is asking for the same rig\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{natives\_df }\OtherTok{\textless{}{-}}\NormalTok{ locness\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{unite}\NormalTok{(}\AttributeTok{col =}\NormalTok{ doc\_id, }\FunctionTok{c}\NormalTok{(}\StringTok{"file\_id"}\NormalTok{, }\StringTok{"essay\_id"}\NormalTok{), }\AttributeTok{sep =} \StringTok{"{-}"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{doc\_id =} \FunctionTok{str\_replace}\NormalTok{(doc\_id, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.txt"}\NormalTok{, }\StringTok{""}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"Native"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(}\AttributeTok{essay\_id =}\NormalTok{ doc\_id, type, student, essay)}

\FunctionTok{glimpse}\NormalTok{(natives\_df)}
\CommentTok{\#\textgreater{} Rows: 411}
\CommentTok{\#\textgreater{} Columns: 4}
\CommentTok{\#\textgreater{} $ essay\_id \textless{}chr\textgreater{} "alevels1{-}1", "alevels1{-}2", "alevels1{-}3", "alevels1{-}4", "alev\textasciitilde{}}
\CommentTok{\#\textgreater{} $ type     \textless{}chr\textgreater{} "Native", "Native", "Native", "Native", "Native", "Native", "\textasciitilde{}}
\CommentTok{\#\textgreater{} $ student  \textless{}chr\textgreater{} "British", "British", "British", "British", "British", "Briti\textasciitilde{}}
\CommentTok{\#\textgreater{} $ essay    \textless{}chr\textgreater{} "The basic dilema facing the UK\textquotesingle{}s rail and road transport sys\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nativeness\_df }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(learners\_df, natives\_df)  }\CommentTok{\# combine}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nativeness\_df }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    janitor}\SpecialCharTok{::}\FunctionTok{tabyl}\NormalTok{(type)}
\CommentTok{\#\textgreater{}     type   n percent}
\CommentTok{\#\textgreater{}  Learner 689   0.626}
\CommentTok{\#\textgreater{}   Native 411   0.374}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nativeness\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{output =} \StringTok{"word"}\NormalTok{, }\AttributeTok{input =} \StringTok{"essay"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{count}\NormalTok{(word, type) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(type) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarize}\NormalTok{(}\AttributeTok{total\_words =} \FunctionTok{sum}\NormalTok{(n))}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 2}
\CommentTok{\#\textgreater{}   type    total\_words}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}         \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Learner      640535}
\CommentTok{\#\textgreater{} 2 Native       324269}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nativeness\_corpus }\OtherTok{\textless{}{-}}\NormalTok{ nativeness\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{corpus}\NormalTok{(}\AttributeTok{text\_field =} \StringTok{"essay"}\NormalTok{)}

\NormalTok{nativeness\_corpus\_summary }\OtherTok{\textless{}{-}}\NormalTok{ nativeness\_corpus }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summary}\NormalTok{(}\AttributeTok{n =} \FunctionTok{ndoc}\NormalTok{(nativeness\_corpus))}

\NormalTok{nativeness\_corpus\_summary }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\CommentTok{\#\textgreater{} Corpus consisting of 1100 documents, showing 1100 documents:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}    Text Types Tokens Sentences essay\_id    type student}
\CommentTok{\#\textgreater{}   text1   210    496        26     A1{-}1 Learner Spanish}
\CommentTok{\#\textgreater{}   text2   253    655        29     A1{-}2 Learner Spanish}
\CommentTok{\#\textgreater{}   text3   280    722        30    A10{-}1 Learner Spanish}
\CommentTok{\#\textgreater{}   text4   177    389        15    A10{-}2 Learner Spanish}
\CommentTok{\#\textgreater{}   text5   245    636        19   A101{-}1 Learner Spanish}
\CommentTok{\#\textgreater{}   text6   219    692        17   A101{-}2 Learner Spanish}
\CommentTok{\#\textgreater{}   text7   213    615        16   A101{-}3 Learner Spanish}
\CommentTok{\#\textgreater{}   text8   319    914        23   A101{-}4 Learner Spanish}
\CommentTok{\#\textgreater{}   text9   253    654        19   A102{-}1 Learner Spanish}
\CommentTok{\#\textgreater{}  text10   310    816        21   A102{-}2 Learner Spanish}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nativeness\_corpus}\SpecialCharTok{$}\NormalTok{doc\_id }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ndoc}\NormalTok{(nativeness\_corpus)}

\NormalTok{nativeness\_corpus }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{docvars}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{}   essay\_id    type student doc\_id}
\CommentTok{\#\textgreater{} 1     A1{-}1 Learner Spanish      1}
\CommentTok{\#\textgreater{} 2     A1{-}2 Learner Spanish      2}
\CommentTok{\#\textgreater{} 3    A10{-}1 Learner Spanish      3}
\CommentTok{\#\textgreater{} 4    A10{-}2 Learner Spanish      4}
\CommentTok{\#\textgreater{} 5   A101{-}1 Learner Spanish      5}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nativeness\_tokens }\OtherTok{\textless{}{-}}\NormalTok{ nativeness\_corpus }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{tokens}\NormalTok{(}\AttributeTok{what =} \StringTok{"word"}\NormalTok{, }\AttributeTok{remove\_punct =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{remove\_numbers =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{remove\_symbols =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{nativeness\_tokens }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{tokens\_group}\NormalTok{(}\AttributeTok{groups =}\NormalTok{ type) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} Tokens consisting of 2 documents and 1 docvar.}
\CommentTok{\#\textgreater{} Learner :}
\CommentTok{\#\textgreater{}  [1] "In"      "our"     "present" "society" "gay"     "people"  "is"     }
\CommentTok{\#\textgreater{}  [8] "asking"  "for"     "the"     "same"    "rights" }
\CommentTok{\#\textgreater{} [ ... and 636,407 more ]}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Native :}
\CommentTok{\#\textgreater{}  [1] "The"       "basic"     "dilema"    "facing"    "the"       "UK\textquotesingle{}s"     }
\CommentTok{\#\textgreater{}  [7] "rail"      "and"       "road"      "transport" "system"    "is"       }
\CommentTok{\#\textgreater{} [ ... and 321,936 more ]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nativeness\_dfm }\OtherTok{\textless{}{-}}\NormalTok{ nativeness\_tokens }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{dfm}\NormalTok{()}

\NormalTok{nativeness\_dfm }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{} Document{-}feature matrix of: 5 documents, 28,129 features (99.24\% sparse) and 4 docvars.}
\CommentTok{\#\textgreater{}        features}
\CommentTok{\#\textgreater{} docs    in our present society gay people is asking for the}
\CommentTok{\#\textgreater{}   text1 12   5       4       5   5     10 12      1   3  23}
\CommentTok{\#\textgreater{}   text2 26   2       1       4   0     10 14      0   1  45}
\CommentTok{\#\textgreater{}   text3 12   4       0       4   9     16 12      0   0  12}
\CommentTok{\#\textgreater{}   text4 15   1       0       1   0      9  7      0   1  10}
\CommentTok{\#\textgreater{}   text5 15   1       1       2   0      7 17      0   6  40}
\CommentTok{\#\textgreater{} [ reached max\_nfeat ... 28,119 more features ]}

\NormalTok{nativeness\_dfm }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{dfm\_group}\NormalTok{(}\AttributeTok{groups =}\NormalTok{ type) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{} Document{-}feature matrix of: 2 documents, 28,129 features (33.74\% sparse) and 1 docvar.}
\CommentTok{\#\textgreater{}          features}
\CommentTok{\#\textgreater{} docs         in  our present society gay people    is asking  for   the}
\CommentTok{\#\textgreater{}   Learner 15518 1279     322    1269 520   5980 12931     33 4993 36728}
\CommentTok{\#\textgreater{}   Native   6357  584     109     422   6   1569  6307     12 3144 21090}
\CommentTok{\#\textgreater{} [ reached max\_nfeat ... 28,119 more features ]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nativeness\_dfm }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{topfeatures}\NormalTok{(}\AttributeTok{n =} \DecValTok{25}\NormalTok{)}
\CommentTok{\#\textgreater{}     the      of      to     and      in    that       a      is      it    this }
\CommentTok{\#\textgreater{}   57818   32821   31155   24158   21875   20847   19981   19238   11259   10357 }
\CommentTok{\#\textgreater{}      be     are    they     not     for      as  people    have    with      or }
\CommentTok{\#\textgreater{}   10096   10085    8438    8196    8137    7977    7549    7506    6007    5568 }
\CommentTok{\#\textgreater{}   their       i      on      by because }
\CommentTok{\#\textgreater{}    5192    5128    4820    4579    4511}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nativeness\_dfm }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{textstat\_frequency}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{, }\AttributeTok{groups =}\NormalTok{ type)}
\CommentTok{\#\textgreater{}    feature frequency rank docfreq   group}
\CommentTok{\#\textgreater{} 1      the     36728    1     689 Learner}
\CommentTok{\#\textgreater{} 2       of     22095    2     689 Learner}
\CommentTok{\#\textgreater{} 3       to     20417    3     689 Learner}
\CommentTok{\#\textgreater{} 4     that     15929    4     689 Learner}
\CommentTok{\#\textgreater{} 5      and     15836    5     689 Learner}
\CommentTok{\#\textgreater{} 6      the     21090    1     410  Native}
\CommentTok{\#\textgreater{} 7       to     10738    2     410  Native}
\CommentTok{\#\textgreater{} 8       of     10726    3     410  Native}
\CommentTok{\#\textgreater{} 9      and      8322    4     409  Native}
\CommentTok{\#\textgreater{} 10       a      6827    5     410  Native}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nativeness\_dfm }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{dfm\_tfidf}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{textstat\_frequency}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{, }\AttributeTok{groups =}\NormalTok{ type, }\AttributeTok{force =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{}      feature frequency rank docfreq   group}
\CommentTok{\#\textgreater{} 1   abortion      1108    1      69 Learner}
\CommentTok{\#\textgreater{} 2  marijuana       901    2      46 Learner}
\CommentTok{\#\textgreater{} 3  education       746    3     204 Learner}
\CommentTok{\#\textgreater{} 4   children       735    4     317 Learner}
\CommentTok{\#\textgreater{} 5        sex       658    5     133 Learner}
\CommentTok{\#\textgreater{} 6         he       803    1     199  Native}
\CommentTok{\#\textgreater{} 7        his       573    2     194  Native}
\CommentTok{\#\textgreater{} 8    candide       522    3      27  Native}
\CommentTok{\#\textgreater{} 9      quote       402    4      26  Native}
\CommentTok{\#\textgreater{} 10  caligula       394    5      11  Native}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{4321}\NormalTok{) }\CommentTok{\# make reproducible}

\NormalTok{num\_docs }\OtherTok{\textless{}{-}} 
\NormalTok{  nativeness\_dfm }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ndoc}\NormalTok{()}

\NormalTok{train\_size }\OtherTok{\textless{}{-}} 
\NormalTok{  (num\_docs }\SpecialCharTok{*}\NormalTok{ .}\DecValTok{75}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get size of sample}
  \FunctionTok{round}\NormalTok{() }\CommentTok{\# round to nearest whole number}

\NormalTok{train\_ids }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{num\_docs, }\CommentTok{\# population}
                   \AttributeTok{size =}\NormalTok{ train\_size, }\CommentTok{\# size of sample}
                   \AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# without replacement}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nativeness\_dfm\_train }\OtherTok{\textless{}{-}}\NormalTok{ nativeness\_dfm }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{dfm\_subset}\NormalTok{(doc\_id }\SpecialCharTok{\%in\%}\NormalTok{ train\_ids)}

\NormalTok{nativeness\_dfm\_test }\OtherTok{\textless{}{-}}\NormalTok{ nativeness\_dfm }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{dfm\_subset}\NormalTok{(}\SpecialCharTok{!}\NormalTok{doc\_id }\SpecialCharTok{\%in\%}\NormalTok{ train\_ids)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nativeness\_dfm }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{docvars}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    janitor}\SpecialCharTok{::}\FunctionTok{tabyl}\NormalTok{(type)}
\CommentTok{\#\textgreater{}     type   n percent}
\CommentTok{\#\textgreater{}  Learner 689   0.626}
\CommentTok{\#\textgreater{}   Native 411   0.374}

\NormalTok{nativeness\_dfm\_train }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{docvars}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    janitor}\SpecialCharTok{::}\FunctionTok{tabyl}\NormalTok{(type)}
\CommentTok{\#\textgreater{}     type   n percent}
\CommentTok{\#\textgreater{}  Learner 516   0.625}
\CommentTok{\#\textgreater{}   Native 309   0.375}

\NormalTok{nativeness\_dfm\_test }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{docvars}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    janitor}\SpecialCharTok{::}\FunctionTok{tabyl}\NormalTok{(type)}
\CommentTok{\#\textgreater{}     type   n percent}
\CommentTok{\#\textgreater{}  Learner 173   0.629}
\CommentTok{\#\textgreater{}   Native 102   0.371}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb1 }\OtherTok{\textless{}{-}} \FunctionTok{textmodel\_nb}\NormalTok{(}\AttributeTok{x =}\NormalTok{ nativeness\_dfm\_train, }\AttributeTok{y =}\NormalTok{ nativeness\_dfm\_train}\SpecialCharTok{$}\NormalTok{type)}

\FunctionTok{summary}\NormalTok{(nb1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} textmodel\_nb.dfm(x = nativeness\_dfm\_train, y = nativeness\_dfm\_train$type)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Class Priors:}
\CommentTok{\#\textgreater{} (showing first 2 elements)}
\CommentTok{\#\textgreater{} Learner  Native }
\CommentTok{\#\textgreater{}     0.5     0.5 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Estimated Feature Scores:}
\CommentTok{\#\textgreater{}             in     our  present society      gay  people     is   asking}
\CommentTok{\#\textgreater{} Learner 0.0230 0.00186 0.000476 0.00193 7.62e{-}04 0.00889 0.0193 5.76e{-}05}
\CommentTok{\#\textgreater{} Native  0.0174 0.00163 0.000315 0.00114 2.22e{-}05 0.00460 0.0176 2.96e{-}05}
\CommentTok{\#\textgreater{}             for    the     same   rights   that heterosexual    some     of}
\CommentTok{\#\textgreater{} Learner 0.00747 0.0544 0.001318 0.000909 0.0238     3.06e{-}04 0.00476 0.0328}
\CommentTok{\#\textgreater{} Native  0.00886 0.0582 0.000563 0.000252 0.0140     1.11e{-}05 0.00157 0.0298}
\CommentTok{\#\textgreater{}            this     are     to      be  allowed      get  married    and}
\CommentTok{\#\textgreater{} Learner 0.01148 0.01114 0.0300 0.01023 0.000326 0.001054 3.67e{-}04 0.0233}
\CommentTok{\#\textgreater{} Native  0.00796 0.00745 0.0296 0.00932 0.000278 0.000763 8.52e{-}05 0.0224}
\CommentTok{\#\textgreater{}            adopt children  request     has    been accepted}
\CommentTok{\#\textgreater{} Learner 2.64e{-}04  0.00268 1.39e{-}05 0.00414 0.00187 0.000137}
\CommentTok{\#\textgreater{} Native  3.71e{-}05  0.00119 2.22e{-}05 0.00435 0.00213 0.000141}
\FunctionTok{coef}\NormalTok{(nb1) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{}          Learner   Native}
\CommentTok{\#\textgreater{} in      0.023045 1.74e{-}02}
\CommentTok{\#\textgreater{} our     0.001856 1.63e{-}03}
\CommentTok{\#\textgreater{} present 0.000476 3.15e{-}04}
\CommentTok{\#\textgreater{} society 0.001929 1.14e{-}03}
\CommentTok{\#\textgreater{} gay     0.000762 2.22e{-}05}
\CommentTok{\#\textgreater{} people  0.008893 4.60e{-}03}

\FunctionTok{predict}\NormalTok{(nb1, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get the predicted document scores}
\NormalTok{  tail }\CommentTok{\# preview predicted probability scores}
\CommentTok{\#\textgreater{}            Learner Native}
\CommentTok{\#\textgreater{} text1092 2.37e{-}164      1}
\CommentTok{\#\textgreater{} text1094  3.18e{-}28      1}
\CommentTok{\#\textgreater{} text1095  8.53e{-}49      1}
\CommentTok{\#\textgreater{} text1096  1.83e{-}37      1}
\CommentTok{\#\textgreater{} text1097  7.45e{-}39      1}
\CommentTok{\#\textgreater{} text1098  2.35e{-}55      1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb1\_predictions }\OtherTok{\textless{}{-}} 
  \FunctionTok{predict}\NormalTok{(nb1, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get the predicted document scores}
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# convert to data frame}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{document =} \FunctionTok{rownames}\NormalTok{(.)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# add the document names to the data frame}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# convert to tibble}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"Learner"}\NormalTok{, }\StringTok{"Native"}\NormalTok{), }\CommentTok{\# convert from wide to long format}
               \AttributeTok{names\_to =} \StringTok{"prediction"}\NormalTok{, }\CommentTok{\# new column for ham/spam predictions}
               \AttributeTok{values\_to =} \StringTok{"probability"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# probablity scores for each}
  \FunctionTok{group\_by}\NormalTok{(document) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# group parameter by document}
  \FunctionTok{slice\_max}\NormalTok{(probability, }\AttributeTok{n =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# keep the document row with highest probablity}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# for predictions that were 50/50 }
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# remove grouping parameter}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{doc\_id =} \FunctionTok{str\_remove}\NormalTok{(document, }\StringTok{"text"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ as.numeric) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# clean up document column so it matches doc\_id in}
  \FunctionTok{arrange}\NormalTok{(doc\_id) }\CommentTok{\# order by doc\_id}

\NormalTok{nb1\_predictions }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\CommentTok{\# preview}
\CommentTok{\#\textgreater{} \# A tibble: 10 x 4}
\CommentTok{\#\textgreater{}    document prediction probability doc\_id}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}    \textless{}chr\textgreater{}            \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 text1    Learner              1      1}
\CommentTok{\#\textgreater{}  2 text3    Learner              1      3}
\CommentTok{\#\textgreater{}  3 text4    Learner              1      4}
\CommentTok{\#\textgreater{}  4 text5    Learner              1      5}
\CommentTok{\#\textgreater{}  5 text6    Learner              1      6}
\CommentTok{\#\textgreater{}  6 text8    Learner              1      8}
\CommentTok{\#\textgreater{}  7 text9    Learner              1      9}
\CommentTok{\#\textgreater{}  8 text11   Learner              1     11}
\CommentTok{\#\textgreater{}  9 text12   Learner              1     12}
\CommentTok{\#\textgreater{} 10 text13   Learner              1     13}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb1\_predictions\_actual }\OtherTok{\textless{}{-}} 
  \FunctionTok{cbind}\NormalTok{(}\AttributeTok{actual =}\NormalTok{ nb1}\SpecialCharTok{$}\NormalTok{y, nb1\_predictions) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# column{-}bind actual classes}
  \FunctionTok{select}\NormalTok{(doc\_id, document, actual, prediction, probability) }\CommentTok{\# organize variables}

\NormalTok{nb1\_predictions\_actual }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{) }\CommentTok{\# preview}
\CommentTok{\#\textgreater{}   doc\_id document  actual prediction probability}
\CommentTok{\#\textgreater{} 1      1    text1 Learner    Learner           1}
\CommentTok{\#\textgreater{} 2      3    text3 Learner    Learner           1}
\CommentTok{\#\textgreater{} 3      4    text4 Learner    Learner           1}
\CommentTok{\#\textgreater{} 4      5    text5 Learner    Learner           1}
\CommentTok{\#\textgreater{} 5      6    text6 Learner    Learner           1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tab\_class }\OtherTok{\textless{}{-}} 
  \FunctionTok{table}\NormalTok{(nb1\_predictions\_actual}\SpecialCharTok{$}\NormalTok{actual, }\CommentTok{\# actual class labels}
\NormalTok{        nb1\_predictions\_actual}\SpecialCharTok{$}\NormalTok{prediction) }\CommentTok{\# predicted class labels}

\NormalTok{caret}\SpecialCharTok{::}\FunctionTok{confusionMatrix}\NormalTok{(tab\_class, }\AttributeTok{mode =} \StringTok{"prec\_recall"}\NormalTok{) }\CommentTok{\# model performance statistics}
\CommentTok{\#\textgreater{} Confusion Matrix and Statistics}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}          }
\CommentTok{\#\textgreater{}           Learner Native}
\CommentTok{\#\textgreater{}   Learner     516      0}
\CommentTok{\#\textgreater{}   Native       10    299}
\CommentTok{\#\textgreater{}                                         }
\CommentTok{\#\textgreater{}                Accuracy : 0.988         }
\CommentTok{\#\textgreater{}                  95\% CI : (0.978, 0.994)}
\CommentTok{\#\textgreater{}     No Information Rate : 0.638         }
\CommentTok{\#\textgreater{}     P{-}Value [Acc \textgreater{} NIR] : \textless{} 2e{-}16       }
\CommentTok{\#\textgreater{}                                         }
\CommentTok{\#\textgreater{}                   Kappa : 0.974         }
\CommentTok{\#\textgreater{}                                         }
\CommentTok{\#\textgreater{}  Mcnemar\textquotesingle{}s Test P{-}Value : 0.00443       }
\CommentTok{\#\textgreater{}                                         }
\CommentTok{\#\textgreater{}               Precision : 1.000         }
\CommentTok{\#\textgreater{}                  Recall : 0.981         }
\CommentTok{\#\textgreater{}                      F1 : 0.990         }
\CommentTok{\#\textgreater{}              Prevalence : 0.638         }
\CommentTok{\#\textgreater{}          Detection Rate : 0.625         }
\CommentTok{\#\textgreater{}    Detection Prevalence : 0.625         }
\CommentTok{\#\textgreater{}       Balanced Accuracy : 0.990         }
\CommentTok{\#\textgreater{}                                         }
\CommentTok{\#\textgreater{}        \textquotesingle{}Positive\textquotesingle{} Class : Learner       }
\CommentTok{\#\textgreater{} }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predicted\_class }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(nb1, }\AttributeTok{newdata =}\NormalTok{ nativeness\_dfm\_test)}

\NormalTok{actual\_class }\OtherTok{\textless{}{-}}\NormalTok{ nativeness\_dfm\_test}\SpecialCharTok{$}\NormalTok{type}

\NormalTok{tab\_class }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(actual\_class, predicted\_class)  }\CommentTok{\# cross{-}tabulate actual and predicted class labels}

\NormalTok{caret}\SpecialCharTok{::}\FunctionTok{confusionMatrix}\NormalTok{(tab\_class, }\AttributeTok{mode =} \StringTok{"prec\_recall"}\NormalTok{)  }\CommentTok{\# model performance statistics}
\CommentTok{\#\textgreater{} Confusion Matrix and Statistics}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}             predicted\_class}
\CommentTok{\#\textgreater{} actual\_class Learner Native}
\CommentTok{\#\textgreater{}      Learner     172      1}
\CommentTok{\#\textgreater{}      Native        9     93}
\CommentTok{\#\textgreater{}                                         }
\CommentTok{\#\textgreater{}                Accuracy : 0.964         }
\CommentTok{\#\textgreater{}                  95\% CI : (0.934, 0.982)}
\CommentTok{\#\textgreater{}     No Information Rate : 0.658         }
\CommentTok{\#\textgreater{}     P{-}Value [Acc \textgreater{} NIR] : \textless{}2e{-}16        }
\CommentTok{\#\textgreater{}                                         }
\CommentTok{\#\textgreater{}                   Kappa : 0.921         }
\CommentTok{\#\textgreater{}                                         }
\CommentTok{\#\textgreater{}  Mcnemar\textquotesingle{}s Test P{-}Value : 0.0269        }
\CommentTok{\#\textgreater{}                                         }
\CommentTok{\#\textgreater{}               Precision : 0.994         }
\CommentTok{\#\textgreater{}                  Recall : 0.950         }
\CommentTok{\#\textgreater{}                      F1 : 0.972         }
\CommentTok{\#\textgreater{}              Prevalence : 0.658         }
\CommentTok{\#\textgreater{}          Detection Rate : 0.625         }
\CommentTok{\#\textgreater{}    Detection Prevalence : 0.629         }
\CommentTok{\#\textgreater{}       Balanced Accuracy : 0.970         }
\CommentTok{\#\textgreater{}                                         }
\CommentTok{\#\textgreater{}        \textquotesingle{}Positive\textquotesingle{} Class : Learner       }
\CommentTok{\#\textgreater{} }
\end{Highlighting}
\end{Shaded}

\hypertarget{preparation-1}{%
\subsection{Preparation}\label{preparation-1}}

Data set transformation

Splitting into training and test sets

\hypertarget{model-training}{%
\subsection{Model training}\label{model-training}}

Aim to use the create an abstraction of the patterns in the dataset

\begin{itemize}
\item
  Feature engineering
\item
  Model selection
\item
  Model evaluation
\end{itemize}

The In Figure () Let's consider the results from a hypothetical model of text classification on the SMS dataset I introduced at in this subsection.

\begin{itemize}
\tightlist
\item
  accuracy (measure of overall correct predictions)
\item
  precision (measure of the quality of the predictions)

  \begin{itemize}
  \tightlist
  \item
    Percentage of predicted `ham' messages that were correct
  \end{itemize}
\item
  recall (measure of the quantity of the predictions)

  \begin{itemize}
  \tightlist
  \item
    Percentage of actual `ham' messages that were correct
  \end{itemize}
\item
  F1-score (summarizes the balance between precision and recall)
\end{itemize}

Avoiding overfitting

\hypertarget{model-testing}{%
\subsection{Model testing}\label{model-testing}}

Aim to test the abstracted model to new observations.

Model testing

\hypertarget{evaluation}{%
\subsection{Evaluation}\label{evaluation}}

Evaluation of results

Relationship between predicted and actual classes in a confusion matrix as seen in Figure \ref{fig:pda-confusion-matrix-image}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/10-prediction/pda-confusion-matrix} 

}

\caption{Confusion matrix}\label{fig:pda-confusion-matrix-image}
\end{figure}

\hypertarget{section}{%
\subsection{\ldots{}}\label{section}}

In Table () we see the top five terms for each class after breaking the messages into terms and then counting up the frequencies.

\hypertarget{summary-11}{%
\subsection{Summary}\label{summary-11}}

\ldots{}

\hypertarget{exploration}{%
\section{Exploration}\label{exploration}}

INCOMPLETE DRAFT

\begin{quote}
Nobody ever figures out what life is all about, and it doesn't matter. Explore the world. Nearly everything is really interesting if you go into it deeply enough.

-- Richard P. Feynman
\end{quote}

\begin{rmdkey}
The essential questions for this chapter are:

\begin{itemize}
\tightlist
\item
  \ldots{}
\end{itemize}
\end{rmdkey}

In this chapter\ldots.

\begin{itemize}
\item
  identify, interrogate, and interpret
\item
  EDA is an inductive approach. That is, it is bottom-up --we do not come into the analysis with strong preconceptions of what the data will tell us (IDA - hypothesis and PDA - target classes). The aim is to uncover and discover patterns that lead to insight based on qualitative interpretation. EDA, the, can be considered a quantitative-supported qualitative analysis.
\item
  Two main classes of exploratory data analysis: (1) descriptive analysis and (2) unsupervised machine learning.

  \begin{itemize}
  \tightlist
  \item
    descriptive analysis can be seen as a more detailed implementation of descriptive assessment, which is a key component of both inferential and predictive analysis approaches. .
  \item
    unsupervised machine learning is a more algorithmic approach to deriving knowledge which leverages \ldots{} to produce knowledge which can be interpreted. This approach falls under the umbrella of machine learning, as we have seen in predictive data analysis, however, whereas PDA assumes a potential relationship between input features and target outcomes, or classes, in unsupervised learning the classes are induced from the data itself and the classes groupings are interpreted and evaluated more \ldots?
  \end{itemize}
\item
  It is, however, important to come to EDA with a research question in which the unit of analysis is clear.
\end{itemize}

Description of the datasets we will use to examine various exploratory methods.

\textbf{Lastfm}

Last.fm webscrape of the top artists by genre which we acquired in Chapter 5 \protect\hyperlink{acquire-data}{``Acquire data''} in the \href{acquire-data.html\#scaling-up}{web scrape section} and transformed in Chapter 6 \href{transform-data.html\#normalize}{``Transform data''}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(lastfm\_df)  }\CommentTok{\# preview dataset structure}
\CommentTok{\#\textgreater{} Rows: 155}
\CommentTok{\#\textgreater{} Columns: 4}
\CommentTok{\#\textgreater{} $ artist \textless{}chr\textgreater{} "Alan Jackson", "Alan Jackson", "Brad Paisley", "Carrie Underwo\textasciitilde{}}
\CommentTok{\#\textgreater{} $ song   \textless{}chr\textgreater{} "Little Bitty", "Remember When", "Mud on the Tires", "Before He\textasciitilde{}}
\CommentTok{\#\textgreater{} $ lyrics \textless{}chr\textgreater{} "Have a little love on a little honeymoon You got a little dish\textasciitilde{}}
\CommentTok{\#\textgreater{} $ genre  \textless{}chr\textgreater{} "country", "country", "country", "country", "country", "country\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

The \texttt{lastfm\_df} dataset contains 155 observations and 4 variables. Each observation corresponds to a particular song.

Let's look at the data dictionary for this dataset.

\begin{table}

\caption{\label{tab:eda-lastfm-data-dictionary-preview}Last.fm lyrics dataset data dictionary.}
\centering
\begin{tabular}[t]{lll}
\toprule
variable\_name & name & description\\
\midrule
artist & Artist Name & The name of the artist\\
song & Song Title & The title of the song\\
lyrics & Song Lyrics & The lyrics for the song\\
genre & Song Genre & The genre of the artist (source last.fm)\\
\bottomrule
\end{tabular}
\end{table}

From the data dictionary we see that each song encodes the artist, the song title, the genre of the song, and the lyrics for the song.

To prepare for the upcoming exploration methods, we will convert the \texttt{lastfm\_df} to a Quanteda corpus object.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create corpus object}
\NormalTok{lastfm\_corpus }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm\_df }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# data frame}
  \FunctionTok{corpus}\NormalTok{(}\AttributeTok{text\_field =} \StringTok{"lyrics"}\NormalTok{) }\CommentTok{\# create corpus}

\NormalTok{lastfm\_corpus }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{) }\CommentTok{\# preview}
\CommentTok{\#\textgreater{} Corpus consisting of 155 documents, showing 5 documents:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Text Types Tokens Sentences           artist                song   genre}
\CommentTok{\#\textgreater{}  text1    84    271         1     Alan Jackson        Little Bitty country}
\CommentTok{\#\textgreater{}  text2   110    203         1     Alan Jackson       Remember When country}
\CommentTok{\#\textgreater{}  text3   130    290         2     Brad Paisley    Mud on the Tires country}
\CommentTok{\#\textgreater{}  text4   114    303         1 Carrie Underwood    Before He Cheats country}
\CommentTok{\#\textgreater{}  text5   171    517        15   Dierks Bentley What Was I Thinkin\textquotesingle{} country}
\end{Highlighting}
\end{Shaded}

\textbf{SOTU}

The quanteda package \citep{R-quanteda} includes various datasets. We will work with the State of the Union Corpus \citep{R-quanteda.corpora}. Let's take a look at the structure of this dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(sotu\_df)  }\CommentTok{\# preview dataset structure}
\CommentTok{\#\textgreater{} Rows: 84}
\CommentTok{\#\textgreater{} Columns: 5}
\CommentTok{\#\textgreater{} $ president \textless{}chr\textgreater{} "Truman", "Truman", "Truman", "Truman", "Truman", "Truman", \textasciitilde{}}
\CommentTok{\#\textgreater{} $ delivery  \textless{}chr\textgreater{} "written", "spoken", "spoken", "spoken", "spoken", "spoken",\textasciitilde{}}
\CommentTok{\#\textgreater{} $ party     \textless{}chr\textgreater{} "Democratic", "Democratic", "Democratic", "Democratic", "Dem\textasciitilde{}}
\CommentTok{\#\textgreater{} $ year      \textless{}dbl\textgreater{} 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1953, 1954, \textasciitilde{}}
\CommentTok{\#\textgreater{} $ text      \textless{}chr\textgreater{} "To the Congress of the United States:\textbackslash{}n\textbackslash{}nA quarter century \textasciitilde{}}
\end{Highlighting}
\end{Shaded}

In the \texttt{sotu\_df} dataset there are 84 observations and 5 variables. Each observation corresponds to a presidential address.

Let's look at the data dictionary to understand what each column measures.

\begin{table}

\caption{\label{tab:eda-sotu-data-dictionary-preview}SOTU dataset data dictionary.}
\centering
\begin{tabular}[t]{lll}
\toprule
variable\_name & name & description\\
\midrule
president & President & Incumbent president\\
delivery & Modality of delivery & Modality of the address (spoken or written)\\
party & Political party & Party affliliation of the president\\
year & Year & Year that the statement was given\\
text & Text & Text or transcription of the address\\
\bottomrule
\end{tabular}
\end{table}

So we see that each observation corresponds to the president that gave the address, the modality of the address, the party the president was affliated with, the year that the address was given, and the address text.

\hypertarget{descriptive-analysis}{%
\subsection{Descriptive analysis}\label{descriptive-analysis}}

\ldots{} overview summary of the aims of descriptive analysis methods\ldots{}

\hypertarget{frequency-analysis}{%
\subsubsection{Frequency analysis}\label{frequency-analysis}}

Explore word frequency.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create tokens object}
\NormalTok{lastfm\_tokens }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm\_corpus }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# corpus object}
  \FunctionTok{tokens}\NormalTok{(}\AttributeTok{what =} \StringTok{"word"}\NormalTok{, }\CommentTok{\# tokenize by word}
         \AttributeTok{remove\_punct =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# remove punctuation}
  \FunctionTok{tokens\_tolower}\NormalTok{() }\CommentTok{\# lowercase tokens}

\NormalTok{lastfm\_tokens }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{) }\CommentTok{\# preview one tokenized document}
\CommentTok{\#\textgreater{} Tokens consisting of 1 document and 3 docvars.}
\CommentTok{\#\textgreater{} text1 :}
\CommentTok{\#\textgreater{}  [1] "have"      "a"         "little"    "love"      "on"        "a"        }
\CommentTok{\#\textgreater{}  [7] "little"    "honeymoon" "you"       "got"       "a"         "little"   }
\CommentTok{\#\textgreater{} [ ... and 252 more ]}
\end{Highlighting}
\end{Shaded}

We see the tokenized output.

Many of the frequency analysis function provided with quanteda require that the dataset be in a document-frequency matrix. So let's create a dfm of the \texttt{lastfm\_corpus} object using the \texttt{dfm()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create document{-}frequency matrix}
\NormalTok{lastfm\_dfm }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm\_tokens }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# tokens object}
  \FunctionTok{dfm}\NormalTok{() }\CommentTok{\# create dfm}

\NormalTok{lastfm\_dfm }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{) }\CommentTok{\# preview 5 documents}
\CommentTok{\#\textgreater{} Document{-}feature matrix of: 5 documents, 3,966 features (97.19\% sparse) and 3 docvars.}
\CommentTok{\#\textgreater{}        features}
\CommentTok{\#\textgreater{} docs    have  a little love on honeymoon you got dish and}
\CommentTok{\#\textgreater{}   text1    1 35     34    1  5         1   4   3    1  10}
\CommentTok{\#\textgreater{}   text2    0  1      1    3  0         0   3   0    0  12}
\CommentTok{\#\textgreater{}   text3    1 21     11    0  9         0   6   5    0   6}
\CommentTok{\#\textgreater{}   text4    1 10      5    0  3         0   1   0    0   4}
\CommentTok{\#\textgreater{}   text5    0 22      7    0  2         0   0   1    0   5}
\CommentTok{\#\textgreater{} [ reached max\_nfeat ... 3,956 more features ]}
\end{Highlighting}
\end{Shaded}

Frequency distributions.

\begin{itemize}
\tightlist
\item
  Very few high frequency terms and many low frequency.
\item
  This results in a long tail when plotted.
\end{itemize}

\begin{center}\includegraphics[width=0.9\linewidth]{11-exploration_files/figure-latex/unnamed-chunk-6-1} \end{center}

Let's take a closer look at the 50 most frequent word terms in \texttt{lastfm\_dfm}. We use the \texttt{textstat\_frequency()} function from the quanteda.textstats package to extract various frequency measures.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_dfm }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{textstat\_frequency}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\CommentTok{\#\textgreater{}    feature frequency rank docfreq group}
\CommentTok{\#\textgreater{} 1        i      2048    1     148   all}
\CommentTok{\#\textgreater{} 2      you      2045    2     139   all}
\CommentTok{\#\textgreater{} 3      the      1787    3     148   all}
\CommentTok{\#\textgreater{} 4      and      1242    4     151   all}
\CommentTok{\#\textgreater{} 5        a      1067    5     135   all}
\CommentTok{\#\textgreater{} 6       to       933    6     147   all}
\CommentTok{\#\textgreater{} 7       me       883    7     132   all}
\CommentTok{\#\textgreater{} 8       my       768    8     116   all}
\CommentTok{\#\textgreater{} 9       it       666    9     118   all}
\CommentTok{\#\textgreater{} 10      in       597   10     131   all}
\end{Highlighting}
\end{Shaded}

We can then use this data frame to plot the frequency of the terms in descending order using \texttt{ggplot()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_dfm }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{textstat\_frequency}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(feature, frequency), }\AttributeTok{y =}\NormalTok{ frequency)) }\SpecialCharTok{+} \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Words"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Raw frequency"}\NormalTok{, }\AttributeTok{title =} \StringTok{"Top 50"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-exploration_files/figure-latex/unnamed-chunk-8-1} \end{center}

Now these are the most common terms for all of the song lyrics. In our case, let's look at the most common 15 terms for each of the genres. We will need at a \texttt{groups\ =} argument to \texttt{textstat\_frequency()} to get the \texttt{genre} and then we need to manipulate the data frame output and the extract the top 15 terms grouping by \texttt{genre}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_dfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dfm}
  \FunctionTok{textstat\_frequency}\NormalTok{(}\AttributeTok{groups =}\NormalTok{ genre) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get frequency statistics}
  \FunctionTok{group\_by}\NormalTok{(group) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# grouping parameters}
  \FunctionTok{slice\_max}\NormalTok{(frequency, }\AttributeTok{n =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract top features}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# remove grouping parameters}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ frequency, }\AttributeTok{y =} \FunctionTok{reorder\_within}\NormalTok{(feature, frequency, group), }\AttributeTok{fill =}\NormalTok{ group)) }\SpecialCharTok{+} \CommentTok{\# mappings (reordering feature by frequency)}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# bar plot}
  \FunctionTok{scale\_y\_reordered}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# clean up y{-}axis labels (features)}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{group, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# organize separate plots by genre}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Raw frequency"}\NormalTok{, }\AttributeTok{y =} \ConstantTok{NULL}\NormalTok{) }\CommentTok{\# labels}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-exploration_files/figure-latex/unnamed-chunk-9-1} \end{center}

\begin{rmdtip}
Note that I've used the plotting function \texttt{facet\_wrap()} to tell
ggplot2 to organize each of the genres in separate bar plots but in the
same plotting space. The \texttt{scales\ =} argument takes either
\texttt{free}, \texttt{free\_x}, or \texttt{free\_y} as a value. This
will let the all the axes or either the x- or y-axis vary freely between
the separate plots.
\end{rmdtip}

Raw frequency is effected by the total number of words in each genre. Therefore we cannot safely make direct comparisons between the frequency counts for individual terms between genres.

To make the term-genre comparisons comparable we normalized the term frequency by the number of terms in each genre. We can use the \texttt{dfm\_weight()} function with the argument \texttt{scheme\ =\ "prop"} to give us the relative frequency of a term per the number of terms in the document it appears in. This weighting is known as Term frequency.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_dfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dfm}
  \FunctionTok{dfm\_weight}\NormalTok{(}\AttributeTok{scheme =} \StringTok{"prop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# weigh by term frequency}
  \FunctionTok{textstat\_frequency}\NormalTok{(}\AttributeTok{groups =}\NormalTok{ genre) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get frequency statistics}
  \FunctionTok{group\_by}\NormalTok{(group) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# grouping parameters}
  \FunctionTok{slice\_max}\NormalTok{(frequency, }\AttributeTok{n =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract top features}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# remove grouping parameters}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ frequency, }\AttributeTok{y =} \FunctionTok{reorder\_within}\NormalTok{(feature, frequency, group), }\AttributeTok{fill =}\NormalTok{ group)) }\SpecialCharTok{+} \CommentTok{\# mappings (reordering feature by frequency)}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# bar plot}
  \FunctionTok{scale\_y\_reordered}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# clean up y{-}axis labels (features)}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{group, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# organize separate plots by genre}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Term frequency"}\NormalTok{, }\AttributeTok{y =} \ConstantTok{NULL}\NormalTok{) }\CommentTok{\# labels}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-exploration_files/figure-latex/unnamed-chunk-11-1} \end{center}

Term frequency makes the frequency scores relative to the genre. This means that the frequencies are directly comparable as the number of words in each genre is taken into account when calculating the term frequency score.

Now our term frequency measures allow us to make direct comparisons, but one problem here is that the most frequent terms tend to be terms that are common across all language use. Since the aim of most frequency analyses which compare sub-groups is to discover what terms are most indicative of each sub-group we need a way to adjust or weigh our measures. The scheme often applied to scale terms according to how common they are is to apply term frequency-inverse document frequency (tf-idf). The tf-idf measure is the result of multiplying the term frequency by the inverse document frequency.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_df }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# data frame}
  \FunctionTok{count}\NormalTok{(genre) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# get number of documents for each genre}
  \FunctionTok{select}\NormalTok{(}\AttributeTok{Genre =}\NormalTok{ genre, }\StringTok{\textasciigrave{}}\AttributeTok{Number of documents}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ n) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
               \AttributeTok{caption =} \StringTok{"Number of documents per genre."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-12}Number of documents per genre.}
\centering
\begin{tabular}[t]{lr}
\toprule
Genre & Number of documents\\
\midrule
country & 44\\
hip-hop & 26\\
pop & 41\\
rock & 44\\
\bottomrule
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_dfm }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{dfm\_weight}\NormalTok{(}\AttributeTok{scheme =} \StringTok{"prop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# term{-}frequency weight}
  \FunctionTok{textstat\_frequency}\NormalTok{(}\AttributeTok{groups =}\NormalTok{ genre) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# include genre as a group}
  \FunctionTok{filter}\NormalTok{(feature }\SpecialCharTok{==} \StringTok{"i"}\NormalTok{) }\CommentTok{\# filter only "i"}
\CommentTok{\#\textgreater{}      feature frequency rank docfreq   group}
\CommentTok{\#\textgreater{} 1          i      1.86    1      42 country}
\CommentTok{\#\textgreater{} 1639       i      1.00    1      26 hip{-}hop}
\CommentTok{\#\textgreater{} 3600       i      1.64    2      38     pop}
\CommentTok{\#\textgreater{} 5012       i      2.08    1      42    rock}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Manually calculate TF{-}IDF scores}
\FloatTok{1.86} \SpecialCharTok{*} \FunctionTok{log10}\NormalTok{(}\DecValTok{44}\SpecialCharTok{/}\DecValTok{42}\NormalTok{)  }\CommentTok{\# i in country}
\CommentTok{\#\textgreater{} [1] 0.0376}
\DecValTok{1} \SpecialCharTok{*} \FunctionTok{log10}\NormalTok{(}\DecValTok{26}\SpecialCharTok{/}\DecValTok{26}\NormalTok{)  }\CommentTok{\# i in hip hop}
\CommentTok{\#\textgreater{} [1] 0}
\FloatTok{1.64} \SpecialCharTok{*} \FunctionTok{log10}\NormalTok{(}\DecValTok{41}\SpecialCharTok{/}\DecValTok{38}\NormalTok{)  }\CommentTok{\# i in pop}
\CommentTok{\#\textgreater{} [1] 0.0541}
\FloatTok{2.08} \SpecialCharTok{*} \FunctionTok{log10}\NormalTok{(}\DecValTok{44}\SpecialCharTok{/}\DecValTok{42}\NormalTok{)  }\CommentTok{\# i in rock}
\CommentTok{\#\textgreater{} [1] 0.042}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_dfm }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{dfm\_tfidf}\NormalTok{(}\AttributeTok{scheme\_tf =} \StringTok{"prop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{textstat\_frequency}\NormalTok{(}\AttributeTok{groups =}\NormalTok{ genre, }\AttributeTok{force =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(feature, }\StringTok{"\^{}(i|yeah)$"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{arrange}\NormalTok{(feature)}
\CommentTok{\#\textgreater{}      feature frequency rank docfreq   group}
\CommentTok{\#\textgreater{} 213        i    0.0373  213      42 country}
\CommentTok{\#\textgreater{} 1873       i    0.0201  235      26 hip{-}hop}
\CommentTok{\#\textgreater{} 3842       i    0.0329  244      38     pop}
\CommentTok{\#\textgreater{} 5197       i    0.0417  186      42    rock}
\CommentTok{\#\textgreater{} 239     yeah    0.0350  239       8 country}
\CommentTok{\#\textgreater{} 1742    yeah    0.0336  104      14 hip{-}hop}
\CommentTok{\#\textgreater{} 3755    yeah    0.0454  157      13     pop}
\CommentTok{\#\textgreater{} 5013    yeah    0.2165    2      17    rock}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_dfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dfm}
  \FunctionTok{dfm\_tfidf}\NormalTok{(}\AttributeTok{scheme\_tf =} \StringTok{"prop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# weigh by tf{-}idf}
  \FunctionTok{textstat\_frequency}\NormalTok{(}\AttributeTok{groups =}\NormalTok{ genre, }\AttributeTok{force =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get frequency statistics}
  \FunctionTok{group\_by}\NormalTok{(group) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# grouping parameters}
  \FunctionTok{slice\_max}\NormalTok{(frequency, }\AttributeTok{n =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract top features}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# remove grouping parameters}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ frequency, }\AttributeTok{y =} \FunctionTok{reorder\_within}\NormalTok{(feature, frequency, group), }\AttributeTok{fill =}\NormalTok{ group)) }\SpecialCharTok{+} \CommentTok{\# mappings (reordering feature by frequency)}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# bar plot}
  \FunctionTok{scale\_y\_reordered}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# clean up y{-}axis labels (features)}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{group, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# organize separate plots by genre}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"TF{-}IDF"}\NormalTok{, }\AttributeTok{y =} \ConstantTok{NULL}\NormalTok{) }\CommentTok{\# labels}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-exploration_files/figure-latex/unnamed-chunk-16-1} \end{center}

TF-IDF works well to identify terms which are particularly indicative of a particular group but there is a shortcoming which is particularly salient when working with song lyrics. That is, that there are terms which are frequent but not common because they appear in one song and are repeated. This is common in song lyrics which tend to have repeated chorus sections. To minimize this influence, we can trim the document-frequency matrix and eliminate terms which only appear in one song.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_dfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dfm}
  \FunctionTok{dfm\_trim}\NormalTok{(}\AttributeTok{min\_docfreq =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# keep terms appearing in 2 or more songs}
  \FunctionTok{dfm\_tfidf}\NormalTok{(}\AttributeTok{scheme\_tf =} \StringTok{"prop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# weigh by tf{-}idf}
  \FunctionTok{textstat\_frequency}\NormalTok{(}\AttributeTok{groups =}\NormalTok{ genre, }\AttributeTok{force =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get frequency statistics}
  \FunctionTok{group\_by}\NormalTok{(group) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# grouping parameters}
  \FunctionTok{slice\_max}\NormalTok{(frequency, }\AttributeTok{n =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract top features}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# remove grouping parameters}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ frequency, }\AttributeTok{y =} \FunctionTok{reorder\_within}\NormalTok{(feature, frequency, group), }\AttributeTok{fill =}\NormalTok{ group)) }\SpecialCharTok{+} \CommentTok{\# mappings (reordering feature by frequency)}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# bar plot}
  \FunctionTok{scale\_y\_reordered}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# clean up y{-}axis labels (features)}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{group, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# organize separate plots by genre}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"TF{-}IDF"}\NormalTok{, }\AttributeTok{y =} \ConstantTok{NULL}\NormalTok{) }\CommentTok{\# labels}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-exploration_files/figure-latex/unnamed-chunk-17-1} \end{center}

Now we are looking a terms which are indicative of their respective genres and appear in at least 2 songs.

Another exploration method is to look at relative frequency, or keyness, measures. This type of analysis compares the relative frequency of terms of a target group in comparison to a reference group. If we set the target to one of our genres then the other genres become the reference. The results show which terms occur significantly more often than they occur in the reference group(s). The \texttt{textstat\_keyness()} function implements this type of analysis in quanteda.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_keywords\_country }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm\_dfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dfm}
  \FunctionTok{dfm\_trim}\NormalTok{(}\AttributeTok{min\_docfreq =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# keep terms appearing in 2 or more songs}
  \FunctionTok{textstat\_keyness}\NormalTok{(}\AttributeTok{target =}\NormalTok{ lastfm\_dfm}\SpecialCharTok{$}\NormalTok{genre }\SpecialCharTok{==} \StringTok{"country"}\NormalTok{) }\CommentTok{\# compare country}

\NormalTok{lastfm\_keywords\_country }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\CommentTok{\# preview}
\CommentTok{\#\textgreater{}    feature  chi2        p n\_target n\_reference}
\CommentTok{\#\textgreater{} 1   little 113.3 0.00e+00       77          43}
\CommentTok{\#\textgreater{} 2   belong  45.6 1.46e{-}11       19           4}
\CommentTok{\#\textgreater{} 3     ring  45.2 1.80e{-}11       22           7}
\CommentTok{\#\textgreater{} 4     he\textquotesingle{}s  43.3 4.82e{-}11       25          11}
\CommentTok{\#\textgreater{} 5     went  43.2 4.90e{-}11       17           2}
\CommentTok{\#\textgreater{} 6     fire  42.2 8.24e{-}11       21           7}
\CommentTok{\#\textgreater{} 7    blues  42.2 8.44e{-}11       14           0}
\CommentTok{\#\textgreater{} 8      him  40.9 1.60e{-}10       34          24}
\CommentTok{\#\textgreater{} 9  country  38.8 4.58e{-}10       13           0}
\CommentTok{\#\textgreater{} 10    road  37.7 8.11e{-}10       23          11}
\end{Highlighting}
\end{Shaded}

The output of the \texttt{textstat\_keyness()} function all terms from most frequent in the target group to the most frequent in the reference group(s). The \texttt{textplot\_keyness()} takes advantage of this and we can see the most contrastive terms in a plot.

Let's look at what terms are most and least indicative of the `country' genre.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_keywords\_country }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{textplot\_keyness}\NormalTok{(}\AttributeTok{n =} \DecValTok{25}\NormalTok{, }\AttributeTok{labelsize =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# plot most contrastive terms}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Chi{-}squared statistic"}\NormalTok{, }
       \AttributeTok{title =} \StringTok{"Term keyness"}\NormalTok{, }
       \AttributeTok{subtitle =} \StringTok{"Country versus other genres"}\NormalTok{) }\CommentTok{\# labels}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-exploration_files/figure-latex/unnamed-chunk-19-1} \end{center}

Interpretation\ldots.

Now let's look at the `Hip hop' genre.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_dfm }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dfm}
  \FunctionTok{dfm\_trim}\NormalTok{(}\AttributeTok{min\_docfreq =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# keep terms appearing in 2 or more songs}
  \FunctionTok{textstat\_keyness}\NormalTok{(}\AttributeTok{target =}\NormalTok{ lastfm\_dfm}\SpecialCharTok{$}\NormalTok{genre }\SpecialCharTok{==} \StringTok{"hip{-}hop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# compare hip hop}
  \FunctionTok{textplot\_keyness}\NormalTok{(}\AttributeTok{n =} \DecValTok{25}\NormalTok{, }\AttributeTok{labelsize =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# plot most contrastive terms}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Chi{-}squared statistic"}\NormalTok{,}
      \AttributeTok{title =} \StringTok{"Term keyness"}\NormalTok{, }
       \AttributeTok{subtitle =} \StringTok{"Hip hop versus other genres"}\NormalTok{) }\CommentTok{\# labels}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-exploration_files/figure-latex/unnamed-chunk-20-1} \end{center}

Interpretation \ldots{}

Now we have been working with words as our tokens/ features but a word is simply a unigram token. We can also consider multi-word tokens, or ngrams. To create bigrams (2-word tokens) we return to the \texttt{lastfm\_tokens} object and add the function \texttt{tokens\_ngrams()} with the argument \texttt{n\ =\ 2} (for bigrams). Then just as before we create a DFM object. I will go ahead and trim the DFM to exclude terms appearing only in one document (i.e.~song).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tokenize by bigrams}
\NormalTok{lastfm\_dfm\_ngrams }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm\_tokens }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# word tokens}
  \FunctionTok{tokens\_ngrams}\NormalTok{(}\AttributeTok{n =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# create 2{-}term ngrams (bigrams)}
  \FunctionTok{dfm}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# create document{-}frequency matrix}
  \FunctionTok{dfm\_trim}\NormalTok{(}\AttributeTok{min\_docfreq =} \DecValTok{2}\NormalTok{) }\CommentTok{\# keep terms appearing in 2 or more songs}

\NormalTok{lastfm\_dfm\_ngrams }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{) }\CommentTok{\# preview 1 document}
\CommentTok{\#\textgreater{} Document{-}feature matrix of: 1 document, 3,232 features (98.73\% sparse) and 3 docvars.}
\CommentTok{\#\textgreater{}        features}
\CommentTok{\#\textgreater{} docs    have\_a a\_little on\_a you\_got got\_a and\_you and\_a well\_it\textquotesingle{}s it\textquotesingle{}s\_alright}
\CommentTok{\#\textgreater{}   text1      1       25    1       3     3       1     7         2            4}
\CommentTok{\#\textgreater{}        features}
\CommentTok{\#\textgreater{} docs    to\_be}
\CommentTok{\#\textgreater{}   text1     4}
\CommentTok{\#\textgreater{} [ reached max\_nfeat ... 3,222 more features ]}
\end{Highlighting}
\end{Shaded}

Interpretation \ldots{}

We can now repeat the same steps we did earlier to explore raw frequency, term frequency, and tf-idf frequency measures by genre. We will skip the visualization of raw frequency as it is inherently incompatible with direct comparisons between sub-groups.

\begin{center}\includegraphics[width=0.9\linewidth]{11-exploration_files/figure-latex/unnamed-chunk-22-1} \end{center}

Interpretation \ldots{}

We can even pull out particular terms and explore them directly.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Term frequency comparison}
\NormalTok{lastfm\_dfm\_ngrams }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{dfm\_weight}\NormalTok{(}\AttributeTok{scheme =} \StringTok{"prop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{textstat\_frequency}\NormalTok{(}\AttributeTok{groups =}\NormalTok{ genre) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(feature, }\StringTok{"i\_ain\textquotesingle{}t"}\NormalTok{))}
\CommentTok{\#\textgreater{}      feature frequency rank docfreq   group}
\CommentTok{\#\textgreater{} 45   i\_ain\textquotesingle{}t     0.110   45       4 country}
\CommentTok{\#\textgreater{} 1944 i\_ain\textquotesingle{}t     0.109   15       8 hip{-}hop}
\CommentTok{\#\textgreater{} 3733 i\_ain\textquotesingle{}t     0.109   54       3     pop}
\end{Highlighting}
\end{Shaded}

Interpretation \ldots{}

\begin{center}\includegraphics[width=0.9\linewidth]{11-exploration_files/figure-latex/unnamed-chunk-24-1} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{11-exploration_files/figure-latex/unnamed-chunk-25-1} \end{center}

Before we leave this introduction to frequency analysis, let's consider another type of metric which can be used to explore term usage in and across documents which aims to estimate lexical diversity, the number of unique terms (types) to the total number of terms (tokens). This is known as the Type-Token Ratio (TTR). The TTR measure is biased when comparison documents or groups differ in the number of total tokens. To mitigate this issue the Moving-Average Type-Token Ratio (MATTR) is often used. MATTR the moving window size must be set to a reasonable size given the size of the documents. In this case we will use 50 as all the lyrics in the datasset have at least this number of words.

I will use box plots to visualize the distribution of the TTR and MATTR estimates across the four genres.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_lexdiv }\OtherTok{\textless{}{-}}\NormalTok{ lastfm\_tokens }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{textstat\_lexdiv}\NormalTok{(}\AttributeTok{measure =} \FunctionTok{c}\NormalTok{(}\StringTok{"TTR"}\NormalTok{, }\StringTok{"MATTR"}\NormalTok{), }\AttributeTok{MATTR\_window =} \DecValTok{50}\NormalTok{)}

\NormalTok{lastfm\_docvars }\OtherTok{\textless{}{-}}\NormalTok{ lastfm\_tokens }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{docvars}\NormalTok{()}

\NormalTok{lastfm\_lexdiv\_meta }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(lastfm\_docvars, lastfm\_lexdiv)}

\NormalTok{p1 }\OtherTok{\textless{}{-}}\NormalTok{ lastfm\_lexdiv\_meta }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(genre, TTR), }\AttributeTok{y =}\NormalTok{ TTR, }\AttributeTok{color =}\NormalTok{ genre)) }\SpecialCharTok{+} \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Genre"}\NormalTok{)}

\NormalTok{p2 }\OtherTok{\textless{}{-}}\NormalTok{ lastfm\_lexdiv\_meta }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(genre, MATTR), }\AttributeTok{y =}\NormalTok{ MATTR, }\AttributeTok{color =}\NormalTok{ genre)) }\SpecialCharTok{+} \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Genre"}\NormalTok{)}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-exploration_files/figure-latex/unnamed-chunk-26-1} \end{center}

We can see that there are similarities and differences between the two estimates of lexical diversity. In both cases, there is a trend towards `country' being the most diverse and `pop' the least diverse. `rock' and `hip-hop' are swapped given the estimate type. It is important, however, to note that the notches in the box plot provide us a rough guide to gauge whether these trends are statistically significant or not. Focusing on the more reliable MATTR and using the notches as our guide, it looks like we can safely say that `country' is more lexically diverse than the other genres. Another potential take-home message is that pop appears to be the most internally variable --that is, there appears to be quite a bit of variability between the lexical diversity in songs in this genre.

\hypertarget{collocation-analysis}{%
\subsubsection{Collocation analysis}\label{collocation-analysis}}

Where frequency analysis focuses on the usage of terms, collocation analysis focuses on the usage of terms in context.

\begin{itemize}
\tightlist
\item
  Keyword in Context

  \begin{itemize}
  \tightlist
  \item
    \texttt{kwic()}
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_tokens }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{tokens\_group}\NormalTok{(}\AttributeTok{groups =}\NormalTok{ genre) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{kwic}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"ain\textquotesingle{}t"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\CommentTok{\#\textgreater{} Keyword{-}in{-}context with 10 matches.                                                            }
\CommentTok{\#\textgreater{}      [rock, 9593] leaving without you cause heaven | ain\textquotesingle{}t |}
\CommentTok{\#\textgreater{}   [hip{-}hop, 4754]       world ain\textquotesingle{}t cake jail bars | ain\textquotesingle{}t |}
\CommentTok{\#\textgreater{}  [hip{-}hop, 12103]            now you and your girl | ain\textquotesingle{}t |}
\CommentTok{\#\textgreater{}   [country, 2758]            i\textquotesingle{}m a redneck woman i | ain\textquotesingle{}t |}
\CommentTok{\#\textgreater{}       [pop, 9507]         scared of your brother i | ain\textquotesingle{}t |}
\CommentTok{\#\textgreater{}   [hip{-}hop, 5603]            you with one though i | ain\textquotesingle{}t |}
\CommentTok{\#\textgreater{}  [hip{-}hop, 11756]        for my child\textquotesingle{}s birthday i | ain\textquotesingle{}t |}
\CommentTok{\#\textgreater{}      [pop, 10320] is thriller thriller night there | ain\textquotesingle{}t |}
\CommentTok{\#\textgreater{}   [hip{-}hop, 1782]               on my hand boy you | ain\textquotesingle{}t |}
\CommentTok{\#\textgreater{}       [pop, 3060]         happen like that cause i | ain\textquotesingle{}t |}
\CommentTok{\#\textgreater{}                              }
\CommentTok{\#\textgreater{}  close in a place like       }
\CommentTok{\#\textgreater{}  golden gates those who fake }
\CommentTok{\#\textgreater{}  speaking no more cause my   }
\CommentTok{\#\textgreater{}  no high class broad i\textquotesingle{}m     }
\CommentTok{\#\textgreater{}  scared of no sheets i       }
\CommentTok{\#\textgreater{}  happy i\textquotesingle{}m feeling glad i    }
\CommentTok{\#\textgreater{}  invited despite it i show   }
\CommentTok{\#\textgreater{}  no second chance against the}
\CommentTok{\#\textgreater{}  my man boy i\textquotesingle{}m just         }
\CommentTok{\#\textgreater{}  no hollaback girl i ain\textquotesingle{}t}
\end{Highlighting}
\end{Shaded}

You can also search for multiword expressions using \texttt{phrase()}. You can use a pattern matching convention to make your key term searches more (`glob' and `regex') or less (`fixed') flexible.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_tokens }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{tokens\_group}\NormalTok{(}\AttributeTok{groups =}\NormalTok{ genre) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{kwic}\NormalTok{(}\AttributeTok{pattern =} \FunctionTok{phrase}\NormalTok{(}\StringTok{"ain\textquotesingle{}t no*"}\NormalTok{), }\AttributeTok{valuetype =} \StringTok{"glob"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\CommentTok{\#\textgreater{} Keyword{-}in{-}context with 10 matches.                                                                   }
\CommentTok{\#\textgreater{}        [pop, 4528:4529]        good to deny it it |   ain\textquotesingle{}t no    |}
\CommentTok{\#\textgreater{}        [pop, 3199:3200]  happen like that cause i |   ain\textquotesingle{}t no    |}
\CommentTok{\#\textgreater{}  [hip{-}hop, 12354:12355] the telephone you know it | ain\textquotesingle{}t nothin  |}
\CommentTok{\#\textgreater{}      [hip{-}hop, 171:172] they wanna fuck but homie | ain\textquotesingle{}t nothing |}
\CommentTok{\#\textgreater{}        [pop, 3092:3093] ain\textquotesingle{}t no hollaback girl i |   ain\textquotesingle{}t no    |}
\CommentTok{\#\textgreater{}        [pop, 3456:3457] ain\textquotesingle{}t no hollaback girl i |   ain\textquotesingle{}t no    |}
\CommentTok{\#\textgreater{}        [pop, 3478:3479]  happen like that cause i |   ain\textquotesingle{}t no    |}
\CommentTok{\#\textgreater{}    [hip{-}hop, 3345:3346] streets it\textquotesingle{}s the d{-}r{-}e it | ain\textquotesingle{}t nothing |}
\CommentTok{\#\textgreater{}        [pop, 3065:3066] ain\textquotesingle{}t no hollaback girl i |   ain\textquotesingle{}t no    |}
\CommentTok{\#\textgreater{}        [pop, 3226:3227]  happen like that cause i |   ain\textquotesingle{}t no    |}
\CommentTok{\#\textgreater{}                             }
\CommentTok{\#\textgreater{}  big deal it\textquotesingle{}s innocent yeah}
\CommentTok{\#\textgreater{}  hollaback girl i ain\textquotesingle{}t no  }
\CommentTok{\#\textgreater{}  to drop a couple stacks    }
\CommentTok{\#\textgreater{}  change hoes down g\textquotesingle{}s up    }
\CommentTok{\#\textgreater{}  hollaback girl ooh ooh this}
\CommentTok{\#\textgreater{}  hollaback girl a few times }
\CommentTok{\#\textgreater{}  hollaback girl i ain\textquotesingle{}t no  }
\CommentTok{\#\textgreater{}  but more hot shit another  }
\CommentTok{\#\textgreater{}  hollaback girl a few times }
\CommentTok{\#\textgreater{}  hollaback girl i ain\textquotesingle{}t no}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Collocation analysis
\end{itemize}

The frequency analysis of ngrams as terms is similar to but distinct from a collocation analysis. In a collocation analysis the frequency with which a two or more terms co-occur is balanced by the frequency of the terms when they do not cooccur. In other words, the sequences occur more than one would expect given the frequency of the individual terms. This provides an estimate of the tendency of a sequence of words to form a cohesive semantic or syntactic unit.

We can apply the \texttt{textstat\_collocations()} function on a tokens object (\texttt{lastfm\_tokens}) and retrieve the most cohesive collocations (using the \(z\)-statistic) for the entire dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_tokens }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{textstat\_collocations}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{}   collocation count count\_nested length lambda    z}
\CommentTok{\#\textgreater{} 1      in the   227            0      2   2.95 33.4}
\CommentTok{\#\textgreater{} 2   yeah yeah    86            0      2   5.32 33.4}
\CommentTok{\#\textgreater{} 3   jump jump    64            0      2   8.44 28.0}
\CommentTok{\#\textgreater{} 4       oh oh    59            0      2   4.78 27.8}
\CommentTok{\#\textgreater{} 5     bum bum    44            0      2   7.48 26.7}
\end{Highlighting}
\end{Shaded}

Add a minimum frequency count (\texttt{min\_count\ =}) to avoid hapaxes (terms which happen infrequently yet when they do occur, the cooccur with another specific term which also occurs infrequently). We can also specify the size of the collocation (the default is 2). If we set it to 3 then we will get three-word collocations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_tokens }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{textstat\_collocations}\NormalTok{(}\AttributeTok{min\_count =} \DecValTok{50}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\CommentTok{\#\textgreater{}    collocation count count\_nested length lambda    z}
\CommentTok{\#\textgreater{} 1       in the   227            0      2   2.95 33.4}
\CommentTok{\#\textgreater{} 2    yeah yeah    86            0      2   5.32 33.4}
\CommentTok{\#\textgreater{} 3    jump jump    64            0      2   8.44 28.0}
\CommentTok{\#\textgreater{} 4        oh oh    59            0      2   4.78 27.8}
\CommentTok{\#\textgreater{} 5        la la    59            0      2   9.13 25.5}
\CommentTok{\#\textgreater{} 6      i\textquotesingle{}m not    64            0      2   3.95 25.0}
\CommentTok{\#\textgreater{} 7     a little    76            0      2   4.45 23.3}
\CommentTok{\#\textgreater{} 8      i don\textquotesingle{}t   140            0      2   2.40 23.1}
\CommentTok{\#\textgreater{} 9       on the   131            0      2   2.29 22.0}
\CommentTok{\#\textgreater{} 10      like a    81            0      2   2.82 21.4}
\end{Highlighting}
\end{Shaded}

If we want to explore the collocations for a specific group in our dataset, we can use the \texttt{tokens\_subset()} function and specify the group that we want to subset and use. Note that the minimum count will need to be lowered (if used at all) as the size of the dataset is now a fraction of what is was when we considered all the documents (not just those from a particular genre).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_tokens }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{tokens\_subset}\NormalTok{(genre }\SpecialCharTok{==} \StringTok{"pop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{textstat\_collocations}\NormalTok{(}\AttributeTok{min\_count =} \DecValTok{10}\NormalTok{, }\AttributeTok{size =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{25}\NormalTok{)}
\CommentTok{\#\textgreater{}        collocation count count\_nested length lambda    z}
\CommentTok{\#\textgreater{} 1         of you i    12            0      3   6.85 6.35}
\CommentTok{\#\textgreater{} 2     i just can\textquotesingle{}t    16            0      3   5.98 4.80}
\CommentTok{\#\textgreater{} 3         cry me a    27            0      3   8.81 4.03}
\CommentTok{\#\textgreater{} 4        up on you    10            0      3   4.01 4.00}
\CommentTok{\#\textgreater{} 5        is not my    17            0      3   7.11 3.84}
\CommentTok{\#\textgreater{} 6    you know that    19            0      3   2.71 3.77}
\CommentTok{\#\textgreater{} 7   don\textquotesingle{}t stop the    25            0      3   7.01 3.76}
\CommentTok{\#\textgreater{} 8   know just just    14            0      3   7.55 3.61}
\CommentTok{\#\textgreater{} 9       you and me    11            0      3   5.16 3.44}
\CommentTok{\#\textgreater{} 10  just just what    14            0      3   7.14 3.43}
\CommentTok{\#\textgreater{} 11  don\textquotesingle{}t you like    13            0      3   5.16 3.42}
\CommentTok{\#\textgreater{} 12    are you okay    40            0      3   6.98 3.37}
\CommentTok{\#\textgreater{} 13     just like a    27            0      3   5.58 3.35}
\CommentTok{\#\textgreater{} 14 where you gonna    27            0      3   5.90 3.30}
\CommentTok{\#\textgreater{} 15    i\textquotesingle{}m not your    10            0      3   6.80 3.28}
\CommentTok{\#\textgreater{} 16        my no he    10            0      3   7.85 3.16}
\CommentTok{\#\textgreater{} 17        me and i    10            0      3   2.45 3.15}
\CommentTok{\#\textgreater{} 18   don\textquotesingle{}t call my    18            0      3   6.36 3.09}
\CommentTok{\#\textgreater{} 19   gonna be okay    10            0      3   7.59 3.06}
\CommentTok{\#\textgreater{} 20    this my shit    33            0      3   7.46 3.00}
\CommentTok{\#\textgreater{} 21  dance gonna be    10            0      3   7.28 2.94}
\CommentTok{\#\textgreater{} 22  because of you    14            0      3   6.19 2.90}
\CommentTok{\#\textgreater{} 23   just what you    14            0      3   2.94 2.90}
\CommentTok{\#\textgreater{} 24   for your call    11            0      3   5.97 2.84}
\CommentTok{\#\textgreater{} 25      am the one    12            0      3   6.05 2.84}
\end{Highlighting}
\end{Shaded}

In this section we have covered some common strategies for doing exploration with descriptive analysis methods. These methods can be extended and combined to dig into and uncover patterns as the research and intermediate findings dictate.

\hypertarget{unsupervised-learning}{%
\subsection{Unsupervised learning}\label{unsupervised-learning}}

We now turn our attention to a second group of methods for conducting exploratory analyses --unsupervised learning.

\begin{itemize}
\tightlist
\item
  Clustering

  \begin{itemize}
  \tightlist
  \item
    \texttt{textstat\_dist()}
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(factoextra)}

\NormalTok{lastfm\_clust }\OtherTok{\textless{}{-}}\NormalTok{ lastfm\_dfm }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{dfm\_weight}\NormalTok{(}\AttributeTok{scheme =} \StringTok{"prop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{textstat\_dist}\NormalTok{(}\AttributeTok{method =} \StringTok{"euclidean"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{as.dist}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{hclust}\NormalTok{(}\AttributeTok{method =} \StringTok{"ward.D2"}\NormalTok{)}

\NormalTok{lastfm\_clust }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{fviz\_dend}\NormalTok{(}\AttributeTok{show\_labels =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{k =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-exploration_files/figure-latex/unnamed-chunk-32-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_clust }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{fviz\_dend}\NormalTok{(}\AttributeTok{show\_labels =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{k =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-exploration_files/figure-latex/unnamed-chunk-33-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clusters }\OtherTok{\textless{}{-}}\NormalTok{ lastfm\_clust }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{cutree}\NormalTok{(}\AttributeTok{k =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{as\_tibble}\NormalTok{(}\AttributeTok{rownames =} \StringTok{"document"}\NormalTok{)}

\NormalTok{clusters}
\CommentTok{\#\textgreater{} \# A tibble: 155 x 2}
\CommentTok{\#\textgreater{}    document value}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}    \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{}  1 text1        1}
\CommentTok{\#\textgreater{}  2 text2        2}
\CommentTok{\#\textgreater{}  3 text3        2}
\CommentTok{\#\textgreater{}  4 text4        1}
\CommentTok{\#\textgreater{}  5 text5        1}
\CommentTok{\#\textgreater{}  6 text6        2}
\CommentTok{\#\textgreater{}  7 text7        3}
\CommentTok{\#\textgreater{}  8 text8        2}
\CommentTok{\#\textgreater{}  9 text9        1}
\CommentTok{\#\textgreater{} 10 text10       1}
\CommentTok{\#\textgreater{} \# ... with 145 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{docvars}\NormalTok{(lastfm\_dfm, }\AttributeTok{field =} \StringTok{"cluster"}\NormalTok{) }\OtherTok{\textless{}{-}}\NormalTok{ clusters}\SpecialCharTok{$}\NormalTok{value}
\NormalTok{lastfm\_dfm}\SpecialCharTok{$}\NormalTok{cluster }\OtherTok{\textless{}{-}}\NormalTok{ clusters}\SpecialCharTok{$}\NormalTok{value}

\NormalTok{lastfm\_dfm }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    docvars }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    head}
\CommentTok{\#\textgreater{}             artist                song   genre cluster}
\CommentTok{\#\textgreater{} 1     Alan Jackson        Little Bitty country       1}
\CommentTok{\#\textgreater{} 2     Alan Jackson       Remember When country       2}
\CommentTok{\#\textgreater{} 3     Brad Paisley    Mud on the Tires country       2}
\CommentTok{\#\textgreater{} 4 Carrie Underwood    Before He Cheats country       1}
\CommentTok{\#\textgreater{} 5   Dierks Bentley What Was I Thinkin\textquotesingle{} country       1}
\CommentTok{\#\textgreater{} 6     Dolly Parton              9 to 5 country       2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_dfm }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{docvars}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    janitor}\SpecialCharTok{::}\FunctionTok{tabyl}\NormalTok{(genre, cluster) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    janitor}\SpecialCharTok{::}\FunctionTok{adorn\_totals}\NormalTok{(}\AttributeTok{where =} \FunctionTok{c}\NormalTok{(}\StringTok{"row"}\NormalTok{, }\StringTok{"col"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    janitor}\SpecialCharTok{::}\FunctionTok{adorn\_percentages}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    janitor}\SpecialCharTok{::}\FunctionTok{adorn\_pct\_formatting}\NormalTok{()}
\CommentTok{\#\textgreater{}    genre     1     2     3  Total}
\CommentTok{\#\textgreater{}  country 47.7\% 40.9\% 11.4\% 100.0\%}
\CommentTok{\#\textgreater{}  hip{-}hop 69.2\% 19.2\% 11.5\% 100.0\%}
\CommentTok{\#\textgreater{}      pop 63.4\% 19.5\% 17.1\% 100.0\%}
\CommentTok{\#\textgreater{}     rock 54.5\% 31.8\% 13.6\% 100.0\%}
\CommentTok{\#\textgreater{}    Total 57.4\% 29.0\% 13.5\% 100.0\%}
\end{Highlighting}
\end{Shaded}

Looking at the assigned clusters and the genres of the songs we see some interesting patterns. For one cluster 1 appears to have the majority of the songs, followed by cluster 2, and 3. In cluster 1 Hip hop and Pop make up the majority of the songs. In cluster 2 Country and Rock tend to dominate and in cluster 3 there is a scattering of genres.

Now we can approach this again with distinct linguistic unit. Where in our current clusters we used words, we could switch to bigrams and see if the results change and how they change.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Clustering: bigram features}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Topic modeling
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# textmodel\_lsa()}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Sentiment analysis
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(vader)  }\CommentTok{\# sentiment analysis for micro{-}blogging text}
\FunctionTok{library}\NormalTok{(syuzhet)  }\CommentTok{\# general sentiment analysis}
\end{Highlighting}
\end{Shaded}

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{section-1}{%
\section{\ldots{}}\label{section-1}}

  \bibliography{coursebook.bib,packages.bib}

\end{document}
