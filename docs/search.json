[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"\nINCOMPLETE DRAFT\n coursebook accompany Linguistics 380 “Language Use Technology” Wake Forest University. working title coursebook Text Data: Introduction Quantitative Text Analysis Reproducible Research R. content currently development. Feedback welcome can provided hypothes.service. toolbar interface service located right sidebar. Note: need register free account make comments suggestions.AuthorDr. Jerid Francom Associate Professor Spanish Linguistics Wake Forest University. research interests focused around quantitative approaches language variation.","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"work Jerid C. Francom licensed Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States License.","code":""},{"path":"course.html","id":"course","chapter":"Course","heading":"Course","text":"\nINCOMPLETE DRAFT\njourney thousand miles begins one step.\n–Lao Tzu\nessential questions chapter :\n\n…\nchapter aims provide brief summary current research trends form context rationale textbook. also provides instructors students overview purpose approach textbook. also include description main components section chapter provide guide conventions used book resources available.","code":""},{"path":"course.html","id":"rationale","chapter":"Course","heading":"0.1 Rationale","text":"recent years growing buzz around term ‘Data Science’ related terms; data analytics, data mining, etc. nutshell data science process investigator leverages statistical methods computational power uncover insight large datasets. Driven large part increase computing power available average individual increasing amount electronic data now available internet, interest data science expanded virtually fields academia areas public sector. Data scientists high demand trend expected continue foreseeable future.coursebook introduction fundamental concepts practical programming skills Data Science increasingly employed variety language-centered fields sub-fields. geared towards advanced undergraduates graduate students linguistics related fields. quantitative research skills quickly becoming core aspect many language programs, coursebook aims provide fundamental understanding theoretical concepts, programming skills, statistical methods quantitative text analysis.","code":""},{"path":"course.html","id":"learning-goals","chapter":"Course","heading":"0.2 Learning goals","text":"course :Data Literacy (DL): learn interpret, assess, contextualize findings based data.ability understand apply data analysis derive insight dataability understand apply data knowledge skills across linguistic language-related disciplinesResearch Skills (RS): learn conduct original research (design, implementation, interpretation, communication).identify applicable area investigation linguistic language-related fielddevelop viable research question hypothesisassess, acquire, document datacurate transform data analysisselect apply relevant analysis methodinterpret communicate findingsProgramming Skills (PS): learn produce research work collaboratively others.demonstrate proficiency implement research R (RD points 3-5)demonstrate ability produce collaborative reproducible research using R, RStudio, GitHubIn chapter coursebook specific learning objectives specified target learning outcomes clear .","code":""},{"path":"course.html","id":"approach","chapter":"Course","heading":"0.3 Approach","text":"Many textbooks ‘Data Science,’ even domain-centric approach, text analysis, tend focus basic ‘tidy’ approach, seen Figure 0.1 Wickham & Grolemund (2017). However resources tend underrepresent importance leading research question. big part, perhaps biggest part quantitative research, research general question addressed. comes orient research approach best address question (questions). move matching data sources, organizing data, modeling data, finally reporting findings.\nFigure 0.1: Workflow diagram R Data Science.\nthink central advantage coursebook language researchers thread project goals conceptual point view without technical implementation mind first., general idea data look like, analyzed, analysis contribute knowledge field, can move towards implementing preliminary formulations R code. essence approach reflects classic separation content format –content research precede format take.coursebook divided four parts:“Foundations,” environmental survey quantitative research across disciplines orient language-based research provided. (Provide historical research context quantitative text analysis)“Orientation” aims build knowledge data , text organized datasets, role statistics play quantitative research types statistical approaches commonly found text analysis research, finally develop research question research blueprint conducting quantitative text analysis research project. (Develop understanding quantitative research approached)“Preparation” covers variety implementation approaches stage deriving dataset ready statistical analysis includes acquiring, curating, transforming data. (Dive coding practices produce data ready statistical analysis)“Modeling” elaborates various statistical approaches data analysis contextualizes application types research questions. (Conducting statistical text analysis)","code":""},{"path":"course.html","id":"prerequisites","chapter":"Course","heading":"0.4 Prerequisites","text":"\nChange subsection:\n\nMove R, RStudio, Packages, Git, GitHub tadr package vignettes/ articles\n\nMake reference tadr package (Coursebook support package)\n\ncontinue, make sure software need book:R: …R: …RStudio: RStudio free open source integrated development environment (IDE) R. …RStudio: RStudio free open source integrated development environment (IDE) R. …R packages: coursebook uses bunch R packages.\ncan install running:\n\ninstall.packages(c(\"bookdown\"))R packages: coursebook uses bunch R packages.\ncan install running:Coursebook support package\ntadr support R package resource site coursebook. package includes data, functions, interactive R programming tutorials make use swirl package. website includes programming demonstrations called ‘Worked’ examples reference documentation resources quantitative research R.Coursebook support package\ntadr support R package resource site coursebook. package includes data, functions, interactive R programming tutorials make use swirl package. website includes programming demonstrations called ‘Worked’ examples reference documentation resources quantitative research R.","code":"\ninstall.packages(c(\"bookdown\"))\ninstall.packages(\"devtools\")\ndevtools::install_github(\"lin380/tadr\")"},{"path":"course.html","id":"programming","chapter":"Course","heading":"0.5 Programming","text":"Reasons program:Flexibility Graphical User Interface (GUI) based software inherently limited. see get. another need, need find tool. another tool implement think need, luck.Transparency taking programming approach research analysis make decisions explicit leave breadcrumb trail everything .Reproducibility clearer also allow share process others (including future self!). Insight grows much faster exposed light. Sharing research collaborators sites GitHub BitBucket brings makes work visible accessible world. Reproducibility gaining momentum fueled programmatic approaches research.Reasons use R:One stop shopping known specifically statistical programming language, R can now round trip tool acquire, curate, transform, visualize, statistically analyze data. also allows robust communication reports data analysis sharing (reproducibility).alone sizable R programming community, especially academics. two tangible benefits; first, likely able find user contributed R packages satisfy many sophisticated programming goals second, able get answers programming questions popular sites like StackOverflow.RStudio RStudio envy many programmers. capable interface R provides convenient access powerful tools allow efficient productive R programmer.","code":""},{"path":"course.html","id":"conventions","chapter":"Course","heading":"0.6 Conventions","text":"coursebook concepts understanding techniques quantitative text analysis R. Therefore intermingling prose code presented. , attempt establish consistent conventions throughout text made signal reader’s attention appropriate. explore concepts, R code incorporated text. may unique textbook compared others seen. created using R –specifically using R language package called bookdown (Xie, 2021). R package makes possible write, execute (‘run’), display code results within text.example, following text block shows actual R code results generated running code. Note hashtag # signals code comment. code follows within text block subsequent text block displays output code.Inline code used code blocks short results needed display. example, code sometimes appear 1 + 1.necessary meta-description code appear. particularly relevant R Markdown documents.terms prose, key concepts signaled using bold italics. Terms appear typeface also appear [glossary] end text. Furthermore, four pose text blocks used signal reader’s attention: key points, notes, tips, warnings.Key points summarize main points covered chapter subsection text.\nchapter learn:\n\ngoals textbook\n\nreasoning using R programming language\n\nimportant text conventions employed textbook\nNotes provide bit information topic find information.\nR powerful statistical programming language, also can used perform necessary steps data science project; including reporting. relatively new addition reporting capabilities R bookdown package (textbook created using package). can find .\nTips used signal helpful hints might otherwise overlooked.\ncourse exploratory work session, many R objects often created test ideas. point inspecting workspace becomes difficult due number objects displayed using ls().\n\nremove objects workspace, use rm(list = ls()).\nErrors inevitable part learning, errors can avoided. text used warning text block highlight typical pitfalls errors.\nHello world!\nwarning.\nAlthough intended -depth introduction statistical techniques, mathematical formulas included text. formulas appear either inline \\(1 + 1 = 2\\) block equations.\\[\\begin{equation}\n  \\hat{c} = \\underset{c \\C} {\\mathrm{argmax}} ~\\hat{P}(c) \\prod_i \\hat{P}(w_i|c)\n  \\tag{0.1}\n\\end{equation}\\]Data analysis leans heavily graphical representations. Figures appear numbered, Figure 0.2.\nFigure 0.2: Test plot mtcars dataset\nTables, Table 0.1 numbered separately figures.Table 0.1: nice table!","code":"\n# Add 1 plus 1\n1 + 1\n#> [1] 2```{r test-code}\n1 + 1\n```\nlibrary(ggplot2) # load graphics package\nggplot(mtcars, aes(x = hp, y = mpg)) + # map 'hp' and 'mpg' to coordinate space\n  geom_point() + # add points\n  geom_smooth(method = \"lm\") + # draw linear trend line\n  labs(x = \"Horsepower\", # label x axis\n       y = \"Miles per gallon\", # label y axis\n       title = \"Test plot\", # add title\n       subtitle = \"From mtcars dataset\") # add subtitle\nknitr::kable(head(iris, 20), caption = \"Here is a nice table!\", booktabs = TRUE)"},{"path":"course.html","id":"build-information","chapter":"Course","heading":"0.7 Build information","text":"coursebook written bookdown inside RStudio. website hosted GitHub Pages complete source available GitHub.version coursebook built R version 4.0.2 (2020-06-22) following packages:","code":""},{"path":"foundations-overview.html","id":"foundations-overview","chapter":"Overview","heading":"Overview","text":"FOUNDATIONSIn section aims (1) provide overview quantitative research applications, highlighting visible applications notable research various fields. (2) hood bit consider quantitative research contributes language research. (3) layout main types research situate quantitative text analysis inside . attention given historical background understand theory (generative usage-based grammar) framed degree continues frame language research. (4) discuss programmatic approaches language, fundamental quantitative text analysis, also provide opportunity science process documentation research reproducibility.","code":""},{"path":"data-language-and-text-analysis.html","id":"data-language-and-text-analysis","chapter":"1 Data, language, and text analysis","heading":"1 Data, language, and text analysis","text":"\nDRAFT\nScience walks forward two feet, namely theory experiment…Sometimes one foot put forward first, sometimes , continuous progress made use .\n—Robert . Millikan (1923)\nessential questions chapter :\n\nrole goals data analysis outside academia?\n\nways quantitative language research approached?\n\napplications text analysis?\n\ncoursebook structured target learning goals?\nchapter aim introduce topic text analysis text analytics frame approach coursebook. goals section work general field data science/ data analysis particular sub-field text analysis (text defined broadly corpus). aim introduce context needed understand text analysis fits larger universe data analysis see commonalities ever-ubiquitous field data analysis, attention language linguistics studies employ data analysis particular area text analysis. round chapter, provide general overview rest coursebook motivating general structure sequencing well setting foundation programmatic approaches data analysis.","code":""},{"path":"data-language-and-text-analysis.html","id":"making-sense-of-a-complex-world","chapter":"1 Data, language, and text analysis","heading":"1.1 Making sense of a complex world","text":"world around us full actions interactions numerous difficult really comprehend. lens individual sees experiences world. gain knowledge world build heuristic knowledge works can interact . happens regardless educational background. humans built . minds process countless sensory inputs many never make conscious mind. underlie skills abilities take granted like able predict happen see someone knock wine glass table onto concrete floor. ’ve never seen object first time ’ve winery, somehow somewhere ‘instinctively’ make effort warn --glass-breaker late. likely stopped consider predictive knowledge come , , may just chalked ‘common sense.’ common may , incredible display brain’s capacity monitor environment, relate events observations take place, store information time making big fuss tell conscious mind ’s .wait, coursebook text analytics language, right? ? Well, two points make relevant framing journey: (1) world full countless information unfold real-time scale daunting (2) power brain works efficiently behind scene making sense world, one individual living one life limited view world large. Let expand two points little .First let’s clear. way one experience things times, .e. omnipotence. even extremely reduced slices reality still vastly outside experiental capacity, least real-time. One can make point since inception internet individual’s ability experience larger slices world increased. imagine reading, watching, listening every file currently accessible web? (see Wayback Machine)? Scale back even ; let’s take Wikipedia, world’s largest encyclopedia. Can imagine reading every wiki entry? large resource Wikipedia ,1 still small fragment written language produced web, just web. Consider moment.second framing point, actually two points one. made underscored efficiency brain’s capacity make sense world. efficiency comes clever evolutionary twists lead brain take world makes shortcuts compress raw experience heuristic understanding. means brain supercomputer. store every experience raw form, access records experience like imagine computer access records logged database. brains excel making associations predictions help us (time) navigate complex world inhabit. point key –brains amazing work, work can give us impression understand world detail actually . Let’s little thought experiment. Close eyes think last time saw best friend. wearing? Can remember colors? like , human, probably pretty confident feeling know answers questions chance right. demonstrated numerous experiments human memory confidence correlate accuracy. (..? JFK, 9/11, …example) ’ve experienced event, real reason lives experienced. ’s little bit scary, sure, magic works ‘good enough’ practical purposes.’s deal: humans (1) clearly unable experience large swaths experience simple fact individuals living individual lives (2) experiences live recorded precision therefore ‘trust’ intuitions, least absolute sense.mean human curiosity world around us ability reliably make sense ? short means need approach understanding world tools science. Science powerful makes strides overcome inherit limitations humans (breadth experience recall relational abilities) bring complex world digestible perspective. Science starts question, identifies collects data, careful selected slices complex world, submits data analysis clearly defined reproducible procedures, reports results others evaluate. process repeated, modifying, manipulating procedures, asking new questions positing new explanations, effort make inroads bring complex tangible view.essence science attempt subvert inherent limitations understanding drawing carefully purposefully collected slices experience letting analysis experience speak, even goes intuitions (powerful sometime spurious heuristics brains use make sense world).","code":""},{"path":"data-language-and-text-analysis.html","id":"data-analysis","chapter":"1 Data, language, and text analysis","heading":"1.2 Data analysis","text":"point ’ve sketched outline strengths limitations humans’ ability make sense world science address limitations. science ’ve described one familiar indespensible tool make sense world. like , description science may associated visions white coats, labs, petri dishes. science’s foundation still stands strong 21st century, series intellectual technological events mid-20th century set motion changes changed aspects science done, done. call Science 2.0, let’s use popularized term “Data Science.” recognized beginnings Data Science attributed work “Statistics Data Analysis Research” department Bell Labs 1960s. Although primarily conceptual theoretic time, framework quantitative data analysis took shape anticipate come: sizable datasets “…require advanced statistical computational techniques … software implement .” (Chambers, 2020) framework emphasized inference-based research traditional science, also embraced exploratory research recognized need address practical considerations arise working deriving insight abundance data.Fast-forward 21st century world machine readable data truly abundance. increased computing power innovative uses technology world wide web took flight. put perspective, focusing language, amount text [add stats amount data added web every day/month/year compared literature .. ..?]. data flood limited language, sensors recording devices ever capture evermore swaths world live (Desjardins, 2019). increased computing power gave rise influx data, also primary methods gathering, preparing, transforming, analyzing, communicating insight derived data (Donoho, 2017). vision laid 1960s Bell Labs come fruition.interest deriving insight available data now almost ubiquitous. science data now reached deep aspects life making sense world sought. Predicting whether loan applicant get loan [cite], whether lump cancerous [cite], films recommend based previous viewing history [cite], players sports team sign (Lewis, 2004) now incorporate common set data analysis tools.advances, however, predicated data alone. envisioned researchers Bell Labs, turning data insight takes computing skills (.e. programming), knowledge statistics, , importantly, substantive/ domain expertise. triad popularly represented Drew Conway Venn diagram 1.1.\nFigure 1.1: Venn diagram Drew Conway\ntoolbelt underlies well-known public-facing language applications. language-capable personal assistant applications, plagiarism detection software, machine translation search, tangible results quantitative approaches language becoming standard fixtures lives.\nFigure 1.2: Well-known language applications\nspread quantitative data analysis taken root academia. Even areas first blush don’t appear approached quantitative manner fields social sciences humanities, data science making important sometimes disisplinary changes way academic research conducted. coursebook focuses domain cuts across many fields; namely language. point let’s turn quantitative approaches language.","code":""},{"path":"data-language-and-text-analysis.html","id":"language-analysis","chapter":"1 Data, language, and text analysis","heading":"1.3 Language analysis","text":"Language defining characteristic species. , study language key concern wide variety fields, just linguists. goals various fields, however, approaches language research, vary. one hand language research traditions, namely closely associated Noam Chomsky, eschewed quantitative approaches language research later half 20th century instead turned qualitative assessment language structure introspective methods. hand many language research programs turned /developed quantitative research methods either necessity theoretical principles. quantitative research trajectories share much common data analysis toolbox described previous section. means large extent language analysis projects share common research language language research also research beyond outside language. However, never one-size-fits approach anything –much less data analysis. quantitative analysis key distinction data collection downstream effects terms procedure also terms interpretation.key distinction, need make point, provide context exploration text analysis, comes approach collecting language data nature data. distinction experimental observational data collection. Experimental approaches start intentionally designed hypothesis lay research methodology appropriate instruments plan collect data shows promise shedding light validity hypothesis. Experimental approaches conducted controlled contexts, usually lab environment, participants recruited perform language related task stimuli carefully curated researchers elicit aspect language behavior interest. Experimental approaches language research heavily influenced procedures adapted psychology. link logical language central area study cognitive psychology. approach looks much like white-coat science made reference earlier , quantitative research, now taken advantage data analysis tool belt collect organize much larger quantities data conduct statistically robust analysis procedures communicate findings efficiently.Observational approaches bit mixed bag terms rationale study; may either start testable hypothesis cases may start open-ended research question explore. fundamental distinction two drawn amount control researcher contexts conditions language behavior data collected produced. Observational approaches seek records language behavior produced language speakers communicative purposes natural(istic) contexts. may take place labs (language development, language disorders, etc.), often , language collected sources speakers performing language part daily lives –whether posting social media, speaking telephone, making political speeches, writing class essays, reporting latest news newspaper, crafting next novel destined New York Times best-seller. , data collected ‘wild’ varies structure relative data collected experimental approaches requires number steps prepare data synch data analysis tool belt.liken distinction experimental observational data collection difference farming foraging. Experimental approaches like farming; groundwork research plan designed, much field prepared seeding, researcher performs series tasks produce data, just farmer waters cares crops, results process bear fruit, data case, data harvested. Observational approaches like foraging; researcher scans available environmental landscape viable sources data naturally existing sources, sources assessed usefulness value address research question, viable selected, data collected.data acquired approaches trade-offs, just farming foraging. Experimental approaches directly elicit language behavior highly controlled conditions. directness level control benefit allowing researchers precisely track particular experimental conditions effect language behavior. conditions explicit part design therefore resulting language behavior can precisely attributed experimental manipulation. primary shortcoming experimental approaches level artificialness directness control. Whether language materials used task, task , fact procedure takes place supervision language behavior elicited can diverge quite significantly language behavior performed natural communicative settings. Observational approaches show complementary strengths shortcomings. Whereas experimental approaches may diverge natural language use, observational approaches strive identify collected language behavior data natural, uncontrolled, unmonitored contexts. way observational approaches question extent language behavior data performed natural communicative act. flipside, contexts natural language communication take place complex relative experimental contexts. Language collected natural contexts nested within complex workings complex world inevitably include host factors conditions can prove challenging disentangle language phenomenon interest must addressed order draw reliable associations conclusions.upshot, , twofold: (1) data collection methods matter research design interpretation (2) single best approach data collection, strengths shortcomings. ideal, robust science language include insight experimental observational approaches (Gilquin & Gries, 2009). evermore greater appreciation complementary nature experimental observational approaches growing body research highlights recognition. Given particular trade-offs observational data often used exploratory starting point help build insight form predictions can submitted experimental conditions. way studies based observational data serve exploratory tool gather better externally valid view language use can serve make prediction can explore precision experimental paradigm. However, always case. Observational data also often used hypothesis-testing contexts well. furthermore, language-related fields, hypothesis-testing ultimate goal deriving knowledge insight.","code":""},{"path":"data-language-and-text-analysis.html","id":"text-analysis","chapter":"1 Data, language, and text analysis","heading":"1.4 Text analysis","text":"Text analysis application data analysis procedures data science derive insight textual data collected observational methods. deliberately chosen term ‘text analysis’ avoid see pitfalls using common terms literature Corpus Linguistics, Computational Linguistics, Digital Humanities. plenty learning resources focus specifically one three fields discussing quantitative analysis text. perspective missing resource underscores fact text analysis research methods employed span across wide variety academic fields applications industry. coursebook aims introduce areas lens data analysis procedures field. approach, hope, provides wider view potential applications using text data inspires either employ quantitative text analysis research raise awareness advantages text analysis making sense language-related linguistic-based phenomenon.applications text analysis? public facing applications stem Computational Linguistic research, often known Natural Language Processing practioners, well-known applications text analysis. Whether using search engines, online translators, submitting paper plagarism detection software, etc. text analysis methods cover play. uses text analysis production-level applications big money behind developing evermore robust text analysis methods.academia use text analysis even widespread, despite lack public fanfare. Let’s run select studies give idea areas employing text analysis, researchers text analysis, whet interest conducting text analysis project.sample studies include research areas translation, stylistics, language variation, dialectology, psychology, psycholinguistics, political science, sociolinguistics highlights diversity fields subareas employ quantitative text analysis. Text analysis center studies share set common goals:detect retrieve patterns text subtle numerous done handTo challenge assumptions /provide views textual sourcesTo explore new questions /provide novel insightLet’s now turn last section chapter provide overview rationale learning text analysis, structure content covered, justification approach take perform text analysis.","code":""},{"path":"data-language-and-text-analysis.html","id":"coursebook-overview","chapter":"1 Data, language, and text analysis","heading":"1.5 Coursebook overview","text":"section provide general overview rest coursebook motivating general structure sequencing well setting foundation programmatic approaches data analysis. Let highlight think valuable area study, hope gain coursebook, structure coursebook configured help scaffold conceptual practical knowledge text analysis.target learning outcomes coursebook following:Data LiteracyResearch SkillsProgramming SkillsData Literacy refers ability interpret, assess, contextualize findings based data. Throughout coursebook explore topics help understand data analysis methods derive insight data. process encouraged critically evaluate connections across linguistic language-related disciplines using data analysis knowledge skills. Data Literacy invaluable skillset academics professionals (cite) also indispensable aptitude 21st century citizens navigate actively participate ‘Information Age’ live (Carmi, Yates, Lockley, & Pawluczuk, 2020).Research skills covers ability conduct original research, communicate findings, make meaningful connections findings literature field. target area differ significantly, spirit, common learning outcomes research methods course: identify area investigation, develop viable research question hypothesis, collect relevant data, analyze data relevant statistical methods, interpret communicate findings. However, working text incur series key steps selection, collection, preparation data unique text analysis projects. addition, stress importance research documentation creating reproducible research integral part modern scientific inquiry.Programming skills aims develop ability implement research skills programmatically produce research replicable collaborative. Modern data analysis, extension, text analysis conducted using programming. various key reasons : (1) programming affords researchers unlimited research freedom –can envision , can program . said --shelf software either proprietary unmaintained –. (2) programming underlies well-documented reproducible research –documenting button clicks menu option selections leads research readily reproduced, either researcher future self! (3) programming forces researchers engage intimately data methods analysis. familiar data methods likely produce higher quality work.Now let turn learning goals integrate shape structure sequencing following chapters.Part II “Orientation” build Data Literacy skills working data insight. progression visualized Figure 1.3.3\nFigure 1.3: Data Insight Hierarchy (DIKI)\nDIKI Hierarchy highlights stages intermediate steps required derive insight data. Chapter 2 “Understanding data” cover Data Information covering conceptual topics populations versus samples language data samples converted information forms can take. Chapter 3 “Statistical approaches” discuss distinction descriptive analytic statistics. brief important data analysis, descriptive statistics serve sanity check dataset submitting interrogation –goal analytic statistics. also cover main distinctions analytics approaches including inference-, exploration-, prediction-based methods. fundamental understanding data, information, knowledge move Chapter 4 “Framing research” discuss develop research plan, call ‘research blueprint.’ point directly address Research Skills elaborate research really comes together; bring speed literature topic, develop research goal hypothesis, select data viable address research goal hypothesis, determine necessary information appropriate measures prepare analysis, perform diagnostic statistics data make adjustments analysis, select perform relevant analytic statistics given research goals, report findings, finally, structure project well-documented reproducible.Part III “Preparation” Part IV “Modeling” serve practical detailed guides R programming strategies conduct text analysis research develop Programming Skills. Chapter 5 “Acquire data” discuss three main strategies accessing data: direct downloads, Automatic Programming Interfaces (APIs), web scraping. Chapter 6 “Curate data” outline process converting augmenting acquired data structured format, therefore creating information. include organizing linguistic non-linguistic metadata one dataset. Chapter 7 “Transform data” describe work curated dataset derive detailed information appropriate dataset structures appropriate upcoming analysis.Chapters 8 “Exploration,” 9 “Inference,” 10 “Prediction” focus different categories statistical analysis associated distinct research goals. Exploration covers ‘bottom-’-style, ‘unsupervised learning,’ analysis methods association measures, clustering, topic modeling, vector-space models. methods aligned research goals aim interpret patterns arise data . Inference deals analysis methods associated standard hypothesis-testing. include common statistical models employed text analysis: chi-squared, logistic regression, linear regression. Prediction explores set statistical methods known ‘supervised learning.’ Similar unsupervised learning, prediction models employed bottom-fashion. However, key distinction dataset includes organizing ‘class’ variable statistical methods aim model order formulate generalization can correctly classify new textual data. cover standard methods text classification including Näive Bayes, k-nearest neighbors (k-NN), decisions tree random forest models.","code":""},{"path":"data-language-and-text-analysis.html","id":"summary","chapter":"1 Data, language, and text analysis","heading":"1.6 Summary","text":"chapter started general observations difficulty making sense complex world. standard approach overcoming inherent human limitations sense making science. 21st century toolbelt scientific research exploration grown terms amount data available, statistical methods analyzing data, computational power manage, store, share data, methods, results quantitative research. methods tools deriving insight data made significant inroads outside academia, increasingly figure quantitative investigation language. Text analysis particular branch enterprise based observational data real-world language used wide variety fields. coursebook aims develop knowledge skills three fundamental areas: Data Literacy, Research Skills, Programming Skills.end hope enjoy exploration text analysis. Although learning curve times may seem steep –experience gain improve data literacy, research skills, programmings skills also enhance appreciation richness human language important role everyday lives.","code":""},{"path":"orientation-overview.html","id":"orientation-overview","chapter":"Overview","heading":"Overview","text":"ORIENTATIONBefore begin working specifics data project, important establish fundamental understanding characteristics levels DIKI Hierarchy (Figure 1.3) roles levels deriving insight data. Chapter 2 explore Data Information levels drawing distinction two main types data (populations samples) cover data structured transformed generate information (datasets) fit statistical analysis. Chapter 3 outline importance distinct types statistical procedures (descriptive analytic) commonly used text analysis. Chapter 4 aims tie concepts together cover required steps preparing research blueprint conduct original text analysis project.","code":""},{"path":"understanding-data.html","id":"understanding-data","chapter":"2 Understanding data","heading":"2 Understanding data","text":"\nDRAFT\nplural anecdote data.\n―– Marc Bekoff\nessential questions chapter :\n\ndistinct types data differ?\n\ninformation form take?\n\nimportance documentation quantitative research?\nchapter cover starting concepts journey understand derive insight data, illustrated DIKI Hierarchy (Figure 1.3), focusing specifically first two levels: Data Information. see commonly referred ‘data’ everyday uses broken three distinct categories, two referred data third known information. also cover importance documentation data datasets quantitative research.","code":""},{"path":"understanding-data.html","id":"data","chapter":"2 Understanding data","heading":"2.1 Data","text":"Data data, right? term ‘data’ common popular vernacular easy assume know mean say ‘data.’ things, common assumptions important details require careful consideration. Let’s turn first key distinction need make start break term ‘data’: difference populations samples.","code":""},{"path":"understanding-data.html","id":"populations","chapter":"2 Understanding data","heading":"2.1.1 Populations","text":"first thing comes many people’s mind term population used human populations.  Say example –’s population Milwuakee? speak population terms talking total sum people living within geographical boundaries Milwaukee. concrete terms, population objective make idealized set objects events reality (cite). Key terms objective idealized. Although can look US Census report Milwaukee retrieve figure population, truly population. ? Well, whatever method used derive numerical figure surely incomplete. incomplete, time someone recorded figure number residents Milwaukee moved , moved , born, passed away –figure longer true population.Likewise talk populations terms language dealing objective idealized aspect reality. Let’s take words English language analog previous example population. case words people English bounding characteristic. Just people, words move , move , born, pass away. compendium words English moment almost instananeously incomplete. true populations, save bounding characteristics select narrow slice reality objectively measurable whose membership fixed (complete works Shakespeare, example).sum, () populations amorphous moving targets. objectively hold exist, practical terms often nail specifics populations. researchers go studying populations theoretically impossible access directly? strategy employed called sampling.","code":""},{"path":"understanding-data.html","id":"sampling","chapter":"2 Understanding data","heading":"2.1.2 Sampling","text":"sample product subjective process selecting finite set observations objective population goal capturing relevant characteristics target population. Although strategies minimize mismatch characteristics subjective sample objective population, important note almost certainly true given sample diverges population aims represent degree. aim, however, employ series sampling decisions, collectively known sampling frame, maximize chance representing population.common sampling strategies? First sample size. larger sample always representative smaller sample. Sample size, however, enough. hard imagine large sample chance captures subset features population. next step enhance sample representativeness apply random sampling. Together large random sample even better chance reflecting main characteristics population better large random sample. , random random , still run risk acquiring skewed sample (.e sample mirror target population).help mitigate issues, two strategies can applied improve sample representativeness. Note, however, size random samples can applied sample little information internal characteristics population, next two strategies require decisions depend presumed internal characteristics population. first informed sampling strategies called stratified sampling. Stratified samples make (educated) assumptions sub-components within population interest. sub-populations mind, large random samples acquired sub-population, strata. minimum, stratified samples can less representative random sampling alone, chances sample better increases. Can problems approach? Yes, two fronts. First knowledge internal components population often based limited incomplete knowledge population. words, strata selected subjectively researchers using various heuristics based sense ‘common knowledge.’ second front stratified sampling can err concerns relative sizes sub-components relative whole population. Even relevant sub-components identified, relative size adds another challenge researchers must face order maximize representativeness sample. attempt align, balance, relative sizes samples strata second population-informed sampling strategy.key feature sample purposely selected. Samples simply collection set data population. Samples rigorously selected explicit target population mind. text analysis purposely sampled collection texts, type defined , known corpus. reason set texts documents selected along purposely selected sampling frame corpus. sampling frame, therefore populations modeled, given corpus likely vary reason safe assumption given corpus equally applicable every research question. Corpus development (.e. sampling) purposeful, characteristics corpus development process made explicit documentation. Therefore vetting corpus sample applicability research goal key step research must take ensure integrity research findings.\nFigure 2.1: Brown Corpus Written American English\n","code":""},{"path":"understanding-data.html","id":"corpora","chapter":"2 Understanding data","heading":"2.1.3 Corpora","text":"","code":""},{"path":"understanding-data.html","id":"types","chapter":"2 Understanding data","heading":"2.1.3.1 Types","text":"notion sampling frames mind, corpora compiled aim general purpose (general reference corpora), much specialized sampling frames (specialized corpora). example, American National Corpus (ANC) British National Corpus (BNC) corpora aim model (represent/ reflect) general characteristics English language, former American English later British English. ambitious projects, require significant investments time corpus design implementation (continued development) usually undertaken research teams (Ädel, 2020).Specialized corpora aim represent specific populations. Santa Barbara Corpus Spoken American English (SBCSAE), can imagine name resource, aims model spoken American English. claim written English included. even specific types corpora attempt model types sub-populations scientific writing, computer-mediated communication (CMC), language use specific regions world, country, etc.Another set specialized corpora resources aim compile texts different languages different language varieties direct indirect comparison. Corpora directly comparable, include source translated texts, called parallel corpora. Parallel corpora include different languages language varieties indexed aligned linguistic level (.e. word, phrase, sentence, paragraph, document) OPUS example. Corpora compiled different languages language varieties directly aligned called comparable corpora. comparable language language varieties sampled similar sampling frame Brown LOB example.aim quantitative text researcher select corpus corpora (plural corpus) best aligns purpose research. Therefore general corpus ANC may better suited address question dealing way American English works, general resource may lack detail certain areas, medical language, may vital research project aimed understanding changes medical terminology.","code":""},{"path":"understanding-data.html","id":"sources","chapter":"2 Understanding data","heading":"2.1.3.2 Sources","text":"common source data used contemporary quantitative research internet. web investigator can access corpora published research purposes language used natural settings can coerced investigator corpus. Many organizations exist around globe provide access corpora browsable catalogs, repositories. repositories dedicated language research, general, Language Data Consortium specific language domains, language acquisition repository TalkBank. always advisable start looking available language data repository. advantage beginning data search repositories repository, especially geared towards linguistic community, make identifying language corpora faster general web search. Furthermore, repositories often require certain standards corpus format documentation publication. standardized resource many times easier interpret evaluate appropriateness particular research project.table ’ve compiled list corpus repositories help get started.Table 2.1: list corpus repositoriesRepositories means source corpora web. Researchers around world provide access corpora data sources sites data sharing platforms. Corpora various sizes scopes often accessible dedicated homepage appear homepage sponsoring institution. Finding resources matter web search word ‘corpus’ list desired attributes, including language, modality, register, etc. part general movement towards reproducibility corpora available web ever . Therefore data sharing platforms supporting reproducible research, GitHub, Zenodo, Re3data, OSF, etc., good place look well, searching repositories targeted web searches yield results.table find list corpus resources datasets.Table 2.2: Corpora language datasets.corpus search ends dead-end, either suitable resource appear exist existing resource unattainable given licensing restrictions fees, may time compile corpus. Turning machine readable texts internet usually logical first step access language new corpus. Language texts may found sites uploaded files, pdf doc (Word) documents, found displayed primary text site. Given wide variety documents uploaded language behavior recorded daily social media, news sites, blogs like, compiling corpus never easier. said , data structured much data needs retrieved can pose practical obstacles collecting data web, particularly approach acquire data hand instead automating task. approach , however, automate process much possible whether means leveraging R package interfaces language data, converting hundreds pdf documents plain text, scraping content web documents.table lists R packages serve interface language data directly R.Table 2.3: R Package interfaces language corpora datasets.Data language research limited (primary) text sources. sources may include processed data previous research; word lists, linguistic features, etc.. Alone combination text sources data can rich viable source data research project.’ve included processed language resources.Table 2.4: Language data previous research meta-studies.list data available language research constantly growing. ’ve document wide variety resources. ’ve included attempts others provide summary corpus data language resources available.Table 2.5: Lists corpus resources.\ncan work real simplified research questions students consider set corpus resources likely better resource.\n","code":""},{"path":"understanding-data.html","id":"formats","chapter":"2 Understanding data","heading":"2.1.3.3 Formats","text":"corpus often include various types non-linguistic attributes, meta-data, well. Ideally include information regarding source(s) data, dates acquired published, author speaker information. may also include number attributes identified potentially important order appropriately document target population. , key match available meta-data goals research. cases corpus may ideal aspects contain key information address research question. may mean need compile corpus fundamental attributes missing. consider compiling corpus, however, worth investigating possibility augmenting available corpus bring inline particular goals. may include adding new language sources, harnessing software linguistic annotation (part--speech, syntactic structure, named entities, etc.), linking available corpus meta-data resources, linguistic non-linguistic.Corpora come various formats, main three : running text, structured documents, databases. format corpus often influenced characteristics data may also reflect author’s individual preferences well. typical corpora meta-data characteristics take form running text.Running text sample Europarle Parallel Corpus.corpora meta-data, header may appended top running text document meta-data may contained separate file appropriate coding coordinate meta-data attributes text corpus.Meta-data header sample Switchboard Dialog Act Corpus.meta-data / linguistic annotation increases complexity common structure corpus document explicitly markup language XML (Extensible Markup Language) organize relationships language meta-data attributes database.XML format meta-data (linguistic annotation) Brown Corpus.Although push towards standardization corpus formats, available resources display degree idiosyncrasy. able parse structure corpus skill develop time. experience working corpora become adept identifying data stored whether content format serve needs analysis.","code":"> Resumption of the session\n> I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n> Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n> You have requested a debate on this subject in the course of the next few days, during this part-session.\n> In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n> Please rise, then, for this minute' s silence.\n> (The House rose and observed a minute' s silence)\n> Madam President, on a point of order.\n> You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n> One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.> FILENAME: 4325_1632_1519\n> TOPIC#:       323\n> DATE:     920323\n> TRANSCRIBER:  glp\n> UTT_CODER:    tc\n> DIFFICULTY:   1\n> TOPICALITY:   3\n> NATURALNESS:  2\n> ECHO_FROM_B:  1\n> ECHO_FROM_A:  4\n> STATIC_ON_A:  1\n> STATIC_ON_B:  1\n> BACKGROUND_A: 1\n> BACKGROUND_B: 2\n> REMARKS:        None.\n> \n> =========================================================================\n> \n> \n> o          A.1 utt1: Okay.  /\n> qw          A.1 utt2: {D So, }\n> \n> qy^d          B.2 utt1: [ [ I guess, +\n> \n> +          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\n> \n> +          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\n> \n> qy          A.5 utt1: Does it say something? /\n> \n> sd          B.6 utt1: I think it usually does.  /\n> ad          B.6 utt2: You might try, {F uh, }  /\n> h          B.6 utt3: I don't know,  /\n> ad          B.6 utt4: hold it down a little longer,  /\n> ad          B.6 utt5: {C and } see if it, {F uh, } -/> <TEI xmlns=\"http://www.tei-c.org/ns/1.0\"><teiHeader><fileDesc><titleStmt><title>Sample A01 from  The Atlanta Constitution<\/title><title type=\"sub\"> November 4, 1961, p.1 \"Atlanta Primary ...\"\n>  \"Hartsfield Files\"\n>  August 17, 1961, \"Urged strongly ...\"\n>  \"Sam Caldwell Joins\"\n>  March 6,1961, p.1 \"Legislators Are Moving\" by Reg Murphy\n>  \"Legislator to fight\" by Richard Ashworth\n>  \"House Due Bid...\"\n>  p.18 \"Harry Miller Wins...\"\n> <\/title><\/titleStmt><editionStmt><edition>A part  of the XML version of the Brown Corpus<\/edition><\/editionStmt><extent>1,988 words 431 (21.7%) quotes 2 symbols<\/extent><publicationStmt><idno>A01<\/idno><availability><p>Used by permission of The Atlanta ConstitutionState News Service (H), and Reg Murphy (E).<\/p><\/availability><\/publicationStmt><sourceDesc><bibl> The Atlanta Constitution<\/bibl><\/sourceDesc><\/fileDesc><encodingDesc><p>Arbitrary Hyphen: multi-million [0520]<\/p><\/encodingDesc><revisionDesc><change when=\"2008-04-27\">Header auto-generated for TEI version<\/change><\/revisionDesc><\/teiHeader>\n> <text xml:id=\"A01\" decls=\"A\">\n> <body><p><s n=\"1\"><w type=\"AT\">The<\/w> <w type=\"NP\" subtype=\"TL\">Fulton<\/w> <w type=\"NN\" subtype=\"TL\">County<\/w> <w type=\"JJ\" subtype=\"TL\">Grand<\/w> <w type=\"NN\" subtype=\"TL\">Jury<\/w> <w type=\"VBD\">said<\/w> <w type=\"NR\">Friday<\/w> <w type=\"AT\">an<\/w> <w type=\"NN\">investigation<\/w> <w type=\"IN\">of<\/w> <w type=\"NPg\">Atlanta's<\/w> <w type=\"JJ\">recent<\/w> <w type=\"NN\">primary<\/w> <w type=\"NN\">election<\/w> <w type=\"VBD\">produced<\/w> <c type=\"pct\">``<\/c> <w type=\"AT\">no<\/w> <w type=\"NN\">evidence<\/w> <c type=\"pct\">''<\/c> <w type=\"CS\">that<\/w> <w type=\"DTI\">any<\/w> <w type=\"NNS\">irregularities<\/w> <w type=\"VBD\">took<\/w> <w type=\"NN\">place<\/w> <c type=\"pct\">.<\/c> <\/s>\n> <\/p>"},{"path":"understanding-data.html","id":"information","chapter":"2 Understanding data","heading":"2.2 Information","text":"Identifying adequate corpus resource target research question first step moving quantitative text research project forward. next step select components characteristics resource relevant research move organize attributes data useful informative format. process converting corpus dataset –tabular representation information leveraged analysis.","code":""},{"path":"understanding-data.html","id":"structure","chapter":"2 Understanding data","heading":"2.2.1 Structure","text":"Data alone informative. explicit organization data way makes relationships accessible data become information. particularly salient hurdle text analysis research. textual data unstructured –, relationships used analysis yet explicitly drawn organized text make relationships meaningful useful analysis.running text Europarle Corpus, know files source text (original) files correspond target text (translation). Table 2.6 see text organized columns corresponding type sentence additional sentence_id column keep index sentences aligned.\nconventional work column names datasets R using conventions used naming objects. matter taste convention used, adopted snake case personal preference. also alternatives. Regardless convention choose, good practice consistent.\n\nalso note column names balanced meaningfulness brevity. brevity practical concern can somewhat opaque. questions meaning column values consult resource’s documentation.\nTable 2.6: First 10 source target sentences Europarle Corpus.corpus resources semi-structured –, characteristics structured, .Switchboard Dialog Act Corpus example semi-structured resource. meta-data associated 1,155 conversations corpus. Table 2.7 language-relevant sub-set meta-data associated utterance.Table 2.7: First 5 utterances Switchboard Dialog Act Corpus.Relatively fewer resources structured. cases high amount meta-data / linguistic annotation included corpus. format convention, however, varies resource resource. formats programming general (.csv, .xml, .json, etc.) others resource specific (.cha, .utt, .prd, etc.). Table 2.8 XML version Brown Corpus represented tabular format. Note along meta-data variables, also contains variable linguistic annotation grammatical category (pos part--speech) word.Table 2.8: First 10 words Brown Corpus.coursebook, selection attributes corpus juxtaposition attributes relational format, dataset, converts data information referred data curation. process data curation minimally involves creating base dataset, derived dataset, establishes main informational associations according philosophical approach outlined Wickham (2014). work, ‘tidy’ dataset refers structural (physical) informational (semantic) organization dataset. Physically, tidy dataset tabular data structure row observation column variable contains measures feature attribute observation. cell given row-column intersect contains value particular attribute particular observation particular observation-feature pair also known data point.Consider adding visual highlight relationshipsSemantic value tidy dataset derived association physical structure along two dimensions rectangular format. First, column variable reflects measures particular attribute. Europarle Corpus dataset, Table 2.6, example, type column measures type text, either Source Target. Columns can contain measures qualitative quantitative, character-based numeric. Second, row observation contains variables associated primary unit observation. primary unit observation variable essential focus informational structure. dataset first observation contains type, sentence_id, sentence. dataset currently structured primary unit investigation sentence variables measures characterize value sentence.decision primary unit observation fundamentally guided research question, therefore highly specific particular research project. Say instead wanted focus words instead sentences. dataset need transformed new variable (words) created contain word corpus.Table 2.9: Europarle Paralle Corpus words primary unit investigation.values variables type sentence_id maintain necessary description word ensure required semantic relationships identify particular attributes word observation. dataset may seem redundant values type sentence_id repeated numerous times ‘redundancy’ makes relationship variable associated primary unit investigation explicit. format makes tidy dataset versatile format researchers conduct analyses powerful flexible way, see throughout coursebook.important make clear data tabular format constitute dataset, tidy sense using. Data can organized many ways make relationships variables observations explicit.Consider adding ‘messy’ data / summary tables reflect relational structure aiming create base research .\nNote cases may convert tidy tabular dataset data formats may required particular statistic approaches times relationship variables maintained line research purpose. touch examples types data formats dive particular statistical approaches require later series (.e. Corpus Document-Term Matrix (DTM) objects R).\n","code":""},{"path":"understanding-data.html","id":"transformation","chapter":"2 Understanding data","heading":"2.2.2 Transformation","text":"point introduced first step data curation original data converted relational dataset (derived dataset) highlighted importance informational structure setting stage data analysis. However, primary derived dataset often final organizational step proceeding statistical analysis. Many times, always, derived dataset requires manipulation transformation prepare dataset specific analysis approach taken. another level human intervention informational organization, therefore another step forward journey data insight step DIKI hierarchy. Common types transformations include cleaning variables (normalization), separating eliminating variables (recoding), creating new variables (generation), incorporating others datasets integrate existing variables (merging). results transformations build manipulate derived dataset produce analysis dataset. Let’s now turn provide select set examples transformations using datasets introduced chapter.","code":""},{"path":"understanding-data.html","id":"normalization","chapter":"2 Understanding data","heading":"2.2.2.1 Normalization","text":"process normalization aims sanitize values within variable set variables. may include removing whitespace, punctuation, numerals, special characters substituting uppercase lowercase characters, numerals word versions, acronyms full forms, irregular incorrect spelling accepted forms, removing common words (stopwords), etc.inspecting Europarle dataset (Table 2.6) see sentence lines represent actual parliment speeches. Table 2.10 see lines.Table 2.10: Non-speech lines Europarle dataset.research project aiming analyze speech want normalize dataset removing lines, seen Table 2.11.Table 2.11: Europarle dataset non-speech lines removed.Another feature dataset may require attention fact English lines include whitespace possessive nouns.Table 2.12: Lines possessives extra whitespace Europarle dataset.may affect another transformation process subsequent analysis, may good idea normalize forms removing extra whitespace.Table 2.13: Europarle dataset whitespace possessives removed.final normalization case scenario involves changing converting text lowercase. goal research count words point fact word starts sentence convention first letter capitalized result distinct counts words essence (.e. “” vs. “”).Table 2.14: Europarle dataset lowercasing applied.Note lowercasing text, normalization steps general, can come cost. example, lowercasing Europarle dataset sentences means lose potentially valuable information; namely ability identify proper names (.e. “Mr Kumar Ponnambalam”) titles (.e. “European Parliament”) directly orthographic forms. , however, transformation steps can applied aim recover ‘lost’ information situations others.","code":""},{"path":"understanding-data.html","id":"recoding","chapter":"2 Understanding data","heading":"2.2.2.2 Recoding","text":"process recoding aims recast values variable set variables new variable set variables enable direct access. may include extracting values variable, stemming lemmatization words, tokenization linguistic forms (words, ngrams, sentences, etc.), calculating lengths linguistic units, removing variables used analysis, etc.Words intuitively associate ‘base’ word can take many forms language use. example word forms ‘investigation,’ ‘investigation,’ ‘investigate,’ ‘investigated,’ etc. intuitively linked. two common methods can applied create new variable facilitate identification associations. first stemming. Stemming rule-based heuristic reduce word forms stem root form.Table 2.15: Results stemming first words Brown Corpus.things note . First number stemming algorithms individual languages distinct languages.4 Second words can stemmed derivative forms (.e. “,” “,” etc.). generally related distinction closed-class (articles, prepositions, conjunctions, etc.) open-class (nouns, verbs, adjectives, etc.) grammatical categories. Third stem generated words can stemmed result forms words . Nonetheless, stems can useful easily extracting set related word forms.example, let’s identify word forms stem ‘investig.’Table 2.16: Results word_stems filter “investig” Brown Corpus.can see results Table 2.16 searching word_stems match ‘investig’ returns set stem-related forms. worth noting forms cut across number grammatical categories. instead want draw distinction grammatical categories, can apply lemmatization. process distinct stemming two important ways: (1) derivative forms grouped grammatical category (2) resulting forms lemmas ‘base’ forms words.Table 2.17: Results lemmatization first words Brown Corpus.appreciate difference stemming lemmatization, let’s compare filter word_lemmas match ‘investigation.’Table 2.18: Results word_lemmas filter “investigation” Brown Corpus.lemma forms ‘investigate’ nouns appear. Let’s run similar search lemma ‘.’Table 2.19: Results word_lemmas filter “” Brown Corpus.words grammatical category returned. case verb ‘’ many derivative forms ‘investigate.’Another form recoding detect pattern values existing variable create new variable whose values extracted pattern register pattern occurs / many times occurs. example, let’s count number disfluencies (‘uh’ ‘um’) occur utterance utterance_text Switchboard Dialog Act Corpus. Note ’ve simplified dataset dropping non-relevant variables example.Table 2.20: Disfluency counts first 10 utterance_text values Switchboard Corpus.One common forms recoding text analysis tokenization. Tokenization process recasting text smaller linguistic units. working text linguistically annotated, feasible linguistic tokens words, ngrams, sentences. word sentence tokens easily understandable, ngram tokens need explanation. ngram sequence either characters words n length sequence. ngram sequences drawn incrementally, bigrams (two-word sequences) sentence “input sentence.” :, , input, input sentenceWe’ve already seen word tokenization exemplified Europarle Corpus subsection Structure Table 2.9, let’s create (word) bigram tokens corpus.Table 2.21: first 10 word bigrams Europarle Corpus.just mentioned, ngrams sequences can formed characters well. character trigram (three-character) sequences.Table 2.22: first 10 character trigrams Europarle Corpus.","code":""},{"path":"understanding-data.html","id":"generation","chapter":"2 Understanding data","heading":"2.2.2.3 Generation","text":"process generation aims augment variable set variables. essence aims make implicit attributes explicit directly accessible. often targeted automatic generation linguistic annotations grammatical category (part--speech) syntactic structure.examples ’ve added linguistic annotation target (English) source (Spanish) example sentence Europarle Parallel Corpus. First, note variables added dataset correspond grammatical category. addition type sentence_id assortment variables replace sentence variable. part process annotation input text annotated sentence tokenized token indexed token_id. upos contains Universal Part Speech tags5, detailed list features included feats. syntactic annotation reflected token_id_source syntactic_relation variables. variables correspond type syntactic parsing done, case Dependency Parsing (using Universal Dependencies framework). Another common syntactic parsing framework phrase constituency parsing (Jurafsky & Martin, 2020).Table 2.23: Automatic linguistic annotation grammatical category syntactic structure example English sentence Europarle CorpusNow compare English example sentence dataset Table 2.23 parallel sentence Spanish. Note grammatical features language specific. example, Spanish gender apparent scanning feats variable.Table 2.24: Automatic linguistic annotation grammatical category syntactic structure example Spanish sentence Europarle CorpusThere much explore linguistic annotation, syntactic parsing particular, point suffice note possible augment dataset grammatical information automatically.strengths shortcomings automatic linguistic annotation research aware . First, automatic linguistic annotation provides quick access rich highly reliable linguistic information large number languages. However, part speech taggers syntactic parsers magic. resources built training computational algorithm recognize patterns manually annotated datasets producing language model. model used predict linguistic annotations new language (just previous examples). shortcomings automatic linguistic annotation first, languages trained language models second, data used train model inevitably reflect particular variety, register, modality, etc. accuracy linguistic annotation highly dependent alignment language sampling frame trained data language data automatically annotated. Many () language models available automatic linguistic annotation based language readily available languages traditionally newswire text. important aware characteristics using linguistic annotation tools.Consider adding ‘creating measures’ ","code":""},{"path":"understanding-data.html","id":"merging","chapter":"2 Understanding data","heading":"2.2.2.4 Merging","text":"process merging aims join variable set variables another variable set variables another dataset. option merge two () datasets requires shared variable indexes aligns datasets.provide example let’s look Switchboard Diaglog Act Corpus. existing, disfluency recoded, version includes following variables.turns corpus website number meta-data files available, including files pertaining speakers topics conversations.speaker meta-data corpus caller_tab.csv file contains speaker_id variable corresponds speaker corpus potentially relevant variables language research project including sex, birth_year, dialect_area, education.Table 2.25: Speaker meta-data Switchboard Dialog Act Corpus.Since datasets contain shared index, speaker_id can merge two datasets. result found Table 2.26.Table 2.26: Merged conversations speaker meta-data Switchboard Dialog Act Corpus.example case dataset merged already structured format (.csv). Many corpus resources contain meta-data stand-files structured.cases researcher like merge information already accompany corpus resource. possible long dataset can created contains variable shared. Without shared variable index datasets merge take place.sum, transformation steps described collectively aim produce higher quality datasets relevant content structure submit analysis. process may include one previous transformations rarely linear often iterative. typical normalization generation, recoding, return normalizing, forth. process highly idiosyncratic given characteristics derived dataset ultimate goals analysis dataset.","code":"#> Rows: 5\n#> Columns: 11\n#> $ doc_id           <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\"\n#> $ speaker_id       <dbl> 1632, 1632, 1519, 1632, 1519\n#> $ topic_num        <dbl> 323, 323, 323, 323, 323\n#> $ topicality       <chr> \"3\", \"3\", \"3\", \"3\", \"3\"\n#> $ naturalness      <chr> \"2\", \"2\", \"2\", \"2\", \"2\"\n#> $ damsl_tag        <chr> \"o\", \"qw\", \"qy^d\", \"+\", \"+\"\n#> $ speaker          <chr> \"A\", \"A\", \"B\", \"A\", \"B\"\n#> $ turn_num         <chr> \"1\", \"1\", \"2\", \"3\", \"4\"\n#> $ utterance_num    <chr> \"1\", \"2\", \"1\", \"1\", \"1\"\n#> $ utterance_text   <chr> \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind …\n#> $ disfluency_count <int> 0, 0, 0, 0, 1"},{"path":"understanding-data.html","id":"documentation","chapter":"2 Understanding data","heading":"2.3 Documentation","text":"seen chapter acquiring data converting data information involves number conscious decisions implementation steps. favor researchers research community, crucial document decisions steps. makes possible retrace steps also provides guide future researchers want reproduce / build research. programmatic approach quantitative research helps ensure implementation steps documented reproducible also vital decisions made documented well. includes creation/ selection corpus data, description variables chosen corpus derived dataset, description variables created derived dataset analysis dataset.Consider adding specifics characteristics formats documenting corpus data datasets; data dictionaries –examples R packages spreadsheets, Rmarkdown files","code":""},{"path":"understanding-data.html","id":"summary-1","chapter":"2 Understanding data","heading":"2.4 Summary","text":"chapter focused data information –first two components DIKI Hierarchy. process visualized Figure 2.2.\nFigure 2.2: Understanding data: visual summary\nFirst distinction made populations samples, latter intentional subjective selection observations world attempt represent population interest. result process known corpus. Whether developing corpus selecting existing corpus important vet sampling frame applicability viability resource given research project.viable corpus identified, corpus converted derived dataset adopts ‘tidy’ dataset format column variable, row observation, intersection columns rows contain values. derived dataset serves establish base informational relationships research stem.derived dataset likely require transformations including normalization, recoding, generation, / merging enhance usefulness information analysis. analysis dataset result process.Although covered end chapter, documentation implemented stage process. Employing programmatic approach establishes documentation implementation steps motivation behind decisions taken content corpus data datasets generated also need documentation ensure transparent reproducible research.","code":""},{"path":"approaching-analysis.html","id":"approaching-analysis","chapter":"3 Approaching analysis","heading":"3 Approaching analysis","text":"\nINCOMPLETE DRAFT\nLies, damn lies, statistics\n—Benjamin Disraeli, popularized Mark Twain\nessential questions chapter :\n\nrole statistics data analysis?\n\nimportance descriptive statistics data analysis?\n\nmain goals three statistical approaches data analysis?\nchapter build notions data information previous chapter. aim statistics quantitative analysis uncover patterns datasets. Thus statistics aimed deriving knowledge information, next step DIKI Hierarchy (Figure 2.2). creation information data involves human intervention conscious decisions, seen, deriving knowledge information involves even conscious subjective decisions assess interrogate information available ultimately interpret findings. first step conduct descriptive assessment information, individual variable level also variables, second interrogate dataset either exploratory, inferential, predictive methods, third interpret report findings.","code":""},{"path":"approaching-analysis.html","id":"descriptive-statistics","chapter":"3 Approaching analysis","heading":"3.1 Descriptive statistics","text":"Descriptive statistics include set diagnostic measures tabular visual summaries provide researchers better understanding structure dataset, prepare researcher make decisions statistical methods / tests appropriate, safeguard false assumptions (missing data, data distributions, etc.). section first cover importance understanding informational value variables can represent move use understanding approach summarizing individual variables relationships variables.ground discussion introduce new dataset. dataset drawn Barcelona English Language Corpus (BELC), found TalkBank repository. ’ve selected “Written composition” task corpus contains writing samples second language learners English different ages. Participants given task writing 15 minutes topic “: past, present future.” Data collected many () participants four times course seven years. Table 3.1 ’ve included first 10 observations dataset reflects structural transformational steps ’ve done start tidy dataset.\nTable 3.1: First 10 observations BELC dataset demonstration.\nentire dataset includes 79 observations 36 participants. observation BELC dataset corresponds individual learner’s composition. includes participant wrote composition (participant_id), age group part time (age_group), sex (sex), number English words produced (num_tokens), number unique English words produced (num_types). final variable (ttr) calculated ratio number unique words (num_types) total words (num_tokens) composition. known Type-Token Ratio standard metric measuring lexical diversity.","code":""},{"path":"approaching-analysis.html","id":"information-values","chapter":"3 Approaching analysis","heading":"3.1.1 Information values","text":"Understanding informational value, level measurement, variable set variables key preparing analysis implications visualization techniques statistical measures can use interrogate dataset. two main levels measurement variable can take: categorical continuous. Categorical variables reflect class group values. Continuous variables reflect values measured along continuum.BELC dataset contains three categorical variables (participant_id, age_group, sex) three continuous variables (num_tokens, num_types, ttr). categorical variables identify class group membership; participant wrote composition, age group , biological sex. continuous variables measure attributes can take range values without fixed limit differences value regular. number words number unique words composition can range 1 \\(n\\) Type-Token Ratio derived two variables also continuous reason. Furthermore, differences values measures defined interval, example composition word count (num_tokens) 40 exactly two times large composition word count 20.distinction categorical continuous levels measurement, mentioned , main two statistical approaches distinction needs made conduct analysis. However, categorical continuous can broken subcategories descriptive analytic purposes distinctions important. categorical variables distinction can made variables structured relationship values . Nominal variables contain values labels denoting membership class relationship labels. Ordinal variables also contain labels classes, contrast nominal variables, relationship classes, namely one precedence relationship order. mind, categorical variables sub-classified. order values participant_id sex therefore nominal whereas values age_group ordered, value refers sequential age group, therefore ordinal.Turning continuous variables, another subdivision can made hinges existence non-arbitrary zero . Interval variables contain values difference values regular defined, measure arbitrary zero value. typically cited example interval variable temperature measurements Fahrenheit scale. value 0 scale mean 0 temperature. Ratio variables properties interval variables also include non-arbitrary definition zero. continuous variables BELC dataset (num_tokens, num_types, ttr) ratio variables value 0 indicate lack attribute.hierarchical overview relationship two main four sub-types levels measurement appear Figure 3.1.\nFigure 3.1: Levels measurement graphic representation.\nnotes practical importance; First, distinction interval ratio variables often applicable text analysis therefore often treated together continuous variables. Second, distinction ordinal interval/continuous variables clear cut may seem. variables contain values ordered relationship. definition values ordinal variable reflect regular intervals units measurement. practice interval/continuous variables defined number values (say Likert scale used survey) may treated ordinal variable may better understood reflecting class membership. Third, continuous variables can converted categorical variables, reverse true. , example, define criterion binning word counts num_tokens composition ordered classes “low,” “mid,” “high.” hand, sex (measured ) take intermediate values unfixed range. upshot variables can -typed -typed. cases preferred treat continuous variables , nature variable permits , -typing continuous data categorical data results loss information –result loss information hence statistical power may lead results obscure meaningful patterns data (Baayen, 2004).","code":""},{"path":"approaching-analysis.html","id":"summaries","chapter":"3 Approaching analysis","heading":"3.1.2 Summaries","text":"always key gain insight shape information numeric, tabular / visual summaries jumping analytic statistical approaches. appropriate form summarizing information depend number informational value(s) target variables. get sense looks, let’s continue work BELC dataset pose different questions data eye towards seeing various combinations variables descriptively explored.","code":""},{"path":"approaching-analysis.html","id":"single-variables","chapter":"3 Approaching analysis","heading":"3.1.2.1 Single variables","text":"way statistically summarize variable single measure derive measure central tendency. continuous variable common measure (arithmetic) mean, avergage, simply sum values divided number values. measure central tendency, however, mean can less--reliable sensitive outliers say data points variable extreme relative overall distribution values variable affect value mean depending extreme deviate. One way assess effects outliers calculate measure dispersion. common standard deviation estimates average amount variability values continuous variable. Another way assess, rather side-step, outliers calculate another measure central tendency, median. median calculated sorting values variable selecting value falls middle values. median less sensitive outliers extreme values () indirectly affect selection middle value. Another measure dispersion calculate quantiles. quantile slices data four percentile ranges providing five value numeric summary spread values continuous variable. spread first third quantile known Interquartile Range (IQR) also used single statistic summarize variability values continuous variable.list central tendency dispersion scores continuous variables BELC dataset.Variable type: numeric\ndescriptive statistics returned generated skimr package.\nsummary, see mean, standard deviation (sd), quantiles (five-number summary, p0, p25, p50, p75, p100). middle quantile (p50) median IQR listed last.important measures assessing central tendency dispersion useful reporting purposes, get better feel variable distributed, nothing beats visual summary. boxplot graphically summarizes many metrics. Figure 3.2 see three continuous variables, now graphical form.\nFigure 3.2: Boxplots continuous variables BELC dataset.\nboxplot, bold line median. surrounding box around median interquantile range. extending lines IQR mark largest lowest value within 1.5 times either 3rd (top box) 1st (bottom box). values fall outside, , extending lines considered statistical outliers marked dots (case red dots).6Boxplots provide robust visually intuitive way assessing central tendency variability continuous variable type plot can complemented looking overall distribution values terms frequencies. histogram provides visualization frequency (density case blue overlay) values across continuous variable binned regular intervals.Figure 3.3 ’ve plotted histograms top row density plots bottom row three continuous variables BELC dataset.\nFigure 3.3: Histograms density plots continuous variables BELC dataset.\nHistograms provide insight distribution data. three continuous variables, distributions happen strikingly distinct. , however, either. explore continuous variables histograms often trying assess whether skew . three general types skew, visualized Figure 3.4 (Bobbitt, 2021).\nFigure 3.4: Examples skew types density plots.\nhistograms/ density plots distribution either left right, median mean aligned. mode, indicates frequent value variable also aligned two measures. left-skewed distribution mean left median left mode whereas right-skewed distribution opposite occurs. distribution absolutely skew three measures . practice measures rarely align perfectly typical three measures approximate alignment. common enough distribution called Normal Distribution7 common real-world data.Another potentially informative way inspect normality distribution create Quantile-Quantile plots (QQ Plot). Figure 3.5 ’ve created QQ plots three continuous variables. line plot normal distribution points fall line, less likely distribution normal.\nFigure 3.5: QQ Plots continuous variables BELC dataset.\nvisual inspection can often enough detect non-normality, cases visually approximate normal distribution () can perform Shapiro-Wilk test normality. inferential test compares variable’s distribution normal distribution. likelihood distribution differs normal distribution reflected \\(p\\)-value. \\(p\\)-value .05 threshold suggests distribution non-normal. Table 3.2 see given criterion distribution num_types normally distributed.\nTable 3.2: Results Shapiro-Wilk test normality continuous variables BELC dataset.\nDownstream analytic analysis, distribution continuous variables need taken account certain statistical tests. Tests assume ‘normality’ parametric tests, non-parametric. Distributions approximate normal distribution can sometimes transformed conform normal distribution either outlier trimming statistical procedures (.e. square root, log, inverse transformation), necessary. stage, however, important thing recognize whether distributions approximate wildly diverge normal distribution.leave continuous variables, let’s consider another approach visually summarizing single continuous variable. Empirical Cumulative Distribution Frequency, ECDF, summary cumulative proportion values continuous variable. ECDF plot can useful determining proportion values fall certain percentage data.Figure 3.6 see ECDF plots three continuous variables.\nFigure 3.6: ECDF plots continuous variables BELC dataset.\nTake, example, number tokens (num_tokens) per composition. ECDF plot tells us 50% values variable 56 words less. three variables plotted, cumulative growth quite steady. cases . , ECDF goes long way provide us glimpse key bends proportions values variable.Now let’s turn descriptive statistics categorical variables. categorical variables, central tendency can calculated well subset measures given reduced informational value categorical variables. nominal variables relationship levels central tendency simply mode. levels ordinal variables, however, relational therefore median, addition mode, can also used measure central tendency. Note variable one mode unimodal, two modes, bimmodal, variables two modes multimodal.\nget numeric value median ordinal variable levels variable need numeric well. Non-numeric levels can recoded numeric purpose necessary.\nlist central tendency metrics categorical variables BELC dataset.Variable type: factorIn practice categorical variable levels common simply summarize counts level table get overview variable. ordinal variables numerous levels, five-score summary (quantiles) can useful summarize distribution. contrast continuous variables graphical representation helpful get perspective shape distribution values, exploration single categorical variables rarely enhanced plots.","code":""},{"path":"approaching-analysis.html","id":"multiple-variables","chapter":"3 Approaching analysis","heading":"3.1.2.2 Multiple variables","text":"addition single variable summaries (univariate), useful understand two (bivariate) variables (multivariate) related add understanding shape relationships dataset. Just univariate summaries, informational values variables frame approach.explore relationship two continuous variables can statistically summarize relationship coefficient correlation measure effect size continuous variables. continuous variables approximate normal distribution Pearson’s r used, Kendall’s tau appropriate measure. correlation coefficient ranges -1 1 0 correlation -1 1 perfect correlation (either negative positive). Let’s assess correlation coefficient variables num_tokens ttr. Since variables normally distributed, use Kendall’s tau. Using measure correlation coefficient \\(-0.563\\) suggesting correlation, particularly strong one.Correlation measures important reporting really appreciate relationship best graphically represent variables scatterplot. Figure 3.7 see relationship num_tokens ttr.\nFigure 3.7: Scatterplot…\nplots ttr y-axis num_tokens x-axis. points correspond intersection variables single observation. left pane points represented. Visually (given correlation coefficient) can see negative relationship number tokens Type-Token ratio: words, tokens composition lower Type-Token Ratio. case trend quite apparent, cases may . provide additional visual cue trend line often added scatterplot. right pane ’ve added linear trend line. line demarcates optimal central tendency across relationship, assuming linear relationship. steeper line, slope, likely correlation strong. band, ribbon, around trend line indicates confidence interval means real central tendency fall anywhere within space. wider ribbon, larger variation observations. case see ribbon widens number tokens either low high. means trend line potentially drawn either steeper (strongly correlated) flatter (less strongly correlated).\nplots comparing two variables, choice variable plot x- y-axis contingent research question / statistical approach. language varies statistical approaches: inferential methods x-axis used plot known dependent variable y-axis independent variable. predictive methods dependent variable known outcome independent variable predictor. Exploratory methods draw distinctions variables along lines choice variable plot along x- y-axis often arbitrary.\nLet’s add another variable mix, case categorical variable sex, taking bivariate exploration multivariate exploration. point corresponds observation values num_tokens ttr intersect. now points given color reflects level sex associated .\nFigure 3.8: Scatterplot visualizing relationship num_tokens ttr.\nmultivariate case, scatterplot without trend line difficult interpret. trend lines levels sex help visually understand variation relationship num_tokensand ttr much better. important note multiple trend lines one slope evaluate. correlation coefficient can calculated level sex (.e. ‘male’ ‘female’) independently relationship slope can visually inspected provide important information regarding level’s relative distribution. trend lines parallel (ignoring ribbons moment), appears case, suggests relationship continuous variables stable across levels categorical variable, males showing lexical diversity females declining similar rate. lines cross, suggest cross point, potentially important difference levels categorical variable (known interaction). Now let’s consider meaning ribbons. Since ribbons reflect range real trend line fall, ribbons overlap, differences levels categorical variable likely distinct. descriptive level, visual summary suggest differences relationship num_tokens ttr distinct levels sex.Characterizing relationship two continuous variables, seen either performed correlation coefficient metric visually. approach summarizing bivariate relationship combines continuous categorical variable distinct. Since categorical variable definition class-oriented variable, descriptive analysis can include tabular representation, type summary statistic. example, consider relationship num_tokens age_group can calculate mean num_tokens level age_group. provide metric dispersion can include either standard error mean (SEM) / confidence interval (CI).Table 3.3 see summary statistics.\nTable 3.3: Summary table tokens age_group.\nSEM metric summarizes variation based number values CI, seen, summarizes potential range mean may fall given likelihood criterion (usually \\(p\\)-value, .05).assessing categorical variable combination continuous variable table available visual summary. said , graphic summary hard beat. following figure (3.9) barplot provided includes means num_tokens level age_group. overlaid bars represent confidence interval mean score.\nFigure 3.9: Barplot comparing mean num_tokens age_group BELC dataset.\nCI ranges overlap, just ribbons scatterplots, likelihood differences levels ‘real’ diminished.gauge effect size relationship can use Spearman’s rho rank-based coefficients. score 0.708 indicating relationship age_group num_tokens quite strong.8Now, want explore multivariate relationship add sex current descriptive summary, can create summary table, let’s jump straight barplot.\nFigure 3.10: Barplot comparing mean num_tokens age_group sex BELC dataset.\nsee Figure 3.10 whole, appears general trend towards tokens composition advanced learner levels. However, non-overlap CI bars ‘12-year-olds’ levels sex (‘male’ ‘female’) suggest 12-year-old females may produce tokens per composition males –potential divergence overall trend.Barplots familiar common visualization summaries continuous variables across levels categorical variables, boxplot another useful visualization type relationship.\nFigure 3.11: Boxplot relationship age_group num_tokens BELC dataset.\nseen summarizing single continuous variables, boxplots provide rich set information concerning distribution continuous variable. case can visually compare continuous variable num_tokens categorical variable age_group. plot right pane includes ‘notches.’ Notches represent confidence interval, boxplots interval surrounds median. compared horizontally across levels categorical variable overlap notched spaces suggest true median may within range.\nAdditionally, confidence interval goes outside interquantile range (box) notches hinge back either 1st (lower) 3rd (higher) IQR range suggests variability high.can also add third variable exploration. barplot Figure 3.10, boxplot Figure 3.12 suggests overall trend towards tokens per composition learner advances experience, except ‘12-year-old’ level appears difference ‘males’ ‘females.’\nFigure 3.12: Boxplot relationship age_group, num_tokens sex BELC dataset.\npoint exploration multiple variables always included least one continuous variable. central tendency continuous variables can summarized multiple ways (mean, median, mode) calculating means medians, measures dispersion also provide helpful information summarize variability. working categorical variables, however, measures central tendency dispersion limited. ordinal variables central tendency can summarized median mode dispersion can assessed interquantile range. nominal variables mode measure central tendency dispersion applicable. reason relationships categorical variables typically summarized using contingency tables provide cross-variable counts level target categorical variables.Let’s explore relationship categorical variables sex age_group. Table 3.4 see contingency table summary counts percentages.\nTable 3.4: Contingency table age_group sex.\nsize contingency table increases, visual inspection becomes difficult. seen, graphical summary often proves helpful detect patterns.\nFigure 3.13: Barplot…\nFigure 3.13 left pane shows counts. Counts alone can tricky evaluate adjusting barplot account proportions males females group, shown right pane, provides clearer picture relationship. barplots can see females study overall particularly 12-year-olds 17-year-olds groups. gauge association strength sex age_group can calculate Cramer’s V , spirit, like correlation coefficients relationship continuous variables. Cramer’s V score relationship 0.12 low, suggesting strong association sex age_group –words, relationship stable.Let’s look complex case three categorical variables. Now dataset, , third categorical variable us explore can recast continuous num_tokens variable categorical variable bin scores groups. ’ve binned tokens three score groups equal ranges new variable called rank_tokens.Adding second categorical independent variable ups complexity analysis result visualization strategy change. numerical summary include individual two-way cross-tabulations levels third variable. case often best use variable fewest levels third variable, case sex.\nTable 3.5: Contingency table age_group, rank_tokens, sex (female).\n\nTable 3.6: Contingency table age_group, rank_tokens, sex (male).\nContingency tables many levels notoriously difficult interpret. plot often used three-way contingency table summaries mosaic plot. Figure 3.14 created mosaic plot three categorical variables previous contingency tables.\nFigure 3.14: Mosaic plot three categorical variables age_group, rank_tokens, sex BELC dataset.\nmosaic plot suggests number tokens per composition increase learner age group increases females show tokens earlier.sum, dataset information observations become numerous complex visually difficult inspect understand pattern level. Descriptive statistics useful provide researcher overview variables (potential) relationships variables dataset. understanding derived exploration prove useful analytically approaching dataset.Consider adding table informational level, central tendency measure, dispersion measure, visualization??","code":""},{"path":"approaching-analysis.html","id":"analytic-statistics","chapter":"3 Approaching analysis","heading":"3.2 Analytic statistics","text":"Overview…Statistical approaches different aims can broken three categories: exploratory, inferential, predictive.","code":""},{"path":"approaching-analysis.html","id":"exploratory-data-analysis-eda","chapter":"3 Approaching analysis","heading":"3.2.1 Exploratory data analysis (EDA)","text":"Bottom-approach, hypothesis generating, deriving novel insight data, discovering patternsOne two statistical learning approaches, statistical approach used uncover potential relationships data gain new insight area predictions hypotheses clearly made. statistical learning, exploration type unsupervised learning. Supervision , Prediction, refers presence absence outcome variable. choosing exploration approach make assumptions (hypotheses) relationships particular variables data. Rather aims investigate extent can induce meaningful patterns wherever may lie.Findings exploratory analyses can provide valuable insight future study safely used generalize larger population, exploratory analyses often known hypothesis generating analyses (rather hypothesis confirming). Given generalizing power curtailed, data can reused multiple times trying various tests.strictly required, data exploratory analysis often partitioned two sets, training validation, roughly 80%/20% split. training set used refining statistical measures test set used evaluate refined measures. Although evaluation results still used generalize, insight can taken stronger evidence potential relationship, set relationships, worthy study.Although quantitative nature, exploratory methods involve high level human interpretation. Human interpretation part stage data analysis, statistical approach, particular, exploratory methods produce results require associative thinking pattern detection distinct two statistical approaches, particular, IDA.? Include methods, visualizations, examples/ applications/ studies?\nKeyword analysis\nClustering\nTopic modeling\nKeyword analysisClusteringTopic modelingNote methods document-level, terms Egbert, Larsson, & Biber (2020) “linguistic descriptive” nature.","code":""},{"path":"approaching-analysis.html","id":"inferential-data-analysis-ida","chapter":"3 Approaching analysis","heading":"3.2.2 Inferential data analysis (IDA)","text":"Top-approach, hypothesis testing, deriving confirmational insight dataAlso commonly known hypothesis testing confirmation, statistical inference aims establish whether reliable generalizable relationship given patterns data. approach makes starting assumption relationship, null hypothesis (\\(H_0\\)) true. relationship reliable, significant, chance null hypothesis false less predetermined threshold; case accept alternative hypothesis (\\(H_1\\)). standard threshold used Social Sciences, Linguistic included, famous p-value \\(p < .05\\). Without digging deeper meaning p-value, nutshell p-value confidence measure suggest relationship investigating robust reliable given data.two considerations keep mind conducting IDA. First, approach data used used . case two categories fo statistical approaches. reason vital identify statistical approach outset research project. Second, failing establish clear hypothesis testable hypothesis sticking hypothesis can lead researchers engage “p-hacking”; practice running multiple tests /parameters data (.e. reusing data) evidence alternative hypothesis appears.? Include methods, visualizations, examples/ applications/ studies?","code":""},{"path":"approaching-analysis.html","id":"predictive-data-analysis-pda","chapter":"3 Approaching analysis","heading":"3.2.3 Predictive data analysis (PDA)","text":"Mixed approach, can used generation hypotheses test hypotheses, deriving intelligent action data, discovering leveraging patternsThe statistical learning approach, Prediction, aims uncover relationships data pertain particular outcome variable. approach known supervised learning. Similar Exploration many ways, approach also makes assumptions potential relationships variables data data can used multiple times refine statistical tests order tease effective method goals. exploratory analysis aims uncover meaningful patterns sort, prediction, however, focused main aim ascertain extent variables data pattern, individually together, way make reliable associations particular outcome variable unseen data. evaluate robustness prediction model data partitioned training validation sets. Depending application amount available data, third ‘development’ set sometimes created pseudo test set facilitate testing multiple approaches final evaluation. proportions vary, good rule thumb reserve 60% data training, 20% development, 20% validation.? Include methods, visualizations, examples/ applications/ studies?? Include methods, visualizations, examples/ applications/ studies?? overfitting, model captures noise training data obscuring target pattern revealed model makes systematic errors testing data (new data)? overfitting, model captures noise training data obscuring target pattern revealed model makes systematic errors testing data (new data)","code":""},{"path":"approaching-analysis.html","id":"reporting","chapter":"3 Approaching analysis","heading":"3.3 Reporting","text":"Descriptive analysis\nProcedures diagnose correct\nProcedures diagnose correctAnalytic analysis\nCommunicate findings statistical appropriate forms\nDepends analytic statistic(s) applied\n\nCommunicate findings statistical appropriate forms\nDepends analytic statistic(s) applied\nDepends analytic statistic(s) applied","code":""},{"path":"framing-research.html","id":"framing-research","chapter":"4 Framing research","heading":"4 Framing research","text":"\nINCOMPLETE DRAFT\ncapital mistake theorize one data. Insensibly one begins twist facts suit theories, instead theories suit facts.\n―—Sir Arthur Conan Doyle, Sherlock Holmes\nchapter learn:\n\ngoals textbook\n\nreasoning using R programming language\n\nimportant text conventions employed textbook\njumping code, every researcher must come project clear idea purpose analysis. means homework order understand exactly want achieve; , need identify research question. first step become versed previous literature topic. written? main findings? Secondly, important become familiar standard methods approaching topic interest. topic approached methodologically? types, sources, quality data employed? statistical approaches employed? particular statistical tests chosen? Getting overview domain-specific findings literature also methodological choices help identify promising plan carrying research.","code":""},{"path":"framing-research.html","id":"chapter-subsection","chapter":"4 Framing research","heading":"4.1 …chapter subsection","text":"text","code":"\n# Packages"},{"path":"framing-research.html","id":"annotated-readdings","chapter":"4 Framing research","heading":"4.2 Annotated readdings","text":"Ignatow, G., & Mihalcea, R. (2017). introduction text mining: Research design, data collection, analysis. Sage Publications.\n(Ignatow & Mihalcea, 2017)Chapter 5 “Designing research project”Research design essentially concerned basic architecture research projects, designing projects systems allow theory, data, research methods interface way maximize project’s ability achieve goals (see Figure 5.1). Research design involves sequence decisions taken project’s early stages, one oversight poor decision can lead results ultimately trivial untrustworthy. Thus, critically important think carefully systematically research design committing time resources acquiring texts mastering software packages programming languages text mining project.Egbert, J., Larsson, T., & Biber, D. (2020). Linguistics Corpus: Methodological Considerations Everyday User. Cambridge University Press. (Egbert, Larsson, & Biber, 2020)Chapter 3 “Research Designs: Linguistically Meaningful Research Questions, Observational Units, Variables, Dispersion”Research questions drive decisions choice observational unit, variables defined, choice research design.Observational units can defined level linguistic feature,\ntext, corpus.Variables can measured qualitatively, according variants \nlinguistic feature, quantitatively, using rates occurrence features.Chapter 7 “Interpreting Quantitative Results”Linguistics done linguists, computers.order useful, quantitative corpus linguistic analysis \ncoupled sound qualitative interpretation.Researchers can rely linguistic context, text-external context, \nlinguistic theory guide interpretation quantitative corpus findings.","code":""},{"path":"preparation-overview.html","id":"preparation-overview","chapter":"Overview","heading":"Overview","text":"Overview…","code":""},{"path":"acquire-data.html","id":"acquire-data","chapter":"5 Acquire data","heading":"5 Acquire data","text":"\nINCOMPLETE DRAFT\n…\nchapter learn:\n\ngoals textbook\n\nreasoning using R programming language\n\nimportant text conventions employed textbook\nOverview…","code":""},{"path":"acquire-data.html","id":"section","chapter":"5 Acquire data","heading":"5.1 …","text":"text","code":""},{"path":"acquire-data.html","id":"acquire-data-packages","chapter":"5 Acquire data","heading":"5.1.1 Packages","text":"","code":"\nlibrary(rvest)  # full-fleged web scraping\nlibrary(datapasta)  # copy/paste approach to HTML tables"},{"path":"curate-data.html","id":"curate-data","chapter":"6 Curate data","heading":"6 Curate data","text":"\nINCOMPLETE DRAFT\n…\nchapter learn:\n\ngoals textbook\n\nreasoning using R programming language\n\nimportant text conventions employed textbook\nOverview…","code":"\n# Packages"},{"path":"curate-data.html","id":"section-1","chapter":"6 Curate data","heading":"6.1 …","text":"textData Organization Spreadsheets (Broman & Woo, 2018). Although based spreadsheets, many best practices discussed apply good data organization regardless technology.","code":""},{"path":"transform-data.html","id":"transform-data","chapter":"7 Transform data","heading":"7 Transform data","text":"\nINCOMPLETE DRAFT\n…\nchapter learn:\n\n…\n\n…\nOverivew …","code":""},{"path":"transform-data.html","id":"section-2","chapter":"7 Transform data","heading":"7.1 …","text":"textNOTE:Cover Corpus Document-Term Matrices (DTM)s","code":"\n# Packages"},{"path":"modeling-overview.html","id":"modeling-overview","chapter":"Overview","heading":"Overview","text":"","code":""},{"path":"exploration.html","id":"exploration","chapter":"8 Exploration","heading":"8 Exploration","text":"\nINCOMPLETE DRAFT\n…\nchapter learn:\n\ngoals textbook\n\nreasoning using R programming language\n\nimportant text conventions employed textbook\ntext","code":""},{"path":"exploration.html","id":"section-3","chapter":"8 Exploration","heading":"8.1 …","text":"text","code":""},{"path":"exploration.html","id":"exploration-packages","chapter":"8 Exploration","heading":"8.1.1 Packages","text":"","code":"\n# Packages"},{"path":"inference.html","id":"inference","chapter":"9 Inference","heading":"9 Inference","text":"\nINCOMPLETE DRAFT\n…\nchapter learn:\n\ngoals textbook\n\nreasoning using R programming language\n\nimportant text conventions employed textbook\ntext","code":""},{"path":"inference.html","id":"section-4","chapter":"9 Inference","heading":"9.1 …","text":"text","code":""},{"path":"inference.html","id":"inference-packages","chapter":"9 Inference","heading":"9.1.1 Packages","text":"","code":"\n# Packages"},{"path":"prediction.html","id":"prediction","chapter":"10 Prediction","heading":"10 Prediction","text":"\nINCOMPLETE DRAFT\n…\nchapter learn:\n\ngoals textbook\n\nreasoning using R programming language\n\nimportant text conventions employed textbook\ntext","code":""},{"path":"prediction.html","id":"section-5","chapter":"10 Prediction","heading":"10.1 …","text":"text","code":""},{"path":"prediction.html","id":"prediction-packages","chapter":"10 Prediction","heading":"10.1.1 Packages","text":"","code":"\n# Packages"},{"path":"section-6.html","id":"section-6","chapter":"A …","heading":"A …","text":"","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
