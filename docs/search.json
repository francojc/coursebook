[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"\nINCOMPLETE DRAFT\ntextbook introduction fundamental concepts practical programming skills Data Science increasingly employed variety language-centered fields sub-fields applied task quantitative text analysis. geared towards advanced undergraduates, graduate students, researchers looking expand methodological toolbox.content currently development. Feedback welcome can provided hypothes.service. toolbar interface service located right sidebar. register free account join “text_as_data” annotation group follow link. Suggestions changes incorporated acknowledged.AuthorDr. Jerid Francom Associate Professor Spanish Linguistics Wake Forest University. research focuses use large-scale language archives (corpora) variety sources (news, social media, internet sources) better understand linguistic cultural similarities differences language varieties scholarly pedagogical projects. published topics including development, annotation, evaluation linguistic corpora analyzed corpora corpus, psycholinguistic, computational methodologies. also experience working teaching statistical programming R.","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"work Jerid C. Francom licensed Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States License.","code":""},{"path":"index.html","id":"credits","chapter":"Welcome","heading":"Credits","text":"","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Welcome","heading":"Acknowledgements","text":"TAD reviewed suggestions changes incorporated based feedback TAD Hypothes.group following people: Andrea Bowling, Caroline Brady, Declan Golsen, Asya Little, …","code":""},{"path":"index.html","id":"build-information","chapter":"Welcome","heading":"Build information","text":"version textbook built R version 4.1.2 (2021-11-01) macOS Big Sur 10.16 following packages:","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"\nBOOK PROPOSAL DRAFT\njourney thousand miles begins one step.— Lao Tzu\nessential questions preface :\n\nrationale textbook?\n\naims approach taken textbook?\n\ntextbook designed support attaining aims?\n\nneeded get started?\nchapter aims provide brief summary current research trends form context rationale textbook. also provides instructors readers overview aims approach textbook, description main components section chapter, guide conventions used book, summary supporting resources available. Additionally information setting computing environment seek support included.","code":""},{"path":"preface.html","id":"rationale","chapter":"Preface","heading":"Rationale","text":"recent years growing buzz around term ‘Data Science’ related terms; data analytics, data mining, etc. nutshell data science process investigator leverages statistical methods computational power uncover insight machine-readable data sources. Driven large part increase computing power available average individual increasing amount electronic data now available internet, interest data science expanded virtually fields academia areas public sector.textbook introduction fundamental concepts practical programming skills data science applied task quantitative text analysis. intended readership textbook primarily undergraduate students may also applicable graduate students researchers looking expand methodological toolbox. textbook aims meet growing interest quantitative data analysis methods taking hold across linguistics subfields language-informed disciplines (digital humanities, political science, economics, inter alia). ensure resource accessible wide variety students researchers assume background linguistics. Additionally, readers interested aforementioned disciplines may lack experience / feel hesitant towards statistical methods / programming. make textbook attractive novices quantitative text analysis methods, make assumptions reader’s experience quantitative data analysis programming, general, programming statistical programming language R, particular.","code":""},{"path":"preface.html","id":"aims","chapter":"Preface","heading":"Aims","text":"textbook aims develop reader’s proficiency three main areas:Data literacy: ability interpret, assess, contextualize findings based data. Throughout textbook explore topics help understand data analysis methods derive insight data. process encouraged critically evaluate connections across linguistic language-related disciplines using data analysis knowledge skills. Data literacy invaluable skillset academics professionals also indispensable aptitude 21st century citizens navigate actively participate ‘Information Age’ live (Carmi, Yates, Lockley, & Pawluczuk, 2020).Research skills: ability conduct original research, communicate findings, make meaningful connections findings literature field. target area differ significantly, spirit, common learning outcomes research methods course: identify area investigation, develop viable research question hypothesis, collect relevant data, analyze data relevant statistical methods, interpret communicate findings. However, working text incur series key steps selection, collection, preparation data unique text analysis projects. addition, stress importance research documentation creating reproducible research integral part modern scientific inquiry (Buckheit & Donoho, 1995).Programming skills: ability implement research skills programmatically produce research replicable collaborative. Modern data analysis, extension, text analysis conducted using programming. various key reasons : (1) programming affords researchers unlimited research freedom –can envision , can program . said --shelf software either proprietary unmaintained –. (2) Programming underlies well-documented reproducible research –documenting button clicks menu option selections leads research readily reproduced, either researcher future self! (3) Programming forces researchers engage intimately data methods analysis. familiar data methods likely produce higher quality work.","code":""},{"path":"preface.html","id":"resources","chapter":"Preface","heading":"Resources","text":"textbook includes three resources support learning areas Data literacy, Research skills, Programming skills systematic chapter-related fashion: 1) textbook includes prose discussion, figures/tables, R code, thought practical exercises, 2) companion website “Text Data Resources” includes programming tutorials demonstrations develop augment reader’s recognition programming strategies implemented, 3) GitHub repository contains set interactive R programming lessons (Swirl) lab exercises guide reader practical hands-programming applications.","code":""},{"path":"preface.html","id":"structure","chapter":"Preface","heading":"Structure","text":"Part “Foundations” aims : 1) provide overview quantitative research applications, highlighting visible applications notable research various fields, 2) consider quantitative research contributes language research, 3) layout main types research situate quantitative text analysis inside .Part II “Orientation” build framework contextualize quantitative data analysis using Data Insight (DIKI) Hierarchy Figure 0.1 1.\nFigure 0.1: Data Insight Hierarchy (DIKI)\nDIKI Hierarchy highlights stages intermediate steps required derive insight data. Chapter 2 “Understanding data” cover Data Information covering conceptual topics populations versus samples language data samples converted information forms can take. Chapter 3 “Approaching analysis” discuss distinction descriptive analytic statistics. brief important conducting data analysis, descriptive statistics serve sanity check dataset submitting interrogation –goal analytic statistics. also cover main distinctions analytics approaches including inference-, exploration-, prediction-based methods. fundamental understanding data, information, knowledge move Chapter 4 “Framing research” discuss develop research plan, call ‘research blueprint’. point directly address Research Skills elaborate research really comes together; bring speed literature topic, develop research goal hypothesis, select data viable address research goal hypothesis, determine necessary information appropriate measures prepare analysis, perform diagnostic statistics data make adjustments analysis, select perform relevant analytic statistics given research goals, report findings, finally, structure project well-documented reproducible.Part III “Preparation” Part IV “Analysis” serve practical detailed guides R programming strategies conduct text analysis research develop Programming Skills. Chapter 5 “Acquire data” discuss three main strategies accessing data: direct downloads, Automatic Programming Interfaces (APIs), web scraping. Chapter 6 “Curate data(sets)” outline process converting augmenting acquired data dataset () structured format, therefore creating information. include organizing linguistic non-linguistic metadata one dataset. Chapter 7 “Transform datasets” describe work curated dataset derive detailed information appropriate dataset structures appropriate subsequent analysis.Chapters 8 “Inference”, 9 “Prediction”, 10 “Exploration” focus different categories statistical analysis associated distinct research goals. Inference deals analysis methods associated standard hypothesis-testing. include common statistical models employed text analysis: chi-squared, logistic regression, linear regression. Prediction covers methods modeling associations data aim accurately predict outcomes using new textual data. cover standard methods text classification including Näive Bayes, k-nearest neighbors (k-NN), decisions tree random forest models. Exploration covers variety analysis methods association measures, clustering, topic modeling, vector-space models. methods aligned research goals aim interpret patterns arise data .Part V “Communication” covers steps presenting findings research research document 11 “Reporting” reproducible research project 12 “Collaboration”. research documents reproducible projects fundamental components modern scientific inquiry. one hand research document provides readers detailed summary main import research study. hand making research project available interested readers ensures scientific community can gain insight process implemented research thus enables researchers vet extend research build robust verifiable research base.","code":""},{"path":"preface.html","id":"conventions","chapter":"Preface","heading":"Conventions","text":"textbook concepts understanding techniques quantitative text analysis R. Therefore intermingling prose code presented. , attempt establish consistent conventions throughout text made signal reader’s attention appropriate.terms prose, key concepts signaled using bold. explore concepts, R code incorporated text.example, following text block shows actual R code results generated running code. Note hashtag # signals code comment. code follows within text block subsequent text block displays output code.Inline code used code blocks short results needed display. example, code sometimes appear 1 + 1.necessary meta-description code appear. particularly relevant R Markdown documents.series text blocks used signal reader’s attention : key points, concept questions, case studies, swirl lessons, programming tutorials, lab exercises, tips, warnings.Key points summarize main points covered chapter subsection text.\nessential questions preface :\n\nrationale textbook?\n\naims approach taken textbook?\n\ntextbook designed support attaining aims?\n\nneeded get started?\ntime time points consider questions explore.\nConsider objectives course: ways can knowledge\nskills learn benefit academic studies / \nprofessional personal life?\nCase studies provided -line highlight key concepts / methodological approaches relevant current topic section.Swirl interactive R programming lessons appear beginning chapter. lessons provide guided environment experience running code R console. instructions install swirl package textbook lessons can found “Text Data Resources” site directly GitHub.\n: Intro SwirlHow: R Console pane load swirl, run\nswirl(), follow prompts select lesson.: familiarize navigating, selecting, \ncompleting swirl lessons.\nend chapter, text block provide readers cue explore applied programming demonstrations called “Recipes” “Text Data Resources” site. Readers may add online annotations using built-social annotation tool hypothes.. Note: Instructors may opt create private Hypothes.annotation group.\n: Literate\nprogramming IHow: Read Recipe 1 participate Hypothes.\nonline social annotation.: introduce concept Literate Programming\nusing R, RStudio, R Markdown.\nHands-lab activities implement extend programming strategies round chapter. labs found GitHub can downloaded / cloned RStudio instance (either computer web RStudio Cloud).\n: Literate programming IHow: Clone, fork, complete steps \nLab 1.: put literate programming techniques covered \nRecipe 1 practice. Specifically, create edit R\nMarkdown document render report PDF format.\nTips used signal helpful hints might otherwise overlooked.\ncourse exploratory work session, many R objects\noften created test ideas. point inspecting workspace\nbecomes difficult due number objects displayed using\nls().\n\nremove objects workspace, use\nrm(list = ls()).\nErrors inevitable part learning code, errors can avoided. text used warning text block highlight common pitfalls errors.\nHello world!\nwarning.\nAlthough intended -depth introduction statistical techniques, mathematical formulas times included text. formulas appear either inline \\(1 + 1 = 2\\) block equations.\\[\\begin{equation}\n  \\hat{c} = \\underset{c \\C} {\\mathrm{argmax}} ~\\hat{P}(c) \\prod_i \\hat{P}(w_i|c)\n  \\tag{0.1}\n\\end{equation}\\]Data analysis leans heavily graphical representations. Figures appear numbered, Figure 0.2.\nFigure 0.2: Test plot mtcars dataset\nTables, Table 0.1 numbered separately figures.Table 0.1: nice table!","code":"\n# Add 1 plus 1\n1 + 1\n#> [1] 2```{r test-code}\n1 + 1\n```"},{"path":"preface.html","id":"getting-started","chapter":"Preface","heading":"Getting started","text":"jumping first chapter textbook, important prepare computing environment understand take advantage resources available, directly associated textbook indirectly associated.","code":""},{"path":"preface.html","id":"r-and-rstudio","chapter":"Preface","heading":"R and RStudio","text":"Programming backbone modern quantitative research. R popular programming language statisticians adopted many fields natural social sciences. freely downloadable R Project Statistical Programming website available macOS, Linux, Windows operating systems.R code can written executed many different environments, RStudio provides powerful interface widely adopted R programmers. RStudio IDE (Integrated Development Environment) serves dashboard working R –therefore must download install R installing RStudio. may choose run RStudio computer (RStudio Desktop) use RStudio web (RStudio Cloud). advantages approaches. Either approach compatible textbook plan continue work R/RStudio future point likely want install desktop version maintain R RStudio environment.details install R RStudio consult RStudio Education page.","code":""},{"path":"preface.html","id":"r-packages","chapter":"Preface","heading":"R packages","text":"Throughout R programming journey take advantage code created R users form packages. package downloadable set functions / datasets aim accomplish given cohesive set related tasks. official R package repositories CRAN (Comprehensive R Archive Network) packages available code-sharing repositories GitHub.\nComprehensive R Archive Network (CRAN) includes groupings \npopular packages related given applied programming task called Task Views. Explore \navailable CRAN Task Views listings. Note variety areas (tasks)\ncovered listing. Now explore detail one \nfollowing task views directly related topics covered \ntextbook noting associated packages descriptions: (1)\nCluster, (2) MachineLearning, (3) NaturalLanguageProcessing, (4)\nReproducibleResearch.\ndownload number packages different stages textbook, set packages key get go. access working R/ RStudio environment, can proceed install following packages.Install following packages CRAN.tidyversermarkdowntinytexdevtoolsusethisswirlYou can running following code RStudio Console pane.can use RStudio Packages pane click ‘Install’ type names packages.textbook includes support package tadr available GitHub (source code). install package GitHub repository, run following code RStudio Console pane:Finally, although package need download interactive R programming lessons textbook accessed swirl package. Download lessons running following code RStudio Console pane.Later Preface beginning subsequent chapter swirl lessons complete. load choose lesson start, run following code RStudio Console pane.follow prompts select complete desired lesson.","code":"\ninstall.packages(c(\"tidyverse\", \"rmarkdown\", \"tinytex\", \"devtools\", \"usethis\", \"swirl\")) # install key packages from CRAN\ndevtools::install_github(\"lin380/tadr\") # install the tadr package from GitHub\nswirl::install_course_github(\"lin380\", \"swirl\") # install the swirl lessons for this textbook\nlibrary(swirl) # load the swirl package\nswirl() # run swirl"},{"path":"preface.html","id":"git-and-github","chapter":"Preface","heading":"Git and GitHub","text":"GitHub code sharing website. Modern computing highly collaborative GitHub popular platform sharing collaborating coding projects. lab exercises textbook shared GitHub. access complete exercises need sign (free) GitHub account set version control software git computing environment. git conduit interfacing GitHub many git already installed computer (cloud computing environment). verify installation (installation instructions) set git configuration, consult useful Happy Git GitHub useR chapter Install Git.","code":""},{"path":"preface.html","id":"getting-help","chapter":"Preface","heading":"Getting help","text":"technologies employed approach text analysis include somewhat steep learning curve. honesty, learning never stops! Experienced programmers novices alike require support. Fortunately large community programmers developed many official support resources actively contribute unofficial discussion forums. Together resources provide ample methods overcoming challenge.easiest convenient place get help either R RStudio RStudio “Help” toolbar menu. find links help resources, guides, manuals. R packages often include “Vignettes” (long-form documentation demonstrations). can accessed either running browseVignettes() RStudio Console pane searching package using search engine web browser consulting package documentation (e.g. usethis). common packages can find cheatsheets RStudio website.Git GitHub recommend Happy Git GitHub useR official Git GitHub documentation pages great resources well.number popular discussion forum websites programming community asks answers questions real-world issues. sites often subsections dedicated particular programming languages software. list useful experience:StackOverflow: R, Git, RStudio, GitHubReddit: R, Git, RStudio, GithubRStudio CommunityThe take-home message alone. many people world-wide learning program / contribute learning others. engage resources communities successful learning . soon able, pay forward. Posting questions offering answers helps community engages refines skills –win-win.","code":""},{"path":"preface.html","id":"activities","chapter":"Preface","heading":"Activities","text":"\n: Intro SwirlHow: R Console pane load swirl, run\nswirl(), follow prompts select lesson.: familiarize navigating, selecting, \ncompleting swirl lessons.\n\n: Literate\nprogramming IHow: Read Recipe 1 participate Hypothes.\nonline social annotation.: introduce concept Literate Programming\nusing R, RStudio, R Markdown.\n\n: Literate programming IHow: Clone, fork, complete steps Lab\n1.: put literate programming techniques covered \nRecipe 1 practice. Specifically, create edit R\nMarkdown document render report PDF format.\n","code":""},{"path":"preface.html","id":"summary","chapter":"Preface","heading":"Summary","text":"preface ’ve provided rationale aims textbook. structure texbook associated resources work scaffold learning proficiency areas Data literacy, Research skills, Programming skills. area Data Science general, quantitative text analysis effectively conducted using programmatic approaches. process without challenges gains well worth effort. ’ve outlined key resources obtain support invaluable novice well seasoned practitioner.","code":""},{"path":"foundations-overview.html","id":"foundations-overview","chapter":"Overview","heading":"Overview","text":"FOUNDATIONSIn section aims : 1) provide overview quantitative research applications, highlighting visible applications notable research various fields, 2) consider quantitative research contributes language research, 3) layout main types research situate quantitative text analysis inside .","code":""},{"path":"text-analysis-chapter.html","id":"text-analysis-chapter","chapter":"1 Text analysis in context","heading":"1 Text analysis in context","text":"\nBOOK PROPOSAL DRAFT\nScience walks forward two feet, namely theory experiment…Sometimes one foot put forward first, sometimes , continuous progress made use .— Robert . Millikan (1923)\nessential questions chapter :\n\nrole goals data analysis outside \nacademia?\n\nways quantitative language research approached?\n\napplications text analysis?\nchapter aim introduce topic text analysis text analytics frame approach textbook. aim introduce context needed understand text analysis fits larger universe data analysis see commonalities ever-ubiquitous field data analysis, attention linguistics language-related studies employ data analysis particular area text analysis.\n: Variables vectors,\nWorkspaceHow: R Console pane load swirl, run\nswirl(), follow prompts select lesson.: explore key building blocks R\nprogramming language examine local workspace R \nunderstand relationship R workspace file system\nmachine.\n","code":""},{"path":"text-analysis-chapter.html","id":"making-sense-of-a-complex-world","chapter":"1 Text analysis in context","heading":"1.1 Making sense of a complex world","text":"world around us full actions interactions numerous difficult really comprehend. lens individual sees experiences world. gain knowledge world build heuristic knowledge works can interact . happens regardless educational background. humans built . minds process countless sensory inputs many never make conscious mind. underlie skills abilities take granted like able predict happen see someone knock wine glass table onto concrete floor. ’ve never seen object first time ’ve winery, somehow somewhere ‘instinctively’ make effort warn --glass-breaker late. likely stopped consider predictive knowledge come , , may just chalked ‘common sense’. common may , incredible display brain’s capacity monitor environment, relate events observations take place, store information time making big fuss tell conscious mind ’s .wait, textbook text analytics language, right? ? Well, two points make relevant framing journey: (1) world full countless information unfold real-time scale daunting (2) power brain works efficiently behind scene making sense world, one individual living one life limited view world large. Let expand two points little .First let’s clear. way one experience things times, .e. omnipotence. even extremely reduced slices reality still vastly outside experiential capacity, least real-time. One can make point since inception internet individual’s ability experience larger slices world increased. imagine reading, watching, listening every file currently accessible web? ? (See Wayback Machine.) Scale even ; let’s take Wikipedia, world’s largest encyclopedia. Can imagine reading every wiki entry? large resource Wikipedia 2, still small fragment written language produced web, just web 3. Consider moment.second framing point, actually two points one. underscored efficiency brain’s capacity make sense world. efficiency comes clever evolutionary twists lead brain take world makes shortcuts compress raw experience heuristic understanding. means brain supercomputer. store every experience raw form, access records experience like imagine computer access records logged database. brains excel making associations predictions help us (time) navigate complex world inhabit. point key –brains amazing work, work can give us impression understand world detail actually . Let’s little thought experiment. Close eyes think last time saw best friend. wearing? Can remember colors? like , human, probably pretty confident feeling know answers questions chance right. demonstrated numerous experiments human memory confidence correlate accuracy (Roediger & McDermott, 2000; Talarico & Rubin, 2003). ’ve experienced event, real reason bet lives experienced. ’s little bit scary, sure, magic works ‘good enough’ practical purposes.’s deal: humans (1) clearly unable experience large swaths experience simple fact individuals living individual lives (2) experiences live recorded precision therefore ‘trust’ intuitions, least absolute sense.mean human curiosity world around us ability reliably make sense ? short means need approach understanding world tools science. Science powerful makes strides overcome inherit limitations humans (breadth experience recall relational abilities) bring complex world digestible perspective. Science starts question, identifies collects data, careful selected slices complex world, submits data analysis clearly defined reproducible procedures, reports results others evaluate. process repeated, modifying, manipulating procedures, asking new questions positing new explanations, effort make inroads bring complex tangible view.essence science attempt subvert inherent limitations understanding drawing carefully purposefully collected slices observable experience letting analysis observations speak, even goes intuitions (powerful sometime spurious heuristics brains use make sense world).","code":""},{"path":"text-analysis-chapter.html","id":"data-analysis","chapter":"1 Text analysis in context","heading":"1.2 Data analysis","text":"point ’ve sketched outline strengths limitations humans’ ability make sense world science used address limitations. science ’ve described one familiar indispensable tool make sense world. like , description science may associated visions white coats, labs, petri dishes. science’s foundation still stands strong 21st century, series intellectual technological events mid-20th century set motion changes changed aspects science done, done. call Science 2.0, let’s use popularized term Data Science. recognized beginnings Data Science attributed work “Statistics Data Analysis Research” department Bell Labs 1960s. Although primarily conceptual theoretic time, framework quantitative data analysis took shape anticipate come: sizable datasets “…require advanced statistical computational techniques … software implement .” (Chambers, 2020) framework emphasized inference-based research traditional science, also embraced exploratory research recognized need address practical considerations arise working deriving insight abundance machine-readable data.Fast-forward 21st century world machine-readable data truly abundance. increased computing power innovative uses technology world wide web took flight. put perspective, 2019 estimated every minute 511 thousand tweets posted, 18.1 million text messages sent, 188 million emails sent (“Data never sleeps 7.0 infographic,” 2019). data flood limited language, sensors recording devices ever capture evermore swaths world live (Desjardins, 2019). increased computing power gave rise influx data, also primary methods gathering, preparing, transforming, analyzing, communicating insight derived data (Donoho, 2017). vision laid 1960s Bell Labs come fruition.interest deriving insight available data now almost ubiquitous. science data now reached deep aspects life making sense world sought. Predicting whether loan applicant get loan (Bao, Lianju, & Yue, 2019), whether lump cancerous (Saxena & Gyanchandani, 2020), films recommend based previous viewing history (Gomez-Uribe & Hunt, 2015), players sports team sign (Lewis, 2004) now incorporate common set data analysis tools.advances, however, predicated data alone. envisioned researchers Bell Labs, turning data insight takes computing skills (.e. programming), statistical knowledge, , importantly, domain expertise. triad popularly represented Venn diagram 1.1.\nFigure 1.1: Data Science Venn Diagram adapted Drew Conway.\ntoolbelt underlies well-known public-facing language applications. language-capable personal assistant applications, plagiarism detection software, machine translation, search engines, tangible results quantitative approaches language becoming standard fixtures lives.\nFigure 1.2: Well-known language applications\nspread quantitative data analysis taken root academia. Even areas first blush don’t appear approachable quantitative manner fields social sciences humanities, data science making important sometimes disciplinary changes way academic research conducted. textbook focuses domain cuts across many fields; namely language. point let’s turn quantitative approaches language.","code":""},{"path":"text-analysis-chapter.html","id":"language-analysis","chapter":"1 Text analysis in context","heading":"1.3 Language analysis","text":"Language defining characteristic species. , study language key concern wide variety fields, just linguistics. goals various fields, however, approaches language research, vary. one hand language research traditions within linguistics, namely closely associated Noam Chomsky, eschewed quantitative approaches language research later half 20th century instead turned qualitative assessment language structure introspective methods. hand many language research programs, outside linguistics, turned /developed quantitative research methods either necessity theoretical principles. quantitative research trajectories share much common data analysis toolbox described previous section. means large extent language analysis projects share common research language language research also research beyond outside language. However, never one-size-fits approach anything –much less data analysis. quantitative analysis key distinction data collection downstream effects terms procedure also terms interpretation.key distinction, need make point, provide context exploration text analysis, comes approach collecting language data nature data. distinction experimental data observational data. Experimental approaches start intentionally designed hypothesis lay research methodology appropriate instruments plan collect data shows promise shedding light validity hypothesis. Experimental approaches conducted controlled contexts, usually lab environment, participants recruited perform language related task stimuli carefully curated researchers elicit aspect language behavior interest. Experimental approaches language research heavily influenced procedures adapted psychology. link logical language central area study cognitive psychology. approach looks much like white-coat science made reference earlier , quantitative research, now taken advantage data analysis tool belt collect organize much larger quantities data conduct statistically robust analysis procedures communicate findings efficiently.Observational approaches bit mixed bag terms rationale study; may either start testable hypothesis cases may start open-ended research question explore. fundamental distinction two drawn amount control researcher contexts conditions language behavior data collected produced. Observational approaches seek records language behavior produced language speakers communicative purposes natural(istic) contexts. may take place labs (language development, language disorders, etc.), often , language collected sources speakers performing language part daily lives –whether posting social media, speaking telephone, making political speeches, writing class essays, reporting latest news newspaper, crafting next novel destined New York Times best-seller. , data collected ‘wild’ varies structure relative data collected experimental approaches requires number steps prepare data sync data analysis toolbelt.liken distinction experimental observational data collection difference farming foraging. Experimental approaches like farming; groundwork research plan designed, much field prepared seeding, researcher performs series tasks produce data, just farmer waters cares crops, results process bear fruit, data case, data harvested. Observational approaches like foraging; researcher scans available environmental landscape viable sources data naturally existing sources, sources assessed usefulness value address research question, viable selected, data collected.data acquired approaches trade-offs, just farming foraging. Experimental approaches directly elicit language behavior highly controlled conditions. directness level control benefit allowing researchers precisely track particular experimental conditions effect language behavior. conditions explicit part design therefore resulting language behavior can precisely attributed experimental manipulation. primary shortcoming experimental approaches level artificialness directness control. Whether language materials used task, task , fact procedure takes place supervision language behavior elicited can diverge quite significantly language behavior performed natural communicative settings. Observational approaches show complementary strengths shortcomings. Whereas experimental approaches may diverge natural language use, observational approaches strive identify collected language behavior data natural, uncontrolled, unmonitored contexts. way observational approaches question extent language behavior data performed natural communicative act. flipside, contexts natural language communication take place complex relative experimental contexts. Language collected natural contexts nested within complex workings complex world inevitably include host factors conditions can prove challenging disentangle language phenomenon interest must addressed order draw reliable associations conclusions.upshot, , twofold: (1) data collection methods matter research design interpretation (2) single best approach data collection, strengths shortcomings. ideal, robust science language include insight experimental observational approaches (Gilquin & Gries, 2009). evermore greater appreciation complementary nature experimental observational approaches growing body research highlights recognition. Given particular trade-offs observational data often used exploratory starting point help build insight form predictions can submitted experimental conditions. way studies based observational data serve exploratory tool gather better externally valid view language use can serve make prediction can explore precision experimental paradigm. However, always case. Observational data also often used hypothesis-testing contexts well. furthermore, language-related fields, hypothesis-testing ultimate goal deriving knowledge insight.","code":""},{"path":"text-analysis-chapter.html","id":"text-analysis","chapter":"1 Text analysis in context","heading":"1.4 Text analysis","text":"Text analysis application data analysis procedures data science derive insight textual data collected observational methods. deliberately chosen term ‘text analysis’ avoid see pitfalls using common terms literature Corpus Linguistics, Computational Linguistics, Digital Humanities. plenty learning resources focus specifically one three fields discussing quantitative analysis text. perspective missing resource underscores fact text analysis research methods employed span across wide variety academic fields applications industry. textbook aims introduce areas lens data analysis procedures particular field. approach, hope, provides wider view potential applications using text data inspires either employ quantitative text analysis research / raise awareness advantages text analysis making sense language-related linguistic-based phenomenon.applications text analysis? public facing applications stem Computational Linguistic research, often known Natural Language Processing practitioners. Whether using search engines, online translators, submitting paper plagiarism detection software, etc. text analysis methods cover play. uses text analysis production-level applications big money behind developing evermore robust text analysis methods.academia use quantitative text analysis even widespread, despite lack public fanfare. Let’s run select studies give idea areas employ text analysis, highlight range topics researchers address text analysis, whet interest conducting text analysis project.sample studies include research areas translation, stylistics, language variation, dialectology, psychology, psycholinguistics, political science, sociolinguistics highlights diversity fields subareas employ quantitative text analysis. Text analysis center studies share set common goals:detect retrieve patterns text subtle numerous done handTo challenge assumptions /provide views textual sourcesTo explore new questions /provide novel insight","code":""},{"path":"text-analysis-chapter.html","id":"actitivies","chapter":"1 Text analysis in context","heading":"Actitivies","text":"\n: Literate\nprogramming IIHow: Read Recipe 2 participate Hypothes.\nonline social annotation.: explore additional functionality R Markdown:\nnumbered sections, table contents, -line citations \ndocument-final references list, cross-referenced tables \nfigures.\n\n: Literate programming IIHow: Clone, fork, complete steps Lab\n2.: put practice R Markdown functionality \ncommunicate aim(s) main finding(s) primary research\narticle interpret related plot.\n","code":""},{"path":"text-analysis-chapter.html","id":"summary-1","chapter":"1 Text analysis in context","heading":"Summary","text":"chapter started general observations difficulty making sense complex world. standard approach overcoming inherent human limitations sense making science. 21st century toolbelt scientific research exploration grown terms amount data available, statistical methods analyzing data, computational power manage, store, share data, methods, results quantitative research. methods tools deriving insight data made significant inroads outside academia, increasingly figure quantitative investigation language. Text analysis particular branch enterprise based observational data real-world language used wide variety fields.end hope enjoy exploration text analysis. Although learning curve times may seem steep –experience gain improve data literacy, research skills, programmings skills also enhance appreciation richness human language important role everyday lives.","code":""},{"path":"orientation-overview.html","id":"orientation-overview","chapter":"Overview","heading":"Overview","text":"ORIENTATIONBefore working specifics data project, important establish fundamental understanding characteristics levels “Data, Information, Knowledge, Insight Hierarchy (DIKI)” (see Figure 0.1) roles levels deriving insight data. Chapter 2 explore Data Information levels drawing distinction two main types data (populations samples) cover data structured transformed generate information (datasets) fit statistical analysis. Chapter 3 outline importance distinct types statistical procedures (descriptive analytic) commonly used text analysis. Chapter 4 aims tie concepts together cover required steps preparing research blueprint conduct original text analysis project.","code":""},{"path":"understanding-data-chapter.html","id":"understanding-data-chapter","chapter":"2 Understanding data","heading":"2 Understanding data","text":"\nBOOK PROPOSAL DRAFT\nplural anecdote data.— Marc Bekoff\nessential questions chapter :\n\ndistinct types data differ?\n\ninformation form take?\n\nimportance documentation reproducible\nresearch?\nchapter cover starting concepts journey understand derive insight data, illustrated DIKI Hierarchy (Figure 0.1), focusing specifically first two levels: Data Information. see commonly referred ‘data’ everyday uses broken three distinct categories, two referred data third known information. also cover importance documentation data datasets quantitative research.\n: Objects, Packages \nfunctionsHow: R Console pane load swirl, run\nswirl(), follow prompts select lesson.: introduce main types objects R\nunderstand role use functions packages R\nprogramming.\n","code":""},{"path":"understanding-data-chapter.html","id":"data","chapter":"2 Understanding data","heading":"2.1 Data","text":"Data data, right? term ‘data’ common popular vernacular easy assume know mean say ‘data’. things, common assumptions important details require careful consideration. Let’s turn first key distinction need make start break term ‘data’: difference populations samples.","code":""},{"path":"understanding-data-chapter.html","id":"populations","chapter":"2 Understanding data","heading":"2.1.1 Populations","text":"first thing comes many people’s mind term population used human populations.  Say example –’s population Milwuakee? speak population terms talking total sum people living within geographical boundaries Milwaukee. concrete terms, population objective make idealized set objects events reality. Key terms objective idealized. Although can look US Census report Milwaukee retrieve figure population, truly population. ? Well, whatever method used derive numerical figure surely incomplete. incomplete, time someone recorded figure number residents Milwaukee moved , moved , born, passed away –figure longer true population.Likewise talk populations terms language dealing objective idealized aspect reality. Let’s take words English language analog previous example population. case words people English bounding characteristic. Just people, words move , move , born, pass away. compendium words English moment almost instananeously incomplete. true populations, save bounding characteristics select narrow slice reality objectively measurable whose membership fixed (complete works Shakespeare, example).sum, () populations amorphous moving targets. objectively hold exist, practical terms often nail specifics populations. researchers go studying populations theoretically impossible access directly? strategy employed called sampling.","code":""},{"path":"understanding-data-chapter.html","id":"sampling","chapter":"2 Understanding data","heading":"2.1.2 Sampling","text":"sample product subjective process selecting finite set observations objective population goal capturing relevant characteristics target population. Although strategies minimize mismatch characteristics subjective sample objective population, important note almost certainly true given sample diverges population aims represent degree. aim, however, employ series sampling decisions, collectively known sampling frame, maximize chance representing population.common sampling strategies? First sample size. larger sample always representative smaller sample. Sample size, however, enough. hard imagine large sample chance captures subset features population. next step enhance sample representativeness apply random sampling. Together large random sample even better chance reflecting main characteristics population better large random sample. , random random , still run risk acquiring skewed sample (.e sample mirror target population).help mitigate issues, two strategies can applied improve sample representativeness. Note, however, size random samples can applied sample little information internal characteristics population, next two strategies require decisions depend presumed internal characteristics population. first informed sampling strategies called stratified sampling. Stratified samples make (educated) assumptions sub-components within population interest. sub-populations mind, large random samples acquired sub-population, strata. minimum, stratified samples can less representative random sampling alone, chances sample better increases. Can problems approach? Yes, two fronts. First knowledge internal components population often based limited incomplete knowledge population. words, strata selected subjectively researchers using various heuristics based sense ‘common knowledge’. second front stratified sampling can err concerns relative sizes sub-components relative whole population. Even relevant sub-components identified, relative size adds another challenge researchers must face order maximize representativeness sample. attempt align, balance, relative sizes samples strata second population-informed sampling strategy.key feature sample purposely selected. Samples simply collection set data population. Samples rigorously selected explicit target population mind. text analysis purposely sampled collection texts, type defined , known corpus. reason set texts documents selected along purposely selected sampling frame corpus. sampling frame, therefore populations modeled, given corpus likely vary reason safe assumption given corpus equally applicable every research question. Corpus development (.e. sampling) purposeful, characteristics corpus development process made explicit documentation. Therefore vetting corpus sample applicability research goal key step research must take ensure integrity research findings.\nFigure 2.1: Brown Corpus Written American English\n","code":""},{"path":"understanding-data-chapter.html","id":"corpora","chapter":"2 Understanding data","heading":"2.1.3 Corpora","text":"","code":""},{"path":"understanding-data-chapter.html","id":"types","chapter":"2 Understanding data","heading":"2.1.3.1 Types","text":"notion sampling frames mind, corpora compiled aim general purpose (general reference corpora), much specialized sampling frames (specialized corpora). example, American National Corpus (ANC) British National Corpus (BNC) corpora aim model (represent/ reflect) general characteristics English language, former American English later British English. ambitious projects, require significant investments time corpus design implementation (continued development) usually undertaken research teams (Ädel, 2020).Specialized corpora aim represent specific populations. Santa Barbara Corpus Spoken American English (SBCSAE), can imagine name resource, aims model spoken American English. claim written English included. even specific types corpora attempt model types sub-populations scientific writing, computer-mediated communication (CMC), language use specific regions world, country, region, etc.Another set specialized corpora resources aim compile texts different languages different language varieties direct indirect comparison. Corpora directly comparable, include source translated texts, called parallel corpora. Parallel corpora include different languages language varieties indexed aligned linguistic level (.e. word, phrase, sentence, paragraph, document), see OPUS. Corpora compiled different languages language varieties directly aligned called comparable corpora. comparable language language varieties sampled similar sampling frame, example Brown LOB corpora.aim quantitative text researcher select corpus corpora (plural corpus) best aligns purpose research. Therefore general corpus ANC may better suited address question dealing way American English works, general resource may lack detail certain areas, medical language, may vital research project aimed understanding changes medical terminology.","code":""},{"path":"understanding-data-chapter.html","id":"sources","chapter":"2 Understanding data","heading":"2.1.3.2 Sources","text":"common source data used contemporary quantitative research internet. web investigator can access corpora published research purposes language used natural settings can coerced investigator corpus. Many organizations exist around globe provide access corpora browsable catalogs, repositories. repositories dedicated language research, general, Language Data Consortium specific language domains, language acquisition repository TalkBank. always advisable start looking available language data repository. advantage beginning data search repositories repository, especially geared towards linguistic community, make identifying language corpora faster general web search. Furthermore, repositories often require certain standards corpus format documentation publication. standardized resource many times easier interpret evaluate appropriateness particular research project.table ’ve compiled list corpus repositories help get started.Table 2.1: list corpus repositoriesRepositories means source corpora web. Researchers around world provide access corpora data sources sites data sharing platforms. Corpora various sizes scopes often accessible dedicated homepage appear homepage sponsoring institution. Finding resources matter web search word ‘corpus’ list desired attributes, including language, modality, register, etc. part general movement towards reproducibility corpora available web ever . Therefore data sharing platforms supporting reproducible research, GitHub, Zenodo, Re3data, OSF, etc., good place look well, searching repositories targeted web searches yield results.table find list corpus resources datasets.Table 2.2: Corpora language datasets.Language corpora prepared researchers research groups listed repositories hosted researchers often first place look data. web, however, contains wealth language language-related data can accessed researcher compile corpus. two primary ways attain language data web. first process web scraping. Web scraping process harvesting data web either manually (semi-)automatically actual public-facing web. second way acquire data web Application Programming Interface (API). __API__s , title suggests, programming interfaces allow access, certain conditions, information website database accessible via web contains.table lists R packages serve interface language data directly R.Table 2.3: R Package interfaces language corpora datasets.Data language research limited (primary) text sources. sources may include processed data previous research; word lists, linguistic features, etc.. Alone combination text sources data can rich viable source data research project.’ve included processed language resources.Table 2.4: Language data previous research meta-studies.list data available language research constantly growing. ’ve document wide variety resources. ’ve included attempts others provide summary corpus data language resources available.Table 2.5: Lists corpus resources.","code":""},{"path":"understanding-data-chapter.html","id":"formats","chapter":"2 Understanding data","heading":"2.1.3.3 Formats","text":"corpus often include various types non-linguistic attributes, meta-data, well. Ideally include information regarding source(s) data, dates acquired published, author speaker information. may also include number attributes identified potentially important order appropriately document target population. , key match available meta-data goals research. cases corpus may ideal aspects contain key information address research question. may mean need compile corpus fundamental attributes missing. consider compiling corpus, however, worth investigating possibility augmenting available corpus bring inline particular goals. may include adding new language sources, harnessing software linguistic annotation (part--speech, syntactic structure, named entities, etc.), linking available corpus meta-data resources, linguistic non-linguistic.Corpora come various formats, main three : running text, structured documents, databases. format corpus often influenced characteristics data may also reflect author’s individual preferences well. typical corpora meta-data characteristics take form running text.Running text sample Europarle Parallel Corpus.corpora meta-data, header may appended top running text document meta-data may contained separate file appropriate coding coordinate meta-data attributes text corpus.Meta-data header sample Switchboard Dialog Act Corpus.meta-data / linguistic annotation increases complexity common structure corpus document explicitly markup language XML (Extensible Markup Language) organize relationships language meta-data attributes database.XML format meta-data (linguistic annotation) Brown Corpus.Although push towards standardization corpus formats, available resources display degree idiosyncrasy. able parse structure corpus skill develop time. experience working corpora become adept identifying data stored whether content format serve needs analysis.","code":"> Resumption of the session\n> I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n> Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n> You have requested a debate on this subject in the course of the next few days, during this part-session.\n> In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n> Please rise, then, for this minute' s silence.\n> (The House rose and observed a minute' s silence)\n> Madam President, on a point of order.\n> You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n> One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.> FILENAME: 4325_1632_1519\n> TOPIC#:       323\n> DATE:     920323\n> TRANSCRIBER:  glp\n> UTT_CODER:    tc\n> DIFFICULTY:   1\n> TOPICALITY:   3\n> NATURALNESS:  2\n> ECHO_FROM_B:  1\n> ECHO_FROM_A:  4\n> STATIC_ON_A:  1\n> STATIC_ON_B:  1\n> BACKGROUND_A: 1\n> BACKGROUND_B: 2\n> REMARKS:        None.\n> \n> =========================================================================\n> \n> \n> o          A.1 utt1: Okay.  /\n> qw          A.1 utt2: {D So, }\n> \n> qy^d          B.2 utt1: [ [ I guess, +\n> \n> +          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\n> \n> +          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\n> \n> qy          A.5 utt1: Does it say something? /\n> \n> sd          B.6 utt1: I think it usually does.  /\n> ad          B.6 utt2: You might try, {F uh, }  /\n> h          B.6 utt3: I don't know,  /\n> ad          B.6 utt4: hold it down a little longer,  /\n> ad          B.6 utt5: {C and } see if it, {F uh, } -/> <TEI xmlns=\"http://www.tei-c.org/ns/1.0\"><teiHeader><fileDesc><titleStmt><title>Sample A01 from  The Atlanta Constitution<\/title><title type=\"sub\"> November 4, 1961, p.1 \"Atlanta Primary ...\"\n>  \"Hartsfield Files\"\n>  August 17, 1961, \"Urged strongly ...\"\n>  \"Sam Caldwell Joins\"\n>  March 6,1961, p.1 \"Legislators Are Moving\" by Reg Murphy\n>  \"Legislator to fight\" by Richard Ashworth\n>  \"House Due Bid...\"\n>  p.18 \"Harry Miller Wins...\"\n> <\/title><\/titleStmt><editionStmt><edition>A part  of the XML version of the Brown Corpus<\/edition><\/editionStmt><extent>1,988 words 431 (21.7%) quotes 2 symbols<\/extent><publicationStmt><idno>A01<\/idno><availability><p>Used by permission of The Atlanta ConstitutionState News Service (H), and Reg Murphy (E).<\/p><\/availability><\/publicationStmt><sourceDesc><bibl> The Atlanta Constitution<\/bibl><\/sourceDesc><\/fileDesc><encodingDesc><p>Arbitrary Hyphen: multi-million [0520]<\/p><\/encodingDesc><revisionDesc><change when=\"2008-04-27\">Header auto-generated for TEI version<\/change><\/revisionDesc><\/teiHeader>\n> <text xml:id=\"A01\" decls=\"A\">\n> <body><p><s n=\"1\"><w type=\"AT\">The<\/w> <w type=\"NP\" subtype=\"TL\">Fulton<\/w> <w type=\"NN\" subtype=\"TL\">County<\/w> <w type=\"JJ\" subtype=\"TL\">Grand<\/w> <w type=\"NN\" subtype=\"TL\">Jury<\/w> <w type=\"VBD\">said<\/w> <w type=\"NR\">Friday<\/w> <w type=\"AT\">an<\/w> <w type=\"NN\">investigation<\/w> <w type=\"IN\">of<\/w> <w type=\"NPg\">Atlanta's<\/w> <w type=\"JJ\">recent<\/w> <w type=\"NN\">primary<\/w> <w type=\"NN\">election<\/w> <w type=\"VBD\">produced<\/w> <c type=\"pct\">``<\/c> <w type=\"AT\">no<\/w> <w type=\"NN\">evidence<\/w> <c type=\"pct\">''<\/c> <w type=\"CS\">that<\/w> <w type=\"DTI\">any<\/w> <w type=\"NNS\">irregularities<\/w> <w type=\"VBD\">took<\/w> <w type=\"NN\">place<\/w> <c type=\"pct\">.<\/c> <\/s>\n> <\/p>"},{"path":"understanding-data-chapter.html","id":"information","chapter":"2 Understanding data","heading":"2.2 Information","text":"Identifying adequate corpus resource target research question first step moving quantitative text research project forward. next step select components characteristics resource relevant research move organize attributes data useful informative format. process converting corpus dataset –tabular representation information leveraged analysis.","code":""},{"path":"understanding-data-chapter.html","id":"structure-1","chapter":"2 Understanding data","heading":"2.2.1 Structure","text":"Data alone informative. explicit organization data way makes relationships accessible data become information. particularly salient hurdle text analysis research. Many textual sources unstructured data –, relationships used analysis yet explicitly drawn organized text make relationships meaningful useful analysis.running text Europarle Corpus, know files source text (original) files correspond target text (translation). Table 2.6 see text organized columns corresponding type sentence additional sentence_id column keep index sentences aligned.\nconventional work column names datasets R using\nconventions used naming objects. matter \ntaste convention used, adopted snake\ncase personal preference. also alternatives.\nRegardless convention choose, good practice \nconsistent.\n\nalso note column names balanced \nmeaningfulness brevity. brevity practical concern can\nsomewhat opaque. questions meaning column \nvalues consult resource’s documentation.\nTable 2.6: First 10 source target sentences Europarle Corpus.corpus resources semi-structured data –, characteristics structured, .Switchboard Dialog Act Corpus example semi-structured resource. meta-data associated 1,155 conversations corpus. Table 2.7 language-relevant sub-set meta-data associated utterance.Table 2.7: First 5 utterances Switchboard Dialog Act Corpus.Relatively fewer resources structured datasets. cases high amount meta-data / linguistic annotation included corpus. format convention, however, varies resource resource. formats programming general (.csv, .xml, .json, etc.) others resource specific (.cha, .utt, .prd, etc.). Table 2.8 XML version Brown Corpus represented tabular format. Note along meta-data variables, also contains variable linguistic annotation grammatical category (pos part--speech) word.Table 2.8: First 10 words Brown Corpus.coursebook, selection attributes corpus juxtaposition attributes relational format, dataset, converts data information referred data curation. process data curation minimally involves creating base dataset, derived dataset, establishes main informational associations according philosophical approach outlined Wickham (2014). work, tidy dataset refers structural (physical) informational (semantic) organization dataset. Physically, tidy dataset tabular data structure row observation column variable contains measures feature attribute observation. cell given row-column intersect contains value particular attribute particular observation particular observation-feature pair also known data point.\nFigure 2.2: Visual summary tidy format.\nSemantic value tidy dataset derived association physical structure along two dimensions rectangular format. First, column variable reflects measures particular attribute. Europarle Corpus dataset, Table 2.6, example, type column measures type text, either Source Target. Columns can contain measures qualitative quantitative, character-based numeric. Second, row observation contains variables associated primary unit observation. primary unit observation variable essential focus informational structure. dataset first observation contains type, sentence_id, sentence. dataset currently structured primary unit investigation sentence variables measures characterize value sentence.decision primary unit observation fundamentally guided research question, therefore highly specific particular research project. Say instead wanted focus words instead sentences. dataset need transformed new variable (words) created contain word corpus.Table 2.9: Europarle Paralle Corpus words primary unit investigation.values variables type sentence_id maintain necessary description word ensure required semantic relationships identify particular attributes word observation. dataset may seem redundant values type sentence_id repeated numerous times ‘redundancy’ makes relationship variable associated primary unit investigation explicit. format makes tidy dataset versatile format researchers conduct analyses powerful flexible way, see throughout coursebook.important make clear data tabular format constitute dataset, tidy sense using. Data can organized many ways make relationships variables observations explicit.\ntabular data ‘tidy’ format \ndescribed . Can think examples tabular information \ntidy format?\n","code":""},{"path":"understanding-data-chapter.html","id":"transformation","chapter":"2 Understanding data","heading":"2.2.2 Transformation","text":"point introduced first step data curation original data converted relational dataset (derived dataset) highlighted importance informational structure setting stage data analysis. However, primary derived dataset often final organizational step proceeding statistical analysis. Many times, always, derived dataset requires manipulation transformation prepare dataset specific analysis approach taken. another level human intervention informational organization, therefore another step forward journey data insight step DIKI hierarchy. Common types transformations include cleaning variables (normalization), separating eliminating variables (recoding), creating new variables (generation), incorporating others datasets integrate existing variables (merging). results transformations build manipulate derived dataset produce analysis dataset. Let’s now turn provide select set examples transformations using datasets introduced chapter.","code":""},{"path":"understanding-data-chapter.html","id":"normalization","chapter":"2 Understanding data","heading":"2.2.2.1 Normalization","text":"process normalization aims sanitize values within variable set variables. may include removing whitespace, punctuation, numerals, special characters substituting uppercase lowercase characters, numerals word versions, acronyms full forms, irregular incorrect spelling accepted forms, removing common words (stopwords), etc.inspecting Europarle dataset (Table 2.6) see sentence lines represent actual parliament speeches. Table 2.10 see lines.Table 2.10: Non-speech lines Europarle dataset.research project aiming analyze speech want normalize dataset removing lines, seen Table 2.11.Table 2.11: Europarle dataset non-speech lines removed.Another feature dataset may require attention fact English lines include whitespace possessive nouns.Table 2.12: Lines possessives extra whitespace Europarle dataset.may affect another transformation process subsequent analysis, may good idea normalize forms removing extra whitespace.Table 2.13: Europarle dataset whitespace possessives removed.final normalization case scenario involves changing converting text lowercase. goal research count words point fact word starts sentence convention first letter capitalized result distinct counts words essence (.e. “” vs. “”).Table 2.14: Europarle dataset lowercasing applied.Note lowercasing text, normalization steps general, can come cost. example, lowercasing Europarle dataset sentences means lose potentially valuable information; namely ability identify proper names (.e. “Mr Kumar Ponnambalam”) titles (.e. “European Parliament”) directly orthographic forms. , however, transformation steps can applied aim recover ‘lost’ information situations others.","code":""},{"path":"understanding-data-chapter.html","id":"recoding","chapter":"2 Understanding data","heading":"2.2.2.2 Recoding","text":"process recoding aims recast values variable set variables new variable set variables enable direct access. may include extracting values variable, stemming lemmatization words, tokenization linguistic forms (words, ngrams, sentences, etc.), calculating lengths linguistic units, removing variables used analysis, etc.Words intuitively associate ‘base’ word can take many forms language use. example word forms ‘investigation’, ‘investigation’, ‘investigate’, ‘investigated’, etc. intuitively linked. two common methods can applied create new variable facilitate identification associations. first stemming. Stemming rule-based heuristic reduce word forms stem root form.Table 2.15: Results stemming first words Brown Corpus.things note . First number stemming algorithms individual languages distinct languages 5. Second words can stemmed alternate morphological forms (.e. “”, “”, etc.). generally related distinction closed-class (articles, prepositions, conjunctions, etc.) open-class (nouns, verbs, adjectives, etc.) grammatical categories. Third stem generated words can stemmed result forms words . Nonetheless, stems can useful easily extracting set related word forms.example, let’s identify word forms stem ‘investig’.Table 2.16: Results filter word stems “investig” Brown Corpus.can see results Table 2.16 searching word_stems match ‘investig’ returns set stem-related forms. worth noting forms cut across number grammatical categories. instead want draw distinction grammatical categories, can apply lemmatization. process distinct stemming two important ways: (1) inflectional forms grouped grammatical category (2) resulting forms lemmas ‘base’ forms words.Table 2.17: Results lemmatization first words Brown Corpus.appreciate difference stemming lemmatization, let’s compare filter word_lemmas match ‘investigation’.Table 2.18: Results filter word stems “investigation” Brown Corpus.lemma forms ‘investigate’ nouns appear. Let’s run similar search lemma ‘’.Table 2.19: Results filter word stems “” Brown Corpus.words grammatical category returned. case verb ‘’ many inflectional forms ‘investigate’.Another form recoding detect pattern values existing variable create new variable whose values extracted pattern register pattern occurs / many times occurs. example, let’s count number disfluencies (‘uh’ ‘um’) occur utterance utterance_text Switchboard Dialog Act Corpus. Note ’ve simplified dataset dropping non-relevant variables example.Table 2.20: Disfluency counts first 10 utterance text values Switchboard Corpus.One common forms recoding text analysis tokenization. Tokenization process recasting text smaller linguistic units. working text linguistically annotated, feasible linguistic tokens words, ngrams, sentences. word sentence tokens easily understandable, ngram tokens need explanation. ngram sequence either characters words n length sequence. ngram sequences drawn incrementally, bigrams (two-word sequences) sentence “input sentence.” :, , input, input sentenceWe’ve already seen word tokenization exemplified Europarle Corpus subsection Structure Table 2.9, let’s create (word) bigram tokens corpus.Table 2.21: first 10 word bigrams Europarle Corpus.just mentioned, ngrams sequences can formed characters well. character trigram (three-character) sequences.Table 2.22: first 10 character trigrams Europarle Corpus.","code":""},{"path":"understanding-data-chapter.html","id":"generation","chapter":"2 Understanding data","heading":"2.2.2.3 Generation","text":"process generation aims augment variable set variables. essence aims make implicit attributes explicit directly accessible. often targeted automatic generation linguistic annotations grammatical category (part--speech) syntactic structure.examples ’ve added linguistic annotation target (English) source (Spanish) example sentence Europarle Parallel Corpus. First, note variables added dataset correspond grammatical category. addition type sentence_id assortment variables replace sentence variable. part process annotation input text annotated sentence tokenized token indexed token_id. upos contains Universal Part Speech tags6, detailed list features included feats. syntactic annotation reflected token_id_source syntactic_relation variables. variables correspond type syntactic parsing done, case Dependency Parsing (using Universal Dependencies framework). Another common syntactic parsing framework phrase constituency parsing (Jurafsky & Martin, 2020).Table 2.23: Automatic linguistic annotation grammatical category syntactic structure example English sentence Europarle CorpusNow compare English example sentence dataset Table 2.23 parallel sentence Spanish. Note grammatical features language specific. example, Spanish gender apparent scanning feats variable.Table 2.24: Automatic linguistic annotation grammatical category syntactic structure example Spanish sentence Europarle CorpusThere much explore linguistic annotation, syntactic parsing particular, point suffice note possible augment dataset grammatical information automatically.strengths shortcomings automatic linguistic annotation research aware . First, automatic linguistic annotation provides quick access rich highly reliable linguistic information large number languages. However, part--speech taggers syntactic parsers magic. resources built training computational algorithm recognize patterns manually annotated datasets producing language model. model used predict linguistic annotations new language (just previous examples). shortcomings automatic linguistic annotation first, languages trained language models second, data used train model inevitably reflect particular variety, register, modality, etc. accuracy linguistic annotation highly dependent alignment language sampling frame trained data language data automatically annotated. Many () language models available automatic linguistic annotation based language readily available languages traditionally newswire text. important aware characteristics using linguistic annotation tools.","code":""},{"path":"understanding-data-chapter.html","id":"merging","chapter":"2 Understanding data","heading":"2.2.2.4 Merging","text":"process merging aims join variable set variables another variable set variables another dataset. option merge two () datasets requires shared variable indexes aligns datasets.provide example let’s look Switchboard Diaglog Act Corpus. existing, disfluency recoded, version includes following variables.turns corpus website number meta-data files available, including files pertaining speakers topics conversations.speaker meta-data corpus caller_tab.csv file contains speaker_id variable corresponds speaker corpus potentially relevant variables language research project including sex, birth_year, dialect_area, education.Table 2.25: Speaker meta-data Switchboard Dialog Act Corpus.Since datasets contain shared index, speaker_id can merge two datasets. result found Table 2.26.Table 2.26: Merged conversations speaker meta-data Switchboard Dialog Act Corpus.example case dataset merged already structured format (.csv). Many corpus resources contain meta-data stand-files structured.cases researcher like merge information already accompany corpus resource. possible long dataset can created contains variable shared. Without shared variable index datasets merge take place.sum, transformation steps described collectively aim produce higher quality datasets relevant content structure submit analysis. process may include one previous transformations rarely linear often iterative. typical normalization generation, recoding, return normalizing, forth. process highly idiosyncratic given characteristics derived dataset ultimate goals analysis dataset.\nNote cases may convert tidy tabular dataset \ndata formats may required particular statistic\napproaches times relationship variables\nmaintained line research purpose. touch \nexamples types data formats (e.g. Corpus \nDocument-Term Matrix (DTM) objects R) dive particular\nstatistical approaches require later coursebook.\n","code":"#> Rows: 5\n#> Columns: 11\n#> $ doc_id           <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\"\n#> $ speaker_id       <dbl> 1632, 1632, 1519, 1632, 1519\n#> $ topic_num        <dbl> 323, 323, 323, 323, 323\n#> $ topicality       <chr> \"3\", \"3\", \"3\", \"3\", \"3\"\n#> $ naturalness      <chr> \"2\", \"2\", \"2\", \"2\", \"2\"\n#> $ damsl_tag        <chr> \"o\", \"qw\", \"qy^d\", \"+\", \"+\"\n#> $ speaker          <chr> \"A\", \"A\", \"B\", \"A\", \"B\"\n#> $ turn_num         <chr> \"1\", \"1\", \"2\", \"3\", \"4\"\n#> $ utterance_num    <chr> \"1\", \"2\", \"1\", \"1\", \"1\"\n#> $ utterance_text   <chr> \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind …\n#> $ disfluency_count <int> 0, 0, 0, 0, 1"},{"path":"understanding-data-chapter.html","id":"documentation","chapter":"2 Understanding data","heading":"2.3 Documentation","text":"seen chapter acquiring data converting data information involves number conscious decisions implementation steps. favor researchers research community, crucial document decisions steps. makes possible retrace steps also provides guide future researchers want reproduce / build research. programmatic approach quantitative research helps ensure implementation steps documented reproducible also vital decisions made documented well. includes creation/ selection corpus data, description variables chosen corpus derived dataset, description variables created derived dataset analysis dataset.existing corpus sample acquired repository (e.g. Switchboard Dialog Act Corpus, Language Data Consortium), research group (e.g. CEDEL2), individual researcher (e.g. SMS Spam Collection), often documentation provided describing key attributes resource. documentation included acquisition corpus added research project. corpus researcher compiles , need generate documentation.curation transformation steps conducted original corpus data produce datasets also documented. steps can included programming scripts code comments (prose using literate programming strategy (e.g. R Markdown)). structure resulting dataset include called data dictionary. table includes variable names, values contain, short prose description variable (e.g. ACTIV-ES Corpus).","code":""},{"path":"understanding-data-chapter.html","id":"activities-1","chapter":"2 Understanding data","heading":"Activities","text":"\n: Reading,\ninspecting, writing dataHow: Read Recipe 3 participate Hypothes.\nonline social annotation.: use literate programming R markdown work\nR coding strategies reading, inspecting, writing\ndatasets.\n\n: Reading, inspecting, writing\ndataHow: Clone, fork, complete steps Lab\n3.: read datasets packages plain-text\nfiles, inspect report characteristics datasets, write\ndatasets plain-text file.\n","code":""},{"path":"understanding-data-chapter.html","id":"summary-2","chapter":"2 Understanding data","heading":"Summary","text":"chapter focused data information –first two components DIKI Hierarchy. process visualized Figure 2.3.\nFigure 2.3: Understanding data: visual summary\nFirst distinction made populations samples, latter intentional subjective selection observations world attempt represent population interest. result process known corpus. Whether developing corpus selecting existing corpus important vet sampling frame applicability viability resource given research project.viable corpus identified, corpus converted derived dataset adopts tidy dataset format column variable, row observation, intersection columns rows contain values. derived dataset serves establish base informational relationships research stem.derived dataset likely require transformations including normalization, recoding, generation, / merging enhance usefulness information analysis. analysis dataset result process.Finally, documentation implemented stage analysis project process. Employing programmatic approach establishes documentation implementation steps motivation behind decisions taken content corpus data datasets generated also need documentation ensure transparent reproducible research.","code":""},{"path":"approaching-analysis-chapter.html","id":"approaching-analysis-chapter","chapter":"3 Approaching analysis","heading":"3 Approaching analysis","text":"\nINCOMPLETE DRAFT\nStatistical thinking one day necessary efficient citizenship ability read write.— H.G. Wells\nessential questions chapter :\n\nrole statistics data analysis?\n\nimportance descriptive assessment data analysis?\n\nways main approaches data analysis similar different?\nchapter build notions data information previous chapter. aim statistics quantitative analysis uncover patterns datasets. Thus statistics aimed deriving knowledge information, next step DIKI Hierarchy (Figure 2.3). creation information data involves human intervention conscious decisions, seen, deriving knowledge information involves even conscious subjective decisions information assess, method select interrogate information, ultimately interpret findings. first step conduct descriptive assessment information, individual variable level also variables, second interrogate dataset either inferential, predictive, exploratory analysis methods, third interpret report findings.","code":""},{"path":"approaching-analysis-chapter.html","id":"description","chapter":"3 Approaching analysis","heading":"3.1 Description","text":"descriptive assessment dataset includes set diagnostic measures tabular visual summaries provide researchers better understanding structure dataset, prepare researcher make decisions statistical methods / tests appropriate, safeguard false assumptions (missing data, data distributions, etc.). section first cover importance understanding informational value variables can represent move use understanding approach summarizing individual variables relationships variables.ground discussion introduce new dataset. dataset drawn Barcelona English Language Corpus (BELC), found TalkBank repository. ’ve selected “Written composition” task corpus contains writing samples second language learners English different ages. Participants given task writing 15 minutes topic “: past, present future”. Data collected many () participants four times course seven years. Table 3.1 ’ve included first 10 observations dataset reflects structural transformational steps ’ve done start tidy dataset.\nTable 3.1: First 10 observations BELC dataset demonstration.\nentire dataset includes 79 observations 36 participants. observation BELC dataset corresponds individual learner’s composition. includes participant wrote composition (participant_id), age group part time (age_group), sex (sex), number English words produced (num_tokens), number unique English words produced (num_types). final variable (ttr) calculated ratio number unique words (num_types) total words (num_tokens) composition. known Type-Token Ratio standard metric measuring lexical diversity.","code":""},{"path":"approaching-analysis-chapter.html","id":"information-values","chapter":"3 Approaching analysis","heading":"3.1.1 Information values","text":"Understanding informational value, level measurement, variable set variables key preparing analysis implications visualization techniques statistical measures can use interrogate dataset. two main levels measurement variable can take: categorical continuous. Categorical variables reflect class group values. Continuous variables reflect values measured along continuum.BELC dataset contains three categorical variables (participant_id, age_group, sex) three continuous variables (num_tokens, num_types, ttr). categorical variables identify class group membership; participant wrote composition, age group , biological sex. continuous variables measure attributes can take range values without fixed limit differences value regular. number words number unique words composition can range 1 \\(n\\) Type-Token Ratio derived two variables also continuous reason. Furthermore, differences values measures defined interval, example composition word count (num_tokens) 40 exactly two times large composition word count 20.distinction categorical continuous levels measurement, mentioned , main two statistical approaches distinction needs made conduct analysis. However, categorical continuous can broken subcategories descriptive analytic purposes distinctions important. categorical variables distinction can made variables structured relationship values . Nominal variables contain values labels denoting membership class relationship labels. Ordinal variables also contain labels classes, contrast nominal variables, relationship classes, namely one precedence relationship order. mind, categorical variables sub-classified. order values participant_id sex therefore nominal whereas values age_group ordered, value refers sequential age group, therefore ordinal.Turning continuous variables, another subdivision can made hinges existence non-arbitrary zero . Interval variables contain values difference values regular defined, measure arbitrary zero value. typically cited example interval variable temperature measurements Fahrenheit scale. value 0 scale mean 0 temperature. Ratio variables properties interval variables also include non-arbitrary definition zero. continuous variables BELC dataset (num_tokens, num_types, ttr) ratio variables value 0 indicate lack attribute.hierarchical overview relationship two main four sub-types levels measurement appear Figure 3.1.\nFigure 3.1: Levels measurement graphic representation.\nnotes practical importance; First, distinction interval ratio variables often applicable text analysis therefore often treated together continuous variables. Second, distinction ordinal interval/continuous variables clear cut may seem. variables contain values ordered relationship. definition values ordinal variable reflect regular intervals units measurement. practice interval/continuous variables defined number values (say Likert scale used survey) may treated ordinal variable may better understood reflecting class membership. Third, continuous variables can converted categorical variables, reverse true. , example, define criterion binning word counts num_tokens composition ordered classes “low”, “mid”, “high”. hand, sex (measured ) take intermediate values unfixed range. upshot variables can -typed -typed. cases preferred treat continuous variables , nature variable permits , -typing continuous data categorical data results loss information –result loss information hence statistical power may lead results obscure meaningful patterns data (R. Harald Baayen, 2004).","code":""},{"path":"approaching-analysis-chapter.html","id":"summaries","chapter":"3 Approaching analysis","heading":"3.1.2 Summaries","text":"always key gain insight shape information numeric, tabular / visual summaries jumping analytic statistical approaches. appropriate form summarizing information depend number informational value(s) target variables. get sense looks, let’s continue work BELC dataset pose different questions data eye towards seeing various combinations variables descriptively explored.","code":""},{"path":"approaching-analysis-chapter.html","id":"single-variables","chapter":"3 Approaching analysis","heading":"3.1.2.1 Single variables","text":"way statistically summarize variable single measure derive measure central tendency. continuous variable common measure (arithmetic) mean, average, simply sum values divided number values. measure central tendency, however, mean can less--reliable sensitive outliers say data points variable extreme relative overall distribution values variable affect value mean depending extreme deviate. One way assess effects outliers calculate measure dispersion. common standard deviation estimates average amount variability values continuous variable. Another way assess, rather side-step, outliers calculate another measure central tendency, median. median calculated sorting values variable selecting value falls middle values. median less sensitive outliers extreme values () indirectly affect selection middle value. Another measure dispersion calculate quantiles. quantile slices data four percentile ranges providing five value numeric summary spread values continuous variable. spread first third quantile known Interquartile Range (IQR) also used single statistic summarize variability values continuous variable.list central tendency dispersion scores continuous variables BELC dataset.Variable type: numeric\ndescriptive statistics returned generated skimr package.\nsummary, see mean, standard deviation (sd), quantiles (five-number summary, p0, p25, p50, p75, p100). middle quantile (p50) median IQR listed last.important measures assessing central tendency dispersion useful reporting purposes, get better feel variable distributed, nothing beats visual summary. boxplot graphically summarizes many metrics. Figure 3.2 see three continuous variables, now graphical form.\nFigure 3.2: Boxplots continuous variables BELC dataset.\nboxplot, bold line median. surrounding box around median interquantile range. extending lines IQR mark largest lowest value within 1.5 times either 3rd (top box) 1st (bottom box). values fall outside, , extending lines considered statistical outliers marked dots (case red dots). 7Boxplots provide robust visually intuitive way assessing central tendency variability continuous variable type plot can complemented looking overall distribution values terms frequencies. histogram provides visualization frequency (density case blue overlay) values across continuous variable binned regular intervals.Figure 3.3 ’ve plotted histograms top row density plots bottom row three continuous variables BELC dataset.\nFigure 3.3: Histograms density plots continuous variables BELC dataset.\nHistograms provide insight distribution data. three continuous variables, distributions happen strikingly distinct. , however, either. explore continuous variables histograms often trying assess whether skew . three general types skew, visualized Figure 3.4.\nFigure 3.4: Examples skew types density plots.\nhistograms/ density plots distribution either left right, median mean aligned. mode, indicates frequent value variable also aligned two measures. left-skewed distribution mean left median left mode whereas right-skewed distribution opposite occurs. distribution absolutely skew three measures . practice measures rarely align perfectly typical three measures approximate alignment. common enough distribution called Normal Distribution 8 common real-world data.Another potentially informative way inspect normality distribution create Quantile-Quantile plots (QQ Plot). Figure 3.5 ’ve created QQ plots three continuous variables. line plot normal distribution points fall line, less likely distribution normal.\nFigure 3.5: QQ Plots continuous variables BELC dataset.\nvisual inspection can often enough detect non-normality, cases visually approximate normal distribution () can perform Shapiro-Wilk test normality. inferential test compares variable’s distribution normal distribution. likelihood distribution differs normal distribution reflected \\(p\\)-value. \\(p\\)-value .05 threshold suggests distribution non-normal. Table 3.2 see given criterion distribution num_types normally distributed.\nTable 3.2: Results Shapiro-Wilk test normality continuous variables BELC dataset.\nDownstream analytic analysis, distribution continuous variables need taken account certain statistical tests. Tests assume ‘normality’ parametric tests, non-parametric. Distributions approximate normal distribution can sometimes transformed conform normal distribution either outlier trimming statistical procedures (e.g. square root, log, inverse transformation), necessary. stage, however, important thing recognize whether distributions approximate wildly diverge normal distribution.leave continuous variables, let’s consider another approach visually summarizing single continuous variable. Empirical Cumulative Distribution Frequency, ECDF, summary cumulative proportion values continuous variable. ECDF plot can useful determining proportion values fall certain percentage data.Figure 3.6 see ECDF plots three continuous variables.\nFigure 3.6: ECDF plots continuous variables BELC dataset.\nTake, example, number tokens (num_tokens) per composition. ECDF plot tells us 50% values variable 56 words less. three variables plotted, cumulative growth quite steady. cases . , ECDF goes long way provide us glimpse key bends proportions values variable.Now let’s turn descriptive assessment categorical variables. categorical variables, central tendency can calculated well subset measures given reduced informational value categorical variables. nominal variables relationship levels central tendency simply mode. levels ordinal variables, however, relational therefore median, addition mode, can also used measure central tendency. Note variable one mode unimodal, two modes, bimmodal, variables two modes multimodal.\nget numeric value median ordinal variable levels variable need numeric well. Non-numeric levels can recoded numeric purpose necessary.\nlist central tendency metrics categorical variables BELC dataset.Variable type: factorIn practice categorical variable levels common simply summarize counts level table get overview variable. ordinal variables numerous levels, five-score summary (quantiles) can useful summarize distribution. contrast continuous variables graphical representation helpful get perspective shape distribution values, exploration single categorical variables rarely enhanced plots.","code":""},{"path":"approaching-analysis-chapter.html","id":"multiple-variables","chapter":"3 Approaching analysis","heading":"3.1.2.2 Multiple variables","text":"addition single variable summaries (univariate), useful understand two (bivariate) variables (multivariate) related add understanding shape relationships dataset. Just univariate summaries, informational values variables frame approach.explore relationship two continuous variables can statistically summarize relationship coefficient correlation measure effect size continuous variables. continuous variables approximate normal distribution Pearson’s r used, Kendall’s tau appropriate measure. correlation coefficient ranges -1 1 0 correlation -1 1 perfect correlation (either negative positive). Let’s assess correlation coefficient variables num_tokens ttr. Since variables normally distributed, use Kendall’s tau. Using measure correlation coefficient \\(-0.563\\) suggesting correlation, particularly strong one.Correlation measures important reporting really appreciate relationship best graphically represent variables scatterplot. Figure 3.7 see relationship num_tokens ttr.\nFigure 3.7: Scatterplot…\nplots ttr y-axis num_tokens x-axis. points correspond intersection variables single observation. left pane points represented. Visually (given correlation coefficient) can see negative relationship number tokens Type-Token ratio: words, tokens composition lower Type-Token Ratio. case trend quite apparent, cases may . provide additional visual cue trend line often added scatterplot. right pane ’ve added linear trend line. line demarcates optimal central tendency across relationship, assuming linear relationship. steeper line, slope, likely correlation strong. band, ribbon, around trend line indicates confidence interval means real central tendency fall anywhere within space. wider ribbon, larger variation observations. case see ribbon widens number tokens either low high. means trend line potentially drawn either steeper (strongly correlated) flatter (less strongly correlated).\nplots comparing two variables, choice variable plot x- y-axis contingent research question / statistical approach. language varies statistical approaches: inferential methods x-axis used plot known dependent variable y-axis independent variable. predictive methods dependent variable known outcome independent variable predictor. Exploratory methods draw distinctions variables along lines choice variable plot along x- y-axis often arbitrary.\nLet’s add another variable mix, case categorical variable sex, taking bivariate exploration multivariate exploration. point corresponds observation values num_tokens ttr intersect. now points given color reflects level sex associated .\nFigure 3.8: Scatterplot visualizing relationship num\\_tokens ttr.\nmultivariate case, scatterplot without trend line difficult interpret. trend lines levels sex help visually understand variation relationship num_tokensand ttr much better. important note multiple trend lines one slope evaluate. correlation coefficient can calculated level sex (.e. ‘male’ ‘female’) independently relationship slope can visually inspected provide important information regarding level’s relative distribution. trend lines parallel (ignoring ribbons moment), appears case, suggests relationship continuous variables stable across levels categorical variable, males showing lexical diversity females declining similar rate. lines cross, suggest cross point, potentially important difference levels categorical variable (known interaction). Now let’s consider meaning ribbons. Since ribbons reflect range real trend line fall, ribbons overlap, differences levels categorical variable likely distinct. descriptive level, visual summary suggest differences relationship num_tokens ttr distinct levels sex.Characterizing relationship two continuous variables, seen either performed correlation coefficient metric visually. approach summarizing bivariate relationship combines continuous categorical variable distinct. Since categorical variable definition class-oriented variable, descriptive evaluation can include tabular representation, type summary statistic. example, consider relationship num_tokens age_group can calculate mean num_tokens level age_group. provide metric dispersion can include either standard error mean (SEM) / confidence interval (CI).Table 3.3 see summary statistics.\nTable 3.3: Summary table tokens age_group.\nSEM metric summarizes variation based number values CI, seen, summarizes potential range mean may fall given likelihood criterion (usually \\(p\\)-value, .05).assessing categorical variable combination continuous variable table available visual summary. said , graphic summary hard beat. following figure (3.9) barplot provided includes means num_tokens level age_group. overlaid bars represent confidence interval mean score.\nFigure 3.9: Barplot comparing mean num\\_tokens age\\_group BELC dataset.\nCI ranges overlap, just ribbons scatterplots, likelihood differences levels ‘real’ diminished.gauge effect size relationship can use Spearman’s rho rank-based coefficients. score 0.708 indicating relationship age_group num_tokens quite strong. 9Now, want explore multivariate relationship add sex current descriptive summary, can create summary table, let’s jump straight barplot.\nFigure 3.10: Barplot comparing mean num_tokens age_group sex BELC dataset.\nsee Figure 3.10 whole, appears general trend towards tokens composition advanced learner levels. However, non-overlap CI bars ‘12-year-olds’ levels sex (‘male’ ‘female’) suggest 12-year-old females may produce tokens per composition males –potential divergence overall trend.Barplots familiar common visualization summaries continuous variables across levels categorical variables, boxplot another useful visualization type relationship.\nFigure 3.11: Boxplot relationship age_group num_tokens BELC dataset.\nseen summarizing single continuous variables, boxplots provide rich set information concerning distribution continuous variable. case can visually compare continuous variable num_tokens categorical variable age_group. plot right pane includes ‘notches’. Notches represent confidence interval, boxplots interval surrounds median. compared horizontally across levels categorical variable overlap notched spaces suggest true median may within range.\nAdditionally, confidence interval goes outside interquantile range (box) notches hinge back either 1st (lower) 3rd (higher) IQR range suggests variability high.can also add third variable exploration. barplot Figure 3.10, boxplot Figure 3.12 suggests overall trend towards tokens per composition learner advances experience, except ‘12-year-old’ level appears difference ‘males’ ‘females’.\nFigure 3.12: Boxplot relationship age_group, num_tokens sex BELC dataset.\npoint exploration multiple variables always included least one continuous variable. central tendency continuous variables can summarized multiple ways (mean, median, mode) calculating means medians, measures dispersion also provide helpful information summarize variability. working categorical variables, however, measures central tendency dispersion limited. ordinal variables central tendency can summarized median mode dispersion can assessed interquantile range. nominal variables mode measure central tendency dispersion applicable. reason relationships categorical variables typically summarized using contingency tables provide cross-variable counts level target categorical variables.Let’s explore relationship categorical variables sex age_group. Table 3.4 see contingency table summary counts percentages.\nTable 3.4: Contingency table age_group sex.\nsize contingency table increases, visual inspection becomes difficult. seen, graphical summary often proves helpful detect patterns.\nFigure 3.13: Barplot…\nFigure 3.13 left pane shows counts. Counts alone can tricky evaluate adjusting barplot account proportions males females group, shown right pane, provides clearer picture relationship. barplots can see females study overall particularly 12-year-olds 17-year-olds groups. gauge association strength sex age_group can calculate Cramer’s V , spirit, like correlation coefficients relationship continuous variables. Cramer’s V score relationship 0.12 low, suggesting strong association sex age_group –words, relationship stable.Let’s look complex case three categorical variables. Now dataset, , third categorical variable us explore can recast continuous num_tokens variable categorical variable bin scores groups. ’ve binned tokens three score groups equal ranges new variable called rank_tokens.Adding second categorical independent variable ups complexity analysis result visualization strategy change. numerical summary include individual two-way cross-tabulations levels third variable. case often best use variable fewest levels third variable, case sex.\nTable 3.5: Contingency table age_group, rank_tokens, sex (female).\n\nTable 3.6: Contingency table age_group, rank_tokens, sex (male).\nContingency tables many levels notoriously difficult interpret. plot often used three-way contingency table summaries mosaic plot. Figure 3.14 created mosaic plot three categorical variables previous contingency tables.\nFigure 3.14: Mosaic plot three categorical variables age_group, rank_tokens, sex BELC dataset.\nmosaic plot suggests number tokens per composition increase learner age group increases females show tokens earlier.sum, dataset information observations become numerous complex visually difficult inspect understand pattern level. descriptive methods described section indispensable providing researcher overview nature variable (potential) relationships variables dataset. Importantly, understanding derived exploration underlies subsequent investigation counted frame approach analysis regardless research goals methods employed derive substantial knowledge.","code":""},{"path":"approaching-analysis-chapter.html","id":"analysis","chapter":"3 Approaching analysis","heading":"3.2 Analysis","text":"identifying target population, selecting data sample represents population, structuring sample dataset, goals research project inform frame process. unsurprising know process selecting approach analysis also intimately linked researcher’s objectives. goal analysis, generally, generate knowledge information. type knowledge generated process generated, however, differ can broadly grouped three analysis types: inferential, predictive, exploratory. section provide overview analysis types tied research goals general goals teach type affect: (1) identify variables interest, (2) interrogate variables, (3) interpret results. structure discussion analysis types moving structured (deductive) least structured (inductive) approach deriving knowledge information aim provide enough information --researcher identify research approaches literature make appropriate decisions approach research adopt.","code":""},{"path":"approaching-analysis-chapter.html","id":"inferential-data-analysis","chapter":"3 Approaching analysis","heading":"3.2.1 Inferential data analysis","text":"commonly recognized three data analysis approaches, inferential data analysis (IDA) bread--butter science. IDA deductive, top-, approach investigation every step research stems premise, hypothesis, nature relationship world aims test whether relationship statistically supported given evidence. aim infer conclusions certain relationship population based statistical evaluation (corpus) sample. , researcher’s aim draw conclusions generalize, , analysis approach researcher take.Given fact approach aims making claims can generalized larger population, IDA approach rigorous set methodological restrictions. First foremost fact testable hypothesis must formulated research begins. hypothesis guides collection data, organization data dataset transformation, selection variables used address hypothesis, interpretation results. conduct analysis draw hypothesis conforms results known “Hypothesis Result Known” (HARKing) (Kerr, 1998) practice violates principles significance testing. second key stipulation reliability sample data, corpus text analysis, provide evidence test hypothesis must representative population. corpus used study misaligned hypothesis undermines ability researcher make valid claims population. essence, IDA good primary data based .point, let elaborate potentially counterintuitive nature hypothesis formulation testing. IDA, Null-Hypothesis Significance Testing (NHST), paradigm fact approached proposing two mutually exclusive hypotheses. first Alternative Hypothesis (\\(H_1\\)). \\(H_1\\) precise statement grounded previous literature outlining predicted relationship (cases directionality relationship). effect research aims investigate. second hypothesis Null Hypothesis (\\(H_0\\)). \\(H_0\\) flip-side hypothesis testing coin states difference relationship. Together \\(H_1\\) \\(H_0\\) cover logical outcomes.provide example consider hypothetical study aimed investigating claim men women differ terms number questions use spontaneous conversations. Alternative Hypothesis formulated way:\\(H_1\\): Men women differ frequency use questions spontaneous conversations.Null Hypothesis, , statement describing remaining logical outcomes. Formally:\\(H_0\\): difference men women use questions spontaneous conversations.Note stated way hypothesis makes prediction directionality difference men women, difference. likely scenario hypothesis stake claim direction difference. directional hypothesis look like :\\(H_1\\): Women use questions men spontaneous conversations.\\(H_0\\): difference men women use questions spontaneous conversations men use questions women.aspect may run counter expectations aim hypothesis testing find evidence support \\(H_1\\), rather aim assess likelihood can reliably reject \\(H_0\\). default assumption \\(H_0\\) true sufficient evidence reject accept \\(H_1\\), alternative. metric used determine sufficient evidence based probability given nature relationship characteristics data, likelihood difference relationship low. threshold likelihood traditionally summarized p-value statistic. Social Sciences, p-value lower .05 considered statistically significant interpreted correctly means 95% chance observed relationship predicted \\(H_0\\). Note working realm probability, absolutes, therefore analysis produces significant result prove \\(H_1\\) correct \\(H_0\\) incorrect, matter. margin error always present.Let’s now turn identification variables, statistical interrogation variables, interpretation statistical results. First, since clearly defined testable hypothesis center IDA approach, variables sense pre-defined. goal researcher select data curate data produce variables operationalized (practically measured) test hypothesis. second consideration roles variables play analysis. standard IDA one variable dependent variable one variables independent variables. dependent variable, sometimes referred outcome response variable, variable contains information predicted depend information independent variable(s). variable whose variation research study seeks explain. independent variable, sometimes referred predictor explanatory variable, variable whose variation predicted explain variation dependent variable.Returning hypothetical study use questions men women spontaneous conversation, frequency questions used speaker dependent variable biological sex speakers independent variable. hypothesis (\\(H_1\\)) states proposition speaker’s sex predict frequency questions used.hypothetical study ’ve identified two variables, one dependent one independent. important keep mind can multiple independent variables cases dependent variable’s variation predicted related multiple variables. relationship need explicitly part original hypothesis, however.Say formulate complex relationship educational level speakers also related number questions. can update hypothesis reflect scenario.\\(H_1\\): Less educated women use questions men spontaneous conversations.\\(H_0\\): difference men women use questions spontaneous conversations regardless educational level, educated women use questions less educated women, men use questions women.hypothesis described predicts known interaction; relationship independent variables predict different variational patterns dependent variable. likely can appreciate independent variables include hypothesis, extension analysis, difficult becomes interpret. Due increasing difficulty interpretation, practice, IDA studies rarely include two three independent variables analysis.Independent variables add complexity study part research focus, specifically hypothesis. , however, common include variables central focus, commonly assumed contribute explanation variation dependent variable. Let’s assume background literature suggests age speakers also plays role number questions men women use spontaneous conversation. Let’s also assume data collected includes information age speakers. like factor potential influence age use questions focus particular independent variables ’ve defined hypothesis, can include age speakers control variable. control variable added statistical analysis documented report included hypothesis interpreted results.\nFigure 3.15: Variable roles inferential analysis.\npoint let’s look main characteristics need taken account statistically interrogate variables chosen test hypothesis. type statistical test one chooses based (1) informational value dependent variable (2) number independent variables included analysis. Together two characteristics go long way determining appropriate class statistical test, considerations distribution particular variables (.e. normality), relationships variables (.e. independence), expected directionality predicted effect may condition appropriate method applied.can imagine, host combinations statistical tests apply particular scenarios, many consider given scope coursebook (see Gries (2013) Paquot & Gries (2020) exhaustive description). ’ve summarized common statistical scenarios associated tests focus juxtaposition informational values number variables, leaving aside alternative tests deal non-normal distributions, ordinal variables, non-independent variables, etc.Table 3.7 see monofactorial tests, tests one independent variable.\nTable 3.7: Common monofactorial tests.\nTable 3.8 includes listing multifactorial tests, tests one independent / control variables.\nTable 3.8: Common multifactorial tests.\nOne key point make turn interpret statistical results concerns use data IDA. contrast two analysis methods cover, data IDA used . say, entire dataset used single time statistically interrogate relationship(s) interest. resulting confidence metrics (p-values, etc.) evaluated findings interpreted. practice running multiple tests statistically significant result found called “p-hacking” (Head, Holman, Lanfear, Kahn, & Jennions, 2015) like HARKing (described earlier) violates statistical hypothesis testing practice. reason vital identify statistical approach outset research project.Now let’s consider approach interpreting results statistical test. now made reference multiple times, results statistical procedure hypothesis testing result confidence metric. standard widely used confidence metrics p-value. p-value provides probability results statistical test explained null hypothesis. probability crosses threshold .05, result considered statistically significant, otherwise ‘null result’ (.e. non-significant). However, sets binary distinction can problematic. one hand one test returns p-value .051 something ‘marginally significant’? According standard practice results statistically significant. important note p-value sensitive sample size. small sample may return non-significant result, larger sample size underlying characteristics may well return significant result. hand, get statistically significant result, move –case closed? just pointed sample size plays role finding statistically significant results, mean results ‘important’ even small effects large samples can return significant p-value.important underscore purpose IDA draw conclusions dataset generalizable population. conclusions require rigorous measures ensure results analysis overgeneralize (suggest relationship one) balance fact don’t want undergeneralize (miss fact relationship population, analysis capable detecting ). Overgeneralization known Type error false positive undergeneralization Type II error false negative.reasons important calculate size magnitude result gauge uncertainty result standardized, sample size-independent way. performed analyzing effect size reporting confidence interval (CI) results. wider CI uncertainty surrounds statistical result, therefore likely significant p-value result Type error. non-significant p-value large effect size result Type II error. addition vetting p-value, CI effect size can help determine significant result reliable ‘important’. Together effect size CIs aid ability realistically interpret confidence metrics statistical hypothesis testing.","code":""},{"path":"approaching-analysis-chapter.html","id":"predictive-data-analysis","chapter":"3 Approaching analysis","heading":"3.2.2 Predictive data analysis","text":"Predictive data analysis (PDA) first two types statistical approaches cover fall machine learning. branch artificial intelligence (AI), machine learning aims develop computer algorithms can essentially learn patterns data automatically. case PDA, also known supervised learning, learning process guided (supervised) directing algorithm associate patterns variable set variables single particular variable. particular variable analogous degree dependent variable IDA, machine learning literature variable known target variable. variable (often ) variables known features. goal PDA develop statistical generalization can accurately predict values target variable using values feature variables. PDA can seen mix deductive (top-) inductive (bottom-) methods target variable determined research goal feature variables choice statistical method (algorithm) fixed can vary depending usefulness effectively predicting target variable. PDA versatile method often employed derive intelligent action data, can also used hypothesis generation even hypothesis testing, certain conditions. researcher’s aim create model can perform language related task, explore association strength target variable various types combinations features, perform emerging alternative approaches hypothesis testing 10, analysis approach researcher take.point let’s consider departures inferential data analysis (IDA) approach covered last subsection important highlight orient overview PDA. First, cornerstone IDA hypothesis, PDA typically case. research question identifies source potential uncertainty area outlines strategy addressing uncertainty sufficient groundwork embark analysis. second divergence, fact data used distinct way. IDA entire dataset statistically interrogated . PDA dataset (minimally) partitioned training set test set. training set used train statistical model test set left test accuracy statistical model. training set typically constitutes larger portion data (typically around 75%) serves test bed iteratively applying one algorithms / feature combinations produce successful learning model. test set reserved final evaluation model’s performance. Depending application amount available data, third development set sometimes created pseudo test set facilitate testing multiple approaches data outside training set final evaluation test set performed. scenario proportions partitions vary, good rule thumb reserve 60% data training, 20% development, 20% testing.Let’s now turn identification variables, statistical interrogation variables, interpretation statistical results. IDA variables (features) pre-determined hypothesis informational values number variables plays significant role selecting statistical procedure (algorithm). Lacking hypothesis, PDA approach’s main goal make accurate predictions target variable free explore number features feature combinations end. target variable variable necessarily fixed light pre-determined.give example, let’s consider language task goal take text messages (SMS) develop language model predict message spam . Minimally need data includes individual text messages text message need labeled either spam legitimate messages (‘ham’ case). Table 3.9 see first ten 4837 observations SMS Spam Collection (v.1) dataset collected Almeida, G’omez Hildago, & Yamakami (2011).\nTable 3.9: First ten observations SMS Spam Collection (v.1)\nstands two variables; sms_type clearly target message contain full messages. question best transform information message variable provide algorithm useful information predict value sms_type. Since informational value sms_type categorical call values classes. process deciding transform information message useful features called feature engineering process much art science. creative side things often helpful mixture relevant domain knowledge clever hacking skills envision features may work best. logistic side things requires knowledge strengths weaknesses various learning algorithms dealing certain number informational value feature combinations.Leaving choice algorithm aside, let’s focus feature engineering. Since message value unique message, chance using message , likely help us make reliable predictions status new message (‘spam’ ‘ham’). simple first-pass approach decomposing message draw similarities distinctions classes may break message words. Now SMS messages average type text –many non-standard forms. definition word may simply character groupings broken apart whitespace. avoid confusion common-sense understanding word types character strings, often case language feature values called terms. term types may work better, n-grams, character sequences, stems/lemmas, even combinations . Certain terms may removed potentially uninformative either based class (stopwords, numerals, punctuation, etc.) due distribution. process systematic isolation terms informative others called dimensionality reduction (Kowsari et al., 2019). experience research become adept recognizing advantages potential issues alternative ways approaching creation features almost always level trial error process. Feature engineering much exploratory process. also iterative. can try set features algorithm produce language model test training set –accurate, great. , can brainstorm –free try engineer features trying new features feature measures (term weights) / change learning algorithm.\nFigure 3.16: Variable roles predictive analysis.\nLet’s now turn considerations take account selecting statistical algorithm. First, just IDA, variable informational value plays role algorithm selection, specifically informational value target variable. target variable categorical, looking classification algorithm. target variable continuous, employ regression algorithm. 11 common classification algorithms listed Table 3.10.\nTable 3.10: common supervised learning algorithms.\nAnother consideration take account whether researcher aims go beyond simply using algorithm make accurate predictions, also wants understand algorithm made predictions contribution features made process. algorithms produce models allow researcher peer understand inner workings (e.g. logistic regression, naïve bayes classifiers, inter alia) (e.g. neural networks, support vector machines, inter alia). called ‘black-box’ algorithms. Neither type assures best prediction accuracy. Important trade-offs need considered, however, best prediction comes black-box method, goal research understand contribution features model’s predictions.identified target variable, engineered promising set features, selected algorithm employ meets research goals, now time interrogate dataset. first step partition dataset training test set. training set dataset use try different features / algorithms aim developing model can accurately predict target variable values training set. second step ’s done first training algorithm associate features (actual) target values. Next, resulting model applied training data, yet target variable removed, hidden, machine learner. target values predicted model observation compared actual target values. predicted actual values target variable coincide, accurate model. model shows high accuracy, ready move evaluate model test set (removing target variable). model accuracy low, ’s back drawing board either returning feature engineering / algorithm selection hopes improve model performance. way, training data can used multiple times, clear divergence standard IDA methods data interrogated analyzed .\nFigure 3.17: Phases predictive analysis.\napplications PDA interpretation prediction model includes metric metrics accuracy comparing extent models predictions actual targets align. cases inner workings model interest, researcher can dive features contributions prediction model exploratory fashion according research goals. exploration features, , varies, time let’s focus metrics prediction accuracy.standard form evaluating model’s performance differs classification models (naive bayes) regression models (linear regression). classification models, cross-tabulation predicted actual classes results contingency table can used calculate accuracy sum correctly predicted observations divided total number observations test set. addition accuracy, various measures aim assess model’s performance gain insight potential - -generalization model (Precision Recall). regression models, differences predicted actual values can assessed using coefficient correlation (typically \\(R^2\\)). , fine-grained detail model’s performance can calculated (Root Mean Square Error).Another component worthy consideration evaluating model’s performance determine performance actually good. One one hand, accuracy rates 90+% range test set usually good sign model performing well. model perform perfect accuracy, however, depending goal research particular error patterns may important, problematic, overall prediction accuracy. hand, another eventuality model performs well training set test set (new data) performance drops significantly. sign training phrase machine learning algorithm learned nuances data (‘noise’) obscure signal pattern learned. problem called overfitting avoid researchers iteratively run evaluations training data using resampling. two common resampling methods bootstrapping (resampling replacement) cross-validation (resampling without replacement). performance multiple models summarized error assessed. goal minimize performance differences models maximizing overall performance. measures go long way avoiding overfitting therefore maximizing chance training phase produce model robust.","code":""},{"path":"approaching-analysis-chapter.html","id":"exploratory-data-analysis","chapter":"3 Approaching analysis","heading":"3.2.3 Exploratory data analysis","text":"last three analysis types, exploratory data analysis (EDA) includes wide range methods whose objective identify structure datasets using data . way, EDA inductive, bottom-approach data analysis, make formal assumptions relationship(s) variables. EDA can roughly broken two subgroups analysis. Unsupervised learning, like supervised learning (PDA), subtype machine learning. However, unlike prediction, unsupervised learning include target variable guide associations. second subgroup EDA methods can seen (robust) extension descriptive analysis methods covered earlier chapter. Either unsupervised learning descriptive methods, EDA employs quantitative methods summarize, reduce, sort complex datasets statistically visually interrogate dataset order provide researcher novel perspective qualitatively assessed. qualitative assessments may prove useful generate hypotheses generate groupings used predictive analyses. , researcher’s aim probe dataset order explore potential relationships area predictions / hypotheses clearly made, analysis approach choose.contrast IDA even PDA assumptions made relationship(s) explore, EDA makes assumptions. Furthermore, given exploratory nature process, EDA approach can used make conclusive generalizations populations (corpus) sample drawn. IDA fidelity sample process selection variables utmost importance ensure statistical results reliably generalizable. Even case PDA, sample variables selected key building robust predictive model. However, contrast IDA, similar PDA, EDA methods may reuse data selecting different variables /methods research goals dictate. machine learning approach EDA adopted, dataset can partitioned training test sets, similar fashion PDA. PDA, training set used refining statistical measures test set used evaluate refined measures. Although evaluation results still used generalize, insight can taken stronger evidence potential relationship, set relationships, worthy study.Another notable point contrast concerns interpretation EDA results. Although quantitative nature, exploratory methods involve high level human interpretation. Human interpretation part stage data analysis, statistical approach, general, exploratory methods produce results require associative thinking pattern detection distinct two analysis approaches, particular, IDA., done two analysis approaches, let’s turn process variable identification, data interrogation, interpretation methods. case PDA, EDA requires research goal. PDA, research goal centered around predicting target variable. EDA, focus. research goal may fact less defined researcher may consider various relationships turn simultaneously. curation variables, however, overlap spirit process feature engineering touched creating variables predictive models. EDA measure gauge whether engineered variables good, left qualitative evaluation researcher.\nFigure 3.18: Variable roles exploratory analysis.\nillustrative purposes let’s consider State Union Corpus (SOTU) (Benoit, 2020). presidential addresses set meta-data variables included corpus. ’ve subsetted corpus include U.S. presidents since 1946. tabular preview first 10 addresses (truncated display) can found Table 3.11.\nTable 3.11: First ten addresses SOTU Corpus.\nCongress United States:Congress United States:dataset one leveraged explore many different types research questions. Key guiding engineering features, however, clarify outset research project entity study , unit analysis. IDA PDA approaches, unit analysis forms explicit part research hypothesis goal. EDA research question may multiple fronts, may reflected differing units analysis. example, based SOTU dataset, interested political rhetoric, language particular presidents, party ideology, etc. Depending perspective interested investigating, choice approach engineering features gain insight vary.token, approaches interrogating dataset can vary widely, within research project, instructive purposes can draw distinction descriptive methods unsupervised learning methods.\nTable 3.12: common EDA analysis methods.\nEDA leans heavily visual representations descriptive unsupervised learning methods. Visualizations enable humans identify extrapolate associative patterns. Visualizations range standard barplots scatterplots network graphs dendrograms . sample visualizations based SOTU Corpus found Figure 3.19.\nFigure 3.19: Sample visualizations SOTU Corpus (1946-2020).\nJust feature selection analysis method, interpretation results EDA much varied analysis methods. EDA methods provide information requires much human intervention associative interpretation. way, EDA can seen quantitatively informed qualitative assessment approach. results one approach can used input another. Findings can lead exploration probing nuances data. Speculative results exploratory methods can highly informative lead new insight inspire study directions may expected.","code":""},{"path":"approaching-analysis-chapter.html","id":"reporting","chapter":"3 Approaching analysis","heading":"3.3 Reporting","text":"Much necessary reporting analysis features prose part write-report article. include descriptive summaries, blueprint method(s) used, results. Descriptive summaries often include assessments individual variables / relationships variables (central tendency, dispersion, association strength, etc.). procedures applied diagnose correct data also included final report. information key helping readers assess results analysis. blueprint methods used describe variable selection process, variables used statistical analysis, information relevant reader understand done done. Reporting results analysis depend type analysis particular method(s) employed. inferential analyses include test statistic(s) (\\(X^2\\), \\(R^2\\), etc.) measure confidence (\\(p\\)-value, confidence interval, effect size). predictive analyses accuracy results related information need reported. exploratory analyses, reporting results vary often include visualizations metrics require human interpretation analysis types.good article write-include vital information understand procedures taken analysis, many details traditionally appear prose. research project conducted programmatically, however, programming files (scripts) used generate analysis can () shared. scripts highly useful researchers consult understand fine-grained detail steps taken, important also recognize research project well documented –organized project directory file structure well code commenting. description instructions run analysis form research compendium ensure research conducted easily understood able reproduced / enhanced researchers.","code":""},{"path":"approaching-analysis-chapter.html","id":"summary-3","chapter":"3 Approaching analysis","heading":"Summary","text":"chapter focused description analysis –third component DIKI Hierarchy. process visually summarized Figure 3.20.\nFigure 3.20: Approaching analysis: visual summary\nBuilding strategies covered Chapter 2 “Understanding data” derive rich relational dataset, chapter outlined key points approaching analysis. first key step analysis perform descriptive assessment individual variables relationships variables. select appropriate descriptive measures covered various informational values variable can take. addition providing key information reporting purposes, descriptive measures important explore researcher can get better feel dataset conducting analysis.covered three data analysis types chapter: inferential, predictive, exploratory. embodies distinct approaches deriving knowledge data. Ultimately choice analysis type highly dependent goals research. Inferential analysis centered around goal testing hypothesis, reason highly structured approach analysis. structure aimed providing mechanisms draw conclusions results can generalized target population. Predictive analysis less-ambitious times relevant goal discovering extent given relationship can extrapolated data provide model language can accurately predict outcome using new data. many times predictive analysis used perform language tasks, can also highly effective methodology applying different algorithmic approaches exploring relationships target variable various configurations variables. ability explore data multiple ways, also key strength employing exploratory analysis. least structured variable analysis types, exploratory analyses powerful approach deriving knowledge data area clear predictions made.rounded chapter short description importance reporting metrics, procedures, results analysis. Reporting, traditional form, documented prose article. reporting aims provide key information reader need understand done, done, done. information also provides necessary information reader’s critical eye understand analysis detail. Yet even detailed reporting write-still leaves many practical, key, points analysis obscured. programming approach provides procedural steps taken shared provide exact methods applied. Together write-research compendium provides scripts run analysis documentation run analysis forms integral part creating reproducible research.","code":""},{"path":"framing-research-chapter.html","id":"framing-research-chapter","chapter":"4 Framing research","heading":"4 Framing research","text":"\nINCOMPLETE DRAFT\nknew , called research, ?―–Albert Einstein\nessential questions chapter :\n\nstrategies selecting research area identifying research problem?\n\nresearch problem research aim frame development research statement?\n\n‘research blueprint’ conceptual practical steps involved developing aid researcher well scientific community?\npoint part coursebook, covered Data, Information, Knowledge Data Insight Hierarchy. goal provide orientation main building blocks text analysis. Insight last component hierarchy. However, practical terms, first step address research project goals research project influence subsequent steps. chapter discuss frame research, position research project’s findings contribute insight understanding world. cover connect literature, selecting research area identifying research problem, design research best positioned return relevant findings connect literature, establishing research aim research question. round chapter guide developing research blueprint –working plan organize conceptual practical steps implement research effectively way supports communicating research findings process findings obtained.Together research area, problem, aim question research blueprint forms conceptual practical scaffolding project ensure outset project solidly grounded main characteristics good research. characteristics, summarized Cross (2006), found Table 4.1.Table 4.1: Characteristics research (Cross, 2006).characteristics mind, let’s get started first component address –connecting literature.","code":""},{"path":"framing-research-chapter.html","id":"connect","chapter":"4 Framing research","heading":"4.1 Connect","text":"","code":""},{"path":"framing-research-chapter.html","id":"research-area","chapter":"4 Framing research","heading":"4.1.1 Research area","text":"area research first decision make terms make contribution understanding. point, aim identify general area interest researcher wants derive insight. established research trajectory language, area research address text analysis likely extension prior work. others, include new researchers researcher’s want explore new areas language research approach area language-based lens, choice area may less obvious. either case, choice research area guided desire contribute something relevant theoretical, social, / practical matter personal interest. Personal relevance goes long way developing carrying purposive inquisitive research.get started? first step reflect areas interest knowledge, academic, professional, personal. Language heart human experience therefore found fashion anywhere one seeks find . big world often general question area explore language use sometimes difficult. get ball rolling, helpful peruse disciplinary encyclopedias handbooks linguistics language-related academic fields (e.g. Encyclopedia Language Linguistics (Brown, 2005), Practical Guide Electronic Resources Humanities (Dubnjakovic & Tomlin, 2010), Routledge encyclopedia translation technology (Chan, 2014))personal, less academic, approach consult online forums, blogs, etc. one already frequents can accessed via online search. example, Reddit wide variety active subreddits (r/LanguageTechnology, r/Linguistics, r/corpuslinguistics, r/DigitalHumanities, etc.). Twitter Facebook also interesting posts linguistics language-related fields worth following. one social media site may find particular people maintain blog worth browsing. example, follow Julia Silge, Rachel Tatman, Ted Underwood, inter alia. Perusing resources can help spark ideas highlight kinds questions interest .Regardless whether inquiry stems academic, professional, personal interest, try connect findings academic areas research. Academic research highly structured well-documented making associations network aid subsequent steps developing research project.","code":""},{"path":"framing-research-chapter.html","id":"research-problem","chapter":"4 Framing research","heading":"4.1.2 Research problem","text":"’ve made rough-cut decision area research, now time take deeper dive subject area jump literature. rich structure disciplinary research provide aid traverse vast world academic knowledge identify research problem. research problem highlights particular topic debate uncertainty existing knowledge worthy study.Surveying relevant literature key ensuring research informed, , connected previous work. Identifying relevant research consult can bit ‘chicken egg’ problem –knowledge area necessary find relevant topics, knowledge topics necessary narrow area research. Many times way forward jump conducting searches. can world-accessible resources (e.g. Google Scholar) limited-access resources provided academic institution (e.g. Linguistics Language Behavior Abstracts), ERIC, PsycINFO, etc.). organizations academic institutions provide research guides help researcher’s access primary literature.Another avenue explore journals dedicated areas linguistics language-related research published. following tables ’ve listed number highly visable journals linguistics, digital humanities, computational linguistics.Table 4.2: list linguistics journals.Table 4.3: list humanities journals.Table 4.4: list computational linguistics journals.explore research related text analysis helpful start (sub)discipline name(s) identified selecting research area, specific terms occur key terms literature, terms ‘corpus study’ ‘corpus-based’. results first searches may turn sources end figuring explicitly research, important skim results publications mine information can useful formulate better targeted searches. Relevant information honing searches can found throughout academic publication (article book). However, pay particular attention abstract, articles, table contents, books, cited references. Abstracts tables contents often include discipline-specific jargon commonly used field. articles even short list key terms listed abstract can extremely useful seed better precise search results. references section contain relevant influential research. Scan references publications appear narrowing topic interest treat like search right.searches begin show promising results time keep track organize references. Whether plan collect thousands references lifetime academic research aim centered around one project, software Zotero12, Mendeley, BibDesk provide powerful, flexible, easy--use tools collect, organize, annotate, search, export references. Citation management software indispensable modern research –often free!list relevant references grows, want start investigation process earnest. Begin skimming (reading) contents publications, starting relevant first13. Annotate publications using highlighting features citation management software identify: (1) stated goal(s) research, (2) data source(s) used, (3) information drawn data source(s), (4) analysis approach employed, (5) main finding(s) research pertain stated goal(s). Next, words, summarize five key areas prose adding summary notes feature citation management software. process allow efficiently gather document references relevant information guide identification research problem, guide formation problem statement, ultimately, support literature review figure project write-.preliminary annotated summaries undoubtedly start recognize overlapping contrasting aspects research literature. aspects may topical, theoretical, methodological, appear along lines. Note aspects continue conduct refine searches, annotate new references, monitor emerging patterns uncertainty debate (gaps) align research interest(s). promising pattern takes shape, time engage detailed reading references appear relevant highlighting potential gap(s) literature. point can focus energy nuanced aspects particular gap literature goal formulate problem statement. problem statement directly acknowledges gap literature puts finer point nature relevance gap understanding. statement reflects first deliberate attempt establish line inquiry. targeted, still somewhat general, statement framing gap literature guide subsequent research design decisions.","code":""},{"path":"framing-research-chapter.html","id":"findings","chapter":"4 Framing research","heading":"4.2 Findings","text":"","code":""},{"path":"framing-research-chapter.html","id":"research-aim","chapter":"4 Framing research","heading":"4.2.1 Research aim","text":"problem statement hand, now time consider goal(s) research. research aim frames type inquiry conducted. research aim explain, evaluate, explore? words, research seek test particular relationship, assess potential strength particular relationship, uncover novel relationships? can appreciate, research aim directly related analysis methods touched upon Chapter 3.gauge frame research aim, reflect literature led problem statement nature problem statement . gap center problem statement lack knowledge, research aim may exploratory. gap concerns conjecture relationship, research may take predictive approach. gap points validation relationship, research likely inferential nature. selecting research aim also helpful consult research aims primary literature led research statement. Consider research statement relates previous literature. aim test hypothesis based previous exploratory analyses? looking generate new knowledge (apparently) uncharted area?general, problem statement addresses smaller, nuanced gap tend adopt similar research aims previous literature larger, divergent gap tend adopt distinct research aim. hard rule, heuristic, however, important familiar previous literature, nature different types analysis, goals research ensure research best-positioned generate findings contribute existing body understanding principled way.","code":""},{"path":"framing-research-chapter.html","id":"research-question","chapter":"4 Framing research","heading":"4.2.2 Research question","text":"next step research design craft research question. research question clearly defined statement identifies aspect uncertainty particular relationships uncertainty concerns. research question extends narrows line inquiry established research statement research aim. research statement can seen content research aim form.form research question vary based analysis approach. inferential-based research, research question actually statement, question. statement makes testable claim nature particular relationship –.e. asserts hypothesis. illustration, let’s return one hypotheses previously sketched Chapter 3, leaving aside implicit null hypothesis.Women use questions men spontaneous conversations.predictive- exploratory-based research, research question fact question. reframing example hypothesis predictive-based research question might looks something like .Can number questions used spontaneous conversations predict speaker male female?similar exploratory-based research question take form.men women differ terms number questions use spontaneous conversations?central research interest behind hypothetical research questions , admittedly, quite basic. simplified examples, able appreciate similarities differences forms research statements correspond distinct research aims.terms content, research question make reference two key components. First, unit analysis. unit analysis entity research aims investigate. three example research aims, unit analysis , namely men women. Note, however, current unit analysis somewhat vague example research questions. precise unit analysis include information population men women drawn (e.g English speakers, American English speakers, American English speakers Southeast, etc.).second key component unit observation. unit observation primary element insight unit analysis derived way constitutes essential organization unit data collected. examples, unit observation, , unchanged spontaneous conversations. Note unit observation key identify forms organizational backbone research, common research derive variables unit provide evidence investigate research question. previous examples, identified number conversations part research question. cases researcher may seek understand aspects questions spontaneous conversations (.e type question, features questions, etc.). unit observation, however, remain .","code":""},{"path":"framing-research-chapter.html","id":"blueprint","chapter":"4 Framing research","heading":"4.3 Blueprint","text":"Efforts craft research question important aspect developing purposive, inquisitive, informed research (returning Cross’s characteristics research). Moving beyond research question project means developing laying research design way research Methodical Communicable. coursebook, method achieve goals development research blueprint. blueprint includes two components: (1) process identifying data, information, methods used (2) creation plan structure document project.Ignatow & Mihalcea (2017) point :Research design essentially concerned basic architecture research projects, designing projects systems allow theory, data, research methods interface way maximize project’s ability achieve goals …. Research design involves sequence decisions taken project’s early stages, one oversight poor decision can lead results ultimately trivial untrustworthy. Thus, critically important think carefully systematically research design committing time resources acquiring texts mastering software packages programming languages text mining project.","code":""},{"path":"framing-research-chapter.html","id":"identify","chapter":"4 Framing research","heading":"4.3.1 Identify","text":"Importance identifying documenting key aspects required conduct research understated. one hand process links concept implementation. , researcher better-positioned conduct research clear view entailed. hand, promising research question, paper, may present challenges may require modification reevaluation viability project. uncommon encounter roadblocks even dead-ends moving well-founded research question forward considering available data, researcher’s (current) technical / research skills, given time frame project. practice, process identifying data, information, methods analysis considered tandem investigative work develop research aim research question. subsection cover main characteristics consider developing research blueprint.first, important, part establishing research blueprint identify viable data source. Regardless find access data, essential vet corpus sample light research question. case research inferential nature, sampling frame corpus primary importance goal generalize findings target population. corpus resource align, extent feasible, target population. predictive exploratory research, goal generalize claim central reason freedom terms representative corpus sample target population. Ideally researcher find able model language population target interest. Since goal, however, test hypothesis, rather explore particular potential relationships, either predictive exploratory fashion, research can often continue stipulation results interpreted light characteristic available corpus sample.second step identify key variables need conduct research ensure information can derived corpus data. research question reference unit analysis unit observation, important point pinpoint key variables . unit observation spontaneous conversations. question aspects conversations used analysis. research questions presented chapter, want envision needs done generate variable measures number questions conversations. research, may features need extracted recoded address research question. variables importance may non-linguistic nature. Provided corpus required meta-data research, variables can normalized, recoded, generated corpus fit research needs. cases meta-data incomplete goals research, sometimes possible merge meta-data sources.third step identify method analysis. selection analysis approach part research aim research question goes long way narrowing methods researcher must consider. number factors make methods appropriate others. inferential research, number information values variables analyzed key importance (Gries, 2013). informational value dependent variable narrow search appropriate method. number independent variables also plays important role. example, study categorical dependent variable single categorical independent variable lead researcher Chi-squared test. study continuous dependent variable multiple independent variables lead linear regression. Another aspect note inference studies consideration distribution continuous variables –normal distribution use parametric test non-normal distribution use non-parametric test. details need nailed point, helpful radar ensure time comes analyze data, appropriate steps taken test normality apply correct test.predictive-based research, informational value target variable key deciding whether prediction classification task numeric prediction task. downstream effects comes time evaluate interpret results. Although feature engineering process predictive analyses means features need specified outset can tweaked changed needed analysis, good idea start basic sense features likely helpful developing robust predictive model. Furthermore, number informational values features (predictor variables) important selecting prediction method (algorithm) inferential analysis methods, important recognize algorithms strengths shortcomings working large numbers / types features (Lantz, 2013).Exploratory research least restricted three types analysis approaches. Although may case research able specify outset project exact analysis methods , attempt consider types analysis methods promising provide results address research question goes long way steering project right direction grounding research. analysis approaches, important aware analysis methods available type information produce light research question.sum, identification data, information, analysis methods used proposed research key ensuring research viable. sure document process prose describe strengths potential shortcomings (1) corpus data selected, (2) information extracted analysis, (3) analysis method(s) appropriate research aim evaluation method . Furthermore, every eventuality can foreseen. helpful include description aspects process may pose challenges include potential contingency plans part prose description.","code":""},{"path":"framing-research-chapter.html","id":"plan","chapter":"4 Framing research","heading":"4.3.2 Plan","text":"next step creating research blueprint consider physically implement project. includes organize files directories fashion provides researcher logical predictable structure work also ensures research Communicable. one hand, communicable research includes strong write-research, , hand, also important research reproducible. Reproducibility strategies benefit researcher (moment future) leads better work habits better teamwork makes changes project easier. Reproducibility also benefit scientific community shared reproducible research enhances replicability encourages cumulative knowledge development (Gandrud, 2015).set guiding principles accomplish goals (Gentleman & Temple Lang, 2007; Marwick, Boettiger, & Mullen, 2018).files plain text means contain formatting information whitespace.clear separation data, method, output research. apparent directory structure.separation original data derived data made. Original data treated ‘read-’. changes original data justified, generated code, documented (see point 6).analysis file (script) represent particular, well-defined step research process.analysis script modular –, file correspond specific goal analysis procedure input output corresponding step.analysis scripts tied together ‘master’ script used coordinate execution analysis steps.Everything documented. includes analysis steps, script code comments, data description data dictionaries, information computing environment packages used conduct analysis, detailed instructions reproduce research.seven principles can physically implemented countless ways. recent years, growing number efforts create R packages templates quickly generate scaffolding tools facilitate reproducible research. notable R packages include workflowr ProjectTemplate many resources R included CRAN Task View Reproducible Research. many advantages working pre-existing frameworks savvy R programmer.coursebook, however, developed project template (available GitHub) believe simplifies makes process transparent beginning intermediate R programmers, directory structure provided .Let now describe template structure aligns seven principles quality reproducible research.files plain text (e.g. .R, .Rmd, .csv, .txt, etc.).three main directories analysis/, data/, ouput/.data/ directory contains sub-directories original (‘read-’) data derived data.analysis/ directory contains five scripts numbered correspond sequential role research process.analysis scripts designed modular; input output must explicit intermediate objects carried analysis scripts. Dataset output written read data/derived/ directory. Figures statistical results written read output/figures/ output/results respectively.analysis scripts, therefore entire project, tied _pipeline.R script. reproduce entire project script need run.Documentation takes place many levels. README.md file first file researcher consult. contains brief description project goals reproduce analysis. Analysis scripts use Rmarkdown format (.Rmd). format allows researchers interleave prose description executable code script. ensures rationale steps taken described prose, code made available consult, code comments can added every line. _sesssion-info.Rmd script merged analysis script provide information computing environment packages used conduct step analysis. template, data datasets appear. However, data acquired data curated transformed, documentation resources documented resource data dictionary along side data(set) .aspects project template described points 1-7 together form backbone reproducible research. template, however, includes additional functionality enhance efficient communicable research. _pipeline.R script executes analysis scripts analysis directory, side effect also produces working website journal-ready article publishing analysis, results, findings web HTML PDF format. index.Rmd file splash page website good place house pre-analysis investigative work including research area, problem, aim, question document research blueprint including identification viable data resource(s), key variables analysis, analysis method, method assessment. Rmarkdown files provide functionality citing organizing references. references.bib file references stored can used include citations support research throughout project.","code":"#> ../project_template/\n#> ├── README.md\n#> ├── _pipeline.R\n#> ├── analysis\n#> │   ├── 1_acquire_data.Rmd\n#> │   ├── 2_curate_dataset.Rmd\n#> │   ├── 3_transform_dataset.Rmd\n#> │   ├── 4_analyze_dataset.Rmd\n#> │   ├── 5_generate_article.Rmd\n#> │   ├── _session-info.Rmd\n#> │   ├── _site.yml\n#> │   ├── index.Rmd\n#> │   └── references.bib\n#> ├── data\n#> │   ├── derived\n#> │   └── original\n#> └── output\n#>     ├── figures\n#>     └── results"},{"path":"framing-research-chapter.html","id":"prepare","chapter":"4 Framing research","heading":"4.3.3 Prepare","text":"template allow organize research design align implementation steps conduct quality reproducible research. prepare analysis, need download fork clone template GitHub repository make adjustments personalize template research.create local copy project template either:Download decompress .zip fileIf git installed machine GitHub account, fork repository GitHub account. open terminal desired location clone repository. using RStudio, can setup new RStudio Project clone using ‘New Project…’ dialog, choosing ‘Version Control’, following steps.begin configuring adding project-specific details template. Reproduce project ‘-’ confirm builds local machine.RStudio R session Terminal application, open console root directory project. run:take time complete, prompt (>) console return. navigate open docs/index.html browser.confirmed project template builds, can begin configure template reflect project. files consider first. files places title project appear.README.md_pipeline.Ranalysis/index.RmdAfter updating files, build project make sure new changes appear like . now ready start research project!","code":"\nsource(\"_pipeline.R\")"},{"path":"framing-research-chapter.html","id":"summary-4","chapter":"4 Framing research","heading":"Summary","text":"aim chapter provide key conceptual practical points guide development viable research project. Good research purposive, inquisitive, informed, methodological, communicable. , however, always linear process. Exploring area(s) interest connecting existing work help couch refine research. practical considerations, existence viable data, technical skills, / time constrains, sometimes pose challenges require researcher rethink / redirect research sometimes small times significant ways. process formulating research question developing viable research plan key supporting viable, successful, insightful research. ensure effort derive insight data value researcher research community, research strive methodological communicable adopting best practices reproducible research.chapter concludes Orientation section coursebook. point fundamental characteristics research place move project towards implementation. next section, Preparation, aims cover acquisition, curation, transformation data preparation analysis. first steps putting research blueprint action coincidence first components Data Insight Hierarchy. Following Preparation section attention turn implementation three analysis approaches covered: inference, prediction, exploration. Throughout next sections maintain aim develop methodological communicable research connecting implementation process reproducible programming strategies.\nFigure 4.1: Framing research: visual summary\n","code":""},{"path":"preparation-overview.html","id":"preparation-overview","chapter":"Overview","heading":"Overview","text":"PREPARATIONAt point turn attention implementing specifics outlined research blueprint. section group components concern acquisition, curation, transformation data dataset prepared submitted analysis. three chapters outline main characteristics consider research steps provide authentic examples working R implement steps. Chapter 5 includes downloads, working APIs, webscraping. Chapter 6 turn organize data rectangular, ‘tidy’, format. Depending data dataset acquired research project, steps necessary shape data base dataset vary, see. Chapter 7 work manipulate curated datasets create datasets aligned research aim research question. often includes normalizing values, recoding variables, generating new variables well sourcing merging information datasets dataset submitted analysis.","code":""},{"path":"acquire-data-chapter.html","id":"acquire-data-chapter","chapter":"5 Acquire data","heading":"5 Acquire data","text":"\nINCOMPLETE DRAFT\nscariest moment always just start.―–Stephen King\nessential questions chapter :\n\ncommon strategies acquiring corpus data?\n\nprogrammatic steps can take ensure acquisition process reproducible?\n\nimportance documenting data?\nthree main ways acquire corpus data using R introduce : downloads, APIs, web scraping. chapter start manual programmatically downloading corpus straightforward process novice R programmer typically incurs least number steps. Along way introduce key R coding concepts including control statements custom functions. Next cover using R packages interface APIs, open-access authentication-based. APIs require us delve detail R objects custom functions. Finally acquiring data web via webscraping idiosyncratic involves knowledge web, sophisticated R skills, often clever hacking skills. start crash course structure web documents (HTML) scale real-world example. round chapter cover process ensuring data documented way provide sufficient information understand key sampling characteristics source drawn.","code":""},{"path":"acquire-data-chapter.html","id":"downloads","chapter":"5 Acquire data","heading":"5.1 Downloads","text":"","code":""},{"path":"acquire-data-chapter.html","id":"manual","chapter":"5 Acquire data","heading":"5.1.1 Manual","text":"first acquisition method cover inherently non-reproducible standpoint programming implementation acquire data based solely running project code . words, requires manual intervention. Manual downloads typical data resources openly accessible public facing web. can resources require institutional private licensing (Language Data Consortium, International Corpus English, BYU Corpora, etc.), require authorization/ registration (Language Archive, COW Corpora, etc.), / accessible via resource search interfaces (Corpus Spanish Southern Arizona, Corpus Escrito del Español como L2 (CEDEL2), etc.).Let’s work CEDEL2 corpus (Lozano, 2009) provides search interface open access data search interface. homepage can seen Figure 5.1.\nFigure 5.1: CEDEL2 Corpus homepage\nFollowing search/ download link can find search interface allows user select sub-corpus interest. ’ve selected subcorpus “Learners L2 Spanish” specified L1 English.\nFigure 5.2: Search download interface CEDEL2 Corpus\n‘Download’ link now appears search criteria. Following link provide user form fill . particular resource allows access different formats download (Texts , Texts metadata, CSV (Excel), CSV (Others)). select ‘CSV (Others)’ option data structured easier processing downstream work curate data next processing step. choose save CSV data/original/ directory project create sub-directory called cedel2/.resources inevitably include unique processes obtaining data, end data archived research structure data/original/ directory treated ‘read-’.","code":"data/\n├── derived\n└── original\n    └── cedel2\n       └── texts.csv"},{"path":"acquire-data-chapter.html","id":"programmatic","chapter":"5 Acquire data","heading":"5.1.2 Programmatic","text":"many resources provide corpus data directly accessible programmatic approaches can applied. Let’s take look works starting sample Switchboard Corpus, corpus 2,400 telephone conversations 543 speakers. First navigate site browser download file looking . case found Switchboard Corpus NLTK data repository site. often file type compressed archive file extension .zip .tz, case . Archive files make downloading large single files multiple files easy grouping files directories one file. R can used download.file() function base R library14. number arguments function may require provide optionally. download.file() function minimally requires two: url destfile. file download location saved disk.can see looking directory structure data/ switchboard.zip file downloaded.archive file downloaded, however, file needs ‘decompressed’ reveal file structure. decompress file use unzip() function arguments zipfile pointing .zip file exdir specifying directory want files extracted .directory structure data/ now look like :point acquired data programmatically code part workflow anyone run code reproduce results. code , however, ideally efficient. Firstly switchboard.zip file strictly needed decompress occupies disk space keep . second, time run code file downloaded remote serve leading unnecessary data transfer server traffic. Let’s tackle issues turn.avoid writing switchboard.zip file disk (long-term) can use tempfile() function open temporary holding space file. space can used store file, unzip , temporary file destroyed. assign temporary space R object name temp tempfile() function. object can now used value argument destfile download.file() function. Let’s also assign web address another object url use value url argument.\nprevious code ’ve used values stored objects url temp download.file() function without specifying argument names –providing names objects. R assume values function map ordering arguments. values map ordering arguments required specify argument name value. view ordering objects hit TAB entering function name consult function documentation prefixing function name ? hitting ENTER.\npoint downloaded file stored temporarily disk can accessed decompressed target directory using temp value argument zipfile unzip() function. ’ve assigned target directory path target_dir used value argument exdir prepare us next tweak approach.directory structure now looks like :second issue raised concerns fact running code part project repeat download time. Since like good citizens avoid unnecessary traffic web nice code checked see already data disk exists, skip download, download .achieve need introduce two new functions () dir.exists(). dir.exists() takes path directory argument returns logical value, TRUE, directory exists, FALSE . () evaluates logical statements processes subsequent code based logical value passed argument. Let’s look toy example.assigned num value 1 created logical evaluation num == whose result passed argument (). statement returns TRUE code withing first set curly braces {...} run. num == 1 false, like code , code withing braces following else run.function () one various functions called control statements. Theses functions provide lot power make dynamic choices code run.get back key objective avoid downloading resources already disk, let introduce another strategy making code powerful ultimately efficient well legible –custom function. Custom functions functions user writes create set procedures can run similar contexts. ’ve created custom function named eval_num() .Let’s take closer look ’s going . function function() creates function user decides arguments necessary code perform task. case necessary argument object store numeric value evaluated. ’ve called num reflects name object toy example, nothing special name. ’s important object names consistently used. ’ve included previous code (except hard-coded assignment num) inside curly braces assigned entire code chunk eval_num.can now use function eval_num() perform task evaluating whether value num equal 1.’ve put coding strategies together previous code custom function named get_zip_data(). lot going . Take look first see can follow logic involved given now know.OK. recognized general steps function: argument url target_dir specify get data write decompressed files, () statement evaluates whether data already exists, (!dir.exists(target_dir)) data downloaded decompressed, exist (else) downloaded.\nprefixed ! logical expression dir.exists(target_dir) returns opposite logical value. needed case target directory exists, expression return FALSE, TRUE, therefore proceed downloading resource.\ncouple key tweaks ’ve added provide additional functionality. one ’ve included function dir.create() create target directory data written. ’ve also added additional argument unzip() function, junkpaths = TRUE. Together additions allow user create arbitrary directory path files, files, extracted disk. discard containing directory .zip file can helpful want add multiple .zip files target directory.practical scenario applies want download data corpus contained multiple .zip files still maintain files single primary data directory. Take example Santa Barbara Corpus. corpus resource includes series interviews one .zip file, SBCorpus.zip contains transcribed interviews another .zip file, metadata.zip organizes meta-data associated speaker. Applying initial strategy download decompress data lead following directory structure:applying new custom function get_zip_data() transcriptions meta-data can better organize data.add data sources can keep logical separate allow data collection scale without creating unnecessary complexity. Let’s add Switchboard Corpus sample using get_zip_data() function see action.point need continue next step data analysis project. go, housekeeping document organize process make work reproducible. take advantage project-template directory structure, seen .First good practice separate custom functions processing scripts. can create file functions/ directory named acquire_functions.R add custom function get_zip_data() .\nNote acquire_functions.R file R script, Rmarkdown document. Therefore code chunks used .Rmd files used, R code .\nuse source() function read function current script make available use needed. good practice source functions SETUP section script.section, sum , ’ve covered access, download, organize data contained .zip files; common format language data found repositories individual sites. included introduction key R programming concepts strategies including using functions, writing custom functions, controlling program flow control statements. approach gather data also keeping mind reproducibility code. end introduced programming strategies avoiding unnecessary web traffic (downloads), scalable directory creation, data documentation.\ncustom function get_zip_data() works .zip files. many compressed file formats (e.g. .gz, .tar, .tgz), however. R package tadr accompanies coursebook, modified version get_zip_data() function, get_compressed_data(), extends logic deal wider range compressed file formats, including .zip files.\n\nExplore function’s documentation (?tadr::get_compressed_data()) / view code (tadr::get_compressed_data) better understand function.\n","code":"\n# Download .zip file and write to disk\ndownload.file(url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip\",\n    destfile = \"../data/original/switchboard.zip\")data\n├── derived\n└── original\n    └── switchboard.zip\n# Decompress .zip file and extract to our target directory\nunzip(zipfile = \"../data/original/switchboard.zip\", exdir = \"../data/original/\")data\n├── derived\n└── original\n    ├── switchboard\n    │   ├── README\n    │   ├── discourse\n    │   ├── disfluency\n    │   ├── tagged\n    │   ├── timed-transcript\n    │   └── transcript\n    └── switchboard.zip\n# Create a temporary file space for our .zip file\ntemp <- tempfile()\n# Assign our web address to `url`\nurl <- \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip\"\n# Download .zip file and write to disk\ndownload.file(url, temp)\n# Assign our target directory to `target_dir`\ntarget_dir <- \"../data/original/\"\n# Decompress .zip file and extract to our target directory\nunzip(zipfile = temp, exdir = target_dir)data\n├── derived\n└── original\n    └── switchboard\n        ├── README\n        ├── discourse\n        ├── disfluency\n        ├── tagged\n        ├── timed-transcript\n        └── transcript\nnum <- 1\nif (num == 1) {\n    cat(num, \"is 1\")\n} else {\n    cat(num, \"is not 1\")\n}\n#> 1 is 1\nnum <- 2\nif (num == 1) {\n    cat(num, \"is 1\")\n} else {\n    cat(num, \"is not 1\")\n}\n#> 2 is not 1\neval_num <- function(num) {\n    if (num == 1) {\n        cat(num, \"is 1\")\n    } else {\n        cat(num, \"is not 1\")\n    }\n}\neval_num(num = 1)\n#> 1 is 1\neval_num(num = 2)\n#> 2 is not 1\neval_num(num = 3)\n#> 3 is not 1\nget_zip_data <- function(url, target_dir) {\n    # Function: to download and decompress a .zip file to a target directory\n\n    # Check to see if the data already exists if data does not exist, download/\n    # decompress\n    if (!dir.exists(target_dir)) {\n        cat(\"Creating target data directory \\n\")  # print status message\n        dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE)  # create target data directory\n        cat(\"Downloading data... \\n\")  # print status message\n        temp <- tempfile()  # create a temporary space for the file to be written to\n        download.file(url = url, destfile = temp)  # download the data to the temp file\n        unzip(zipfile = temp, exdir = target_dir, junkpaths = TRUE)  # decompress the temp file in the target directory\n        cat(\"Data downloaded! \\n\")  # print status message\n    } else {\n        # if data exists, don't download it again\n        cat(\"Data already exists \\n\")  # print status message\n    }\n}data\n├── derived\n└── original\n    ├── SBCorpus\n    │   ├── TRN\n    │   └── __MACOSX\n    │       └── TRN\n    └── metadata\n        └── __MACOSX\n# Download corpus transcriptions\nget_zip_data(url = \"http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip\",\n    target_dir = \"../data/original/sbc/transcriptions/\")\n\n# Download corpus meta-data\nget_zip_data(url = \"http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip\",\n    target_dir = \"../data/original/sbc/meta-data/\")data\n├── derived\n└── original\n    └── sbc\n        ├── meta-data\n        └── transcriptions\n# Download corpus\nget_zip_data(url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip\",\n    target_dir = \"../data/original/scs/\")data\n├── derived\n└── original\n    ├── sbc\n    │   ├── meta-data\n    │   └── transcriptions\n    └── scs\n        ├── README\n        ├── discourse\n        ├── disfluency\n        ├── tagged\n        ├── timed-transcript\n        └── transcript├── README.md\n├── _pipeline.R\n├── analysis\n│   ├── 1_acquire_data.Rmd\n│   ├── 2_curate_dataset.Rmd\n│   ├── 3_transform_dataset.Rmd\n│   ├── 4_analyze_dataset.Rmd\n│   ├── 5_generate_article.Rmd\n│   ├── _session-info.Rmd\n│   ├── _site.yml\n│   ├── index.Rmd\n│   └── references.bib\n├── data\n│   ├── derived\n│   └── original\n│       ├── sbc\n│       └── scs\n├── functions\n└── output\n    ├── figures\n    └── results\n# Load custom functions for this project\nsource(file = \"../functions/acquire_functions.R\")"},{"path":"acquire-data-chapter.html","id":"apis","chapter":"5 Acquire data","heading":"5.2 APIs","text":"convenient alternative method acquiring data R package interfaces web services. interfaces built using R code make connections resources web Application Programming Interfaces (APIs). Websites Project Gutenberg, Twitter, Facebook, many others provide APIs allow access data certain conditions, limiting data collection others. Programmers (like !) R community take task wrapping calls API R code make accessing data R possible. example, gutenbergr provides access Project Gutenberg, rtweet Twitter, Rfacebook Facebook.15","code":""},{"path":"acquire-data-chapter.html","id":"open-access","chapter":"5 Acquire data","heading":"5.2.1 Open access","text":"Using R package interfaces, however, often requires knowledge R objects functions. Let’s take look access data Project Gutenberg gutenbergr package. Along way touch upon various functions concepts key working R data types vectors data frames including filtering writing tabular data disk plain-text format.get started let’s install / load gutenbergr package. package part R base library, assume user package library. standard approach installing loading package using install.packages() function calling library().approach works just fine, luck R package installing loading packages! pacman package includes set functions managing packages. useful one p_load() look package system, load found, install load found. helps potentially avoid using unnecessary bandwidth install packages may already exist user’s system. , use pacman need include code install load functions install.packages() library(). ’ve included code mimic behavior p_load() installing pacman , can see elegant, luckily ’s used add SETUP section master file, _pipeline.R.Now pacman installed loaded R session, let’s use p_load() function make sure install/ load two packages need upcoming tasks. following along project_template, add code within SETUP section 1_acquire_data.Rmd file.\nNote arguments tidyverse gutenbergr comma-separated quoted using p_load(). using install.packages() install, package names need quoted (character strings). library() can take quotes quotes, one package time.\nProject Gutenberg provides access thousands texts public domain. gutenbergr package contains set tables, data frames R speak, index meta-data texts broken text (gutenberg_metadata), author (gutenberg_authors), subject (gutenberg_subjects). ’ll use glimpse() function loaded tidyverse package 16 summarize structure data frames.gutenberg_metadata, gutenberg_authors, gutenberg_subjects periodically updated. check see data frame last updated run:attr(gutenberg_metadata, \"date_updated\")download text use gutenberg_download() function takes one required argument, gutenberg_id. gutenberg_download() function known ‘vectorized’, , can take single value multiple values argument gutenberg_id. Vectorization refers process applying function elements stored vector –primary object type R. vector grouping values one various types including character (chr), integer (int), double (dbl), logical (lgl) data frame grouping vectors. gutenberg_download() function takes integer vector can manually added selected gutenberg_metadata gutenberg_subjects data frames using $ operator (e.g. gutenberg_metadata$gutenberg_id).Let’s first add manually toy example generating vector integers 1 5 assigned variable name ids.download works Project Gutenberg corresponding gutenberg_ids 1 5, pass ids object gutenberg_download() function.Two attributes returned: gutenberg_id text. text column contains values line text (delimited carriage return) 5 works downloaded. many attributes available Project Gutenberg API can accessed passing character vector attribute names argument meta_fields. column names gutenberg_metadata data frame contains available attributes.Let’s augment previous download title author works. create character vector use c() function, , quote delimit individual elements vector comma.Now, practical scenario like select values gutenberg_id principled query works specific author, language, subject. first query either gutenberg_metadata data frame gutenberg_subjects data frame. Let’s say want download random sample 10 works English Literature (Library Congress Classification, “PR”). Using dplyr::filter() function (dplyr part tidyverse package set) first extract Gutenberg ids gutenberg_subjects subject_type == \"lcc\" subject == \"PR\" assigning result ids.17\noperators = == equivalents. == used logical evaluation = alternate notation variable assignment (<-).\ngutenberg_subjects data frame contain information whether gutenberg_id associated plain-text version. limit query English Literature works text, filter gutenberg_metadata data frame ids selected ids attribute has_text gutenberg_metadata data frame.can see number works text fewer number works listed, 7100 versus 6724. Now can safely random selection 10 works, function slice_sample() confident ids select contain text take next step downloading data.point data move processing dataset preparation analysis. However, aiming reproducible workflow code conform principle modularity: subsequent step analysis depend running code first. Furthermore, running code creates issues bandwidth, previous examples direct downloads. address modularity write dataset disk plain-text format. way subsequent step analysis can access dataset locally. address bandwidth concerns, devise method checking see dataset already downloaded skip download, possible, avoid accessing Project Gutenberg server unnecessarily.write data frame disk export standard plain-text format two-dimensional datasets: CSV file (comma-separated value). CSV structure dataset look like :first line contains names columns subsequent lines observations. Data points contain commas (e.g. “Shaw, Bernard”) quoted avoid misinterpreting commas deliminators data. write dataset disk use reader::write_csv() function.avoid downloading dataset already resides disk, let’s implement similar strategy one used direct downloads (get_zip_data()). ’ve incorporated code sampling downloading data particular subject Project Gutenberg control statement check dataset file already exists function named get_gutenberg_subject(). Take look function .Adding function function script functions/acquire_functions.R, can now source function analysis/1_acquire_data.Rmd script download multiple subjects store disk file.Let’s download American Literature now (LCC code “PQ”).Applying function English American Literature datasets, data directory structure now looks like :","code":"\ninstall.packages(\"gutenbergr\")  # install `gutenbergr` package\nlibrary(gutenbergr)  # load the `gutenbergr` package\n# Load `pacman`. If not installed, install then load.\nif (!require(\"pacman\", character.only = TRUE)) {\n    install.packages(\"pacman\")\n    library(\"pacman\", character.only = TRUE)\n}\n# Script-specific options or packages\npacman::p_load(tidyverse, gutenbergr)\nglimpse(gutenberg_metadata)  # summarize text meta-data\n#> Rows: 51,997\n#> Columns: 8\n#> $ gutenberg_id        <int> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …\n#> $ title               <chr> NA, \"The Declaration of Independence of the United…\n#> $ author              <chr> NA, \"Jefferson, Thomas\", \"United States\", \"Kennedy…\n#> $ gutenberg_author_id <int> NA, 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7,…\n#> $ language            <chr> \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n#> $ gutenberg_bookshelf <chr> NA, \"United States Law/American Revolutionary War/…\n#> $ rights              <chr> \"Public domain in the USA.\", \"Public domain in the…\n#> $ has_text            <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\nglimpse(gutenberg_authors)  # summarize authors meta-data\n#> Rows: 16,236\n#> Columns: 7\n#> $ gutenberg_author_id <int> 1, 3, 4, 5, 7, 8, 9, 10, 12, 14, 16, 17, 18, 20, 2…\n#> $ author              <chr> \"United States\", \"Lincoln, Abraham\", \"Henry, Patri…\n#> $ alias               <chr> NA, NA, NA, NA, \"Dodgson, Charles Lutwidge\", NA, \"…\n#> $ birthdate           <int> NA, 1809, 1736, NA, 1832, NA, 1819, 1860, 1805, 17…\n#> $ deathdate           <int> NA, 1865, 1799, NA, 1898, NA, 1891, 1937, 1844, 18…\n#> $ wikipedia           <chr> NA, \"http://en.wikipedia.org/wiki/Abraham_Lincoln\"…\n#> $ aliases             <chr> NA, \"United States President (1861-1865)/Lincoln, …\nglimpse(gutenberg_subjects)  # summarize subjects meta-data\n#> Rows: 140,173\n#> Columns: 3\n#> $ gutenberg_id <int> 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, …\n#> $ subject_type <chr> \"lcc\", \"lcsh\", \"lcsh\", \"lcc\", \"lcc\", \"lcsh\", \"lcsh\", \"lcc…\n#> $ subject      <chr> \"E201\", \"United States. Declaration of Independence\", \"Un…\nids <- 1:5  # integer vector of values 1 to 5\nids\n#> [1] 1 2 3 4 5\nworks_sample <- gutenberg_download(gutenberg_id = ids)  # download works with `gutenberg_id` 1-5\nglimpse(works_sample)  # summarize `works` dataset\n#> Rows: 2,959\n#> Columns: 2\n#> $ gutenberg_id <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#> $ text         <chr> \"December, 1971  [Etext #1]\", \"\", \"\", \"The Project Gutenb…\nnames(gutenberg_metadata)  # print the column names of the `gutenberg_metadata` data frame\n#> [1] \"gutenberg_id\"        \"title\"               \"author\"             \n#> [4] \"gutenberg_author_id\" \"language\"            \"gutenberg_bookshelf\"\n#> [7] \"rights\"              \"has_text\"\n# download works with `gutenberg_id` 1-5 including `title` and `author` as\n# attributes\nworks_sample <- gutenberg_download(gutenberg_id = ids, meta_fields = c(\"title\", \"author\"))\nglimpse(works_sample)  # summarize dataset\n#> Rows: 2,959\n#> Columns: 4\n#> $ gutenberg_id <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#> $ text         <chr> \"December, 1971  [Etext #1]\", \"\", \"\", \"The Project Gutenb…\n#> $ title        <chr> \"The Declaration of Independence of the United States of …\n#> $ author       <chr> \"Jefferson, Thomas\", \"Jefferson, Thomas\", \"Jefferson, Tho…\n# filter for only English literature\nids <- filter(gutenberg_subjects, subject_type == \"lcc\", subject == \"PR\")\nglimpse(ids)\n#> Rows: 7,100\n#> Columns: 3\n#> $ gutenberg_id <int> 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43, 46, 58, 60, 8…\n#> $ subject_type <chr> \"lcc\", \"lcc\", \"lcc\", \"lcc\", \"lcc\", \"lcc\", \"lcc\", \"lcc\", \"…\n#> $ subject      <chr> \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR…\n# Filter for only those works that have text\nids_has_text <- \n  filter(gutenberg_metadata, \n         gutenberg_id %in% ids$gutenberg_id, \n         has_text == TRUE)\nglimpse(ids_has_text)\n#> Rows: 6,724\n#> Columns: 8\n#> $ gutenberg_id        <int> 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43, 46, 58…\n#> $ title               <chr> \"Alice's Adventures in Wonderland\", \"Through the L…\n#> $ author              <chr> \"Carroll, Lewis\", \"Carroll, Lewis\", \"Carroll, Lewi…\n#> $ gutenberg_author_id <int> 7, 7, 7, 10, 17, 17, 23, 30, 30, 35, 35, 37, 17, 4…\n#> $ language            <chr> \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n#> $ gutenberg_bookshelf <chr> \"Children's Literature\", \"Children's Literature/Be…\n#> $ rights              <chr> \"Public domain in the USA.\", \"Public domain in the…\n#> $ has_text            <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\nset.seed(123)  # make the sampling reproducible\nids_sample <- slice_sample(ids_has_text, n = 10)  # sample 10 works\nglimpse(ids_sample)  # summarize the dataset\n#> Rows: 10\n#> Columns: 8\n#> $ gutenberg_id        <int> 10564, 10784, 9316, 1540, 24450, 13821, 7595, 3818…\n#> $ title               <chr> \"Fairy Gold\\nShip's Company, Part 4.\", \"Sentence D…\n#> $ author              <chr> \"Jacobs, W. W. (William Wymark)\", \"Jacobs, W. W. (…\n#> $ gutenberg_author_id <int> 1865, 1865, 2364, 65, 999, 2685, 761, 1317, 3564, …\n#> $ language            <chr> \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n#> $ gutenberg_bookshelf <chr> NA, NA, NA, NA, \"Adventure\", \"Fantasy\", NA, NA, NA…\n#> $ rights              <chr> \"Public domain in the USA.\", \"Public domain in the…\n#> $ has_text            <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\nworks_pr <- gutenberg_download(gutenberg_id = ids_sample$gutenberg_id, meta_fields = c(\"author\",\n    \"title\"))\nglimpse(works_pr)  # summarize the dataset\n#> Rows: 47,515\n#> Columns: 4\n#> $ gutenberg_id <int> 1540, 1540, 1540, 1540, 1540, 1540, 1540, 1540, 1540, 154…\n#> $ text         <chr> \"cover \", \"\", \"\", \"\", \"THE TEMPEST\", \"\", \"\", \"\", \"by Will…\n#> $ author       <chr> \"Shakespeare, William\", \"Shakespeare, William\", \"Shakespe…\n#> $ title        <chr> \"The Tempest\", \"The Tempest\", \"The Tempest\", \"The Tempest…\nworks_pr %>%\n    head() %>%\n    format_csv() %>%\n    cat()\n#> gutenberg_id,text,author,title\n#> 1540,cover ,\"Shakespeare, William\",The Tempest\n#> 1540,,\"Shakespeare, William\",The Tempest\n#> 1540,,\"Shakespeare, William\",The Tempest\n#> 1540,,\"Shakespeare, William\",The Tempest\n#> 1540,THE TEMPEST,\"Shakespeare, William\",The Tempest\n#> 1540,,\"Shakespeare, William\",The Tempest\nwrite_csv(works_pr, file = \"../data/original/gutenberg_works_pr.csv\")\nget_gutenberg_subject <- function(subject, target_file, sample_size = 10) {\n  # Function: to download texts from Project Gutenberg with \n  # a specific LCC subject and write the data to disk.\n  \n  pacman::p_load(tidyverse, gutenbergr) # install/load necessary packages\n  \n  # Check to see if the data already exists\n  if(!file.exists(target_file)) { # if data does not exist, download and write\n    target_dir <- dirname(target_file) # generate target directory for the .csv file\n    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE) # create target data directory\n    cat(\"Downloading data... \\n\") # print status message\n    # Select all records with a particular LCC subject\n    ids <- \n      filter(gutenberg_subjects, \n             subject_type == \"lcc\", subject == subject) # select subject\n    # Select only those records with plain text available\n    set.seed(123) # make the sampling reproducible\n    ids_sample <- \n      filter(gutenberg_metadata, \n             gutenberg_id %in% ids$gutenberg_id, # select ids in both data frames \n             has_text == TRUE) %>% # select those ids that have text\n      slice_sample(n = sample_size) # sample N works \n    # Download sample with associated `author` and `title` metadata\n    works_sample <- \n      gutenberg_download(gutenberg_id = ids_sample$gutenberg_id, \n                         meta_fields = c(\"author\", \"title\"))\n    # Write the dataset to disk in .csv format\n    write_csv(works_sample, file = target_file)\n    cat(\"Data downloaded! \\n\") # print status message\n  } else { # if data exists, don't download it again\n    cat(\"Data already exists \\n\") # print status message\n  }\n}\n# Download Project Gutenberg text for subject 'PQ' (American Literature) and\n# then write this dataset to disk in .csv format\nget_gutenberg_subject(subject = \"PQ\", target_file = \"../data/original/gutenberg/works_pq.csv\")data\n├── derived\n└── original\n    ├── gutenberg\n    │   ├── works_pq.csv\n    │   └── works_pr.csv\n    ├── sbc\n    │   ├── meta-data\n    │   └── transcriptions\n    └── scs\n        ├── README\n        ├── discourse\n        ├── disfluency\n        ├── documentation\n        ├── tagged\n        ├── timed-transcript\n        └── transcript"},{"path":"acquire-data-chapter.html","id":"authentication","chapter":"5 Acquire data","heading":"5.2.2 Authentication","text":"APIs R interfaces provide access require authentication. may either interactive process mediated R web service / visiting developer website particular API. either case, extra step necessary make connect API access data.Let’s take look popular micro-blogging platform Twitter. rtweet package (Kearney, 2020) provides access tweets various ways. get started install /load rtweet package.Now researcher can access data Twitter rtweet, authentication token must setup made accessible. following steps setting authentication token saving , token can accessed auth_as() function.Now R session authenticated, can explore popular method querying Twitter API searchs tweets (search_tweets) posted recent past (6-9 days).Let’s look typical query using search_tweets() function.Looking arguments function, see ’ve specified query term ‘latinx’. single word query query included multiple words, spaces interpreted logical (match tweets individual terms). one like include multi-word expressions, expressions enclosed single quotes (.e. q = \"'spanish speakers' latinx\"). Another approach include logical (match tweets either terms). Multi-word expressions can included previous case. note, hashtags acceptable terms, q = \"#latinx\" match tweets hashtag.number results set ‘100’, default, left . can increase number desired tweets. rate limits cap number tweets can access given 15-minute time period.Another argument importance type argument. argument three possible attributes popular, recent, mixed. popular attribute Twitter API tend return fewer tweets specified n. recent mixed likely get n specified (note mixed mix popular recent).final argument note include_rts whose attribute logical. FALSE retweets included results. often language researcher want.Now, search_tweets query run, large number variables included resulting data frame. ’s overview names variables vector types variable.Table 5.1: Variables variable types returned Twitter API via rtweet’s search_tweets() function.Twitter API documentation standard Search Tweets call, search_tweets() interfaces quite variables (35 exact). many purposes necessary keep variables. Furthermore, since want write plain-text file disk part project, need either convert eliminate variables marked type list. common variable convert coordinates variable, contain geolocation codes Twitter users’ tweets captured query geolocation enabled device. note, however, using search_tweets() without specifying tweets geocodes captured (geocode =) tend return , , tweets geolocation information majority Twitter users geolocation enabled.Let’s assume want keep variables type list. One option use select() name variable want keep. hand can use combination select() negated !() select variables lists (is_list). Let’s later approach.Now 30 variables can written disk plain-text file. Let’s go ahead , wrap function work ’ve just laid one function. addition check see query run, skip running query dataset disk.Let’s run function query .appropriate directory structure file written disk.sum, subsection provided overview acquiring data web service APIs R packages. took closer look gutenbergr package provides programmatic access works available Project Gutenberg rtweet package provides authenticated access Twitter. Working package interfaces requires knowledge R including loading/ installing packages, working vectors data frames, exporting data R session. touched programming concepts also outlined method create reproducible workflow.","code":"\npacman::p_load(rtweet)  # install/load rtweet package\nauth_as(twitter_auth)  # load the saved `twitter_auth` token\nrt_latinx <- \n  search_tweets(q = \"latinx\", # query term\n                n = 100, # number of tweets desired\n                type = \"mixed\", # a mix of `recent` and `popular` tweets\n                include_rts = FALSE) # do not include RTs\nrt_latinx_subset <- \n  rt_latinx %>% # dataset\n  select(!where(is_list))  # select all variables that are NOT lists\n\nrt_latinx_subset %>% # subsetted dataset\n  glimpse() # overview\n#> Rows: 100\n#> Columns: 30\n#> $ created_at                    <chr> \"Sun Sep 26 17:38:06 +0000 2021\", \"Sun S…\n#> $ id                            <dbl> 1.44e+18, 1.44e+18, 1.44e+18, 1.44e+18, …\n#> $ id_str                        <chr> \"1442181701967302659\", \"1442196629801488…\n#> $ full_text                     <chr> \"If we call it Latinx Mass they can't ca…\n#> $ truncated                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n#> $ display_text_range            <dbl> 57, 177, 166, 23, 261, 153, 202, 211, 57…\n#> $ source                        <chr> \"<a href=\\\"https://mobile.twitter.com\\\" …\n#> $ in_reply_to_status_id         <dbl> NA, NA, NA, 1.44e+18, NA, NA, NA, NA, 1.…\n#> $ in_reply_to_status_id_str     <chr> NA, NA, NA, \"1437436224042635269\", NA, N…\n#> $ in_reply_to_user_id           <dbl> NA, NA, NA, 4.26e+08, NA, NA, NA, NA, 2.…\n#> $ in_reply_to_user_id_str       <chr> NA, NA, NA, \"426159377\", NA, NA, NA, NA,…\n#> $ in_reply_to_screen_name       <chr> NA, NA, NA, \"MorganStanley\", NA, NA, NA,…\n#> $ geo                           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ contributors                  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ is_quote_status               <lgl> FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n#> $ retweet_count                 <int> 351, 124, 62, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n#> $ favorite_count                <int> 3902, 898, 280, 0, 0, 0, 0, 0, 0, 7, 0, …\n#> $ favorited                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n#> $ retweeted                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n#> $ lang                          <chr> \"en\", \"en\", \"es\", \"en\", \"en\", \"en\", \"en\"…\n#> $ possibly_sensitive            <lgl> NA, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n#> $ quoted_status_id              <dbl> NA, NA, NA, NA, 1.44e+18, NA, NA, NA, NA…\n#> $ quoted_status_id_str          <chr> NA, NA, NA, NA, \"1442475408058830856\", N…\n#> $ text                          <chr> \"If we call it Latinx Mass they can't ca…\n#> $ favorited_by                  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ display_text_width            <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ retweeted_status              <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ quoted_status_permalink       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ query                         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ possibly_sensitive_appealable <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\nwrite_search_tweets <- \n  function(query, path, n = 100, type = \"mixed\", include_rts = FALSE) {\n    # Function\n    # Conduct a Twitter search query and write the results to a csv file\n    \n    if(!file.exists(path)) { # check to see if the file already exists\n      cat(\"File does not exist \\n\") # message\n      \n      library(rtweet) # to use Twitter API\n      library(tidyverse) # to manipulate data\n      \n      auth_get() # get authentication token\n      \n      results <- # query results\n        search_tweets(q = query, # query term\n                      n = n, # number of tweets desired (default 100)\n                      type = type, # type of query\n                      include_rts = include_rts) %>%  # to include RTs\n        select(!where(is_list))  # remove list variables\n      \n      if(!dir.exists(dirname(path))) { # isolate directory and check if exists\n        cat(\"Creating directory \\n\") # message\n        \n        dir.create(path = dirname(path), # isolate and create directory (remove file name)\n                   recursive = TRUE, # create embedded directories if necessary\n                   showWarnings = FALSE) # do not report warnings\n      }\n      \n      write_csv(x = results, file = path) # write results to csv file \n      cat(\"Twitter search results written to disk \\n\") # message\n      \n    } else {\n      cat(\"File already exists! \\n\") # message\n    }\n  }\nwrite_search_tweets(query = \"latinx\", path = \"../data/original/twitter/rt_latinx.csv\")data/original/twitter/\n└── rt_latinx.csv"},{"path":"acquire-data-chapter.html","id":"web-scraping","chapter":"5 Acquire data","heading":"5.3 Web scraping","text":"many resources available manula direct downloads repositories individual sites R package interfaces web resources APIs, resources relatively limited amount public-facing textual data recorded web. case want acquire data webpages, R can used access web programmatically process known web scraping. complexity web scrapes can vary general requires advanced knowledge R well structure language web: HTML (Hypertext Markup Language).","code":""},{"path":"acquire-data-chapter.html","id":"a-toy-example","chapter":"5 Acquire data","heading":"5.3.1 A toy example","text":"HTML cousin XML (eXtensible Markup Language) organizes web documents hierarchical format read browser navigate web. Take example toy webpage created demonstration Figure 5.3.\nFigure 5.3: Example web page.\nfile accessed browser render webpage test.html plain-text format looks like :element file delineated opening closing tag, <head><\/head>. Tags nested within tags create structural hierarchy. Tags can take class id labels distinguish tags often contain attributes dictate tag behave rendered visually browser. example, two <div> tags toy example: one label class = \"intro\" class = \"conc\". <div> tags often used separate sections webpage may require special visual formatting. <> tag, hand, creates web link. part tag’s function, requires attribute href= web protocol –case link email address mailto:francojc@wfu.edu. often , however, href= contains URL (Uniform Resource Locator). working example might look like : <href=\"https://francojc.github.io/\">homepage<\/>.aim web scrape download HTML file, parse document structure, extract elements containing relevant information wish capture. Let’s attempt extract information toy example. need rvest(Wickham, 2021) package. First, install/load package, , read parse HTML character vector named web_file assigning result html.read_html() parses raw HTML object class xml_document. summary output shows tags HTML structure parsed ‘elements’. tag elements can accessed using html_elements() function specifying tag isolate.Notice html_elements(\"div\") returned div tags. isolate one tags class, add class name tag separating ..Great. Now say want drill isolate subordinate <p> nodes. can add p node filter.extract text contained within node use html_text() function.result character vector two elements corresponding text contained <p> tag. paying close attention might noticed second element vector includes extra whitespace period. trim leading trailing whitespace text can add trim = TRUE argument html_text().work organize text format want store write results disk. Let’s leave writing data disk later chapter. now keep focus working rvest acquire data html documents working practical example.","code":"\n<html>\n  <head>\n    <title>My website<\/title>\n  <\/head>\n  <body>\n    <div class=\"intro\">\n      <p>Welcome!<\/p>\n      <p>This is my first website. <\/p>\n    <\/div>\n    <table>\n      <tr>\n        <td>Contact me:<\/td>\n        <td>\n          <a href=\"mailto:francojc@wfu.edu\">francojc@wfu.edu<\/a>\n        <\/td>\n      <\/tr>\n    <\/table>\n    <div class=\"conc\">\n      <p>Good-bye!<\/p>\n    <\/div>\n  <\/body>\n<\/html>\npacman::p_load(rvest)  # install/ load `rvest`\n\nhtml <- read_html(web_file)  # read raw html and parse to xml\nhtml\n#> {html_document}\n#> <html>\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n#> [2] <body>\\n    <div class=\"intro\">\\n      <p>Welcome!<\/p>\\n      <p>This is  ...\nhtml %>%\n    html_elements(\"div\")\n#> {xml_nodeset (2)}\n#> [1] <div class=\"intro\">\\n      <p>Welcome!<\/p>\\n      <p>This is my first web ...\n#> [2] <div class=\"conc\">\\n      <p>Good-bye!<\/p>\\n    <\/div>\nhtml %>%\n    html_elements(\"div.intro\")\n#> {xml_nodeset (1)}\n#> [1] <div class=\"intro\">\\n      <p>Welcome!<\/p>\\n      <p>This is my first web ...\nhtml %>%\n    html_elements(\"div.intro p\")\n#> {xml_nodeset (2)}\n#> [1] <p>Welcome!<\/p>\n#> [2] <p>This is my first website. <\/p>\nhtml %>%\n    html_elements(\"div.intro p\") %>%\n    html_text()\n#> [1] \"Welcome!\"                   \"This is my first website. \"\nhtml %>%\n    html_elements(\"div.intro p\") %>%\n    html_text(trim = TRUE)\n#> [1] \"Welcome!\"                  \"This is my first website.\""},{"path":"acquire-data-chapter.html","id":"a-practical-example","chapter":"5 Acquire data","heading":"5.3.2 A practical example","text":"basic understanding HTML use rvest package, let’s turn realistic example. Say want acquire lyrics online music website database last.fm. first step web scrape investigate site page(s) want scrape ascertain licensing restrictions. Many, websites, include plain text file robots.txt root main URL. file declares webpages ‘robot’ (including web scraping scripts) can access. can use robotstxt package find URLs accessible 18.next step includes identifying URL want target exploring structure HTML document. Take following webpage identified, seen Figure 5.4.\nFigure 5.4: Lyrics page last.fm\ntoy example, first want feed HTML web address read_html() function parse tags elements. assign result html.point captured parsed raw HTML assigning object named html. next step identify html elements contain information want extract page. helpful use browser inspect specific elements webpage. browser equipped command can enable hovering mouse element page want target using right click select “Inspect” (Chrome) “Inspect Element” (Safari, Brave). split browser window vertical horizontally showing raw HTML underlying webpage.\nFigure 5.5: Using “Inspect Element” command explore raw html.\nFigure 5.6 see element want target contained within <><\/> tag. Now tag common don’t want extract every use class header-new-crumb specify want artist name. Using convention described toy example, can isolate artist lyrics page.can extract text html_text().Let’s extract song title way.Now inspect HTML lyrics page, notice lyrics contained <p><\/p> tags class lyrics-paragraph.\nFigure 5.6: Using “Inspect Element” command explore raw html.\nSince multiple elements want extract, need use html_elements() function instead html_element() targets one element.point, isolated extracted artist, song, lyrics webpage. elements stored character vectors R session. complete task need write data disk plain text. eye towards tidy dataset, ideal format store data CSV file column corresponds one elements scrape row observation. CSV file tabular format can write data disk let’s coerce data tabular format. use tibble() function streamline data frame creation. 19 Feeding vectors artist, song, lyrics arguments tibble() creates tabular format looking .Notice seven rows data frame, one corresponding paragraph lyrics. R bias towards working vectors length. vectors (artist, song) replicated, recycled, length longest vector lyrics, length seven.good documentation let’s add object lyrics_url data frame, contains actual web link page, assign result song_lyrics.final step write data disk. use write_csv() function.","code":"\npacman::p_load(robotstxt)  # load/ install `robotstxt`\n\npaths_allowed(paths = \"https://www.last.fm/\")  # check permissions\n#> [1] TRUE\n# read and parse html as an xml object\nlyrics_url <- \"https://www.last.fm/music/Radiohead/_/Karma+Police/+lyrics\"\nhtml <- read_html(lyrics_url)  # read raw html and parse to xml\nhtml#> {html_document}\n#> <html lang=\"en\" class=\"\n#>         no-js\n#>         playbar-masthead-release-shim\n#>         youtube-provider-not-ready\n#>     \">\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n#> [2] <body>\\n<div id=\"initial-tealium-data\" data-require=\"tracking/tealium-uta ...\nhtml %>%\n    html_element(\"a.header-new-crumb\")\n#> {html_node}\n#> <a class=\"header-new-crumb\" itemprop=\"url\" href=\"/music/Radiohead\">\n#> [1] <span itemprop=\"name\">Radiohead<\/span>\nartist <- html %>%\n    html_element(\"a.header-new-crumb\") %>%\n    html_text()\nartist\n#> [1] \"Radiohead\"\nsong <- html %>%\n    html_element(\"h1.header-new-title\") %>%\n    html_text()\nsong\n#> [1] \"Karma Police\"\nlyrics <- html %>%\n    html_elements(\"p.lyrics-paragraph\") %>%\n    html_text()\nlyrics\n#> [1] \"Karma policeArrest this manHe talks in mathsHe buzzes like a fridgeHe's like a detuned radio\"      \n#> [2] \"Karma policeArrest this girlHer Hitler hairdoIs making me feel illAnd we have crashed her party\"   \n#> [3] \"This is what you'll getThis is what you'll getThis is what you'll getWhen you mess with us\"        \n#> [4] \"Karma policeI've given all I canIt's not enoughI've given all I canBut we're still on the payroll\" \n#> [5] \"This is what you'll getThis is what you'll getThis is what you'll getWhen you mess with us\"        \n#> [6] \"For a minute thereI lost myself, I lost myselfPhew, for a minute thereI lost myself, I lost myself\"\n#> [7] \"For a minute thereI lost myself, I lost myselfPhew, for a minute thereI lost myself, I lost myself\"\ntibble(artist, song, lyrics) %>%\n    glimpse()\n#> Rows: 7\n#> Columns: 3\n#> $ artist <chr> \"Radiohead\", \"Radiohead\", \"Radiohead\", \"Radiohead\", \"Radiohead\"…\n#> $ song   <chr> \"Karma Police\", \"Karma Police\", \"Karma Police\", \"Karma Police\",…\n#> $ lyrics <chr> \"Karma policeArrest this manHe talks in mathsHe buzzes like a f…\nsong_lyrics <- tibble(artist, song, lyrics, lyrics_url)\nwrite_csv(x = song_lyrics, path = \"../data/original/lyrics.csv\")"},{"path":"acquire-data-chapter.html","id":"scaling-up","chapter":"5 Acquire data","heading":"5.3.3 Scaling up","text":"point may think, ‘Great, can download data single page, downloading multiple pages?’ Good question. ’s really strength programming approach takes hold. Extracting information multiple pages fundamentally different working single page. However, require sophisticated understanding web R coding strategies, particular iteration.get iteration, let’s first create couple functions make possible efficiently reuse code developed far:get_lyrics function wraps code scraping single lyrics webpage last.fm.get_lyrics function includes code developed previously, also includes: (1) output messages (cat()), (2) processing pause (Sys.sleep()), (3) code manage opening closing web connections (url() close()).write_content writes webscraped data local machine, including functionality create necessary directory structure target file path choose.just two functions, can take lyrics URL last.fm scrape write data disk like .Now manually search copy URLs run function pipeline. fine just particular URLs wanted scrape. want , say, scrape set lyrics grouped genre. probably want programmatic approach. good news can leverage understanding webscraping scrape last.fm harvest information needed create store links songs genre. can pass links pipeline, similar previous one, scrape lyrics many songs store results files grouped genre.Last.fm provides genres page top genres listed can explored.\nFigure 5.7: Genre page last.fm\nDiving particular genre, ‘rock’ example, get listing top tracks genre.\nFigure 5.8: Tracks genre list page last.fm\ninspect HTML elements track names Figure 5.8, can see relative URL found track. case, ‘Smells Like Teen Spirit’ Nirvana highlighted inspector. follow link track page lyrics track, notice relative URL track listings page unique information. web domain https://www.last.fm post-pended /+lyrics missing.can put together function gets track listing last.fm genre, scrapes relative URLs tracks, creates full absolute URL lyrics page.function, need identify verbatim way last.fm lists genres. Rock, rock Hip Hop, hip+hop.now method scrape URLs genre list vector. approach, , pass lyrics URLs existing pipeline downloads lyrics (get_lyrics()) writes disk (write_content()).approach, however, couple problems. (1) get_lyrics() function takes one URL time, result get_genre_lyrics_urls() produce many URLs. able solve iteration using purrr package, specifically map() function iteratively map URL output get_genre_lyrics_urls() get_lyrics() turn. (2) output iterative application get_lyrics() produce tibble URL, sets problem writing tibbles disk write_content() function. avoid want combine tibbles one single tibble send written disk. bind_rows() function just .preceding pipeline conceptually work. However, testing, turns URLs generated get_genre_lyrics_urls() exist site. , song listed lyrics added song site. mean URL sent get_lyrics() function, error attempting download parse page read_html() halt entire process. avoid error, can wrap get_lyrics() function function designed attempt download parse URL (tryCatch()), error, skip move next URL without stopping processing. approach reflected get_lyrics_catch() function .Updating pipeline get_lyrics_catch() function look like :work, discussed one goals acquiring data reproducible research project make sure developing efficient code burden site’s server scraping . case, like check see data already downloaded. , script run. , script run. course perfect use conditional statement. make single function can call, ’ve wrapped functions created getting lyric URLs last.fm, scraping URLs, writing results disk download_lastfm_lyrics() function . also added line add last_fm_genre column combined tibble store name genre scraped (.e. mutate(genre = last_fm_genre).Now can call function genre last.fm site download top 50 song lyrics genre (provided lyrics pages).Now can see web scrape data organized similar fashion data acquired chapter., important add custom functions acquire_functions.R script functions/ directory can access scripts efficiently make analysis steps succinct legible.section covered scraping language data web. rvest package provides host functions downloading parsing HTML. first looked toy example get basic understanding HTML works moved applying knowledge practical example. maintain reproducible workflow, code developed example grouped task-oriented functions turn joined wrapped function provided convenient access workflow avoided unnecessary downloads (case data already exists disk).built previously introduced R coding concepts demonstrated various others. Web scraping often requires knowledge familiarity R well web technologies. Rest assured, however, practice increase confidence abilities. encourage practice websites. encounter problems. Consult R documentation RStudio online lean R community web sites Stack Overflow inter alia.","code":"\nget_lyrics <- function(lyrics_url) {\n    # Function: Scrape last.fm lyrics page for: artist, song, and lyrics from a\n    # provided content link.  Return as a tibble/data.frame\n\n    cat(\"Scraping song lyrics from:\", lyrics_url, \"\\n\")\n\n    pacman::p_load(tidyverse, rvest)  # install/ load package(s)\n\n    url <- url(lyrics_url, \"rb\")  # open url connection \n    html <- read_html(url)  # read and parse html as an xml object\n    close(url)  # close url connection\n\n    artist <- html %>%\n        html_element(\"a.header-new-crumb\") %>%\n        html_text()\n\n    song <- html %>%\n        html_element(\"h1.header-new-title\") %>%\n        html_text()\n\n    lyrics <- html %>%\n        html_elements(\"p.lyrics-paragraph\") %>%\n        html_text()\n\n    cat(\"...one moment \")\n\n    Sys.sleep(1)  # sleep for 1 second to reduce server load\n\n    song_lyrics <- tibble(artist, song, lyrics, lyrics_url)\n\n    cat(\"... done! \\n\")\n\n    return(song_lyrics)\n}\nwrite_content <- function(content, target_file) {\n    # Function: Write the tibble content to disk. Create the directory if it\n    # does not already exist.\n\n    pacman::p_load(tidyverse)  # install/ load packages\n\n    target_dir <- dirname(target_file)  # identify target file directory structure\n    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE)  # create directory\n    write_csv(content, target_file)  # write csv file to target location\n\n    cat(\"Content written to disk!\\n\")\n}\nlyrics_url <- \"https://www.last.fm/music/Pixies/_/Where+Is+My+Mind%3F/+lyrics\"\n\nlyrics_url %>%\n    get_lyrics() %>%\n    write_content(target_file = \"../data/original/lastfm/lyrics.csv\")data/original/lastfm/\n└── lyrics.csv\nget_genre_lyrics_urls <- function(last_fm_genre) {\n  # Function: Scrapes a given last.fm genre title for top tracks in\n  # that genre and then creates links to the lyrics pages for these tracks\n  \n  cat(\"Scraping top songs from:\", last_fm_genre, \"genre: \\n\")\n  \n  pacman::p_load(tidyverse, rvest) # install/ load packages\n  \n  # create web url for the genre listing page\n  genre_listing_url <- \n    paste0(\"https://www.last.fm/tag/\", last_fm_genre, \"/tracks\") \n  \n  genre_lyrics_urls <- \n    read_html(genre_listing_url) %>% # read raw html and parse to xml\n    html_elements(\"td.chartlist-name a\") %>% # isolate the track elements\n    html_attr(\"href\") %>% # extract the href attribute\n    paste0(\"https://www.last.fm\", ., \"/+lyrics\") # join the domain, relative artist path, and the post-pended /+lyrics to create an absolute URL\n  \n  return(genre_lyrics_urls)\n}\nget_genre_lyrics_urls(\"hip+hop\") %>%  # get urls for top hip hop tracks\n  head(n = 10) # only display 10 tracks#> Scraping top songs from: hip+hop genre:\n#>  [1] \"https://www.last.fm/music/Juzhin/_/Charlie+Conscience+(feat.+MMAIO)/+lyrics\"\n#>  [2] \"https://www.last.fm/music/Juzhin/_/Railways/+lyrics\"                        \n#>  [3] \"https://www.last.fm/music/Juzhin/_/Coming+Down/+lyrics\"                     \n#>  [4] \"https://www.last.fm/music/Juzhin/_/Tupona/+lyrics\"                          \n#>  [5] \"https://www.last.fm/music/Juzhin/_/Sakhalin/+lyrics\"                        \n#>  [6] \"https://www.last.fm/music/Juzhin/_/3+Simple+Minutes/+lyrics\"                \n#>  [7] \"https://www.last.fm/music/Juzhin/_/Lost+Sense/+lyrics\"                      \n#>  [8] \"https://www.last.fm/music/Juzhin/_/Wonderful/+lyrics\"                       \n#>  [9] \"https://www.last.fm/music/Gina+Moryson/_/Vanilla+Smoothy+(Live)/+lyrics\"    \n#> [10] \"https://www.last.fm/music/Juzhin/_/Flunk-Down+(Juzhin+Remix)/+lyrics\"\n# Note: will not run\nget_genre_lyrics_urls(\"hip+hop\") %>% # get lyrics urls for specific genre\n  get_lyrics() %>% # scrape lyrics url\n  write_content(target_file = \"../data/original/lastfm/hip_hop.csv\") # write to disk\n# Note: will run, but with occasional errors\nget_genre_lyrics_urls(\"hip+hop\") %>% # get lyrics urls for specific genre\n  map(get_lyrics) %>%  # scrape lyrics url\n  bind_rows() %>% # combine tibbles into one\n  write_content(target_file = \"../data/original/lastfm/hip_hop.csv\") # write to disk\n# Wrap the `get_lyrics()` function with `tryCatch()` to skip URLs that have no\n# lyrics\n\nget_lyrics_catch <- function(lyrics_url) {\n    tryCatch(get_lyrics(lyrics_url), error = function(e) return(NULL))  # no, URL, return(NULL)/ skip\n}\n# Note: will run, but we can do better\nget_genre_lyrics_urls(\"hip+hop\") %>% # get lyrics urls for specific genre\n  map(get_lyrics_catch) %>%  # scrape lyrics url\n  bind_rows() %>% # combine tibbles into one\n  write_content(target_file = \"../data/original/lastfm/hip_hop.csv\") # write to disk\ndownload_lastfm_lyrics <- function(last_fm_genre, target_file) {\n    # Function: get last.fm lyric urls by genre and write them to disk\n\n    if (!file.exists(target_file)) {\n\n        cat(\"Downloading data.\\n\")\n\n        get_genre_lyrics_urls(last_fm_genre) %>%\n            map(get_lyrics_catch) %>%\n            bind_rows() %>%\n            mutate(genre = last_fm_genre) %>%\n            write_content(target_file)\n\n    } else {\n        cat(\"Data already downloaded!\\n\")\n    }\n}\n# Scrape lyrics for 'pop'\ndownload_lastfm_lyrics(last_fm_genre = \"pop\", target_file = \"../data/original/lastfm/pop.csv\")\n\n# Scrape lyrics for 'rock'\ndownload_lastfm_lyrics(last_fm_genre = \"rock\", target_file = \"../data/original/lastfm/rock.csv\")\n\n# Scrape lyrics for 'hip hop'\ndownload_lastfm_lyrics(last_fm_genre = \"hip+hop\", target_file = \"../data/original/lastfm/hip_hop.csv\")\n\n# Scrape lyrics for 'metal'\ndownload_lastfm_lyrics(last_fm_genre = \"metal\", target_file = \"../data/original/lastfm/metal.csv\")data/\n├── derived/\n└── original/\n    ├── cedel2/\n    │   └── texts.csv\n    ├── gutenberg/\n    │   ├── works_pq.csv\n    │   └── works_pr.csv\n    ├── lastfm/\n    │   ├── country.csv\n    │   ├── hip_hop.csv\n    │   ├── lyrics.csv\n    │   ├── metal.csv\n    │   ├── pop.csv\n    │   └── rock.csv\n    ├── sbc/\n    │   ├── meta-data/\n    │   └── transcriptions/\n    ├── scs/\n    │   ├── README\n    │   ├── discourse\n    │   ├── disfluency\n    │   ├── documentation/\n    │   ├── tagged\n    │   ├── timed-transcript\n    │   └── transcript\n    └── twitter/\n        └── rt_latinx.csv"},{"path":"acquire-data-chapter.html","id":"documentation-1","chapter":"5 Acquire data","heading":"5.4 Documentation","text":"part data acquisition process important include documentation describes data resource(s) serve base research project. resources data include much information possible outlines sampling frame data (Ädel, 2020). corpus sample acquired repository often include documentation outline sampling frame –likely information leads researcher select resource project hand. important include documentation (HTML PDF file) reference documentation (article citation link20) within reproducible project’s directory structure.cases data acquisition process formulated conducted researcher specific aims research (.e. API web scraping approaches), researcher make effort document aspects key study, may also interest researchers similar research questions. may include language characteristics modality, register, genre, etc., speaker/ writer characteristics demographics, time period(s), context linguistic communication, etc. process characteristics source data, process acquisition, date acquisition, etc. However, important recognize language sample resource drawn unique. general rule thumb, researcher document resource resource encounter first time. archive information, standard practice include README file relevant directory data stored.existing corpora data samples acquired researcher also important signal conditions / licensing restrictions one heed using potentially sharing data. cases existing corpus data come restrictions data sharing. can quite restrictive ultimately require corpus data included publically available reproducible project data can shared derived format. case, important document steps legally acquire data researcher can acquire license take full advantage reproducible project.case data APIs web scraping, may stipulations sharing data. growing number data sources apply one available Creative Common Licenses. Check source data information member research institution likely specialist Copyright Fair Use.","code":"data/\n├── derived/\n└── original/\n    ├── cedel2/\n    │   ├── documentation/\n    │   └── texts.csv\n    ├── gutenberg/\n    │   ├── README.md\n    │   ├── works_pq.csv\n    │   └── works_pr.csv\n    ├── lastfm/\n    │   ├── README.md\n    │   ├── country.csv\n    │   ├── hip_hop.csv\n    │   ├── lyrics.csv\n    │   ├── metal.csv\n    │   ├── pop.csv\n    │   └── rock.csv\n    ├── sbc/\n    │   ├── meta-data/\n    │   └── transcriptions/\n    ├── scs/\n    │   ├── README\n    │   ├── discourse\n    │   ├── disfluency\n    │   ├── documentation/\n    │   ├── tagged\n    │   ├── timed-transcript\n    │   └── transcript\n    └── twitter/\n        ├── README.md\n        └── rt_latinx.csv"},{"path":"acquire-data-chapter.html","id":"summary-5","chapter":"5 Acquire data","heading":"Summary","text":"chapter covered lot ground. surface discussed three methods acquiring corpus data use text analysis. process delved various aspects R programming language. key concepts include writing custom functions working function iterative manner. also considered topics general nature concern interacting data found internet.methods approached way transparent researcher -collaborators general research community. reason documentation steps taken acquire data key code human-facing documentation.point bird’s eye view data available web strategies access great majority . now time turn next step data analysis project: data curation. next posts cover wrangle raw data tidy dataset. include working incorporating meta-data well augmenting dataset linguistic annotations.","code":""},{"path":"curate-datasets-chapter.html","id":"curate-datasets-chapter","chapter":"6 Curate data(sets)","heading":"6 Curate data(sets)","text":"\nINCOMPLETE DRAFT\nhardest bit information extract first piece.―–Robert Ferrigno\nessential questions chapter :\n\nformats data can take?\n\nR programming strategies used read formats tabular, tidy dataset structures?\n\nimportance maintaining modularity data data processing reproducible research project?\nchapter now look next step text analysis project: data curation. , process converting original data acquire tidy dataset. Acquired data can come wide variety formats depend largely richness metadata included, also can reflect individual preferences. chapter consider three general types formats: (1) unstructured data, (2) structured data, (3) semi-structured data. Regardless file type structure data, necessary consider curate dataset structure reflects basic unit analysis wish investigate (see Chapter 4, section 4.2. resulting dataset base work transform dataset aligns analysis method(s) implement. previous implementation steps, discuss important role documentation.","code":""},{"path":"curate-datasets-chapter.html","id":"unstructured","chapter":"6 Curate data(sets)","heading":"6.1 Unstructured","text":"bulk text available wild unstructured variety. Unstructured data data organized make information contained within explicit. Explicit information included data called metadata. Metadata can linguistic non-linguistic nature. unstructured data little metadata directly associated data. information needs added derived purposes research, either manual inspection (semi-)automatic processes. now, however, job just get unstructured data structured format minimal set metadata can derive resource.example unstructured source text data, let’s take look Europarle Parallel Corpus, introduced Chapter 2 “Understanding data”. data contains parallel texts (source translated documents) European Parliamentary proceedings 21 European languages. focus translation Spanish English (Spanish-English).","code":""},{"path":"curate-datasets-chapter.html","id":"orientation","chapter":"6 Curate data(sets)","heading":"6.1.1 Orientation","text":"data downloaded data/original/europarle/ directory see two files. One corresponding source language (Spanish) one target language (English).Looking first 10 lines first file, can see running text.meta information can surmise files fact know one source language one target language sentence aligned (parallel) lines file.’d like create data frame following structure.Table 6.1: Idealized structure Europarle Corpus dataset.","code":"data/original/europarle/\n├── europarl-v7.es-en.en\n└── europarl-v7.es-en.esResumption of the session\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\nAlthough, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\nYou have requested a debate on this subject in the course of the next few days, during this part-session.\nIn the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\nPlease rise, then, for this minute' s silence.\n(The House rose and observed a minute' s silence)\nMadam President, on a point of order.\nYou will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\nOne of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago."},{"path":"curate-datasets-chapter.html","id":"tidy-the-data","chapter":"6 Curate data(sets)","heading":"6.1.2 Tidy the data","text":"create dataset structure lets’s read files readtext() function readtext package assign meaningful variable.\nreadtext() function can read many different types file formats, structured unstructured. However, depends large part extension file recognize algorithm use reading file. particular case Europarle files typical extension (.en .es). readtext() function treat plain text (.txt), throw warning. suppress warning can add verbosity = 0 argument function.\nNow couple things note thbe europarle_en europarle_es objects. inspect structure, find dimensions data frame created one row two columns.columns doc_id text. doc_id created readtext index file read . text column text appears. fact one row means text entire file contained one cell! want break cell rows sentence, now let’s work getting columns line idealized dataset structure.\nNote ’ve used str() function instead glimpse() preview structure. glimpse() try provide preview cells particular case cell text large take long time render.\nFirst let’s change type data frame working tibble. make sure don’t accidentally print hundreds lines R Markdown output / R Console. rename doc_id column type change value column “Target” (English) “Source” (Spanish).two objects now, one corresponding ‘Source’ ‘Target’ parallel texts. Let’s now join two datasets, one top –, rows. wil use bind_rows() function .europarle dataset now 2 columns, , 2 rows –corresponding distinct language types (Source/ Target).Remember goal create dataset structure three columns type, sentence_id, sentence. moment type text –text sentences type cell. going want break text column sentences, group sentences created type, number sentences aligned distinct types.break text sentences going turn tidytext package. package extremely useful function unnest_tokens() provides effective way break text various units (see ?tidytext::unnest_tokens full list token types). Since know looking raw text sentence line, best strategy break text sentence units find way break line new row dataset. need use token = \"regex\" (Regular Expression) use pattern = \"\\\\n\" tells R look carriage returns use breaking criterion.\nRegular Expressions powerful pattern matching syntax. used extensively text manipulation see . good website practice Regular Expressions RegEx101. can also install regexplain package R get access useful RStudio Addin.\nnew europarle_sentences object data frame almost 4 million rows! final step get envisioned dataset structure add sentence_id column calculated grouping data type assigning row number sentences group.Table 6.2: First ten sentences Europarle Corpus curated dataset.","code":"\n# Read the Europarle files\neuroparle_en <-  # English target text\n  readtext::readtext(\"../data/original/europarle/europarl-v7.es-en.en\", # path to the data\n                     verbosity = 0) # don't show warnings\n\neuroparle_es <- # Spanish source text\n  readtext::readtext(\"../data/original/europarle/europarl-v7.es-en.es\", # path to the data\n                     verbosity = 0) # don't show warnings\nstr(europarle_en)  # inspect the structure of the object\n#> Classes 'readtext' and 'data.frame': 1 obs. of  2 variables:\n#>  $ doc_id: chr \"europarl-v7.es-en.en\"\n#>  $ text  : chr \"Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Friday 17 Dece\"| __truncated__\neuroparle_target <- \n  europarle_en %>% # readtext data frame\n  as_tibble() %>% # convert to tibble\n  rename(type = doc_id) %>% # rename doc_id to type\n  mutate(type = \"Target\") # change type value to 'Target'\n\neuroparle_source <- \n  europarle_es %>% # readtext data frame\n  as_tibble() %>% # convert to tibble\n  rename(type = doc_id) %>% # rename doc_id to type\n  mutate(type = \"Source\") # change type value to 'Source'\neuroparle <- bind_rows(europarle_target, europarle_source)\n\nstr(europarle)  # inspect the structure of the object\n#> tibble [2 × 2] (S3: tbl_df/tbl/data.frame)\n#>  $ type: chr [1:2] \"Target\" \"Source\"\n#>  $ text: chr [1:2] \"Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Friday 17 Dece\"| __truncated__ \"Reanudación del período de sesiones\\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrump\"| __truncated__\neuroparle_sentences <- \n  europarle %>% \n  tidytext::unnest_tokens(output = sentence, # new column\n                          input = text, # column to find text\n                          token = \"regex\", # use a regular expression to break up the text\n                          pattern = \"\\\\n\", # break text by carriage returns (returns after lines)\n                          to_lower = FALSE) # do not lowercase the text\n\nglimpse(europarle_sentences) # preview the structure\n#> Rows: 3,926,375\n#> Columns: 2\n#> $ type     <chr> \"Target\", \"Target\", \"Target\", \"Target\", \"Target\", \"Target\", \"…\n#> $ sentence <chr> \"Resumption of the session\", \"I declare resumed the session o…\neuroparle_sentences_id <- \n  europarle_sentences %>% # dataset\n  group_by(type) %>% # group by type\n  mutate(sentence_id = row_number()) %>% # add a row number for each sentence for each level of type\n  select(type, sentence_id, sentence) %>% # select the relevant columns to keep\n  ungroup() %>%  # ungroup by type\n  arrange(sentence_id, type) # arrange the dataset\n\neuroparle_sentences_id %>% \n  slice_head(n = 10) %>% \n  knitr::kable(booktabs = TRUE,\n        caption = \"First ten sentences in the Europarle Corpus curated dataset.\")"},{"path":"curate-datasets-chapter.html","id":"write-dataset","chapter":"6 Curate data(sets)","heading":"6.1.3 Write dataset","text":"point curated dataset (europarle_sentences_id) tidy format. dataset, however, current R session. want write dataset disk next step text analysis workflow (transform data) able start work dataset make changes needed fit analysis needs.leverage project directory structure distinct directories original/ derived/ data(sets).Since dataset derived work, added derived/ directory. ’ll create europarle/ directory just keep things organized.directory structure derived/ directory looks now.","code":"data/\n├── derived\n└── original\n# Write the curated dataset to disk\nfs::dir_create(path = \"../data/derived/europarle/\") # create the europarle directory\nwrite_csv(x = europarle_sentences_id, # object to write\n          file = \"../data/derived/europarle/europarle_curated.csv\") # target file location/ namedata/\n├── derived\n│   └── europarle\n│       └── europarle_curated.csv\n└── original\n    └──europarle\n        ├── europarl-v7.es-en.en\n        └── europarl-v7.es-en.es"},{"path":"curate-datasets-chapter.html","id":"summary-6","chapter":"6 Curate data(sets)","heading":"6.1.4 Summary","text":"section worked unstructured data looked read data R session manipulate data form tidy dataset columns derive based information corpus.discussion worked step step curate Europarle Corpus, adding intermediate steps illustration purposes. However, realistic case code likely make extensive use piping (%>%) reduce number intermediate objects make code legible. ’ve included sample code might look like.","code":"\n# Read data and set up `type` column\neuroparle_en <-  \n  readtext::readtext(\"../data/original/europarle/europarl-v7.es-en.en\", # path to the data\n                     verbosity = 0) %>% # don't show warnings\n  as_tibble() %>% # covert to tibble\n  rename(type = doc_id) %>% # rename doc_id to type\n  mutate(type = \"Target\") # change type value to 'Target'\n\neuroparle_es <- \n  readtext::readtext(\"../data/original/europarle/europarl-v7.es-en.en\", # path to the data\n                     verbosity = 0) %>%  # don't show warnings\n  as_tibble() %>% # covert to tibble\n  rename(type = doc_id) %>% # rename doc_id to type\n  mutate(type = \"Source\") # change type value to 'Source'\n\n# Join the datasets by rows\neuroparle <- \n  bind_rows(europarle_en, europarle_es)\n\n# Segment the `text` column into `sentence` units\neuroparle <- \n  europarle %>% # dataset\n  tidytext::unnest_tokens(output = sentence, # new column\n                          input = text, # column to find text\n                          token = \"regex\", # use a regular expression to break up the text\n                          pattern = \"\\\\n\", # break text by carriage returns (returns after lines)\n                          to_lower = FALSE) # do not lowercase the text\n\n# Add `sentence_id` to each `type`\neuroparle <- \n  europarle %>% # dataset\n  group_by(type) %>% # group by type\n  mutate(sentence_id = row_number()) %>% # add a row number for each sentence for each level of type\n  select(type, sentence_id, sentence) %>% # select the relevant columns to keep\n  ungroup() %>%  # ungroup by type\n  arrange(sentence_id, type) # arrange the dataset\n\n# Write the curated dataset to disk\nfs::dir_create(path = \"../data/derived/europarle/\") # create the europarle directory\nwrite_csv(x = europarle_sentences_id, # object to write\n          file = \"../data/derived/europarle/europarle_curated.csv\") # target file location/ name"},{"path":"curate-datasets-chapter.html","id":"structured","chapter":"6 Curate data(sets)","heading":"6.2 Structured","text":"opposite side spectrum unstructured data, structured data includes metadata information –often much . association metadata language analyzed means data already curated degree, therefore apt discuss structured data dataset. two questions, however, need taken account. One, logistical question, file format dataset read R. second, research-based, whether data curated fashion makes sense current research. Let’s look questions briefly get practical example.file formats purposely designed storing structured datasets. common file types .csv, .xml, .json, etc. data within files explicitly organized. example, .csv file, dataset structure represented delimiting columns rows commas.read R, .csv file format converted data frame appropriate structure.Table 6.3: Example .csv file RWith understanding information encoding file, can now turn considerations original dataset structure structure used given research project. curation process reflected structured dataset may may initially align goals research either terms type(s) information unit analysis structured dataset. aim, , take advantage information curate align.example case curating structured datasets, look song lyric datasets acquired Last.fm previous chapter.","code":"column_1,column_2,column_3\nrow 1 value 1,row 1 value 2,row 1 value 3\nrow 2 value 1,row 2 value 2,row 2 value 3"},{"path":"curate-datasets-chapter.html","id":"orientation-1","chapter":"6 Curate data(sets)","heading":"6.2.1 Orientation","text":"individual datasets Last.fm webscrape found inside data/original/lastfm/ directory, includes README.md documentation file.Let’s take look structure one genres set lyrics familiarize structure.Table 6.4: Example file fromt Last.fm dataset song lyrics.can see couple important characteristics preview dataset. First, see columns include artist, song, lyrics, lyrics_url, genre. Second, see son lyrics segmented across multiple rows.\nmay notice addition lyrics separated line, appears artifact original webscrape data individual lyric lines run next. example lyrics “… hurt todayTosee still feelI focus…”. address issue comes time normalize dataset transform process.\nGiven fact files include genre label, means able read files one operation distinction genres recoverable. next thing think want curate dataset purposes. , base structure curated dataset look like?Let’s make assumption want columns artist, song, lyrics, genre. lyrics_url useful documentation purposes, text analysis appear relevant –drop . second aspect concerns observations. stands, dataset observations reflect formatting website lyrics drawn. potentially better organization observation correspond lyrics single song. case want collapse current lyrics column’s values lyrics entire song –maintaining measure columns.structure mind, shooting idealized structure one .Table 6.5: Idealized structure Last.fm dataset.","code":"data/\n├── derived/\n└── original/\n    └── lastfm/\n        ├── README.md\n        ├── country.csv\n        ├── hip_hop.csv\n        ├── lyrics.csv\n        ├── metal.csv\n        ├── pop.csv\n        └── rock.csv\nlf_country <- read_csv(file = \"../data/original/lastfm/country.csv\") # read the csv file\nlf_country %>% # dataset\n  slice_head(n = 10) %>% # first 10 observations \nknitr::kable(booktabs = TRUE, # print pretty table\n             caption = \"Example file fromt the Last.fm dataset of song lyrics.\") # add caption"},{"path":"curate-datasets-chapter.html","id":"tidy-the-datasets","chapter":"6 Curate data(sets)","heading":"6.2.2 Tidy the datasets","text":"objectives set, let’s first read files. use readtext() function. instead reading one file time read files interest (.csv extension) one go. readtext() function allows use ‘wildcard’ notation (*) file(s) path enable pattern matching.files data/original/lastfm/ directory look like .want files, except REAME.md file. want path look like :wildcard * replaces genre names effectively matches files ending .csv.Great, capture files looking working readtext() need set text_field argument column corresponds text dataset. lyrics column. Let’s go ahead convert result tibble.Looking preview data frame now lastfm couple things note. First, see column doc_id added. column used readtext() index file data read. case since already sufficient information index dataset, can drop column. Next see lyrics column renamed text. set text_field read files. can easily rename column, ’ll leave later.Let’s go ahead drop columns decided figure curated dataset. can use select() function either select columns want keep using - operator, identify columns want drop. decision ‘selecting’ ‘deselecting’ usually one personal choice code succinctness. case, dropping two columns keeping four, let’s deselect. assign result name current dataset, effectively overwritting dataset.Now let’s work collapse lyrics text column distinct artist, song, genre combination. use group_by() function create artist song genre groupings use summarize() create new column text field collapsed song lyrics grouping. Inside summarize() function use str_flatten() argument collapse = \" \" collapse observation text leaving single whitespace observations (otherwise line joined contiguously next).Let’s take look first 5 observations collapsed dataset.Table 6.6: Sample lyrics Last.fm dataset collapsed artist, song, genre.point, thing left get dataset align idealized dataset structure organize column ordering (using select()). also arrange dataset alphabetically genre artist (using arrange()).Table 6.7: Sample lyrics curated Last.fm dataset.","code":"../data/original/lastfm/README.md\n../data/original/lastfm/country.csv\n../data/original/lastfm/hip_hop.csv\n../data/original/lastfm/lyrics.csv\n../data/original/lastfm/metal.csv\n../data/original/lastfm/pop.csv\n../data/original/lastfm/rock.csv../data/original/lastfm/*.csv\nlastfm <- \n  readtext(file = \"../data/original/lastfm/*.csv\", # files to match using *.csv\n           text_field = \"lyrics\") %>% # text column from the datasets\n  as_tibble() # convert to a tibble\n\nglimpse(lastfm) # preview#> Rows: 2,172\n#> Columns: 6\n#> $ doc_id     <chr> \"country.csv.1\", \"country.csv.2\", \"country.csv.3\", \"country…\n#> $ text       <chr> \"I hurt myself todayTo see if I still feelI focus on the pa…\n#> $ artist     <chr> \"Johnny Cash\", \"Johnny Cash\", \"Johnny Cash\", \"Johnny Cash\",…\n#> $ song       <chr> \"Hurt\", \"Hurt\", \"Hurt\", \"Hurt\", \"Hurt\", \"Ring of Fire\", \"Ri…\n#> $ lyrics_url <chr> \"https://www.last.fm/music/Johnny+Cash/_/Hurt/+lyrics\", \"ht…\n#> $ genre      <chr> \"country\", \"country\", \"country\", \"country\", \"country\", \"cou…\nlastfm <- # new dataset\n  select(lastfm, # original dataset\n         -doc_id, -lyrics_url) # drop these columns\n\nglimpse(lastfm) # preview\n#> Rows: 2,172\n#> Columns: 4\n#> $ text   <chr> \"I hurt myself todayTo see if I still feelI focus on the painTh…\n#> $ artist <chr> \"Johnny Cash\", \"Johnny Cash\", \"Johnny Cash\", \"Johnny Cash\", \"Jo…\n#> $ song   <chr> \"Hurt\", \"Hurt\", \"Hurt\", \"Hurt\", \"Hurt\", \"Ring of Fire\", \"Ring o…\n#> $ genre  <chr> \"country\", \"country\", \"country\", \"country\", \"country\", \"country…\nlastfm <- \n  lastfm %>% # dataset\n  group_by(artist, song, genre) %>% # grouping\n  summarise(lyrics = str_flatten(text, collapse = \" \")) %>%  # collapse text into the new column `lyrics` (dropping `text`)\n  ungroup() # unset the groupings\n\nglimpse(lastfm) # preview\n#> Rows: 199\n#> Columns: 4\n#> $ artist <chr> \"3 Doors Down\", \"3 Doors Down\", \"50 Cent\", \"a-ha\", \"ABBA\", \"Aer…\n#> $ song   <chr> \"Here Without You\", \"Kryptonite\", \"In Da Club\", \"Take On Me\", \"…\n#> $ genre  <chr> \"rock\", \"rock\", \"hip-hop\", \"pop\", \"pop\", \"rock\", \"country\", \"co…\n#> $ lyrics <chr> \"A hundred days have made me olderSince the last time that I sa…\nlastfm %>% # dataset\n  slice_head(n = 5) %>% # first 5 observations\n  knitr::kable(booktabs = TRUE, # print pretty table\n               caption = \"Sample lyrics from Last.fm dataset collapsed by artist, song, and genre.\") # add caption to the table\nlastfm <- \n  lastfm %>% # original dataset\n  select(artist, song, lyrics, genre) %>% # order columns (and rename `text` to `lyrics`)\n  arrange(genre, artist) # arrange rows by `genre` and `artist`\n\nlastfm %>% # curated dataset\n  slice_head(n = 5) %>% # first 5 observations\n  knitr::kable(booktabs = TRUE, # print pretty table\n               caption = \"Sample lyrics from curated Last.fm dataset.\") # add caption to the table"},{"path":"curate-datasets-chapter.html","id":"write-dataset-1","chapter":"6 Curate data(sets)","heading":"6.2.3 Write dataset","text":"now curated dataset can write disk. , Europarle Corpus dataset curated , write dataset data/derived/ directory –effectively ensuring clear dataset created project work.’s overview new directory structure.","code":"\nfs::dir_create(path = \"../data/derived/lastfm/\")  # create lastfm subdirectory\nwrite_csv(lastfm, file = \"../data/derived/lastfm/lastfm_curated.csv\")  # write lastfm to disk and label as the curated datasetdata/\n├── derived/\n│   └── lastfm/\n│       └── lastfm_curated.csv\n└── original/\n    └── lastfm/\n        ├── README.md\n        ├── country.csv\n        ├── hip_hop.csv\n        ├── lyrics.csv\n        ├── metal.csv\n        ├── pop.csv\n        └── rock.csv"},{"path":"curate-datasets-chapter.html","id":"summary-7","chapter":"6 Curate data(sets)","heading":"6.2.4 Summary","text":", summarize, code accomplish steps covered section curating structured datasets.","code":"\n# Read Last.fm lyrics and subset relevant columns\nlastfm <- \n  readtext(file = \"../data/original/lastfm/*.csv\", # files to match using *.csv\n           text_field = \"lyrics\") %>% # text column from the datasets\n  as_tibble() %>% # convert to a tibble\n  select(-doc_id, -lyrics_url) # drop these columns\n\n# Collapse text by artist, song, and genre grouping\nlastfm <- \n  lastfm %>% # dataset\n  group_by(artist, song, genre) %>% # grouping\n  summarise(lyrics = str_flatten(text, collapse = \" \")) %>%  # collapse text into the new column `lyrics` (dropping `text`)\n  ungroup() # unset the groupings\n\n# Order columns and arrange rows\nlastfm <- \n  lastfm %>% # original dataset\n  select(artist, song, lyrics, genre) %>% # order columns (and rename `text` to `lyrics`)\n  arrange(genre, artist) # arrange rows by `genre` and `artist`\n\n# Write curated dataset to disk\nfs::dir_create(path = \"../data/derived/lastfm/\") # create lastfm subdirectory\nwrite_csv(lastfm, \n          file = \"../data/derived/lastfm/lastfm_curated.csv\") # write lastfm to disk and label as the curated dataset"},{"path":"curate-datasets-chapter.html","id":"semi-structured","chapter":"6 Curate data(sets)","heading":"6.3 Semi-structured","text":"point discussed curating unstructured data structured datasets. two extremes falls semi-structured data. name suggests, hybrid unstructured structured data. means important structured metadata included unstructured elements. file formats approaches encoding structured aspects data vary widely resource resource therefore often requires detailed attention structure data often includes sophisticated programming strategies curate data produce tidy dataset.example work Switchboard Dialog Act Corpus (SDAC) extends Switchboard Corpus speech act annotation. (ADD CITATION)\nSDAC dialogues (swb1_dialogact_annot.tar.gz) available free download LDC. download, decompress, organize resource, follow strategies discussed “Acquire data” Direct Downloads. tadr package provides tadr::get_compressed_data() function accomplish step.\n","code":""},{"path":"curate-datasets-chapter.html","id":"orientation-2","chapter":"6 Curate data(sets)","heading":"6.3.1 Orientation","text":"main directory structure SDAC data looks like :README file contains basic information resource, doc/ directory contains detailed information dialog annotations, following directories prefixed sw... contain individual conversation files. ’s peek internal structure first couple directories.Let’s take look first conversation file (sw_0001_4325.utt) see structured.things take note . First see conversation files meta-data header offset conversation text line = characters. Second header contains meta-information various types. Third, text interleaved annotation scheme.information may readily understandable, various pieces meta-data header, get better understanding information encoded let’s take look README file. file get birds eye view going . short, data includes 1155 telephone conversations two people annotated 42 ‘DAMSL’ dialog act labels. README file refers us doc/manual.august1.html file information scheme.point open doc/manual.august1.html file browser investigation. find ‘DAMSL’ stands ‘Discourse Annotation Markup System Labeling’ first characters line conversation text correspond one combination labels utterance. first utterances :utterance also labeled speaker (‘’ ‘B’), speaker turn (‘1’, ‘2’, ‘3’, etc.), utterance within turn (‘utt1’, ‘utt2’, etc.). annotation provided withing utterance, enough get us started conversations.Now let’s turn meta-data header. see information creation file: ‘FILENAME’, ‘TOPIC’, ‘DATE’, etc. doc/manual.august1.html file doesn’t much say information returned LDC Documentation found information Online Documentation section. poking around documentation discovered meta-data speaker corpus found caller_tab.csv file. tabular file contain column names, caller_doc.txt . inspecting files manually comparing information conversation file noticed ‘FILENAME’ information contained three pieces useful information delimited underscores _.first information document id (4325), second third correspond speaker number: first speaker (1632) second speaker B (1519).sum, 1155 conversation files. file two parts, header text section, separated line = characters. header section contains ‘FILENAME’ line document id, ids speaker speaker B. text section annotated DAMSL tags beginning line, followed speaker, turn number, utterance number, utterance text. knowledge hand, let’s set create tidy dataset following column structure:Table 6.8: Idealized structure SDAC dataset.","code":"data/\n├── derived/\n└── original/\n    └── sdac/\n        ├── README\n        ├── doc/\n        ├── sw00utt/\n        ├── sw01utt/\n        ├── sw02utt/\n        ├── sw03utt/\n        ├── sw04utt/\n        ├── sw05utt/\n        ├── sw06utt/\n        ├── sw07utt/\n        ├── sw08utt/\n        ├── sw09utt/\n        ├── sw10utt/\n        ├── sw11utt/\n        ├── sw12utt/\n        └── sw13utt/├── README\n├── doc\n│   └── manual.august1.html\n├── sw00utt\n│   ├── sw_0001_4325.utt\n│   ├── sw_0002_4330.utt\n│   ├── sw_0003_4103.utt\n│   ├── sw_0004_4327.utt\n│   ├── sw_0005_4646.utt*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n*x*                                                                     *x*\n*x*            Copyright (C) 1995 University of Pennsylvania            *x*\n*x*                                                                     *x*\n*x*    The data in this file are part of a preliminary version of the   *x*\n*x*    Penn Treebank Corpus and should not be redistributed.  Any       *x*\n*x*    research using this corpus or based on it should acknowledge     *x*\n*x*    that fact, as well as the preliminary nature of the corpus.      *x*\n*x*                                                                     *x*\n*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n\n\nFILENAME:   4325_1632_1519\nTOPIC#:     323\nDATE:       920323\nTRANSCRIBER:    glp\nUTT_CODER:  tc\nDIFFICULTY: 1\nTOPICALITY: 3\nNATURALNESS:    2\nECHO_FROM_B:    1\nECHO_FROM_A:    4\nSTATIC_ON_A:    1\nSTATIC_ON_B:    1\nBACKGROUND_A:   1\nBACKGROUND_B:   2\nREMARKS:        None.\n\n=========================================================================\n  \n\no          A.1 utt1: Okay.  /\nqw          A.1 utt2: {D So, }   \n\nqy^d          B.2 utt1: [ [ I guess, +   \n\n+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /o = \"Other\"\nqw = \"Wh-Question\"\nqy^d = \"Declarative Yes-No-Question\"\n+ = \"Segment (multi-utterance)\"*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n\n\nFILENAME:   4325_1632_1519\nTOPIC#:     323\nDATE:       920323\nTRANSCRIBER:    glp"},{"path":"curate-datasets-chapter.html","id":"tidy-the-data-1","chapter":"6 Curate data(sets)","heading":"6.3.2 Tidy the data","text":"Let’s begin reading one conversation files R character vector using read_lines() function readr package.isolate vector element contains document speaker ids, use str_detect() stringr package. function takes two arguments, string pattern, returns logical value, TRUE pattern matched FALSE . can use output function, , subset doc character vector return vector element (line) contains digits_digits_digits regular expression. expression combines digit matching operator \\\\d + operator match 1 contiguous digits. separate three groups \\\\d+ underscores _. result \\\\d+_\\\\d+_\\\\d+.\nstringr package handy function str_view() str_view_all() allow interactive pattern matching. also RStudio Addin regexplain package also can helpful developing regular expression syntax.\nnext step extract three digit sequences correspond doc_id, speaker_a_id, speaker_b_id. First extract pattern identified str_extract() can break single character vector multiple parts based underscore _. str_split() function takes string pattern use split character vector. return list character vectors.list special object type R. unordered collection objects whose lengths can differ (contrast data frame collection objects whose lengths –hence tabular format). case list length 1, whose sole element character vector length 3 –one element per segment returned split. desired result cases pass multiple character vectors str_split() function don’t want results conflated single character vector blurring distinction individual character vectors. like conflate, flatten list, can use unlist() function.Let’s flatten list case, single character vector, assign result doc_speaker_info.doc_speaker_info now character vector length three. Let’s subset elements assign meaningful variable names can conveniently use later tidying process.next step isolate text section extracting rest document. noted previously, sequence = separates header section text section. need index point character vector doc line occurs subset doc point end character vector. Let’s first find point = sequence occurs. use str_detect() function find pattern looking (contiguous sequence =), pass logical result () function return element index number match.file 31 index doc = sequence occurs. Now important keep mind working single file sdac/ data. need cautious create pattern may matched multiple times another document corpus. =+ pattern match =, ==, ===, etc. implausible believe might = character line one files. Let’s update regular expression avoid potential scenario matching sequences three =. case make use curly bracket operators {}.get result file, safeguard bit unlikely find multiple matches ===, ====, etc.31 index = sequence, want next line start reading text section. increment index 1.index end text simply length doc vector. can use length() function get index.now bookends, speak, text section. extract text subset doc vector indices.text extra whitespace lines blank lines well. cleaning moving forward organize data. get rid whitespace use str_trim() function default remove leading trailing whitespace line.remove blank lines use logical expression subset text vector. text != \"\" means return TRUE lines blank, FALSE .first step towards tidy dataset now combine doc_id element text data frame.Table 6.9: First 5 observations prelim data curation SDAC data.data now data frame, time parse text column extract damsl tags, speaker, speaker turn, utterance number, utterance text separate columns. make extensive use regular expressions. aim find consistent pattern distinguishes piece information text given row data$text extract .best way learn regular expressions use . end ’ve included link interactive regular expression practice website regex101.Open site copy text ‘TEST STRING’ field.\nFigure 6.1: RegEx101.\nNow manually type following regular expressions ‘REGULAR EXPRESSION’ field one--one (separate line). Notice matched type ’ve finished typing. can find exactly component parts expression toggling top right icon window hovering mouse relevant parts expression.can now see, regular expressions match damsl tags, speaker speaker turn, utterance number, utterance text. apply expressions data extract information separate columns make use mutate() str_extract() functions. mutate() take data frame create new columns values match extract row data frame str_extract(). Notice str_extract() different str_extract_all(). work mutate() row evaluated turn, therefore need make one match per row data$text.’ve chained steps code , dropping original text column select(-text), overwriting data results.\nOne twist notice regular expressions R require double backslashes (\\\\) programming environments use single backslash (\\).\ncouple things left columns extracted text move finishing tidy dataset. First, need separate speaker_turn column speaker turn_num columns second need remove unwanted characters damsl_tag, utterance_num, utterance_text columns.separate values column two columns use separate() function. takes column separate character vector names new columns create. default values input column separated non-alphanumeric characters. case means . separator.remove unwanted leading trailing whitespace apply str_trim() function. removing characters matching character(s) replace empty string (\"\") str_replace() function. , ’ve chained functions together overwritten data results.round tidy dataset single conversation file connect speaker_a_id speaker_b_id speaker B current dataset adding new column speaker_id. case_when() function exactly : allows us map rows speaker value “” speaker_a_id rows value “B” speaker_b_id.now tidy dataset set create. dataset includes one conversation file! want apply code 1155 conversation files sdac/ corpus. approach create custom function groups code ’ve done single file iterative send file corpus function combine results one data frame.’s custom function extra code print progress message file runs.sanity check run extract_sdac_metadata() function conversation file just working make sure works expected.Looks good!now ’s time create vector paths conversation files. fs::dir_ls() interfaces OS file system return paths files specified directory. also add pattern match conversation files (regexp = \\\\.utt$) don’t accidentally include files corpus. recurse set TRUE means get full path file.pass conversation file vector paths conversation files iteratively extract_sdac_metadata() function use map(). apply function conversation file return data frame . bind_rows() join resulting data frames rows give us single tidy dataset 1155 conversations. Note lot processing going patient.now see 223606 observations (individual utterances dataset).","code":"\ndoc <- read_lines(file = \"../data/original/sdac/sw00utt/sw_0001_4325.utt\")  # read a single file as character vector\ndoc[str_detect(doc, pattern = \"\\\\d+_\\\\d+_\\\\d+\")]  # isolate pattern\n#> [1] \"FILENAME:\\t4325_1632_1519\"\ndoc[str_detect(doc, \"\\\\d+_\\\\d+_\\\\d+\")] %>% # isolate pattern\n  str_extract(pattern = \"\\\\d+_\\\\d+_\\\\d+\") %>% # extract the pattern\n  str_split(pattern = \"_\") # split the character vector\n#> [[1]]\n#> [1] \"4325\" \"1632\" \"1519\"\ndoc[str_detect(doc, \"\\\\d+_\\\\d+_\\\\d+\")] %>% # isolate pattern\n  str_extract(pattern = \"\\\\d+_\\\\d+_\\\\d+\") %>% # extract the pattern\n  str_split(pattern = \"_\") %>% # split the character vector\n  unlist() # flatten the list to a character vector\n#> [1] \"4325\" \"1632\" \"1519\"\ndoc_speaker_info <- \n  doc[str_detect(doc, \"\\\\d+_\\\\d+_\\\\d+\")] %>% # isolate pattern\n  str_extract(pattern = \"\\\\d+_\\\\d+_\\\\d+\") %>% # extract the pattern\n  str_split(pattern = \"_\") %>%  # split the character vector\n  unlist() # flatten the list to a character vector\ndoc_id <- doc_speaker_info[1]  # extract by index\nspeaker_a_id <- doc_speaker_info[2]  # extract by index\nspeaker_b_id <- doc_speaker_info[3]  # extract by index\ndoc %>% \n  str_detect(pattern = \"=+\") %>% # match 1 or more `=`\n  which() # find vector index\n#> [1] 31\ndoc %>% \n  str_detect(pattern = \"={3,}\") %>% # match 3 or more `=`\n  which() # find vector index\n#> [1] 31\ntext_start_index <- \n  doc %>% \n  str_detect(pattern = \"={3,}\") %>% # match 3 or more `=` \n  which() # find vector index\ntext_start_index <- text_start_index + 1 # increment index by 1\ntext_end_index <- length(doc)\ntext <- doc[text_start_index:text_end_index]  # extract text\nhead(text)  # preview first lines of `text`\n#> [1] \"  \"                                       \n#> [2] \"\"                                         \n#> [3] \"o          A.1 utt1: Okay.  /\"            \n#> [4] \"qw          A.1 utt2: {D So, }   \"        \n#> [5] \"\"                                         \n#> [6] \"qy^d          B.2 utt1: [ [ I guess, +   \"\ntext <- str_trim(text)  # remove leading and trailing whitespace\nhead(text)  # preview first lines of `text`\n#> [1] \"\"                                      \n#> [2] \"\"                                      \n#> [3] \"o          A.1 utt1: Okay.  /\"         \n#> [4] \"qw          A.1 utt2: {D So, }\"        \n#> [5] \"\"                                      \n#> [6] \"qy^d          B.2 utt1: [ [ I guess, +\"\ntext <- text[text != \"\"]  # remove blank lines\nhead(text)  # preview first lines of `text`\n#> [1] \"o          A.1 utt1: Okay.  /\"                                                                  \n#> [2] \"qw          A.1 utt2: {D So, }\"                                                                 \n#> [3] \"qy^d          B.2 utt1: [ [ I guess, +\"                                                         \n#> [4] \"+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\"\n#> [5] \"+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\"                        \n#> [6] \"qy          A.5 utt1: Does it say something? /\"\ndata <- data.frame(doc_id, text) # tidy format `doc_id` and `text`\nslice_head(data, n = 5) %>% # preview first lines of `text`\n  knitr::kable(booktabs = TRUE,\n               caption = \"First 5 observations of prelim data curation of the SDAC data.\")o          A.1 utt1: Okay.  /\nqw          A.1 utt2: {D So, }\nqy^d          B.2 utt1: [ [ I guess, +\n+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\n+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\nqy          A.5 utt1: Does it say something? /\nsd          B.6 utt1: I think it usually does.  /\nad          B.6 utt2: You might try, {F uh, }  /\nh          B.6 utt3: I don't know,  /\nad          B.6 utt4: hold it down a little longer,  /^.+?\\s\n[AB]\\.\\d+\nutt\\d+\n:.+$\n# Extract column information from `text`\ndata <- \n  data %>% # current dataset\n  mutate(damsl_tag = str_extract(string = text, pattern = \"^.+?\\\\s\")) %>%  # extract damsl tags\n  mutate(speaker_turn = str_extract(string = text, pattern = \"[AB]\\\\.\\\\d+\")) %>% # extract speaker_turn pairs\n  mutate(utterance_num = str_extract(string = text, pattern = \"utt\\\\d+\")) %>% # extract utterance number\n  mutate(utterance_text = str_extract(string = text, pattern = \":.+$\")) %>%  # extract utterance text\n  select(-text) # drop the `text` column\n\nglimpse(data) # preview the data set\n#> Rows: 159\n#> Columns: 5\n#> $ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n#> $ damsl_tag      <chr> \"o \", \"qw \", \"qy^d \", \"+ \", \"+ \", \"qy \", \"sd \", \"ad \", …\n#> $ speaker_turn   <chr> \"A.1\", \"A.1\", \"B.2\", \"A.3\", \"B.4\", \"A.5\", \"B.6\", \"B.6\",…\n#> $ utterance_num  <chr> \"utt1\", \"utt2\", \"utt1\", \"utt1\", \"utt1\", \"utt1\", \"utt1\",…\n#> $ utterance_text <chr> \": Okay.  /\", \": {D So, }\", \": [ [ I guess, +\", \": What…\ndata <-\n  data %>% # current dataset\n  separate(col = speaker_turn, # source column\n           into = c(\"speaker\", \"turn_num\")) # separate speaker_turn into distinct columns: speaker and turn_num\n\nglimpse(data) # preview the data set\n#> Rows: 159\n#> Columns: 6\n#> $ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n#> $ damsl_tag      <chr> \"o \", \"qw \", \"qy^d \", \"+ \", \"+ \", \"qy \", \"sd \", \"ad \", …\n#> $ speaker        <chr> \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n#> $ turn_num       <chr> \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n#> $ utterance_num  <chr> \"utt1\", \"utt2\", \"utt1\", \"utt1\", \"utt1\", \"utt1\", \"utt1\",…\n#> $ utterance_text <chr> \": Okay.  /\", \": {D So, }\", \": [ [ I guess, +\", \": What…\n# Clean up column information\ndata <- \n  data %>% # current dataset\n  mutate(damsl_tag = str_trim(damsl_tag)) %>% # remove leading/ trailing whitespace\n  mutate(utterance_num = str_replace(string = utterance_num, pattern = \"utt\", replacement = \"\")) %>% # remove 'utt'\n  mutate(utterance_text = str_replace(string = utterance_text, pattern = \":\\\\s\", replacement = \"\")) %>% # remove ': '\n  mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace\n\nglimpse(data) # preview the data set\n#> Rows: 159\n#> Columns: 6\n#> $ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n#> $ damsl_tag      <chr> \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n#> $ speaker        <chr> \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n#> $ turn_num       <chr> \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n#> $ utterance_num  <chr> \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n#> $ utterance_text <chr> \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n# Link speaker with speaker_id\ndata <- \n  data %>% # current dataset\n  mutate(speaker_id = case_when( # create speaker_id\n    speaker == \"A\" ~ speaker_a_id, # speaker_a_id value when A\n    speaker == \"B\" ~ speaker_b_id # speaker_b_id value when B\n  ))\n\nglimpse(data) # preview the data set\n#> Rows: 159\n#> Columns: 7\n#> $ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n#> $ damsl_tag      <chr> \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n#> $ speaker        <chr> \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n#> $ turn_num       <chr> \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n#> $ utterance_num  <chr> \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n#> $ utterance_text <chr> \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n#> $ speaker_id     <chr> \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…\nextract_sdac_metadata <- function(file) {\n  # Function: to read a Switchboard Corpus Dialogue file and extract meta-data\n  cat(\"Reading\", basename(file), \"...\")\n  \n  # Read `file` by lines\n  doc <- read_lines(file) \n  \n  # Extract `doc_id`, `speaker_a_id`, and `speaker_b_id`\n  doc_speaker_info <- \n    doc[str_detect(doc, \"\\\\d+_\\\\d+_\\\\d+\")] %>% # isolate pattern\n    str_extract(\"\\\\d+_\\\\d+_\\\\d+\") %>% # extract the pattern\n    str_split(pattern = \"_\") %>% # split the character vector\n    unlist() # flatten the list to a character vector\n  doc_id <- doc_speaker_info[1] # extract `doc_id`\n  speaker_a_id <- doc_speaker_info[2] # extract `speaker_a_id`\n  speaker_b_id <- doc_speaker_info[3] # extract `speaker_b_id`\n  \n  # Extract `text`\n  text_start_index <- # find where header info stops\n    doc %>% \n    str_detect(pattern = \"={3,}\") %>% # match 3 or more `=`\n    which() # find vector index\n  \n  text_start_index <- text_start_index + 1 # increment index by 1\n  text_end_index <- length(doc) # get the end of the text section\n  \n  text <- doc[text_start_index:text_end_index] # extract text\n  text <- str_trim(text) # remove leading and trailing whitespace\n  text <- text[text != \"\"] # remove blank lines\n  \n  data <- data.frame(doc_id, text) # tidy format `doc_id` and `text`\n  \n  # Extract column information from `text`\n  data <- \n    data %>% \n    mutate(damsl_tag = str_extract(string = text, pattern = \"^.+?\\\\s\")) %>%  # extract damsl tags\n    mutate(speaker_turn = str_extract(string = text, pattern = \"[AB]\\\\.\\\\d+\")) %>% # extract speaker_turn pairs\n    mutate(utterance_num = str_extract(string = text, pattern = \"utt\\\\d+\")) %>% # extract utterance number\n    mutate(utterance_text = str_extract(string = text, pattern = \":.+$\")) %>%  # extract utterance text\n    select(-text)\n  \n  # Separate speaker_turn into distinct columns\n  data <-\n    data %>% # current dataset\n    separate(col = speaker_turn, # source column\n             into = c(\"speaker\", \"turn_num\")) # separate speaker_turn into distinct columns: speaker and turn_num\n  \n  # Clean up column information\n  data <- \n    data %>% \n    mutate(damsl_tag = str_trim(damsl_tag)) %>% # remove leading/ trailing whitespace\n    mutate(utterance_num = str_replace(string = utterance_num, pattern = \"utt\", replacement = \"\")) %>% # remove 'utt'\n    mutate(utterance_text = str_replace(string = utterance_text, pattern = \":\\\\s\", replacement = \"\")) %>% # remove ': '\n    mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace\n  \n  # Link speaker with speaker_id\n  data <- \n    data %>% # current dataset\n    mutate(speaker_id = case_when( # create speaker_id\n      speaker == \"A\" ~ speaker_a_id, # speaker_a_id value when A\n      speaker == \"B\" ~ speaker_b_id # speaker_b_id value when B\n    ))\n  cat(\" done.\\n\")\n  return(data) # return the data frame object\n}\nextract_sdac_metadata(file = \"../data/original/sdac/sw00utt/sw_0001_4325.utt\") %>%\n    glimpse()#> Reading sw_0001_4325.utt ... done.\n#> Rows: 159\n#> Columns: 7\n#> $ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n#> $ damsl_tag      <chr> \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n#> $ speaker        <chr> \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n#> $ turn_num       <chr> \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n#> $ utterance_num  <chr> \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n#> $ utterance_text <chr> \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n#> $ speaker_id     <chr> \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…\nsdac_files <- \n  fs::dir_ls(path = \"../data/original/sdac/\", # source directory\n             recurse = TRUE, # traverse all sub-directories\n             type = \"file\", # only return files\n             regexp = \"\\\\.utt$\") # only return files ending in .utt\nhead(sdac_files) # preview file paths../data/original/sdac/sw00utt/sw_0001_4325.utt\n../data/original/sdac/sw00utt/sw_0002_4330.utt\n../data/original/sdac/sw00utt/sw_0003_4103.utt\n../data/original/sdac/sw00utt/sw_0004_4327.utt\n../data/original/sdac/sw00utt/sw_0005_4646.utt\n../data/original/sdac/sw00utt/sw_0006_4108.utt\n# Read files and return a tidy dataset\nsdac <- \n  sdac_files %>% # pass file names\n  map(extract_sdac_metadata) %>% # read and tidy iteratively \n  bind_rows() # bind the results into a single data frame\nglimpse(sdac)  # preview complete curated dataset\n#> Rows: 223,606\n#> Columns: 7\n#> $ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n#> $ damsl_tag      <chr> \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n#> $ speaker        <chr> \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n#> $ turn_num       <chr> \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n#> $ utterance_num  <chr> \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n#> $ utterance_text <chr> \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n#> $ speaker_id     <chr> \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…"},{"path":"curate-datasets-chapter.html","id":"write-datasets","chapter":"6 Curate data(sets)","heading":"6.3.3 Write datasets","text":"previous cases, write dataset disk prepare next step text analysis project.directory structure now looks like :","code":"\nfs::dir_create(path = \"../data/derived/sdac/\")  # create sdac subdirectory\nwrite_csv(sdac, file = \"../data/derived/sdac/sdac_curated.csv\")  # write sdac to disk and label as the curated datasetdata/\n├── derived/\n│   └── sdac/\n│       └── sdac_curated.csv\n└── original/\n    └── sdac/\n        ├── README\n        ├── doc/\n        ├── sw00utt/\n        ├── sw01utt/\n        ├── sw02utt/\n        ├── sw03utt/\n        ├── sw04utt/\n        ├── sw05utt/\n        ├── sw06utt/\n        ├── sw07utt/\n        ├── sw08utt/\n        ├── sw09utt/\n        ├── sw10utt/\n        ├── sw11utt/\n        ├── sw12utt/\n        └── sw13utt/"},{"path":"curate-datasets-chapter.html","id":"summary-8","chapter":"6 Curate data(sets)","heading":"6.3.4 Summary","text":"section looked semi-structured data. type data often requires work organize tidy dataset. continued work many R programming strategies introduced point coursebook. also made extensive use regular expressions pick information semi-structured document format.round section ’ve provided code summary steps involved conduct curation Switchboard Dialogue Act Corpus files. Note ’ve added extract_sdac_metadata() custom function file called curate_functions.R sourced file. make code succinct legible , well research projects.","code":"\n\n# Source the `extract_sdac_metadata()` function\nsource(\"../functions/curate_functions.R\") \n\n# Get list of the corpus files (.utt)\nsdac_files <- \n  fs::dir_ls(path = \"../data/original/sdac/\", # source directory\n             recurse = TRUE, # traverse all sub-directories\n             type = \"file\", # only return files\n             regexp = \"\\\\.utt$\") # only return files ending in .utt\n\n# Read files and return a tidy dataset\nsdac <- \n  sdac_files %>% # pass file names\n  map(extract_sdac_metadata) %>% # read and tidy iteratively \n  bind_rows() # bind the results into a single data frame\n  \n# Write curated dataset to disk\nfs::dir_create(path = \"../data/derived/sdac/\") # create sdac subdirectory\nwrite_csv(sdac, \n          file = \"../data/derived/sdac/sdac_curated.csv\") # write sdac to disk and label as the curated dataset"},{"path":"curate-datasets-chapter.html","id":"documentation-2","chapter":"6 Curate data(sets)","heading":"6.4 Documentation","text":"stage want ensure data derived well-documented. data acquisition process documentation focused sampling frame, curated datasets require documentation describes structure now rectangular dataset attributes. documentation known data dictionary. curation stage documentation often contains following information (“make data dictionary,” 2021):names variables (appear dataset)human-readable names variablesshort prose descriptions variables, including units measurement (applicable)data dictionary take format table can stored tabular-oriented file format (.csv). often easier work spreadsheet create documentation. suggest creating .csv file basic structure documentation. can however choose, suggest using something along lines seen following custom function, data_dic_starter().Running function R Console curated dataset (case sdac dataset), provide structure.Table 6.10: Data dictionary starter structure SDAC curated dataset.resulting .csv file can opened spreadsheet software (MS Excel, Google Sheets, etc.) edited.21Save file .csv file replace original starter file. Note important use plain-text file format official documentation file avoid proprietary formats ensure open accessibility future compatibility.22Our data/derived/ directory now looks like .","code":"\ndata_dic_starter <- function(data, file_path) {\n  # Function:\n  # Creates a .csv file with the basic information\n  # to document a curated dataset\n  \n  tibble(variable_name = names(data), # column with existing variable names \n       name = \"\", # column for human-readable names\n       description = \"\") %>% # column for prose description\n  write_csv(file = file_path) # write to disk\n}data/\n└── derived/\n    └── sdac/\n        ├── sdac_curated.csv\n        └── data_dictionary_sdac.csv"},{"path":"curate-datasets-chapter.html","id":"summary-9","chapter":"6 Curate data(sets)","heading":"6.5 Summary","text":"chapter looked process structuring data dataset. included discussion three main types data –unstructured, structured, semi-structured. level structure original data(set) vary resource resource token file format used support level meta-information included. results data curation resulted curated dataset saved separate original data maintain modularity data(set) looked like intervene afterwards. addition code use derived curated dataset’s structure, also include data dictionary documents names variables provides sufficient description variables clear dataset contains.important recognized curated dataset form base next step text analysis project last step data preparation analysis: dataset transformation. last step preparing data analysis convert curated dataset dataset directly aligned research aims (.e. analysis method(s)) project. Since can multiple analysis approaches applied original data research project, curated dataset serves point departure subsequent datasets derived transformational steps.","code":""},{"path":"transform-datasets-chapter.html","id":"transform-datasets-chapter","chapter":"7 Transform datasets","heading":"7 Transform datasets","text":"\nINCOMPLETE DRAFT\nNothing lost. Everything transformed.— Michael Ende, Neverending Story\nessential questions chapter :\n\nrole data transformation text analysis project?\n\ngeneral processes preparing datasets analysis?\n\ngeneral processes transform datasets?\nchapter turn attention process moving curated dataset one step closer analysis. process curating data dataset goal derived tidy dataset contained main relational characteristics data text analysis project, transformation step refines potential expands characteristics line analysis aims. chapter grouped various transformation steps four categories: normalization, recoding, generation, merging. note categories ordered covered separately descriptive reasons. practice ordering transformation apply another highly idiosyncratic requires researcher evaluate characteristics dataset desired results.Furthermore, since given project may one analysis may performed data, may distinct transformation steps correspond analysis approach. Therefore possible one transformed dataset created curated dataset. one reasons create curated dataset instead derived transformed dataset original data. curated dataset serves point departure multiple transformational methods can derive distinct formats distinct analyses.Let’s now turn demonstrations common transformational steps using datasets now familiar","code":""},{"path":"transform-datasets-chapter.html","id":"normalize","chapter":"7 Transform datasets","heading":"7.1 Normalize","text":"process normalizing datasets essence santize values variable set variables artifacts contaminate subsequent processing. may case non-linguistic metadata may require normalization often linguistic information common target normalization text often includes artifacts acquisition process desired analysis.Europarle CorpusConsider curated Europarle Corpus dataset. read dataset. Since dataset quite large, also subsetted dataset keeping first 1,000 observations value type demonstration purposes.Simply looking first 14 lines dataset, can see goal work transcribed (‘Source’) translated (‘Target’) language, lines appear interest.Table 7.1: Europarle Corpus curated dataset preview.sentence_id 1 appears title sentence_id 7 reflects description parliamentary session. artifacts like remove dataset.remove lines can turn programming strategies ’ve previously worked . Namely use filter() filter observations combination str_detect() detect matches pattern indicative lines want remove lines want keep.remove lines, let’s try craft search pattern identify lines, exclude lines want keep. Condition one lines start opening parenthesis (. Condition two lines end standard sentence punctuation (., !, ?). ’ve added conditions one filter() using logical operator (|) ensure either condition matched output.Table 7.2: Non-speech lines Europarle dataset.Since search appears match lines want preserve, let’s move now eliminate lines dataset. use regular expression patterns, now condition ’s filter() call str_detect() negated prefixed !.Let’s look first 14 lines , now eliminated artifacts.Table 7.3: Europarle Corpus non-speech lines removed.One issue may want resolve concerns fact whitespaces possessive forms (.e. “minute’ s silence”). case can employ str_replace_all() inside mutate() function overwrite sentence values match apostrophe ' whitespace (\\\\s) s.Now normalized text sentence column Europarle dataset.Last FM LyricsLet’s look another dataset worked coursebook: Lastfm lyrics. Reading lastfm_curated dataset data/derived/ directory can see structure curated structure.Table 7.4: Last fm lyrics dataset preview one artist/ song per genre lyrics text truncated 200 characters display purposes.things might want clean lyrics column’s values. First, lines original webscrape end one stanza runs next without whitespace (.e. “honeymoonYou”). reflect contiguous end-new line segments stanzas joined curation process. Second, see appear backing vocals appear parentheses (.e. “(Take )”).cases use mutate(). contiguous end-new line segments use str_replace_all() inside backing vocals parentheses use str_remove_all().pattern match end-new lines stanzas use regular expression magic. base pattern includes finding pair lowercase-uppercase letters (.e. “nY”, “honeymoonYou”). can use pattern [-z][-Z]. replace pattern using lowercase letter space uppercase letter take advantage grouping syntax regular expressions (...). add parentheses around two groups capture like ([-z])([-Z]). replacement argument str_replace_all() function specify use captured groups order appear \\\\1 lowercase letter match \\\\2 uppercase letter match.Now, ’ve looked extensively lyrics column found combinations joined stanzas. Namely ', !, ,, ), ?, also may precede uppercase letter. make sure capture possibilities well ’ve updated regular expression ([-z'!,.)?])([-Z]).Now remove backing vocals, regex pattern \\\\(.+?\\\\) –match parentheses everything within parentheses. added ? + operator known ‘lazy’ operator. specifies .+ match minimal string enclosed trailing ). include get matches span first parenthesis ( way last, match real lyrics, just backing vocals.Putting work let’s clean lyrics column.Table 7.5: Last fm lyrics cleaned lyrics…Now given fact songs poems, many lines complete sentences practical way try segment grammatical sentence units. case, seems like good stopping point normalizing lastfm dataset.","code":"\neuroparle <- read_csv(file = \"../data/derived/europarle/europarle_curated.csv\") %>%  # read curated dataset\n  filter(sentence_id < 1001) # keep first 1000 observations for each type\n\nglimpse(europarle)#> Rows: 2,000\n#> Columns: 3\n#> $ type        <chr> \"Source\", \"Target\", \"Source\", \"Target\", \"Source\", \"Target\"…\n#> $ sentence_id <dbl> 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, …\n#> $ sentence    <chr> \"Reanudación del período de sesiones\", \"Resumption of the …\n# Identify non-speech lines\neuroparle %>% \n  filter(str_detect(sentence, \"^\\\\(\") | str_detect(sentence, \"[^.!?]$\")) %>% # filter lines that detect a match for either condition 1 or 2\n  slice_sample(n = 10) %>% # random sample of 10 observations\n  knitr::kable(booktabs = TRUE,\n        caption = 'Non-speech lines in the Europarle dataset.')\neuroparle <- \n  europarle %>% # dataset\n  filter(!str_detect(sentence, pattern = \"^\\\\(\")) %>% # remove lines starting with (\n  filter(!str_detect(sentence, pattern = \"[^.!?]$\")) # remove lines not ending in ., !, or ?\neuroparle <- \n  europarle %>% # dataset\n  mutate(sentence = str_replace_all(string = sentence, \n                                    pattern = \"'\\\\ss\", \n                                    replacement = \"'s\")) # replace ' s with `s\nlastfm <- read_csv(file = \"../data/derived/lastfm/lastfm_curated.csv\")  # read in lastfm_curated dataset\nlastfm <- \n  lastfm %>% # dataset\n  mutate(lyrics = \n           str_replace_all(string = lyrics, \n                           pattern = \"([a-z'!,.)?I])([A-Z])\", # find contiguous end/ new line segments\n                           replacement = \"\\\\1 \\\\2\")) %>%  # replace with whitespace between\n  mutate(lyrics = str_remove_all(lyrics, \"\\\\(.+?\\\\)\")) # remove backing vocals (Take On Me)"},{"path":"transform-datasets-chapter.html","id":"recode","chapter":"7 Transform datasets","heading":"7.2 Recode","text":"Normalizing text can seen extension dataset curation extent structure dataset maintained. Europarle Lastfm cases saw true. case recoding, transformational steps, aim modify dataset structure either rows, columns, . Recoding processes can characterized creation structural changes derived values variables effectively recasting values new variables enable direct access analyses.Switchboard Dialogue Act CorpusThe Switchboard Dialogue Act Corpus dataset curated previous chapter contains number variables describing conversations speakers American English.Let’s read dataset take closer look.Among number metadata variables, curated dataset includes utterance_text column contains dialogue conversations interleaved disfluency annotation scheme.Table 7.6: 20 randomly sampled lines SDAC curated dataset.Let’s drop variables dataset rein focus. keep doc_id, speaker_id, utterance_text.Table 7.7: First 10 lines simplified SDAC curated dataset.disfluency annotation system, various conventions used non-sentence elements. say, example, researcher interested understanding use filled pauses (‘uh’ ‘uh’), aim identify lines {F ...} annotation used around utterances ‘uh’ ‘um’.turn str_count() function. function count number matches found pattern. can use regular expression identify pattern interest instances {F followed either uh um. Since disfluencies may start utterance, therefore capitalized need formulate regular expression allows either U u disfluency type. result disfluency match added new column. create new column wrap str_count() mutate() give new column meaningful name. case ’ve opted uh um.Table 7.8: First 20 lines SDAC dataset counts disfluencies ‘uh’ ‘um’.Now two new columns, uh um indicate many times relevant pattern matched given utterance. choosing focus disfluencies, however, made decision change unit observation utterance use filled pauses (uh um). means dataset stands, tidy format –observation corresponds observational unit. datasets misaligned particular way, known ‘wide’ format. want , , restructure dataset row corresponds unit observation –case filled pause type.convert current (wide) dataset one filler type listed counts measured utterance turn pivot_longer() function. function creates two new columns, one column names listed one values column names.Table 7.9: First 20 lines SDAC dataset tidy format fillers unit observation.Last fmIn previous example, used matching approach extract information embedded one column dataset recoded dataset maintain fidelity particular unit observation metadata.Another common approach recoding datasets text analysis projects involves recoding linguistic units smaller units; process known tokenization.Let’s return lastfm object normalized earlier chapter see various ways one can choose tokenize linguistic information.Table 7.10: Last fm dataset normalized lyrics.current lastfm dataset, unit observation lyrics entire artist, song, genre combination. , however, like change unit say words, like word used appear row, still maintaining relevant attributes associated word.tidytext package includes useful function unnest_tokens() allows us tokenize textual input smaller linguistic units. ‘unnest’ part function name refers process extracting unit interest maintaining relevant attributes. Let’s see action.Table 7.11: First 10 observations lastfm dataset tokenized words.can see output, word appears separate line order appearance input text (lyrics). Furthermore, output tidy format words still associated relevant attribute values (artist, song, genre). default tokenized text output lowercased original text input column dropped. can overridden, however, desired.addition ‘words’, unnest_tokens() function provides easy access number common tokenized units including ‘characters’, ‘sentences’, ‘paragraphs’.Table 7.12: First 10 observations lastfm dataset tokenized characters.two built-options ‘sentences’ ‘paragraphs’ depend punctuation / line breaks function, particular dataset, options work given particular characteristics lyrics variable.even options allow creation sequences linguistic units. Say want tokenize lyrics two-word sequences, can specify token ‘ngrams’ add argument n = 2 reflect want two-word sequences.Table 7.13: First 10 observations lastfm dataset tokenized bigramsThe ‘n’ ‘ngram’ refers number word-sequence units want tokenize. Two-word sequences known ‘bigrams’, three-word sequences ‘trigrams’, .","code":"\nsdac <- read_csv(file = \"../data/derived/sdac/sdac_curated.csv\")  # read curated dataset\nsdac_simplified <- \n  sdac %>% # dataset\n  select(doc_id, speaker_id, utterance_text) # columns to retain\nsdac_disfluencies <- \n  sdac_simplified %>% # dataset\n  mutate(uh = str_count(utterance_text, \"\\\\{F [Uu]h\")) %>% # match {F Uh or {F uh}\n  mutate(um = str_count(utterance_text, \"\\\\{F [Uu]m\")) # match {F Um or {F um}\nsdac_disfluencies <- \n  sdac_disfluencies %>% # dataset\n  pivot_longer(cols = c(\"uh\", \"um\"), # columns to convert\n               names_to = \"filler\", # column for the column names (i.e. filler types)\n               values_to = \"count\") # column for the column values (i.e. counts)\nlastfm %>% # dataset\n  unnest_tokens(output = word, # column for tokenized output\n                input = lyrics, # input column\n                token = \"words\") %>% # tokenize unit type\n  slice_head(n = 10) %>%  # preview first 10 lines\n  kable(booktabs = TRUE,\n        caption = \"First 10 observations for lastfm dataset tokenized by words.\")\nlastfm %>% # dataset\n  unnest_tokens(output = character, # column for tokenized output\n                input = lyrics, # input column\n                token = \"characters\") %>% # tokenize unit type\n  slice_head(n = 10) %>%  # preview first 10 lines\n  kable(booktabs = TRUE,\n        caption = \"First 10 observations for lastfm dataset tokenized by characters.\")\nlastfm %>% \n  unnest_tokens(output = bigram, # column for tokenized output\n                input = lyrics, # input column\n                token = \"ngrams\", # tokenize unit type\n                n = 2) %>%  # size of word sequences \n  slice_head(n = 10) %>%  # preview first 10 lines\n  kable(booktabs = TRUE,\n        caption = \"First 10 observations for lastfm dataset tokenized by bigrams\")"},{"path":"transform-datasets-chapter.html","id":"generate","chapter":"7 Transform datasets","heading":"7.3 Generate","text":"process recoding dataset transformation dataset works information already explicit. process generation, however, aims make implicit information explicit. common type operation involved generation process addition linguistic annotation. process can accomplished manually researcher research team automatically use pre-trained linguistic resources / software. Ideally annotation linguistic information can conducted automatically.important considerations, however, need taken account choosing whether linguistic annotation can conducted automatically. First foremost type annotation desired. Information part speech (grammatical category) morpho-syntactic information common types linguistic annotation can conducted automatically. Second degree resource used annotate linguistic information aligned language variety /register also key consideration. noted, automatic linguistic annotation methods contingent pre-trained resources. language language variety used develop resources may available language investigation, , language variety / register may align. degree resource align linguistic information targeted annotation directly related quality final annotations. clear, annotation method, whether manual automatic guaranteed perfectly accurate.Let’s take look annotation language Europarle dataset normalized.Table 7.14: First 10 lines English normalized SDAC dataset.use cleanNLP package linguistic annotation. annotation process depends pre-trained language models. list models available access. load_model_udpipe() custom function downloads specified language model initialized udpipe engine (cnlp_init_udpipe()) conducting annotations.test case, let’s load ‘english’ model annotate sentence line Europarle dataset illustrate basic workflow.see structure returned cnlp_annotate() function list. list contains two data frames (tibbles). One tokens (annotation information) document (metadata information). can inspect annotation characteristics one sentence targetting $tokens data frame. Let’s take look linguistic annotation information returned.Table 7.15: Annotation information single English sentence Europarle dataset.quite bit information returned cnlp_annotate(). First note input sentence tokenized word. token includes token, lemma, part speech (upos xpos), morphological features (feats), syntactic relationships (tid_source relation). also key note doc_id, sid tid maintain relational attributes original dataset –therefore maintains annotated dataset tidy format.Let’s now annotate sentence Europarle corpus Source (‘Spanish’) note similarities differences.Table 7.16: Annotation information single Spanish sentence Europarle dataset.Spanish version sentence, see variables. However, feats variable morphological information specific Spanish –notably gender mood.","code":"\neuroparle %>%\n    filter(type == \"Target\") %>%\n    slice_head(n = 10) %>%\n    kable(booktabs = TRUE, caption = \"First 10 lines in English from the normalized SDAC dataset.\")\nload_model_udpipe <- function(model_lang) {\n  # Function\n  # Download and load the specified udpipe language model\n  \n  cnlp_init_udpipe(model_lang) # to download the model, if not downloaded\nbase_path <- system.file(\"extdata\", package = \"cleanNLP\") # get the base path\n  model_name <- # extract the model_name\n    base_path %>% # extract the base path\n    dir() %>% # get the directory\n    stringr::str_subset(pattern = paste0(\"^\", model_lang)) # extract the name of the model\n  \n  udpipe::udpipe_load_model(file = file.path(base_path, model_name, fsep = \"/\")) %>% # create the path to the downloaded model stored on disk\n    return()\n}\neng_model <- load_model_udpipe(\"english\") # load and initialize the language model, 'english' in this case.\n\neng_annotation <- \n  europarle %>% # dataset \n  filter(type == \"Target\" & sentence_id == 6) %>% # select English and sentence_id 6\n  cnlp_annotate(text_name = \"sentence\", # input text (sentence)\n                doc_name = \"sentence_id\") # specify the grouping column (sentence_id)\n\nglimpse(eng_annotation) # preview structure\n#> List of 2\n#>  $ token   : tibble [11 × 11] (S3: tbl_df/tbl/data.frame)\n#>   ..$ doc_id       : num [1:11] 6 6 6 6 6 6 6 6 6 6 ...\n#>   ..$ sid          : int [1:11] 1 1 1 1 1 1 1 1 1 1 ...\n#>   ..$ tid          : chr [1:11] \"1\" \"2\" \"3\" \"4\" ...\n#>   ..$ token        : chr [1:11] \"Please\" \"rise\" \",\" \"then\" ...\n#>   ..$ token_with_ws: chr [1:11] \"Please \" \"rise\" \", \" \"then\" ...\n#>   ..$ lemma        : chr [1:11] \"please\" \"rise\" \",\" \"then\" ...\n#>   ..$ upos         : chr [1:11] \"INTJ\" \"VERB\" \"PUNCT\" \"ADV\" ...\n#>   ..$ xpos         : chr [1:11] \"UH\" \"VB\" \",\" \"RB\" ...\n#>   ..$ feats        : chr [1:11] NA \"Mood=Imp|VerbForm=Fin\" NA \"PronType=Dem\" ...\n#>   ..$ tid_source   : chr [1:11] \"2\" \"0\" \"2\" \"10\" ...\n#>   ..$ relation     : chr [1:11] \"discourse\" \"root\" \"punct\" \"advmod\" ...\n#>  $ document: tibble [1 × 2] (S3: tbl_df/tbl/data.frame)\n#>   ..$ type  : chr \"Target\"\n#>   ..$ doc_id: num 6\n#>  - attr(*, \"class\")= chr [1:2] \"cnlp_annotation\" \"list\"\nspa_model <- load_model_udpipe(\"spanish\") # load and initialize the language model, 'spanish' in this case.\n\nspa_annotation <- \n  europarle %>% # dataset \n  filter(type == \"Source\" & sentence_id == 6) %>% # select Spanish and sentence_id 6\n  cnlp_annotate(text_name = \"sentence\", # input text (sentence)\n                doc_name = \"sentence_id\") # specify the grouping column (sentence_id)"},{"path":"transform-datasets-chapter.html","id":"merge","chapter":"7 Transform datasets","heading":"7.4 Merge","text":"One final class transformations can applied curated datasets enhance informativeness research project process merging two datasets. merge datasets required datasets share one attributes. common attribute two datasets can joined coordinate attributes one dataset effectively adding attributes one dataset extended information. Another approach join datasets goal filtering one datasets given matching attribute.Let’s see practice. Take lastfm dataset. Let’s tokenize dataset words, using unnest_tokens() unit observation words.Table 7.17: First 10 observations lastfm_words dataset.Consider get_sentiments() function returns words classified ‘positive’- ‘negative’-biased, lexicon set ‘bing’ (Hu & Liu, 2004).Since sentiments_bing dataset lastfm_words dataset share column word (type values) can join two datasets. sentiments_bing dataset 6786 unique words. Let’s check many distinct words lastfm_words dataset .One thing note sentiments_bing dataset include function words, words associated closed-class categories (pronouns, determiners, prepositions, etc.) words semantic content along lines positive negative. many words appear lastfm_words matched. thing note sentiments_bing lexicon undoubtly words appear lastfm_words vice versa.want keep words lastfm_words add sentiment information words match datasets, can use left_join() function. lastfm_words dataset ‘left’ therefore rows dataset retained.Table 7.18: First 10 observations lastfm_words sentiments_bing` left join.see quite words lastfm_words matched. focus words lastfm_words match, ’ll run join operation filter rows sentiment empty (.e. match sentiments_bing lexicon).Table 7.19: First 10 observations lastfm_words sentiments_bing` left join.Let’s turn another type join: anti-join. purpose anti-join eliminate matches. makes sense quick dirty approach removing function words (.e. grammatical words little semantic content). case use get_stopwords() function get dataset. ’ll specify English language ’ll use default lexicon (‘Snowball’).Now want eliminate stopwords lastfm_words dataset use anti_join(). observations lastfm_words match english_stopwords returned.Table 7.20: First 10 observations lastfm_words filtering English stopwords.can also merge datasets generate analysis import sources. can useful cases corpus associated metadata contained files separate corpus . case Switchboard Dialogue Act Corpus.existing, disfluency recoded, version includes following variables.online documentation page provides key file caller_tab.csv contains speaker metadata information. Included .csv file column caller_no contains speaker_id currently sdac_disfluencies dataset. Let’s read file R session renaming caller_no speaker_id prepare join datasets.Now join sdac_disfluencies sdac_speaker_meta. Let’s turn left_join() want retain observations (rows) sdac_disfluencies add columns sdac_speaker_meta speaker_id column values match.Now metadata columns may want keep others may want drop may importance analysis. ’m going assume want keep sex, birth_year, dialect_area, education drop rest.Table 7.21: First 10 observations sdac_disfluencies dataset speaker metadata.","code":"\nlastfm_words <- \n  lastfm %>% # dataset\n  unnest_tokens(output = \"word\", # output column\n                input = \"lyrics\", # input column\n                token = \"words\") # tokenized unit (words)\n\nlastfm_words %>% # dataset\n  slice_head(n = 10) %>% # first 10 observations\n  kable(booktabs = TRUE,\n        caption = \"First 10 observations for `lastfm_words` dataset.\")\nsentiments_bing <- get_sentiments(lexicon = \"bing\")  # get 'bing' lexicon\n\nsentiments_bing %>%\n    slice_head(n = 10)  # preview first 10 observations\n#> # A tibble: 10 × 2\n#>    word        sentiment\n#>    <chr>       <chr>    \n#>  1 2-faces     negative \n#>  2 abnormal    negative \n#>  3 abolish     negative \n#>  4 abominable  negative \n#>  5 abominably  negative \n#>  6 abominate   negative \n#>  7 abomination negative \n#>  8 abort       negative \n#>  9 aborted     negative \n#> 10 aborts      negative\nlastfm_words %>% # dataset\n  distinct(word) %>% # find unique words\n  nrow() # count distinct rows/ words\n#> [1] 4614\nleft_join(lastfm_words, sentiments_bing) %>% \n  slice_head(n = 10) %>% # first 10 observations\n  kable(booktabs = TRUE,\n        caption = \"First 10 observations for the `lastfm_words` sentiments_bing` left join.\")\nleft_join(lastfm_words, sentiments_bing) %>%\n  filter(sentiment != \"\") %>% # return matched sentiments\n  slice_head(n = 10) %>% # first 10 observations\n  kable(booktabs = TRUE,\n        caption = \"First 10 observations for the `lastfm_words` sentiments_bing` left join.\")\nenglish_stopwords <- get_stopwords(language = \"en\")  # get English stopwords from the Snowball lexicon\n\nenglish_stopwords %>%\n    slice_head(n = 10)  # preview first 10 observations\n#> # A tibble: 10 × 2\n#>    word      lexicon \n#>    <chr>     <chr>   \n#>  1 i         snowball\n#>  2 me        snowball\n#>  3 my        snowball\n#>  4 myself    snowball\n#>  5 we        snowball\n#>  6 our       snowball\n#>  7 ours      snowball\n#>  8 ourselves snowball\n#>  9 you       snowball\n#> 10 your      snowball\nanti_join(lastfm_words, english_stopwords) %>%\n    slice_head(n = 10) %>%\n    kable(booktabs = TRUE, caption = \"First 10 observations in `lastfm_words` after filtering for English stopwords.\")\nsdac_disfluencies %>% # dataset\n  slice_head(n = 10) # preview first 10 observations\n#> # A tibble: 10 × 5\n#>    doc_id speaker_id utterance_text                                 filler count\n#>     <dbl>      <dbl> <chr>                                          <chr>  <int>\n#>  1   4325       1632 Okay.  /                                       uh         0\n#>  2   4325       1632 Okay.  /                                       um         0\n#>  3   4325       1632 {D So, }                                       uh         0\n#>  4   4325       1632 {D So, }                                       um         0\n#>  5   4325       1519 [ [ I guess, +                                 uh         0\n#>  6   4325       1519 [ [ I guess, +                                 um         0\n#>  7   4325       1632 What kind of experience [ do you, + do you ] … uh         0\n#>  8   4325       1632 What kind of experience [ do you, + do you ] … um         0\n#>  9   4325       1519 I think, ] + {F uh, } I wonder ] if that work… uh         1\n#> 10   4325       1519 I think, ] + {F uh, } I wonder ] if that work… um         0\nsdac_speaker_meta <- \n  read_csv(file = \"https://catalog.ldc.upenn.edu/docs/LDC97S62/caller_tab.csv\", \n           col_names = c(\"speaker_id\", # changed from `caller_no`\n                         \"pin\",\n                         \"target\",\n                         \"sex\",\n                         \"birth_year\",\n                         \"dialect_area\",\n                         \"education\",\n                         \"ti\",\n                         \"payment_type\",\n                         \"amt_pd\",\n                         \"con\",\n                         \"remarks\",\n                         \"calls_deleted\",\n                         \"speaker_partition\"))\n\nglimpse(sdac_speaker_meta)\n#> Rows: 543\n#> Columns: 14\n#> $ speaker_id        <dbl> 1000, 1001, 1002, 1003, 1004, 1005, 1007, 1008, 1010…\n#> $ pin               <dbl> 32, 102, 104, 5656, 123, 166, 274, 322, 445, 461, 57…\n#> $ target            <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y…\n#> $ sex               <chr> \"FEMALE\", \"MALE\", \"FEMALE\", \"MALE\", \"FEMALE\", \"FEMAL…\n#> $ birth_year        <dbl> 1954, 1940, 1963, 1947, 1958, 1956, 1965, 1939, 1932…\n#> $ dialect_area      <chr> \"SOUTH MIDLAND\", \"WESTERN\", \"SOUTHERN\", \"NORTH MIDLA…\n#> $ education         <dbl> 1, 3, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 3, 3, 2, 3…\n#> $ ti                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ payment_type      <chr> \"CASH\", \"GIFT\", \"GIFT\", \"NONE\", \"GIFT\", \"GIFT\", \"CAS…\n#> $ amt_pd            <dbl> 15, 10, 11, 7, 11, 22, 20, 3, 11, 9, 25, 9, 1, 16, 1…\n#> $ con               <chr> \"N\", \"N\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"N\", \"N…\n#> $ remarks           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ calls_deleted     <dbl> 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0…\n#> $ speaker_partition <chr> \"DN2\", \"XP\", \"XP\", \"DN2\", \"XP\", \"ET\", \"DN1\", \"DN1\", …\nsdac_disfluencies <- left_join(sdac_disfluencies, sdac_speaker_meta)  # join by ``speaker_id`\n\nglimpse(sdac_disfluencies)\n#> Rows: 447,212\n#> Columns: 18\n#> $ doc_id            <dbl> 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325…\n#> $ speaker_id        <dbl> 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519…\n#> $ utterance_text    <chr> \"Okay.  /\", \"Okay.  /\", \"{D So, }\", \"{D So, }\", \"[ […\n#> $ filler            <chr> \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\"…\n#> $ count             <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n#> $ pin               <dbl> 7713, 7713, 7713, 7713, 775, 775, 7713, 7713, 775, 7…\n#> $ target            <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n#> $ sex               <chr> \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"F…\n#> $ birth_year        <dbl> 1962, 1962, 1962, 1962, 1971, 1971, 1962, 1962, 1971…\n#> $ dialect_area      <chr> \"WESTERN\", \"WESTERN\", \"WESTERN\", \"WESTERN\", \"SOUTH M…\n#> $ education         <dbl> 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1…\n#> $ ti                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ payment_type      <chr> \"CASH\", \"CASH\", \"CASH\", \"CASH\", \"CASH\", \"CASH\", \"CAS…\n#> $ amt_pd            <dbl> 10, 10, 10, 10, 4, 4, 10, 10, 4, 4, 10, 10, 4, 4, 4,…\n#> $ con               <chr> \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y…\n#> $ remarks           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ calls_deleted     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ speaker_partition <chr> \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UN…\nsdac_disfluencies <- \n  sdac_disfluencies %>% # dataset\n  select(doc_id:count, sex:education) # subset key columns"},{"path":"transform-datasets-chapter.html","id":"documentation-3","chapter":"7 Transform datasets","heading":"7.5 Documentation","text":"Documentation transformed dataset just important curated dataset. Therefore use process covered previous chapter. First write transformed dataset disk work provide data dictionary dataset. ’ve included data_dic_starter() custom function apply dataset(s).Let’s apply function sdac_disfluencies dataset using R console (part project script avoid overwriting documentation!).Open data_dictionary_sdac_disfluencies.csv file spreadsheet software add relevant description dataset.","code":"\ndata_dic_starter <- function(data, file_path) {\n  # Function:\n  # Creates a .csv file with the basic information\n  # to document a curated dataset\n  \n  tibble(variable_name = names(data), # column with existing variable names \n       name = \"\", # column for human-readable names\n       description = \"\") %>% # column for prose description\n  write_csv(file = file_path) # write to disk\n}\ndata_dic_starter(data = sdac_disfluencies, file_path = \"../data/derived/sdac/sdac_disfluencies_data_dictionary.csv\")data/derived/\n└── sdac/\n    ├── data_dictionary_sdac.csv\n    ├── sdac_curated.csv\n    ├── sdac_disfluencies.csv\n    └── sdac_disfluencies_data_dictionary.csv"},{"path":"transform-datasets-chapter.html","id":"summary-10","chapter":"7 Transform datasets","heading":"Summary","text":"chapter covered process transforming datasets. goal manipulate curated dataset make align better analysis.\nfour general types transformation steps: normalization, recoding, generation, merging. given research project steps employed –necessarily order presented chapter. Furthermore may also various datasets generated stage distinct analysis focus mind. case important write datasets disk document according principles established previous chapter.chapter concludes section data/ dataset preparation. next section turn analyzing datasets. stage interrogate datasets derive knowledge insight either inference, prediction, / exploratory methods.","code":""},{"path":"analysis-overview.html","id":"analysis-overview","chapter":"Overview","heading":"Overview","text":"section turn analysis datasets, evaluation results, interpretation findings. outline three main types statistical analyses: Inferential Data Analysis (IDA), Predictive Data Analysis (PDA), Exploratory Data Analysis (EDA). analysis types distinct, non-overlapping aims therefore determined outset research project included part research blueprint. aim section establish clearer picture goals, methods, value approaches.","code":""},{"path":"inference-chapter.html","id":"inference-chapter","chapter":"8 Inference","heading":"8 Inference","text":"\nINCOMPLETE DRAFT\nPeople generally see look , hear listen .– Harper Lee, Kill Mockingbird\nessential questions chapter :\n\nthree main types inferential analysis approaches?\n\ninformational value dependent variable relate statistical approach adopted?\n\ndescriptive, statistical, evaluative steps work together produce reliable results?\nchapter consider approaches deriving knowledge information can generalized population data sampled. process known statistical inference. discussion builds concepts developed Chapter 3 “Approaching analysis” implements descriptive assessments, statistical tests, evaluation procedures series contexts common analysis corpus-based data. chapter structured three main sections correspond number variables included statistical procedure. sections includes subsection dedicated informational value dependent variable; variable whose variation explained.discussion two datasets used base pose various questions submit interrogation. note questions subsequent sections posited highlight various descriptive, statistic, evaluation procedures reflect standard approach hypothesis testing assumes null alternative hypotheses developed outset research project.process inferential data analysis section include three steps: (1) descriptive assessment, (2) statistical interrogation, (3) evaluation results.","code":""},{"path":"inference-chapter.html","id":"preparation","chapter":"8 Inference","heading":"8.1 Preparation","text":"point let’s now get familiar datasets prepare analysis. first dataset consider dative dataset. dataset can loaded languageR package (R. H. Baayen & Shafaei-Bajestan, 2019).glimpse() can see dataset contains 3,263 observations 15 columns.R Documentation can consulted using ?dative R Console. description states:Data describing realization dative NP PP Switchboard corpus Treebank Wall Street Journal collection.bit context, dative phrase reflects entity takes recipient role ditransitive clause. English, recipient (dative) can realized either noun phrase (NP) seen (1) prepositional phrase (PP) seen (2) .give [NP] drug test.give drug test [PP].Together two syntactic options known Dative Alternation.observational unit dataset RealizationOfRecipient variable either ‘NP’ ‘PP’. purposes chapter select subset key variables use upcoming analyses drop others.Table 8.1: First 10 observations simplified dative dataset.Table 8.2 ’ve created data dictionary describing variables new dative dataset based variable descriptions languageR::dative documentation.Table 8.2: Data dictionary dative dataset.second dataset use chapter sdac_disfluencies dataset worked derived previous chapter. Let’s read dataset preview structure.prepared data dictionary reflects transformed dataset. Let’s read file view Table 8.3.Table 8.3: Data dictionary sdac_disfluencies dataset.analysis purposes reduce dataset, dative dataset, retaining variables interest upcoming analyses.Let’s preview simplified sdac_disfluencies dataset.Table 8.4: First 10 observations simplified sdac_disfluencies dataset.Now sdac_disfluencies dataset needs extra transformation better prepare statistical interrogation. one hand variables birth_year education maximally informative. First ideal birth_year reflect age speaker time conversation(s) furthermore coded values education explicit far numeric values refer .second issue preparing sdac_disfluencies dataset statistical analysis. involves converting column types correct vector types statistical methods. Specifically need convert categorical variables R type ‘factor’ (fct). includes current variables character vectors, also speaker_id education appear numeric reflect continuous variables; one merely code uniquely labels speaker ordinal list educational levels.three step process, first normalize birth_year reflect age speaker, second convert relevant categorical variables factors, third convert education variable factor adding meaningful labels levels factor.Consulting online manual corpus, see recording date conversations took place 1992, can simply subtract birth_year 1992 get participant’s age. ’ll rename new column age drop birth_year column.Now let’s convert variables character vectors. can using factor() function; first speaker_id , help mutate_if(), variables character vectors.know data dictionary education column contains four values (0, 1, 2, 3, 9). , consulting corpus manual can see values mean.let’s convert education factor adding descriptions factor level labels. function factor() can take argument labels = can manually assign label names factor levels order factor levels. Since original values numeric, factor level ordering defaults ascending order.let’s take look sdac_disfluencies dataset ’ve prepared analysis.Now datasets dative sdac_disfluencies ready statistically interrogated.","code":"\ndative <- \n  languageR::dative %>% # load the `dative` dataset  \n  as_tibble() # convert the data frame to a tibble object\n  \nglimpse(dative) # preview structure \n#> Rows: 3,263\n#> Columns: 15\n#> $ Speaker                <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ Modality               <fct> written, written, written, written, written, wr…\n#> $ Verb                   <fct> feed, give, give, give, offer, give, pay, bring…\n#> $ SemanticClass          <fct> t, a, a, a, c, a, t, a, a, a, a, a, t, a, c, a,…\n#> $ LengthOfRecipient      <int> 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1,…\n#> $ AnimacyOfRec           <fct> animate, animate, animate, animate, animate, an…\n#> $ DefinOfRec             <fct> definite, definite, definite, definite, definit…\n#> $ PronomOfRec            <fct> pronominal, nonpronominal, nonpronominal, prono…\n#> $ LengthOfTheme          <int> 14, 3, 13, 5, 3, 4, 4, 1, 11, 2, 3, 3, 5, 2, 8,…\n#> $ AnimacyOfTheme         <fct> inanimate, inanimate, inanimate, inanimate, ina…\n#> $ DefinOfTheme           <fct> indefinite, indefinite, definite, indefinite, d…\n#> $ PronomOfTheme          <fct> nonpronominal, nonpronominal, nonpronominal, no…\n#> $ RealizationOfRecipient <fct> NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP,…\n#> $ AccessOfRec            <fct> given, given, given, given, given, given, given…\n#> $ AccessOfTheme          <fct> new, new, new, new, new, new, new, new, accessi…\ndative <- \n  dative %>% # dataset\n  select(RealizationOfRecipient, Modality, LengthOfRecipient, LengthOfTheme) %>% # select key variables\n  janitor::clean_names() # normalize variable names\nsdac_disfluencies <- read_csv(file = \"../data/derived/sdac/sdac_disfluencies.csv\")  # read transformed dataset\n\nglimpse(sdac_disfluencies)  # preview structure#> Rows: 447,212\n#> Columns: 9\n#> $ doc_id         <dbl> 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4…\n#> $ speaker_id     <dbl> 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519, 1…\n#> $ utterance_text <chr> \"Okay.  /\", \"Okay.  /\", \"{D So, }\", \"{D So, }\", \"[ [ I …\n#> $ filler         <chr> \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"…\n#> $ count          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0…\n#> $ sex            <chr> \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMA…\n#> $ birth_year     <dbl> 1962, 1962, 1962, 1962, 1971, 1971, 1962, 1962, 1971, 1…\n#> $ dialect_area   <chr> \"WESTERN\", \"WESTERN\", \"WESTERN\", \"WESTERN\", \"SOUTH MIDL…\n#> $ education      <dbl> 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1…\nsdac_disfluencies_dictionary <- read_csv(file = \"../data/derived/sdac/sdac_disfluencies_data_dictionary.csv\")  # read data dictionary\nsdac_disfluencies <- \n  sdac_disfluencies %>% # dataset\n  select(speaker_id, filler, count, sex, birth_year, education) # select key variables\nsdac_disfluencies <- \n  sdac_disfluencies %>% # dataset\n  mutate(age = (1992 - birth_year)) %>% # calculate age\n  select(-birth_year) # drop `birth_year` column\nsdac_disfluencies <- \n  sdac_disfluencies %>% # dataset\n  mutate(speaker_id = factor(speaker_id)) %>% # convert numeric to factor\n  mutate_if(is.character, factor) # convert all character to factorEDUCATION    COUNT\n--------------------\n\n0            14      less than high school\n1            39      less than college\n2            309     college\n3            176     more than college\n9            4       unknown\nsdac_disfluencies <- \n  sdac_disfluencies %>% # dataset\n  mutate(education = factor(education, \n                            labels = c(\"less than high school\", # value 0\n                                       \"less than college\", # value 1\n                                       \"college\", # value 2\n                                       \"more than college\", # value 3 \n                                       \"unknown\"))) # value 9\nglimpse(sdac_disfluencies)\n#> Rows: 447,212\n#> Columns: 6\n#> $ speaker_id <dbl> 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519, 1519,…\n#> $ filler     <chr> \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\",…\n#> $ count      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n#> $ sex        <chr> \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\",…\n#> $ education  <chr> \"college\", \"college\", \"college\", \"college\", \"less than coll…\n#> $ age        <dbl> 30, 30, 30, 30, 21, 21, 30, 30, 21, 21, 30, 30, 21, 21, 21,…"},{"path":"inference-chapter.html","id":"univariate-analysis","chapter":"8 Inference","heading":"8.2 Univariate analysis","text":"follows provide description inferential data analysis one variable interrogated. known univariate analysis, one-variable analysis. consider case variable categorical continuous.","code":""},{"path":"inference-chapter.html","id":"categorical","chapter":"8 Inference","heading":"8.2.1 Categorical","text":"example univariate analysis variable used analysis categorical look dative dataset. analysis may interested knowing whether recipient role ditransitive construction realized ‘NP’ ‘PP’.Descriptive assessmentThe realization_of_recipient variable contains relevant information. Let’s take first look using skimr package.Variable type: factorThe output skim() produces various pieces information can helpful. one hand get diagnostics tell us missing cases (NA values), proportion complete cases , factor ordered, many distinct levels factor , well level counts.Looking top_counts can see 3,263 observations, 2,414 dative expressed ‘NP’ 849 ‘PP’. Numerically can see difference use alternation types. visualization often helpful descriptive purposes statistical analysis. particular case, however, considering single categorical variable two levels (values) visualization likely informative numeric values already obtained. demonstration purposes get familiar building plots, let’s create visualization.question want address, however, whether numerical difference fact statistical difference.Statistical interrogationTo statistical assess distribution categorical variable, turn Chi-squared test. test aims gauge whether numerical differences ‘NP’ ‘PP’ counts observed data greater expected chance. Chance case two possible outcome levels 50/50. particular data 3,263 observations half ‘NP’ half ‘PP’ –specifically 1631.5 .run test first need create cross-tabulation variable. use xtabs() function create table.new information , format (.e. object class ‘table’) important input argument chisq.test() function use run test.preview c1 object reveals main information interest including Chi-squared statistic, degrees freedom, \\(p\\)-value (albeit scientific notation). However, c1 ‘htest’ object includes number pieces information test.purposes let’s simply confirm \\(p\\)-value lower standard .05 threshold statistical significance.information can organized readable format using broom package’s augment() function.can see observed expected counts proportions level realization_of_recipient. also get additional information concerning residuals, leave aside.EvaluationAt point may think done. statistically interrogated realization_of_recipient variable found difference ‘NP’ ‘PP’ realization datives dataset statistically significant. However, need evaluate size (‘effect size’) reliability effect (‘confidence interval’). effectsize package provides function effectsize() can provide us effect size confidence interval.effectsize() recognizes type test results c1 calculates appropriate effect size measure generates confidence interval. Since effect statistic (“Phi”) falls 95% confidence interval suggests results reliably interpreted (chances Type (false positive) Type II (false negative) low).Now, remaining question evaluate whether significant result strong effect . can pass effect size measure interpret_r() function.Turns strong effect; realization dative alternation heavily favors ‘NP’ form data. potential reasons considered univariate analysis, return question later add independent variables statistical analysis.","code":"\ndative %>% # dataset\n  select(realization_of_recipient) %>% # select the variable\n  skim() %>% # get data summary\n  yank(\"factor\") # only show factor-oriented information\ndative %>% # dataset\n  ggplot(aes(x = realization_of_recipient)) + # mapping\n  geom_bar() + # geometry\n  labs(x = \"Realization of recipient\", y = \"Count\") # labels\nror_table <- \n  xtabs(formula = ~ realization_of_recipient, # formula selecting the variable\n        data = dative) # dataset\n\nror_table # preview\n#> realization_of_recipient\n#>   NP   PP \n#> 2414  849\nc1 <- chisq.test(x = ror_table)  # apply the chi-squared test to `ror_table`\n\nc1  # preview the test results\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  ror_table\n#> X-squared = 751, df = 1, p-value <2e-16\nnames(c1)  # preview column names\n#> [1] \"statistic\" \"parameter\" \"p.value\"   \"method\"    \"data.name\" \"observed\" \n#> [7] \"expected\"  \"residuals\" \"stdres\"\nc1$p.value < 0.05  # confirm p-value below .05\n#> [1] TRUE\nc1 %>% # statistical result\n  augment() # view detailed statistical test information\n#> # A tibble: 2 × 6\n#>   realization_of_recipient .observed .prop .expected .resid .std.resid\n#>   <fct>                        <int> <dbl>     <dbl>  <dbl>      <dbl>\n#> 1 NP                            2414 0.740     1632.   19.4       27.4\n#> 2 PP                             849 0.260     1632.  -19.4      -27.4\neffects <- effectsize(c1, type = \"phi\")  # evaluate effect size and generate a confidence interval (phi type given 2x1 contingency table)\n\neffects  # preview effect size and confidence interval\n#> Phi  |           95% CI\n#> -----------------------\n#> 0.48 | [0.45,      Inf]\n#> \n#> - One-sided CIs: upper bound fixed at (Inf).\ninterpret_r(effects$phi)  # interpret the effect size \n#> [1] \"very large\"\n#> (Rules: funder2019)"},{"path":"inference-chapter.html","id":"continuous","chapter":"8 Inference","heading":"8.2.2 Continuous","text":"Now let’s turn case variable aim interrogate non-categorical. case turn sdac_disfluencies dataset. Specifically aim test whether use fillers normally distributed across speakers.\nimportant step working numeric dependent variables type distribution dictate decisions whether use parametric non-parametric tests consider extent independent variable (variables) can explain variation dependent variable.\nSince dataset currently organized around fillers observational unit, first transform dataset sum use fillers speaker dataset.Table 8.5: First 10 observations sdac_speaker_fillers dataset.Descriptive assessmentLet’s perform descriptive assessement variable interest sum. First let’s apply skim() function retrieve just relevant numeric descriptors yank(). One twist , however, ’ve customized skim() function using skim_with() remove default histogram add Interquartile Range (IQR) output. new skim function num_skim() take place skim().Variable type: numericWe see mean use fillers 87.1 across speakers. However, standard deviation IQR large relative mean indicates dispersion quite large, words suggests large differences speakers. Furthermore, since median (p50) smaller mean, distribution right skewed.Let’s look couple visualizations distribution appreciate descriptives. histogram provide us view distribution using counts values sum density plot provide smooth curve represents scaled distribution observed data.\nplots initial intuitions distribution sum correct. large dispersion speakers data distribution right skewed.\nNote ’ve used patchwork package organizing display plots including plot annotation label.\nSince aim test normality, can generate Quantile-Quantile plots (QQ Plot).Since many points fall expected normal distribution line even evidence support notion distribution sum non-normal.Statistical interrogationAlthough descriptives visualizations strongly suggest normally distributed data let’s run normality test. turn shapiro.test() function performs Shapiro-Wilk test normality. pass sum variable function run test.saw results chisq.test() function, shapiro.test() function produces object information test including \\(p\\)-value. Let’s run logical test see test statistically significant.EvaluationThe results Shapiro-Wilk Normality Test tell us distribution sum statistically found differ normal distribution. case, statistical significance suggests sum used parametric dependent variable. aims evaluation required. Effect size confidence intervals applicable.note, however, expectation variable sum conform normal distribution low outset working count data. Count data, frequencies, strict sense continuous, rather discrete –meaning real numbers (whole numbers always positive). common informational type encounter text analysis.","code":"\nsdac_speaker_fillers <- \n  sdac_disfluencies %>% # dataset\n  group_by(speaker_id) %>% # group by each speaker\n  summarise(sum = sum(count)) %>% # add up all fillers used\n  ungroup() # remove grouping parameter\nnum_skim <- \n  skim_with(numeric = sfl(hist = NULL, # remove hist skim\n                                   iqr = IQR)) # add IQR to skim\n\nsdac_speaker_fillers %>% # dataset\n  select(sum) %>% # variable of interest\n  num_skim() %>% # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\np1 <- \n  sdac_speaker_fillers %>% # dataset\n  ggplot(aes(sum)) + # mapping\n  geom_histogram() +  # geometry\n  labs(x = \"Fillers\", y = \"Count\")\n\np2 <- \n  sdac_speaker_fillers %>% # dataset\n  ggplot(aes(sum)) + # mapping\n  geom_density() + # geometry\n  geom_rug() +  # visualize individual observations\n  labs(x = \"Fillers\", y = \"Density\")\n\np1 + p2 + plot_annotation(\"Filler count distributions.\")\nsdac_speaker_fillers %>% # dataset\n  ggplot(aes(sample = sum)) + # mapping\n  stat_qq() + # calculate expected quantile-quantile distribution\n  stat_qq_line() # plot the qq-line\ns1 <- shapiro.test(sdac_speaker_fillers$sum)  # apply the normality test to `sum`\n\ns1  # preview the test results\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  sdac_speaker_fillers$sum\n#> W = 0.8, p-value <2e-16\ns1$p.value < 0.05  # \n#> [1] TRUE"},{"path":"inference-chapter.html","id":"bivariate-analysis","chapter":"8 Inference","heading":"8.3 Bivariate analysis","text":"common scenario statistical analysis consideration relationship two-variables, known bivariate analysis.","code":""},{"path":"inference-chapter.html","id":"categorical-1","chapter":"8 Inference","heading":"8.3.1 Categorical","text":"Let’s build univariate analysis realization_of_recipient include explanatory, independent variable explore test whether can explain earlier finding ‘NP’ datives common ‘PP’ datives. question test, , whether modality explains distribution realization_of_recipient.Descriptive assessmentBoth realization_of_recipient modality variables categorical, specifically nominal can see using skim().Variable type: factorFor reason measures central tendency applicable turn contingency table summarize relationship. janitor package set functions, primary function tabyl(). functions used adorn contingency table totals, percentages, format output readability.Table 8.6: Contingency table realization_of_recipient modality.gain better appreciation relationship let’s generate couple plots one shows cross-tabulated counts calculated proportions.Looking count plot (left pane) see large difference realization dative ‘NP’ ‘PP’ obscures degree ability see degree modality related realization dative. , proportion plot (right pane) standardizes level realization_of_recipient provide comparable view. proportion plot see appears trend towards use ‘PP’ ‘NP’ written modality.Statistical interrogationAlthough proportion plot visually helpful, use raw counts statistically analyze relationship. , working categorical variables, now dependent independent variable, use Chi-squared test. need create cross-tabulation table pass chisq.test() perform test.can preview result provide confirmation \\(p\\)-value. evidence suggests difference distribution dative realization according modality.can also see details test.EvaluationNow want calculate effect size confidence interval provide measures assurance finding robust.get effect size confidence interval information. Note effect size, reflected Cramer’s V, relationship weak. points important aspect evaluation statistical tests. fact test significant mean meaningful. small effect size suggests cautious extent significant finding robust population data sampled.","code":"\ndative %>% \n  select(realization_of_recipient, modality) %>% # select key variables\n  skim() %>% # get custom data summary\n  yank(\"factor\") # only show factor-oriented information\ndative %>% \n  tabyl(realization_of_recipient, modality) %>% # cross-tabulate\n  adorn_totals(c(\"row\", \"col\")) %>% # provide row and column totals\n  adorn_percentages(\"col\") %>% # add percentages to the columns\n  adorn_pct_formatting(rounding = \"half up\", digits = 0) %>% # round the digits\n  adorn_ns() %>% # add observation number\n  adorn_title(\"combined\") %>% # add a header title\n  kable(booktabs = TRUE, # pretty table\n        caption = \"Contingency table for `realization_of_recipient` and `modality`.\") # caption\np1 <- \n  dative %>% # dataset\n  ggplot(aes(x = realization_of_recipient, fill = modality)) + # mappings\n  geom_bar() + # geometry\n  labs(y = \"Count\", x = \"Realization of recipient\") # labels\n\np2 <- \n  dative %>% # dataset\n  ggplot(aes(x = realization_of_recipient, fill = modality)) + # mappings\n  geom_bar(position = \"fill\") + # geometry, with fill for proportion plot\n  labs(y = \"Proportion\", x = \"Realization of recipient\", fill = \"Modality\") # labels\n\np1 <- p1 + theme(legend.position = \"none\") # remove legend from left plot\n\np1 + p2 + plot_annotation(\"Relationship between Realization of recipient and Modality.\")\nror_mod_table <- \n  xtabs(formula = ~ realization_of_recipient + modality, # formula \n        data = dative) # dataset\n\nc2 <- chisq.test(ror_mod_table) # apply the chi-squared test to `ror_mod_table`\n\nc2 # # preview the test results\n#> \n#>  Pearson's Chi-squared test with Yates' continuity correction\n#> \n#> data:  ror_mod_table\n#> X-squared = 101, df = 1, p-value <2e-16\n\nc2$p.value < .05 # confirm p-value below .05\n#> [1] TRUE\nc2 %>% # statistical result\n  augment() # view detailed statistical test information\n#> # A tibble: 4 × 9\n#>   realization_of_… modality .observed .prop .row.prop .col.prop .expected .resid\n#>   <fct>            <fct>        <int> <dbl>     <dbl>     <dbl>     <dbl>  <dbl>\n#> 1 NP               spoken        1859 0.570     0.770     0.788     1746.   2.71\n#> 2 PP               spoken         501 0.154     0.590     0.212      614.  -4.56\n#> 3 NP               written        555 0.170     0.230     0.615      668.  -4.37\n#> 4 PP               written        348 0.107     0.410     0.385      235.   7.38\n#> # … with 1 more variable: .std.resid <dbl>\neffects <- effectsize(c2)  # evaluate effect size and generate a confidence interval\n\neffects  # preview effect size and confidence interval\n#> Cramer's V |       95% CI\n#> -------------------------\n#> 0.18       | [0.14, 0.21]\n\ninterpret_r(effects$Cramers_v)  # interpret the effect size\n#> [1] \"small\"\n#> (Rules: funder2019)"},{"path":"inference-chapter.html","id":"continuous-1","chapter":"8 Inference","heading":"8.3.2 Continuous","text":"bivariate analysis dependent variable categorical, turn sdac_disfluencies dataset. question pose test whether use fillers related type filler (‘uh’ ‘um’).Descriptive assessmentThe key variables assess case variables count filler. start explore relationship need transform dataset speaker’s use levels filler summed. use group_by() group speaker_id filler combinations use summarize() sum counts filler type speakerLet’s preview transformation.Table 8.7: First 10 observations sdac_fillers dataset.Let’s take look together grouping dataset filler using custom skim function num_skim() numeric variablecount.Variable type: numericWe see standard deviation IQR ‘uh’ ‘um’ relatively large respective means (71.4 15.7) suggesting distribution quite dispersed. Let’s take look boxplot visualize counts sum level filler.plot left pane see couple things. First, appears fact quite bit dispersion quite outliers (dots) lines extending boxes. Recall boxes represent first third quantile, IQR notches represent confidence interval. Second, compare boxes notches see little overlap (looking horizontally). right pane ’ve zoomed bit trimming outliers get better view relationship boxes. Since overlap minimal particular notches overlap , good indication significant trend.descriptive statistics visual summary appears filler ‘uh’ common ‘um’. ’s now time submit statistical interrogation.Statistical interrogationIn bivariate (multivariate) analysis dependent variable non-categorical apply Linear Regression Modeling (LM). default assumption linear models, however, dependent variable normally distributed. seen variable sum conform normal distribution. know tests univariate case, mentioned end section, working count data nature understood discrete continuous strict technical sense. instead using linear model regression analysis use Generalized Linear Model (GLM) (R. Harald Baayen, 2008; Gries, 2013).function glm() implements generalized linear models. addition formula (sum ~ filler) dataset use, also include appropriate distribution family dependent variable. count frequency data appropriate family “Poisson” distribution.Let’s focus coefficients, specifically ‘fillerum’ line. Since factor filler two levels one level used reference contrast level. case default first level used reference. Therefore coefficients see ‘fillerum’ ‘um’ contrast ‘uh’. Without digging details parameter statistics, let’s focus last column contains \\(p\\)-value. convenient aspect summary() function applied regression model results provides statistical significance codes. case can see contrast ‘uh’ ‘um’ signficant \\(p < .001\\) course lower standard threshold \\(.05\\).Therefore can say confidence filler ‘uh’ frequent ‘um’.EvaluationGiven found significant effect filler, let’s look evaluating effect size confidence interval. , use effectsize() function. can preview effects object. Note effect size interest second row coefficient (Std_Coefficient) subset column extract effect coefficient filler contrast.coefficient statistic falls within confidence interval effect size strong can confident findings reliable given data.","code":"\nsdac_fillers <- \n  sdac_disfluencies %>% # dataset\n  group_by(speaker_id, filler) %>% # grouping parameters\n  summarize(sum = sum(count)) %>% # summed counts for each speaker-filler combination\n  ungroup() # remove the grouping parameters\nsdac_fillers %>% # dataset\n  group_by(filler) %>% # grouping parameter\n  num_skim() %>% # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\np1 <- \n  sdac_fillers %>% # dataset\n  ggplot(aes(x = filler, y = sum)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  labs(x = \"Filler\", y = \"Counts\") # labels\n\np2 <- \n  sdac_fillers %>% # dataset\n  ggplot(aes(x = filler, y = sum)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  ylim(0, 100) + # scale the y axis to trim outliers\n  labs(x = \"Filler\", y = \"\") # labels\n\np1 + p2\nm1 <- \n  glm(formula = sum ~ filler, # formula\n      data = sdac_fillers, # dataset\n      family = \"poisson\") # distribution family\n\nsummary(m1) # preview the test results\n#> \n#> Call:\n#> glm(formula = sum ~ filler, family = \"poisson\", data = sdac_fillers)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -11.95   -5.61   -3.94    0.80   41.99  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  4.26794    0.00564     757   <2e-16 ***\n#> fillerum    -1.51308    0.01327    -114   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 72049  on 881  degrees of freedom\n#> Residual deviance: 55071  on 880  degrees of freedom\n#> AIC: 58524\n#> \n#> Number of Fisher Scoring iterations: 6\neffects <- effectsize(m1)  # evaluate effect size and generate a confidence interval\n\neffects  # preview effect size and confidence interval\n#> # Standardization method: refit\n#> \n#> Parameter   | Coefficient (std.) |         95% CI\n#> -------------------------------------------------\n#> (Intercept) |               4.27 | [ 4.26,  4.28]\n#> fillerum    |              -1.51 | [-1.54, -1.49]\n#> \n#> (Response is unstandardized)\n\ninterpret_r(effects$Std_Coefficient[2])  # interpret the effect size\n#> [1] \"very large\"\n#> (Rules: funder2019)"},{"path":"inference-chapter.html","id":"multivariate-analysis","chapter":"8 Inference","heading":"8.4 Multivariate analysis","text":"last case consider one independent variable want use assess potential relationship dependent variable. consider categorical non-categorical dependent variable. , case implementation methods quite similar, see.","code":""},{"path":"inference-chapter.html","id":"categorical-2","chapter":"8 Inference","heading":"8.4.1 Categorical","text":"categorical multivariate case consider dative dataset build previous analyses. question posed whether modality combination length recipient (length_of_recipient) together explain distribution realization recipient (realization_of_recipient).Descriptive assessmentNow three variables, summarize get descriptive information. Luckily, however, process can applied three () variables using group_by() function passed skim(). case two categorical variables one numeric variable. group categorical variables pass numeric variable custom num_skim() function –pulling relevant descriptive information numeric variables yank().Variable type: numericThere much information now considering multiple independent variables, look measures dispersion can see median IQR relatively similar respective means suggesting fewer outliers relativley little skew.Let’s take look visualization information. Since working categorical dependent variable one non-categorical variable can use boxplot. addition include color mapping provide distinct box level modality (‘written’ ‘spoken’).left pane see entire visualization including outliers. view appears potential trend length recipient larger realization recipient ‘PP’. also potential trend modality written language showing longer recipient lengths overall. pane right scaled get better view boxes scaling y-axis trimming outliers. plot shows clearly length recipient longer recipient realized ‘PP’. , contrast modality also potential trend, boxes (color), particularly spoken modality overlap degree.trends mind help us interpret statistical interrogation let’s move next.Statistical interrogationOnce involve two variables, choice statistical method turns towards regression. case dependent variable categorical, however, use Logistic Regression. workhorse function glm() can used series regression models, including logistic regression. requirement, however, specify family distribution. logistic regression family “binomial”. formula includes dependent variable function two variables, separated + operator.results model provide wealth information. key information focus coefficients. particular coefficients independent variables modality length_of_recipient. notice, \\(p\\)-value length_of_recipient significant, contrast ‘written’ ‘spoken’ modality . recall, used dataset explore modality single indpendent variable earlier –found significant. now ? answer multiple variables used explain distribution measure (dependent variable) variable now adds information explain dependent variable –’s contribution. Since length_of_recipient significant, suggests explanatory power modality weak, especially compared length_of_recipient. make sense saw earlier model fact effect size modality strong now evident length_of_recipient included model.EvaluationNow let’s move gauge effect size calculate confidence interval length_of_recipient model. apply effectsize() function model use interpret_r() coefficient interest (fourth row Std_Coefficients column).see coefficient falls within confidence interval effect size large. can saw confidence length recipient significant predictor use ‘PP’ realization recipient dative alternation.","code":"\ndative %>% # dataset\n  select(realization_of_recipient, modality, length_of_recipient) %>% # select key variables\n  group_by(realization_of_recipient, modality) %>% # grouping parameters\n  num_skim() %>% # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\np1 <- \n  dative %>% # dataset\n  ggplot(aes(x = realization_of_recipient, y = length_of_recipient, color = modality)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  labs(x = \"Realization of recipient\", y = \"Length of recipient (in words)\", color = \"Modality\") # labels\n\np2 <- \n  dative %>% # dataset\n  ggplot(aes(x = realization_of_recipient, y = length_of_recipient, color = modality)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  ylim(0, 15) + # scale the y axis to trim outliers\n  labs(x = \"Realization of recipient\", y = \"\", color = \"Modality\") # labels\n\np1 <- p1 + theme(legend.position = \"none\") # remove the legend from the left pane plot\n\np1 + p2\nm1 <- glm(formula = realization_of_recipient ~ modality + length_of_recipient, # formula\n          data = dative, # dataset\n          family = \"binomial\") # distribution family\n\nsummary(m1) # preview the test results\n#> \n#> Call:\n#> glm(formula = realization_of_recipient ~ modality + length_of_recipient, \n#>     family = \"binomial\", data = dative)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -4.393  -0.598  -0.598   0.132   1.924  \n#> \n#> Coefficients:\n#>                     Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)          -2.3392     0.0797  -29.35   <2e-16 ***\n#> modalitywritten      -0.0483     0.1069   -0.45     0.65    \n#> length_of_recipient   0.7081     0.0420   16.86   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 3741.1  on 3262  degrees of freedom\n#> Residual deviance: 3104.7  on 3260  degrees of freedom\n#> AIC: 3111\n#> \n#> Number of Fisher Scoring iterations: 5\neffects <- effectsize(m1)  # evaluate effect size and generate a confidence interval\n\neffects  # preview effect size and confidence interval\n#> # Standardization method: refit\n#> \n#> Parameter           | Coefficient (std.) |         95% CI\n#> ---------------------------------------------------------\n#> (Intercept)         |              -1.03 | [-1.15, -0.92]\n#> modalitywritten     |              -0.05 | [-0.26,  0.16]\n#> length_of_recipient |               1.46 | [ 1.30,  1.64]\n#> \n#> (Response is unstandardized)\n\ninterpret_r(effects$Std_Coefficient[4])  # interpret the effect size\n#> [1] NA\n#> (Rules: funder2019)"},{"path":"inference-chapter.html","id":"continuous-2","chapter":"8 Inference","heading":"8.4.2 Continuous","text":"last case consider dependent variable non-categorical one independent variable. question pose whether type filler sex speaker can explain use fillers conversational speech.need prepare data get started current data frame sdac_fillers filler sum count filler grouped speaker –include sex speaker. sdac_disfluencies data frame sex column, grouped speaker. let’s transform sdac_disfluencies summarizing get speaker_id sex combinations. result data frame 441 observations, one observation speaker corpus.Let’s preview first 10 observations form transformation.Table 8.8: First 10 observations sdac_speakers_sex data frame.Great, now speaker_id sex 441 speakers. One thing note, however, speaker ‘155’ value sex –seems error metadata need deal proceed analysis. Let’s move join new sdac_speakers_sex data frame sdac_fillers data frame.Now complete dataset speaker_id sex now join dataset sdac_fillers dataset effectively adding column sex. want keep observations sdac_fillers add column sex observations correspond data frame column speaker_id use left join function left_join() sdac_fillers dataset left.Now let’s preview first observations new sdac_fillers_sex data frame.Table 8.9: First 10 observations sdac_fillers_sex data frame.point let’s drop speaker sdac_speakers_sex data frame.now ready proceed analysis.Descriptive assessmentThe process now quite routine getting descriptive statistics: select key variables, group categorical variables, finally pull descriptives numeric variable.Variable type: numericLooking descriptives, seems like quite bit variability combinations others. short, ’s mixed bag. Let’s try make sense numbers boxplot.can see ‘uh’ used ‘um’ overall. whereas men women use ‘uh’ similar ways, women use ‘um’ men. known interaction. approach statistical analysis mind.Statistical interrogationWe use generalized linear model glm() function conduct test. distribution family using sum dependent variable contains discrete count values. formula use, however, new. Instead adding new variable independent variables, test possible interaction filler sex noted descriptive assessment. encode interaction * operator used. formula take form sum ~ filler * sex. Let’s generate model view summary test results done .looking coefficients something new. First see row filler contrast sex contrast also interaction filler sex (‘fillerum:sexMALE’). rows show significant effects. important note interaction explored found significant, simple effects, known main effects (‘fillerum’ ‘sexMALE’), ignored. higer-order effect considered significant.Now ‘fillerum:sexMALE’ row mean. means interaction filler sex. directionality interaction interpreted using descriptive assessment, particular visual boxplots generated. sum, women use ‘um’ men stated another way men use ‘um’ less women.EvaluationWe finalize analysis looking effect size confidence intervals.can conclude, , strong interaction effect filler sex women use ‘um’ men.","code":"\nsdac_speakers_sex <- \n  sdac_disfluencies %>% # dataset\n  distinct(speaker_id, sex) # summarize for distinct `speaker_id` and `sex` values\nsdac_fillers_sex <- left_join(sdac_fillers, sdac_speakers_sex)  # join\nsdac_fillers_sex <- \n  sdac_fillers_sex %>% # dataset\n  filter(speaker_id != \"155\") # drop speaker_id 155\nsdac_fillers_sex %>% # dataset\n  select(sum, filler, sex) %>% # select key variables\n  group_by(filler, sex) %>% # grouping parameters\n  num_skim() %>% # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\np1 <- \n  sdac_fillers_sex %>% # dataset\n  ggplot(aes(x = filler, y = sum, color = sex)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  labs(x = \"Filler\", y = \"Counts\", color = \"Sex\") # labels\n\np2 <- \n  sdac_fillers_sex %>% # dataset\n  ggplot(aes(x = filler, y = sum, color = sex)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  ylim(0, 200) + # scale the y axis to trim outliers\n  labs(x = \"Filler\", y = \"\", color = \"Sex\") # labels\n\np1 <- p1 + theme(legend.position = \"none\") # drop the legend from the left pane plot\n\np1 + p2\nm1 <- \n  glm(formula = sum ~ filler * sex, # formula\n      data = sdac_fillers_sex, # dataset\n      family = \"poisson\") # distribution family\n\nsummary(m1) # preview the test results\n#> \n#> Call:\n#> glm(formula = sum ~ filler * sex, family = \"poisson\", data = sdac_fillers_sex)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -12.55   -6.21   -3.64    1.08   40.60  \n#> \n#> Coefficients:\n#>                  Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)       4.14660    0.00876   473.2   <2e-16 ***\n#> fillerum         -1.03827    0.01714   -60.6   <2e-16 ***\n#> sexMALE           0.21955    0.01145    19.2   <2e-16 ***\n#> fillerum:sexMALE -1.03344    0.02791   -37.0   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 71956  on 879  degrees of freedom\n#> Residual deviance: 53543  on 876  degrees of freedom\n#> AIC: 56994\n#> \n#> Number of Fisher Scoring iterations: 6\neffects <- effectsize(m1)  # evaluate effect size and generate a confidence interval\n\neffects  # preview effect size and confidence interval\n#> # Standardization method: refit\n#> \n#> Parameter        | Coefficient (std.) |         95% CI\n#> ------------------------------------------------------\n#> (Intercept)      |               4.15 | [ 4.13,  4.16]\n#> fillerum         |              -1.04 | [-1.07, -1.00]\n#> sexMALE          |               0.22 | [ 0.20,  0.24]\n#> fillerum:sexMALE |              -1.03 | [-1.09, -0.98]\n#> \n#> (Response is unstandardized)\n\ninterpret_r(effects$Std_Coefficient[4])  # interpret the effect size\n#> [1] \"very large\"\n#> (Rules: funder2019)"},{"path":"inference-chapter.html","id":"summary-11","chapter":"8 Inference","heading":"8.5 Summary","text":"chapter discussed various approaches conducting inferential data analysis. configuration, however, always includes descriptive assessment, statistical interrogation, evaluation results. considered univariate, bivariate, multivariate analyses using categorical non-categorical dependent variables explore similarities differences approaches.","code":""},{"path":"prediction-chapter.html","id":"prediction-chapter","chapter":"9 Prediction","heading":"9 Prediction","text":"\nINCOMPLETE DRAFT\n…\nessential questions chapter :\n\n…\n\n…\n\n…\nchapter willOrientation question(s) dataset(s) explored …","code":"\nwricle_df <- read_csv(file = \"resources/10-prediction/data/derived/wricle_formal_curated.csv\")\n\nlocness_df <- read_csv(file = \"resources/10-prediction/data/derived/locness_curated.csv\")\nlearners_df <- wricle_df %>%\n    filter(native_language == \"Spanish\") %>%\n    mutate(student = \"Spanish\") %>%\n    mutate(type = \"Learner\") %>%\n    select(essay_id = id, type, student, essay)\n\nglimpse(learners_df)\n#> Rows: 689\n#> Columns: 4\n#> $ essay_id <chr> \"A1-1\", \"A1-2\", \"A10-1\", \"A10-2\", \"A101-1\", \"A101-2\", \"A101-3…\n#> $ type     <chr> \"Learner\", \"Learner\", \"Learner\", \"Learner\", \"Learner\", \"Learn…\n#> $ student  <chr> \"Spanish\", \"Spanish\", \"Spanish\", \"Spanish\", \"Spanish\", \"Spani…\n#> $ essay    <chr> \"In our present society gay people is asking for the same rig…\nnatives_df <- locness_df %>%\n    unite(col = doc_id, c(\"file_id\", \"essay_id\"), sep = \"-\") %>%\n    mutate(doc_id = str_replace(doc_id, \"\\\\.txt\", \"\")) %>%\n    mutate(type = \"Native\") %>%\n    select(essay_id = doc_id, type, student, essay)\n\nglimpse(natives_df)\n#> Rows: 411\n#> Columns: 4\n#> $ essay_id <chr> \"alevels1-1\", \"alevels1-2\", \"alevels1-3\", \"alevels1-4\", \"alev…\n#> $ type     <chr> \"Native\", \"Native\", \"Native\", \"Native\", \"Native\", \"Native\", \"…\n#> $ student  <chr> \"British\", \"British\", \"British\", \"British\", \"British\", \"Briti…\n#> $ essay    <chr> \"The basic dilema facing the UK's rail and road transport sys…\nnativeness_df <- rbind(learners_df, natives_df)  # combine\nnativeness_df %>%\n    janitor::tabyl(type)\n#>     type   n percent\n#>  Learner 689   0.626\n#>   Native 411   0.374\nnativeness_df %>%\n    unnest_tokens(output = \"word\", input = \"essay\") %>%\n    count(word, type) %>%\n    group_by(type) %>%\n    summarize(total_words = sum(n))\n#> # A tibble: 2 × 2\n#>   type    total_words\n#>   <chr>         <int>\n#> 1 Learner      640535\n#> 2 Native       324269\nnativeness_corpus <- nativeness_df %>%\n    corpus(text_field = \"essay\")\n\nnativeness_corpus_summary <- nativeness_corpus %>%\n    summary(n = ndoc(nativeness_corpus))\n\nnativeness_corpus_summary %>%\n    slice_head(n = 10)\n#> Corpus consisting of 1100 documents, showing 1100 documents:\n#> \n#>    Text Types Tokens Sentences essay_id    type student\n#>   text1   210    496        26     A1-1 Learner Spanish\n#>   text2   253    655        29     A1-2 Learner Spanish\n#>   text3   280    722        30    A10-1 Learner Spanish\n#>   text4   177    389        15    A10-2 Learner Spanish\n#>   text5   245    636        19   A101-1 Learner Spanish\n#>   text6   219    692        17   A101-2 Learner Spanish\n#>   text7   213    615        16   A101-3 Learner Spanish\n#>   text8   319    914        23   A101-4 Learner Spanish\n#>   text9   253    654        19   A102-1 Learner Spanish\n#>  text10   310    816        21   A102-2 Learner Spanish\nnativeness_corpus$doc_id <- 1:ndoc(nativeness_corpus)\n\nnativeness_corpus %>%\n    docvars() %>%\n    slice_head(n = 5)\n#>   essay_id    type student doc_id\n#> 1     A1-1 Learner Spanish      1\n#> 2     A1-2 Learner Spanish      2\n#> 3    A10-1 Learner Spanish      3\n#> 4    A10-2 Learner Spanish      4\n#> 5   A101-1 Learner Spanish      5\nnativeness_tokens <- nativeness_corpus %>%\n    tokens(what = \"word\", remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)\n\nnativeness_tokens %>%\n    tokens_group(groups = type) %>%\n    head()\n#> Tokens consisting of 2 documents and 1 docvar.\n#> Learner :\n#>  [1] \"In\"      \"our\"     \"present\" \"society\" \"gay\"     \"people\"  \"is\"     \n#>  [8] \"asking\"  \"for\"     \"the\"     \"same\"    \"rights\" \n#> [ ... and 636,407 more ]\n#> \n#> Native :\n#>  [1] \"The\"       \"basic\"     \"dilema\"    \"facing\"    \"the\"       \"UK's\"     \n#>  [7] \"rail\"      \"and\"       \"road\"      \"transport\" \"system\"    \"is\"       \n#> [ ... and 321,936 more ]\nnativeness_dfm <- nativeness_tokens %>%\n    dfm()\n\nnativeness_dfm %>%\n    head(n = 5)\n#> Document-feature matrix of: 5 documents, 28,129 features (99.24% sparse) and 4 docvars.\n#>        features\n#> docs    in our present society gay people is asking for the\n#>   text1 12   5       4       5   5     10 12      1   3  23\n#>   text2 26   2       1       4   0     10 14      0   1  45\n#>   text3 12   4       0       4   9     16 12      0   0  12\n#>   text4 15   1       0       1   0      9  7      0   1  10\n#>   text5 15   1       1       2   0      7 17      0   6  40\n#> [ reached max_nfeat ... 28,119 more features ]\n\nnativeness_dfm %>%\n    dfm_group(groups = type) %>%\n    head(n = 5)\n#> Document-feature matrix of: 2 documents, 28,129 features (33.74% sparse) and 1 docvar.\n#>          features\n#> docs         in  our present society gay people    is asking  for   the\n#>   Learner 15518 1279     322    1269 520   5980 12931     33 4993 36728\n#>   Native   6357  584     109     422   6   1569  6307     12 3144 21090\n#> [ reached max_nfeat ... 28,119 more features ]\nnativeness_dfm %>%\n    topfeatures(n = 25)\n#>     the      of      to     and      in    that       a      is      it    this \n#>   57818   32821   31155   24158   21875   20847   19981   19238   11259   10357 \n#>      be     are    they     not     for      as  people    have    with      or \n#>   10096   10085    8438    8196    8137    7977    7549    7506    6007    5568 \n#>   their       i      on      by because \n#>    5192    5128    4820    4579    4511\nnativeness_dfm %>%\n    textstat_frequency(n = 5, groups = type)\n#>    feature frequency rank docfreq   group\n#> 1      the     36728    1     689 Learner\n#> 2       of     22095    2     689 Learner\n#> 3       to     20417    3     689 Learner\n#> 4     that     15929    4     689 Learner\n#> 5      and     15836    5     689 Learner\n#> 6      the     21090    1     410  Native\n#> 7       to     10738    2     410  Native\n#> 8       of     10726    3     410  Native\n#> 9      and      8322    4     409  Native\n#> 10       a      6827    5     410  Native\nnativeness_dfm %>%\n    dfm_tfidf() %>%\n    textstat_frequency(n = 5, groups = type, force = TRUE)\n#>      feature frequency rank docfreq   group\n#> 1   abortion      1108    1      69 Learner\n#> 2  marijuana       901    2      46 Learner\n#> 3  education       746    3     204 Learner\n#> 4   children       735    4     317 Learner\n#> 5        sex       658    5     133 Learner\n#> 6         he       803    1     199  Native\n#> 7        his       573    2     194  Native\n#> 8    candide       522    3      27  Native\n#> 9      quote       402    4      26  Native\n#> 10  caligula       394    5      11  Native\nset.seed(4321) # make reproducible\n\nnum_docs <- \n  nativeness_dfm %>% \n  ndoc()\n\ntrain_size <- \n  (num_docs * .75) %>% # get size of sample\n  round() # round to nearest whole number\n\ntrain_ids <- sample(x = 1:num_docs, # population\n                   size = train_size, # size of sample\n                   replace = FALSE) # without replacement\nnativeness_dfm_train <- nativeness_dfm %>%\n    dfm_subset(doc_id %in% train_ids)\n\nnativeness_dfm_test <- nativeness_dfm %>%\n    dfm_subset(!doc_id %in% train_ids)\nnativeness_dfm %>%\n    docvars() %>%\n    janitor::tabyl(type)\n#>     type   n percent\n#>  Learner 689   0.626\n#>   Native 411   0.374\n\nnativeness_dfm_train %>%\n    docvars() %>%\n    janitor::tabyl(type)\n#>     type   n percent\n#>  Learner 516   0.625\n#>   Native 309   0.375\n\nnativeness_dfm_test %>%\n    docvars() %>%\n    janitor::tabyl(type)\n#>     type   n percent\n#>  Learner 173   0.629\n#>   Native 102   0.371\nnb1 <- textmodel_nb(x = nativeness_dfm_train, y = nativeness_dfm_train$type)\n\nsummary(nb1)\n#> \n#> Call:\n#> textmodel_nb.dfm(x = nativeness_dfm_train, y = nativeness_dfm_train$type)\n#> \n#> Class Priors:\n#> (showing first 2 elements)\n#> Learner  Native \n#>     0.5     0.5 \n#> \n#> Estimated Feature Scores:\n#>             in     our  present society      gay  people     is   asking\n#> Learner 0.0230 0.00186 0.000476 0.00193 7.62e-04 0.00889 0.0193 5.76e-05\n#> Native  0.0174 0.00163 0.000315 0.00114 2.22e-05 0.00460 0.0176 2.96e-05\n#>             for    the     same   rights   that heterosexual    some     of\n#> Learner 0.00747 0.0544 0.001318 0.000909 0.0238     3.06e-04 0.00476 0.0328\n#> Native  0.00886 0.0582 0.000563 0.000252 0.0140     1.11e-05 0.00157 0.0298\n#>            this     are     to      be  allowed      get  married    and\n#> Learner 0.01148 0.01114 0.0300 0.01023 0.000326 0.001054 3.67e-04 0.0233\n#> Native  0.00796 0.00745 0.0296 0.00932 0.000278 0.000763 8.52e-05 0.0224\n#>            adopt children  request     has    been accepted\n#> Learner 2.64e-04  0.00268 1.39e-05 0.00414 0.00187 0.000137\n#> Native  3.71e-05  0.00119 2.22e-05 0.00435 0.00213 0.000141\ncoef(nb1) %>% head()\n#>          Learner   Native\n#> in      0.023045 1.74e-02\n#> our     0.001856 1.63e-03\n#> present 0.000476 3.15e-04\n#> society 0.001929 1.14e-03\n#> gay     0.000762 2.22e-05\n#> people  0.008893 4.60e-03\n\npredict(nb1, type = \"prob\") %>% # get the predicted document scores\n  tail # preview predicted probability scores\n#>            Learner Native\n#> text1092 2.37e-164      1\n#> text1094  3.18e-28      1\n#> text1095  8.53e-49      1\n#> text1096  1.83e-37      1\n#> text1097  7.45e-39      1\n#> text1098  2.35e-55      1\nnb1_predictions <- \n  predict(nb1, type = \"prob\") %>% # get the predicted document scores\n  as.data.frame() %>% # convert to data frame\n  mutate(document = rownames(.)) %>% # add the document names to the data frame\n  as_tibble() %>% # convert to tibble\n  pivot_longer(cols = c(\"Learner\", \"Native\"), # convert from wide to long format\n               names_to = \"prediction\", # new column for ham/spam predictions\n               values_to = \"probability\") %>% # probablity scores for each\n  group_by(document) %>% # group parameter by document\n  slice_max(probability, n = 1) %>% # keep the document row with highest probablity\n  slice_head(n = 1) %>% # for predictions that were 50/50 \n  ungroup() %>% # remove grouping parameter\n  mutate(doc_id = str_remove(document, \"text\") %>% as.numeric) %>% # clean up document column so it matches doc_id in\n  arrange(doc_id) # order by doc_id\n\nnb1_predictions %>% \n  slice_head(n = 10) # preview\n#> # A tibble: 10 × 4\n#>    document prediction probability doc_id\n#>    <chr>    <chr>            <dbl>  <dbl>\n#>  1 text1    Learner              1      1\n#>  2 text3    Learner              1      3\n#>  3 text4    Learner              1      4\n#>  4 text5    Learner              1      5\n#>  5 text6    Learner              1      6\n#>  6 text8    Learner              1      8\n#>  7 text9    Learner              1      9\n#>  8 text11   Learner              1     11\n#>  9 text12   Learner              1     12\n#> 10 text13   Learner              1     13\nnb1_predictions_actual <- \n  cbind(actual = nb1$y, nb1_predictions) %>% # column-bind actual classes\n  select(doc_id, document, actual, prediction, probability) # organize variables\n\nnb1_predictions_actual %>% \n  slice_head(n = 5) # preview\n#>   doc_id document  actual prediction probability\n#> 1      1    text1 Learner    Learner           1\n#> 2      3    text3 Learner    Learner           1\n#> 3      4    text4 Learner    Learner           1\n#> 4      5    text5 Learner    Learner           1\n#> 5      6    text6 Learner    Learner           1\ntab_class <- \n  table(nb1_predictions_actual$actual, # actual class labels\n        nb1_predictions_actual$prediction) # predicted class labels\n\ncaret::confusionMatrix(tab_class, mode = \"prec_recall\") # model performance statistics\n#> Confusion Matrix and Statistics\n#> \n#>          \n#>           Learner Native\n#>   Learner     516      0\n#>   Native       10    299\n#>                                         \n#>                Accuracy : 0.988         \n#>                  95% CI : (0.978, 0.994)\n#>     No Information Rate : 0.638         \n#>     P-Value [Acc > NIR] : < 2e-16       \n#>                                         \n#>                   Kappa : 0.974         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.00443       \n#>                                         \n#>               Precision : 1.000         \n#>                  Recall : 0.981         \n#>                      F1 : 0.990         \n#>              Prevalence : 0.638         \n#>          Detection Rate : 0.625         \n#>    Detection Prevalence : 0.625         \n#>       Balanced Accuracy : 0.990         \n#>                                         \n#>        'Positive' Class : Learner       \n#> \npredicted_class <- predict(nb1, newdata = nativeness_dfm_test)\n\nactual_class <- nativeness_dfm_test$type\n\ntab_class <- table(actual_class, predicted_class)  # cross-tabulate actual and predicted class labels\n\ncaret::confusionMatrix(tab_class, mode = \"prec_recall\")  # model performance statistics\n#> Confusion Matrix and Statistics\n#> \n#>             predicted_class\n#> actual_class Learner Native\n#>      Learner     172      1\n#>      Native        9     93\n#>                                         \n#>                Accuracy : 0.964         \n#>                  95% CI : (0.934, 0.982)\n#>     No Information Rate : 0.658         \n#>     P-Value [Acc > NIR] : <2e-16        \n#>                                         \n#>                   Kappa : 0.921         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.0269        \n#>                                         \n#>               Precision : 0.994         \n#>                  Recall : 0.950         \n#>                      F1 : 0.972         \n#>              Prevalence : 0.658         \n#>          Detection Rate : 0.625         \n#>    Detection Prevalence : 0.629         \n#>       Balanced Accuracy : 0.970         \n#>                                         \n#>        'Positive' Class : Learner       \n#> "},{"path":"prediction-chapter.html","id":"preparation-1","chapter":"9 Prediction","heading":"9.1 Preparation","text":"Data set transformationSplitting training test sets","code":""},{"path":"prediction-chapter.html","id":"model-training","chapter":"9 Prediction","heading":"9.2 Model training","text":"Aim use create abstraction patterns datasetFeature engineeringFeature engineeringModel selectionModel selectionModel evaluationModel evaluationThe Figure () Let’s consider results hypothetical model text classification SMS dataset introduced subsection.accuracy (measure overall correct predictions)precision (measure quality predictions)\nPercentage predicted ‘ham’ messages correct\nPercentage predicted ‘ham’ messages correctrecall (measure quantity predictions)\nPercentage actual ‘ham’ messages correct\nPercentage actual ‘ham’ messages correctF1-score (summarizes balance precision recall)Avoiding overfitting","code":""},{"path":"prediction-chapter.html","id":"model-testing","chapter":"9 Prediction","heading":"9.3 Model testing","text":"Aim test abstracted model new observations.Model testing","code":""},{"path":"prediction-chapter.html","id":"evaluation","chapter":"9 Prediction","heading":"9.4 Evaluation","text":"Evaluation resultsRelationship predicted actual classes confusion matrix seen Figure 9.1.\nFigure 9.1: Confusion matrix\n","code":""},{"path":"prediction-chapter.html","id":"section","chapter":"9 Prediction","heading":"9.5 …","text":"Table () see top five terms class breaking messages terms counting frequencies.","code":""},{"path":"prediction-chapter.html","id":"summary-12","chapter":"9 Prediction","heading":"9.6 Summary","text":"…","code":""},{"path":"exploration-chapter.html","id":"exploration-chapter","chapter":"10 Exploration","heading":"10 Exploration","text":"\nINCOMPLETE DRAFT\nNobody ever figures life , doesn’t matter. Explore world. Nearly everything really interesting go deeply enough.– Richard P. Feynman\nessential questions chapter :\n\n…\nchapter….identify, interrogate, interpretidentify, interrogate, interpretEDA inductive approach. , bottom-–come analysis strong preconceptions data tell us (IDA - hypothesis PDA - target classes). aim uncover discover patterns lead insight based qualitative interpretation. EDA, , can considered quantitative-supported qualitative analysis.EDA inductive approach. , bottom-–come analysis strong preconceptions data tell us (IDA - hypothesis PDA - target classes). aim uncover discover patterns lead insight based qualitative interpretation. EDA, , can considered quantitative-supported qualitative analysis.Two main classes exploratory data analysis: (1) descriptive analysis (2) unsupervised machine learning.\ndescriptive analysis can seen detailed implementation descriptive assessment, key component inferential predictive analysis approaches. .\nunsupervised machine learning algorithmic approach deriving knowledge leverages … produce knowledge can interpreted. approach falls umbrella machine learning, seen predictive data analysis, however, whereas PDA assumes potential relationship input features target outcomes, classes, unsupervised learning classes induced data classes groupings interpreted evaluated …?\nTwo main classes exploratory data analysis: (1) descriptive analysis (2) unsupervised machine learning.descriptive analysis can seen detailed implementation descriptive assessment, key component inferential predictive analysis approaches. .unsupervised machine learning algorithmic approach deriving knowledge leverages … produce knowledge can interpreted. approach falls umbrella machine learning, seen predictive data analysis, however, whereas PDA assumes potential relationship input features target outcomes, classes, unsupervised learning classes induced data classes groupings interpreted evaluated …?, however, important come EDA research question unit analysis clear., however, important come EDA research question unit analysis clear.Description datasets use examine various exploratory methods.LastfmLast.fm webscrape top artists genre acquired Chapter 5 “Acquire data” web scrape section transformed Chapter 6 “Transform data”.lastfm_df dataset contains 155 observations 4 variables. observation corresponds particular song.Let’s look data dictionary dataset.Table 10.1: Last.fm lyrics dataset data dictionary.data dictionary see song encodes artist, song title, genre song, lyrics song.prepare upcoming exploration methods, convert lastfm_df Quanteda corpus object.SOTUThe quanteda package (Benoit et al., 2022) includes various datasets. work State Union Corpus (Benoit, 2020). Let’s take look structure dataset.sotu_df dataset 84 observations 5 variables. observation corresponds presidential address.Let’s look data dictionary understand column measures.Table 10.2: SOTU dataset data dictionary.see observation corresponds president gave address, modality address, party president affliated , year address given, address text.","code":"\nglimpse(lastfm_df)  # preview dataset structure\n#> Rows: 200\n#> Columns: 4\n#> $ artist <chr> \"Alan Jackson\", \"Alan Jackson\", \"Brad Paisley\", \"Carrie Underwo…\n#> $ song   <chr> \"Little Bitty\", \"Remember When\", \"Mud on the Tires\", \"Before He…\n#> $ lyrics <chr> \"Have a little love on a little honeymoon You got a little dish…\n#> $ genre  <chr> \"country\", \"country\", \"country\", \"country\", \"country\", \"country…\n# Create corpus object\nlastfm_corpus <- \n  lastfm_df %>% # data frame\n  corpus(text_field = \"lyrics\") # create corpus\n\nlastfm_corpus %>% \n  summary(n = 5) # preview\n#> Corpus consisting of 155 documents, showing 5 documents:\n#> \n#>   Text Types Tokens Sentences           artist                song   genre\n#>  text1    84    271         1     Alan Jackson        Little Bitty country\n#>  text2   110    203         1     Alan Jackson       Remember When country\n#>  text3   130    290         2     Brad Paisley    Mud on the Tires country\n#>  text4   114    303         1 Carrie Underwood    Before He Cheats country\n#>  text5   171    517        15   Dierks Bentley What Was I Thinkin' country\nglimpse(sotu_df)  # preview dataset structure\n#> Rows: 84\n#> Columns: 5\n#> $ president <chr> \"Truman\", \"Truman\", \"Truman\", \"Truman\", \"Truman\", \"Truman\", …\n#> $ delivery  <fct> written, spoken, spoken, spoken, spoken, spoken, spoken, wri…\n#> $ party     <fct> Democratic, Democratic, Democratic, Democratic, Democratic, …\n#> $ year      <dbl> 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1953, 1954, …\n#> $ text      <chr> \"To the Congress of the United States:\\n\\nA quarter century …"},{"path":"exploration-chapter.html","id":"descriptive-analysis","chapter":"10 Exploration","heading":"10.1 Descriptive analysis","text":"… overview summary aims descriptive analysis methods…","code":""},{"path":"exploration-chapter.html","id":"frequency-analysis","chapter":"10 Exploration","heading":"10.1.1 Frequency analysis","text":"Explore word frequency.see tokenized output.Many frequency analysis function provided quanteda require dataset document-frequency matrix. let’s create dfm lastfm_corpus object using dfm() function.Frequency distributions.high frequency terms many low frequency.results long tail plotted.Let’s take closer look 50 frequent word terms lastfm_dfm. use textstat_frequency() function quanteda.textstats package extract various frequency measures.can use data frame plot frequency terms descending order using ggplot().\nNow common terms song lyrics. case, let’s look common 15 terms genres. need groups = argument textstat_frequency() get genre need manipulate data frame output extract top 15 terms grouping genre.\nNote ’ve used plotting function facet_wrap() tell ggplot2 organize genres separate bar plots plotting space. scales = argument takes either free, free_x, free_y value. let axes either x- y-axis vary freely separate plots.\nRaw frequency effected total number words genre. Therefore safely make direct comparisons frequency counts individual terms genres.make term-genre comparisons comparable normalized term frequency number terms genre. can use dfm_weight() function argument scheme = \"prop\" give us relative frequency term per number terms document appears . weighting known Term frequency.Term frequency makes frequency scores relative genre. means frequencies directly comparable number words genre taken account calculating term frequency score.Now term frequency measures allow us make direct comparisons, one problem frequent terms tend terms common across language use. Since aim frequency analyses compare sub-groups discover terms indicative sub-group need way adjust weigh measures. scheme often applied scale terms according common apply term frequency-inverse document frequency (tf-idf). tf-idf measure result multiplying term frequency inverse document frequency.Table 10.3: Number documents per genre.TF-IDF works well identify terms particularly indicative particular group shortcoming particularly salient working song lyrics. , terms frequent common appear one song repeated. common song lyrics tend repeated chorus sections. minimize influence, can trim document-frequency matrix eliminate terms appear one song.Another exploration method look relative frequency, keyness, measures. type analysis compares relative frequency terms target group comparison reference group. set target one genres genres become reference. results show terms occur significantly often occur reference group(s). textstat_keyness() function implements type analysis quanteda.output textstat_keyness() function terms frequent target group frequent reference group(s). textplot_keyness() takes advantage can see contrastive terms plot.Let’s look terms least indicative ‘country’ genre.Interpretation….Now let’s look ‘Hip hop’ genre.Interpretation …Now working words tokens/ features word simply unigram token. can also consider multi-word tokens, ngrams. create bigrams (2-word tokens) return lastfm_tokens object add function tokens_ngrams() argument n = 2 (bigrams). just create DFM object. go ahead trim DFM exclude terms appearing one document (.e. song).Interpretation …can now repeat steps earlier explore raw frequency, term frequency, tf-idf frequency measures genre. skip visualization raw frequency inherently incompatible direct comparisons sub-groups.Interpretation …can even pull particular terms explore directly.Interpretation …leave introduction frequency analysis, let’s consider another type metric can used explore term usage across documents aims estimate lexical diversity, number unique terms (types) total number terms (tokens). known Type-Token Ratio (TTR). TTR measure biased comparison documents groups differ number total tokens. mitigate issue Moving-Average Type-Token Ratio (MATTR) often used. MATTR moving window size must set reasonable size given size documents. case use 50 lyrics datasset least number words.use box plots visualize distribution TTR MATTR estimates across four genres.can see similarities differences two estimates lexical diversity. cases, trend towards ‘country’ diverse ‘pop’ least diverse. ‘rock’ ‘hip-hop’ swapped given estimate type. important, however, note notches box plot provide us rough guide gauge whether trends statistically significant . Focusing reliable MATTR using notches guide, looks like can safely say ‘country’ lexically diverse genres. Another potential take-home message pop appears internally variable –, appears quite bit variability lexical diversity songs genre.","code":"\n# Create tokens object\nlastfm_tokens <- \n  lastfm_corpus %>% # corpus object\n  tokens(what = \"word\", # tokenize by word\n         remove_punct = TRUE) %>% # remove punctuation\n  tokens_tolower() # lowercase tokens\n\nlastfm_tokens %>% \n  head(n = 1) # preview one tokenized document\n#> Tokens consisting of 1 document and 3 docvars.\n#> text1 :\n#>  [1] \"have\"      \"a\"         \"little\"    \"love\"      \"on\"        \"a\"        \n#>  [7] \"little\"    \"honeymoon\" \"you\"       \"got\"       \"a\"         \"little\"   \n#> [ ... and 252 more ]\n# Create document-frequency matrix\nlastfm_dfm <- \n  lastfm_tokens %>% # tokens object\n  dfm() # create dfm\n\nlastfm_dfm %>% \n  head(n = 5) # preview 5 documents\n#> Document-feature matrix of: 5 documents, 3,966 features (97.19% sparse) and 3 docvars.\n#>        features\n#> docs    have  a little love on honeymoon you got dish and\n#>   text1    1 35     34    1  5         1   4   3    1  10\n#>   text2    0  1      1    3  0         0   3   0    0  12\n#>   text3    1 21     11    0  9         0   6   5    0   6\n#>   text4    1 10      5    0  3         0   1   0    0   4\n#>   text5    0 22      7    0  2         0   0   1    0   5\n#> [ reached max_nfeat ... 3,956 more features ]\nlastfm_dfm %>%\n    textstat_frequency() %>%\n    slice_head(n = 10)\n#>    feature frequency rank docfreq group\n#> 1        i      2048    1     148   all\n#> 2      you      2045    2     139   all\n#> 3      the      1787    3     148   all\n#> 4      and      1242    4     151   all\n#> 5        a      1067    5     135   all\n#> 6       to       933    6     147   all\n#> 7       me       883    7     132   all\n#> 8       my       768    8     116   all\n#> 9       it       666    9     118   all\n#> 10      in       597   10     131   all\nlastfm_dfm %>%\n    textstat_frequency() %>%\n    slice_head(n = 50) %>%\n    ggplot(aes(x = reorder(feature, frequency), y = frequency)) + geom_col() + coord_flip() +\n    labs(x = \"Words\", y = \"Raw frequency\", title = \"Top 50\")\nlastfm_dfm %>% # dfm\n  textstat_frequency(groups = genre) %>% # get frequency statistics\n  group_by(group) %>% # grouping parameters\n  slice_max(frequency, n = 15) %>% # extract top features\n  ungroup() %>% # remove grouping parameters\n  ggplot(aes(x = frequency, y = reorder_within(feature, frequency, group), fill = group)) + # mappings (reordering feature by frequency)\n  geom_col(show.legend = FALSE) + # bar plot\n  scale_y_reordered() + # clean up y-axis labels (features)\n  facet_wrap(~group, scales = \"free_y\") + # organize separate plots by genre\n  labs(x = \"Raw frequency\", y = NULL) # labels\nlastfm_dfm %>% # dfm\n  dfm_weight(scheme = \"prop\") %>% # weigh by term frequency\n  textstat_frequency(groups = genre) %>% # get frequency statistics\n  group_by(group) %>% # grouping parameters\n  slice_max(frequency, n = 15) %>% # extract top features\n  ungroup() %>% # remove grouping parameters\n  ggplot(aes(x = frequency, y = reorder_within(feature, frequency, group), fill = group)) + # mappings (reordering feature by frequency)\n  geom_col(show.legend = FALSE) + # bar plot\n  scale_y_reordered() + # clean up y-axis labels (features)\n  facet_wrap(~group, scales = \"free_y\") + # organize separate plots by genre\n  labs(x = \"Term frequency\", y = NULL) # labels\nlastfm_df %>% # data frame\n  count(genre) %>%  # get number of documents for each genre\n  select(Genre = genre, `Number of documents` = n) %>% \n  knitr::kable(booktabs = TRUE,\n               caption = \"Number of documents per genre.\")\nlastfm_dfm %>% \n  dfm_weight(scheme = \"prop\") %>% # term-frequency weight\n  textstat_frequency(groups = genre) %>% # include genre as a group\n  filter(feature == \"i\") # filter only \"i\"\n#>      feature frequency rank docfreq   group\n#> 1          i      1.86    1      42 country\n#> 1639       i      1.00    1      26 hip-hop\n#> 3600       i      1.64    2      38     pop\n#> 5012       i      2.08    1      42    rock\n# Manually calculate TF-IDF scores\n1.86 * log10(44/42)  # i in country\n#> [1] 0.0376\n1 * log10(26/26)  # i in hip hop\n#> [1] 0\n1.64 * log10(41/38)  # i in pop\n#> [1] 0.0541\n2.08 * log10(44/42)  # i in rock\n#> [1] 0.042\nlastfm_dfm %>%\n    dfm_tfidf(scheme_tf = \"prop\") %>%\n    textstat_frequency(groups = genre, force = TRUE) %>%\n    filter(str_detect(feature, \"^(i|yeah)$\")) %>%\n    arrange(feature)\n#>      feature frequency rank docfreq   group\n#> 213        i    0.0373  213      42 country\n#> 1873       i    0.0201  235      26 hip-hop\n#> 3842       i    0.0329  244      38     pop\n#> 5197       i    0.0417  186      42    rock\n#> 239     yeah    0.0350  239       8 country\n#> 1742    yeah    0.0336  104      14 hip-hop\n#> 3755    yeah    0.0454  157      13     pop\n#> 5013    yeah    0.2165    2      17    rock\nlastfm_dfm %>% # dfm\n  dfm_tfidf(scheme_tf = \"prop\") %>%  # weigh by tf-idf\n  textstat_frequency(groups = genre, force = TRUE) %>% # get frequency statistics\n  group_by(group) %>% # grouping parameters\n  slice_max(frequency, n = 15) %>% # extract top features\n  ungroup() %>% # remove grouping parameters\n  ggplot(aes(x = frequency, y = reorder_within(feature, frequency, group), fill = group)) + # mappings (reordering feature by frequency)\n  geom_col(show.legend = FALSE) + # bar plot\n  scale_y_reordered() + # clean up y-axis labels (features)\n  facet_wrap(~group, scales = \"free_y\") + # organize separate plots by genre\n  labs(x = \"TF-IDF\", y = NULL) # labels\nlastfm_dfm %>% # dfm\n  dfm_trim(min_docfreq = 2) %>% # keep terms appearing in 2 or more songs\n  dfm_tfidf(scheme_tf = \"prop\") %>%  # weigh by tf-idf\n  textstat_frequency(groups = genre, force = TRUE) %>% # get frequency statistics\n  group_by(group) %>% # grouping parameters\n  slice_max(frequency, n = 15) %>% # extract top features\n  ungroup() %>% # remove grouping parameters\n  ggplot(aes(x = frequency, y = reorder_within(feature, frequency, group), fill = group)) + # mappings (reordering feature by frequency)\n  geom_col(show.legend = FALSE) + # bar plot\n  scale_y_reordered() + # clean up y-axis labels (features)\n  facet_wrap(~group, scales = \"free_y\") + # organize separate plots by genre\n  labs(x = \"TF-IDF\", y = NULL) # labels\nlastfm_keywords_country <- \n  lastfm_dfm %>% # dfm\n  dfm_trim(min_docfreq = 2) %>% # keep terms appearing in 2 or more songs\n  textstat_keyness(target = lastfm_dfm$genre == \"country\") # compare country\n\nlastfm_keywords_country %>% \n  slice_head(n = 10) # preview\n#>    feature  chi2        p n_target n_reference\n#> 1   little 113.3 0.00e+00       77          43\n#> 2   belong  45.6 1.46e-11       19           4\n#> 3     ring  45.2 1.80e-11       22           7\n#> 4     he's  43.3 4.82e-11       25          11\n#> 5     went  43.2 4.90e-11       17           2\n#> 6     fire  42.2 8.24e-11       21           7\n#> 7    blues  42.2 8.44e-11       14           0\n#> 8      him  40.9 1.60e-10       34          24\n#> 9  country  38.8 4.58e-10       13           0\n#> 10    road  37.7 8.11e-10       23          11\nlastfm_keywords_country %>% \n  textplot_keyness(n = 25, labelsize = 2) + # plot most contrastive terms\n  labs(x = \"Chi-squared statistic\", \n       title = \"Term keyness\", \n       subtitle = \"Country versus other genres\") # labels\nlastfm_dfm %>% # dfm\n  dfm_trim(min_docfreq = 2) %>% # keep terms appearing in 2 or more songs\n  textstat_keyness(target = lastfm_dfm$genre == \"hip-hop\") %>% # compare hip hop\n  textplot_keyness(n = 25, labelsize = 2) + # plot most contrastive terms\n  labs(x = \"Chi-squared statistic\",\n      title = \"Term keyness\", \n       subtitle = \"Hip hop versus other genres\") # labels\n# Tokenize by bigrams\nlastfm_dfm_ngrams <- \n  lastfm_tokens %>% # word tokens\n  tokens_ngrams(n = 2) %>% # create 2-term ngrams (bigrams)\n  dfm() %>% # create document-frequency matrix\n  dfm_trim(min_docfreq = 2) # keep terms appearing in 2 or more songs\n\nlastfm_dfm_ngrams %>% \n  head(n = 1) # preview 1 document\n#> Document-feature matrix of: 1 document, 3,232 features (98.73% sparse) and 3 docvars.\n#>        features\n#> docs    have_a a_little on_a you_got got_a and_you and_a well_it's it's_alright\n#>   text1      1       25    1       3     3       1     7         2            4\n#>        features\n#> docs    to_be\n#>   text1     4\n#> [ reached max_nfeat ... 3,222 more features ]\n# Term frequency comparison\nlastfm_dfm_ngrams %>%\n    dfm_weight(scheme = \"prop\") %>%\n    textstat_frequency(groups = genre) %>%\n    filter(str_detect(feature, \"i_ain't\"))\n#>      feature frequency rank docfreq   group\n#> 45   i_ain't     0.110   45       4 country\n#> 1944 i_ain't     0.109   15       8 hip-hop\n#> 3733 i_ain't     0.109   54       3     pop\nlastfm_lexdiv <- lastfm_tokens %>%\n    textstat_lexdiv(measure = c(\"TTR\", \"MATTR\"), MATTR_window = 50)\n\nlastfm_docvars <- lastfm_tokens %>%\n    docvars()\n\nlastfm_lexdiv_meta <- cbind(lastfm_docvars, lastfm_lexdiv)\n\np1 <- lastfm_lexdiv_meta %>%\n    ggplot(aes(x = reorder(genre, TTR), y = TTR, color = genre)) + geom_boxplot(notch = TRUE,\n    show.legend = FALSE) + labs(x = \"Genre\")\n\np2 <- lastfm_lexdiv_meta %>%\n    ggplot(aes(x = reorder(genre, MATTR), y = MATTR, color = genre)) + geom_boxplot(notch = TRUE,\n    show.legend = FALSE) + labs(x = \"Genre\")\n\np1 + p2"},{"path":"exploration-chapter.html","id":"collocation-analysis","chapter":"10 Exploration","heading":"10.1.2 Collocation analysis","text":"frequency analysis focuses usage terms, collocation analysis focuses usage terms context.Keyword Context\nkwic()\nkwic()can also search multiword expressions using phrase(). can use pattern matching convention make key term searches (‘glob’ ‘regex’) less (‘fixed’) flexible.Collocation analysisThe frequency analysis ngrams terms similar distinct collocation analysis. collocation analysis frequency two terms co-occur balanced frequency terms cooccur. words, sequences occur one expect given frequency individual terms. provides estimate tendency sequence words form cohesive semantic syntactic unit.can apply textstat_collocations() function tokens object (lastfm_tokens) retrieve cohesive collocations (using \\(z\\)-statistic) entire dataset.Add minimum frequency count (min_count =) avoid hapaxes (terms happen infrequently yet occur, cooccur another specific term also occurs infrequently). can also specify size collocation (default 2). set 3 get three-word collocations.want explore collocations specific group dataset, can use tokens_subset() function specify group want subset use. Note minimum count need lowered (used ) size dataset now fraction considered documents (just particular genre).section covered common strategies exploration descriptive analysis methods. methods can extended combined dig uncover patterns research intermediate findings dictate.","code":"\nlastfm_tokens %>%\n    tokens_group(groups = genre) %>%\n    kwic(pattern = \"ain't\") %>%\n    slice_sample(n = 10)\n#> Keyword-in-context with 10 matches.                                                        \n#>   [country, 4468]       to find another for it | ain't |\n#>   [hip-hop, 6158]         i got lyrics but you | ain't |\n#>     [hip-hop, 66]         i'm into havin sex i | ain't |\n#>       [pop, 3060]     happen like that cause i | ain't |\n#>    [hip-hop, 336]         i'm into havin sex i | ain't |\n#>   [country, 9027]            no pool no pets i | ain't |\n#>   [hip-hop, 9996]     in his cage the audience | ain't |\n#>  [hip-hop, 10005]     ain't gone clap and they | ain't |\n#>    [hip-hop, 378]         i'm into havin sex i | ain't |\n#>   [hip-hop, 5398] maybe we're crazy probably i | ain't |\n#>                                  \n#>  right that she should live      \n#>  got none if you come            \n#>  into makin love so come         \n#>  no hollaback girl i ain't       \n#>  into makin love so come         \n#>  got no cigarettes ah but        \n#>  fazed and they ain't gone       \n#>  gone praise they want everything\n#>  into makin love so come         \n#>  happy i'm feeling glad i\nlastfm_tokens %>%\n    tokens_group(groups = genre) %>%\n    kwic(pattern = phrase(\"ain't no*\"), valuetype = \"glob\") %>%\n    slice_sample(n = 10)\n#> Keyword-in-context with 10 matches.                                                                        \n#>    [hip-hop, 171:172]        they wanna fuck but homie | ain't nothing |\n#>  [hip-hop, 3345:3346]        streets it's the d-r-e it | ain't nothing |\n#>      [pop, 3451:3452]         happen like that cause i |   ain't no    |\n#>      [pop, 3065:3066]        ain't no hollaback girl i |   ain't no    |\n#>    [pop, 10320:10321] is thriller thriller night there |   ain't no    |\n#>      [pop, 3231:3232]        ain't no hollaback girl i |   ain't no    |\n#>      [pop, 3060:3061]         happen like that cause i |   ain't no    |\n#>  [country, 2758:2759]            i'm a redneck woman i |   ain't no    |\n#>      [pop, 3368:3369]         happen like that cause i |   ain't no    |\n#>      [pop, 3373:3374]        ain't no hollaback girl i |   ain't no    |\n#>                                 \n#>  change hoes down g's up        \n#>  but more hot shit another      \n#>  hollaback girl i ain't no      \n#>  hollaback girl a few times     \n#>  second chance against the thing\n#>  hollaback girl ooh ooh this    \n#>  hollaback girl i ain't no      \n#>  high class broad i'm just      \n#>  hollaback girl i ain't no      \n#>  hollaback girl ooh ooh this\nlastfm_tokens %>%\n    textstat_collocations() %>%\n    slice_head(n = 5)\n#>   collocation count count_nested length lambda    z\n#> 1      in the   227            0      2   2.95 33.4\n#> 2   yeah yeah    86            0      2   5.32 33.4\n#> 3   jump jump    64            0      2   8.44 28.0\n#> 4       oh oh    59            0      2   4.78 27.8\n#> 5     bum bum    44            0      2   7.48 26.7\nlastfm_tokens %>%\n    textstat_collocations(min_count = 50, size = 2) %>%\n    slice_head(n = 10)\n#>    collocation count count_nested length lambda    z\n#> 1       in the   227            0      2   2.95 33.4\n#> 2    yeah yeah    86            0      2   5.32 33.4\n#> 3    jump jump    64            0      2   8.44 28.0\n#> 4        oh oh    59            0      2   4.78 27.8\n#> 5        la la    59            0      2   9.13 25.5\n#> 6      i'm not    64            0      2   3.95 25.0\n#> 7     a little    76            0      2   4.45 23.3\n#> 8      i don't   140            0      2   2.40 23.1\n#> 9       on the   131            0      2   2.29 22.0\n#> 10      like a    81            0      2   2.82 21.4\nlastfm_tokens %>%\n    tokens_subset(genre == \"pop\") %>%\n    textstat_collocations(min_count = 10, size = 3) %>%\n    slice_head(n = 25)\n#>        collocation count count_nested length lambda    z\n#> 1         of you i    12            0      3   6.85 6.35\n#> 2     i just can't    16            0      3   5.98 4.80\n#> 3         cry me a    27            0      3   8.81 4.03\n#> 4        up on you    10            0      3   4.01 4.00\n#> 5        is not my    17            0      3   7.11 3.84\n#> 6    you know that    19            0      3   2.71 3.77\n#> 7   don't stop the    25            0      3   7.01 3.76\n#> 8   know just just    14            0      3   7.55 3.61\n#> 9       you and me    11            0      3   5.16 3.44\n#> 10  just just what    14            0      3   7.14 3.43\n#> 11  don't you like    13            0      3   5.16 3.42\n#> 12    are you okay    40            0      3   6.98 3.37\n#> 13     just like a    27            0      3   5.58 3.35\n#> 14 where you gonna    27            0      3   5.90 3.30\n#> 15    i'm not your    10            0      3   6.80 3.28\n#> 16        my no he    10            0      3   7.85 3.16\n#> 17        me and i    10            0      3   2.45 3.15\n#> 18   don't call my    18            0      3   6.36 3.09\n#> 19   gonna be okay    10            0      3   7.59 3.06\n#> 20    this my shit    33            0      3   7.46 3.00\n#> 21  dance gonna be    10            0      3   7.28 2.94\n#> 22  because of you    14            0      3   6.19 2.90\n#> 23   just what you    14            0      3   2.94 2.90\n#> 24   for your call    11            0      3   5.97 2.84\n#> 25      am the one    12            0      3   6.05 2.84"},{"path":"exploration-chapter.html","id":"unsupervised-learning","chapter":"10 Exploration","heading":"10.2 Unsupervised learning","text":"now turn attention second group methods conducting exploratory analyses –unsupervised learning.Clustering\ntextstat_dist()\ntextstat_dist()Looking assigned clusters genres songs see interesting patterns. one cluster 1 appears majority songs, followed cluster 2, 3. cluster 1 Hip hop Pop make majority songs. cluster 2 Country Rock tend dominate cluster 3 scattering genres.Now can approach distinct linguistic unit. current clusters used words, switch bigrams see results change change.Topic modelingSentiment analysis","code":"\nlibrary(factoextra)\n\nlastfm_clust <- lastfm_dfm %>%\n    dfm_weight(scheme = \"prop\") %>%\n    textstat_dist(method = \"euclidean\") %>%\n    as.dist() %>%\n    hclust(method = \"ward.D2\")\n\nlastfm_clust %>%\n    fviz_dend(show_labels = FALSE, k = 4)\nlastfm_clust %>%\n    fviz_dend(show_labels = FALSE, k = 3)\nclusters <- lastfm_clust %>%\n    cutree(k = 3) %>%\n    as_tibble(rownames = \"document\")\n\nclusters\n#> # A tibble: 155 × 2\n#>    document value\n#>    <chr>    <int>\n#>  1 text1        1\n#>  2 text2        2\n#>  3 text3        2\n#>  4 text4        1\n#>  5 text5        1\n#>  6 text6        2\n#>  7 text7        3\n#>  8 text8        2\n#>  9 text9        1\n#> 10 text10       1\n#> # … with 145 more rows\ndocvars(lastfm_dfm, field = \"cluster\") <- clusters$value\nlastfm_dfm$cluster <- clusters$value\n\nlastfm_dfm %>%\n    docvars %>%\n    head\n#>             artist                song   genre cluster\n#> 1     Alan Jackson        Little Bitty country       1\n#> 2     Alan Jackson       Remember When country       2\n#> 3     Brad Paisley    Mud on the Tires country       2\n#> 4 Carrie Underwood    Before He Cheats country       1\n#> 5   Dierks Bentley What Was I Thinkin' country       1\n#> 6     Dolly Parton              9 to 5 country       2\nlastfm_dfm %>%\n    docvars() %>%\n    janitor::tabyl(genre, cluster) %>%\n    janitor::adorn_totals(where = c(\"row\", \"col\")) %>%\n    janitor::adorn_percentages() %>%\n    janitor::adorn_pct_formatting()\n#>    genre     1     2     3  Total\n#>  country 47.7% 40.9% 11.4% 100.0%\n#>  hip-hop 69.2% 19.2% 11.5% 100.0%\n#>      pop 63.4% 19.5% 17.1% 100.0%\n#>     rock 54.5% 31.8% 13.6% 100.0%\n#>    Total 57.4% 29.0% 13.5% 100.0%\n# Clustering: bigram features\n# textmodel_lsa()\nlibrary(vader)  # sentiment analysis for micro-blogging text\nlibrary(syuzhet)  # general sentiment analysis"},{"path":"communication-overview.html","id":"communication-overview","chapter":"Overview","heading":"Overview","text":"section cover steps presenting findings research research document reproducible research project. research documents reproducible projects fundamental components modern scientific inquiry. one hand research document provides readers detailed summary main import research study. hand making research project available interested readers ensures scientific community can gain insight process implemented research thus enables researchers vet extend research build robust verifiable research base.","code":""},{"path":"reporting-chapter.html","id":"reporting-chapter","chapter":"11 Reporting","heading":"11 Reporting","text":"\nINCOMPLETE DRAFT\n… quote ….\nessential questions chapter :\n\n….\nchapter first discuss purpose research document structure document supports clearly documents project’s rationale, goals, procedure, results, findings. turn incorporating citations references figures tables R Markdown. Finally, explore apply field-specific publishing house formats various document formats including Word, PDF, HTML, ePub.…","code":""},{"path":"collaboration-chapter.html","id":"collaboration-chapter","chapter":"12 Collaboration","heading":"12 Collaboration","text":"\nINCOMPLETE DRAFT\n… quote …\nessential questions chapter :\n\n…\nWhether researchers future self, creating research well-documented reproducible fundamental part conducting modern scientific inquiry. chapter emphasize importance endeavor outline strategies ensuring research project reproducible. include directory file structure, key documentation files well effectively use existing software resources frameworks publishing research (either private use, journal requirements, general public consumption) popular repositories GitHub Open Science Framework (OSF).","code":""},{"path":"appendix-data.html","id":"appendix-data","chapter":"A Data","heading":"A Data","text":"Listing data resources, description, licensing. (Example: SMLR data appendix)RepositoriesTalkbankLinguistic Data Consortium…CorporaCEDEL2Santa Barbara Corpus Spoken American English…WebsitesLast.fm","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
