[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"\nINCOMPLETE DRAFT\n coursebook accompany Linguistics 380 “Language Use Technology” Wake Forest University. working title coursebook Text Data: Introduction Quantitative Text Analysis Reproducible Research R.content currently development. Feedback welcome can provided hypothes.service. toolbar interface service located right sidebar. register free account join “TAD” annotation group follow link. Suggestions changes incorporated acknowledged.AuthorDr. Jerid Francom Associate Professor Spanish Linguistics Wake Forest University. research interests focused around quantitative approaches language variation.","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"work Jerid C. Francom licensed Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States License.","code":""},{"path":"index.html","id":"credits","chapter":"Welcome","heading":"Credits","text":"","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Welcome","heading":"Acknowledgements","text":"TAD reviewed suggestions changes incorporated based feedback TAD Hypothes.group following people: …","code":""},{"path":"foundations-overview.html","id":"foundations-overview","chapter":"Overview","heading":"Overview","text":"FOUNDATIONSIn section aims : (1) provide overview quantitative research applications, highlighting visible applications notable research various fields. (2) hood bit consider quantitative research contributes language research. (3) layout main types research situate quantitative text analysis inside . (4) provide overview rest coursebook highlighting learning goals, structure coursebook, structure support robust knowledge text analysis , used, conduct research.","code":""},{"path":"data-language-and-text-analysis.html","id":"data-language-and-text-analysis","chapter":"1 Data, language, and text analysis","heading":"1 Data, language, and text analysis","text":"\nDRAFT\nScience walks forward two feet, namely theory experiment…Sometimes one foot put forward first, sometimes , continuous progress made use .— Robert . Millikan (1923)\nessential questions chapter :\n\nrole goals data analysis outside academia?\n\nways quantitative language research approached?\n\napplications text analysis?\n\ncoursebook structured target learning goals?\nchapter aim introduce topic text analysis text analytics frame approach coursebook. goals section work general field data science/ data analysis particular sub-field text analysis (text defined broadly corpus). aim introduce context needed understand text analysis fits larger universe data analysis see commonalities ever-ubiquitous field data analysis, attention language linguistics studies employ data analysis particular area text analysis. round chapter, provide general overview rest coursebook motivating general structure sequencing well setting foundation programmatic approaches data analysis.","code":""},{"path":"data-language-and-text-analysis.html","id":"making-sense-of-a-complex-world","chapter":"1 Data, language, and text analysis","heading":"1.1 Making sense of a complex world","text":"world around us full actions interactions numerous difficult really comprehend. lens individual sees experiences world. gain knowledge world build heuristic knowledge works can interact . happens regardless educational background. humans built . minds process countless sensory inputs many never make conscious mind. underlie skills abilities take granted like able predict happen see someone knock wine glass table onto concrete floor. ’ve never seen object first time ’ve winery, somehow somewhere ‘instinctively’ make effort warn --glass-breaker late. likely stopped consider predictive knowledge come , , may just chalked ‘common sense.’ common may , incredible display brain’s capacity monitor environment, relate events observations take place, store information time making big fuss tell conscious mind ’s .wait, coursebook text analytics language, right? ? Well, two points make relevant framing journey: (1) world full countless information unfold real-time scale daunting (2) power brain works efficiently behind scene making sense world, one individual living one life limited view world large. Let expand two points little .First let’s clear. way one experience things times, .e. omnipotence. even extremely reduced slices reality still vastly outside experiental capacity, least real-time. One can make point since inception internet individual’s ability experience larger slices world increased. imagine reading, watching, listening every file currently accessible web? ? (See Wayback Machine.) Scale even ; let’s take Wikipedia, world’s largest encyclopedia. Can imagine reading every wiki entry? large resource Wikipedia ,1 still small fragment written language produced web, just web.2 Consider moment.second framing point, actually two points one. made underscored efficiency brain’s capacity make sense world. efficiency comes clever evolutionary twists lead brain take world makes shortcuts compress raw experience heuristic understanding. means brain supercomputer. store every experience raw form, access records experience like imagine computer access records logged database. brains excel making associations predictions help us (time) navigate complex world inhabit. point key –brains amazing work, work can give us impression understand world detail actually . Let’s little thought experiment. Close eyes think last time saw best friend. wearing? Can remember colors? like , human, probably pretty confident feeling know answers questions chance right. demonstrated numerous experiments human memory confidence correlate accuracy (Roediger & McDermott, 2000; Talarico & Rubin, 2003). ’ve experienced event, real reason lives experienced. ’s little bit scary, sure, magic works ‘good enough’ practical purposes.’s deal: humans (1) clearly unable experience large swaths experience simple fact individuals living individual lives (2) experiences live recorded precision therefore ‘trust’ intuitions, least absolute sense.mean human curiosity world around us ability reliably make sense ? short means need approach understanding world tools science. Science powerful makes strides overcome inherit limitations humans (breadth experience recall relational abilities) bring complex world digestible perspective. Science starts question, identifies collects data, careful selected slices complex world, submits data analysis clearly defined reproducible procedures, reports results others evaluate. process repeated, modifying, manipulating procedures, asking new questions positing new explanations, effort make inroads bring complex tangible view.essence science attempt subvert inherent limitations understanding drawing carefully purposefully collected slices experience letting analysis experience speak, even goes intuitions (powerful sometime spurious heuristics brains use make sense world).","code":""},{"path":"data-language-and-text-analysis.html","id":"data-analysis","chapter":"1 Data, language, and text analysis","heading":"1.2 Data analysis","text":"point ’ve sketched outline strengths limitations humans’ ability make sense world science address limitations. science ’ve described one familiar indespensible tool make sense world. like , description science may associated visions white coats, labs, petri dishes. science’s foundation still stands strong 21st century, series intellectual technological events mid-20th century set motion changes changed aspects science done, done. call Science 2.0, let’s use popularized term “Data Science.” recognized beginnings Data Science attributed work “Statistics Data Analysis Research” department Bell Labs 1960s. Although primarily conceptual theoretic time, framework quantitative data analysis took shape anticipate come: sizable datasets “…require advanced statistical computational techniques … software implement .” (Chambers, 2020) framework emphasized inference-based research traditional science, also embraced exploratory research recognized need address practical considerations arise working deriving insight abundance data.Fast-forward 21st century world machine readable data truly abundance. increased computing power innovative uses technology world wide web took flight. put perspective, focusing language, amount text [add stats amount data added web every day/month/year compared literature .. ..?]. data flood limited language, sensors recording devices ever capture evermore swaths world live (Desjardins, 2019). increased computing power gave rise influx data, also primary methods gathering, preparing, transforming, analyzing, communicating insight derived data (Donoho, 2017). vision laid 1960s Bell Labs come fruition.interest deriving insight available data now almost ubiquitous. science data now reached deep aspects life making sense world sought. Predicting whether loan applicant get loan [cite], whether lump cancerous [cite], films recommend based previous viewing history [cite], players sports team sign (Lewis, 2004) now incorporate common set data analysis tools.advances, however, predicated data alone. envisioned researchers Bell Labs, turning data insight takes computing skills (.e. programming), knowledge statistics, , importantly, substantive/ domain expertise. triad popularly represented Venn diagram 1.1.\nFigure 1.1: Data Science Venn Diagram adapted Drew Conway.\ntoolbelt underlies well-known public-facing language applications. language-capable personal assistant applications, plagiarism detection software, machine translation search, tangible results quantitative approaches language becoming standard fixtures lives.\nFigure 1.2: Well-known language applications\nspread quantitative data analysis taken root academia. Even areas first blush don’t appear approached quantitative manner fields social sciences humanities, data science making important sometimes disisplinary changes way academic research conducted. coursebook focuses domain cuts across many fields; namely language. point let’s turn quantitative approaches language.","code":""},{"path":"data-language-and-text-analysis.html","id":"language-analysis","chapter":"1 Data, language, and text analysis","heading":"1.3 Language analysis","text":"Language defining characteristic species. , study language key concern wide variety fields, just linguists. goals various fields, however, approaches language research, vary. one hand language research traditions, namely closely associated Noam Chomsky, eschewed quantitative approaches language research later half 20th century instead turned qualitative assessment language structure introspective methods. hand many language research programs turned /developed quantitative research methods either necessity theoretical principles. quantitative research trajectories share much common data analysis toolbox described previous section. means large extent language analysis projects share common research language language research also research beyond outside language. However, never one-size-fits approach anything –much less data analysis. quantitative analysis key distinction data collection downstream effects terms procedure also terms interpretation.key distinction, need make point, provide context exploration text analysis, comes approach collecting language data nature data. distinction experimental observational data collection. Experimental approaches start intentionally designed hypothesis lay research methodology appropriate instruments plan collect data shows promise shedding light validity hypothesis. Experimental approaches conducted controlled contexts, usually lab environment, participants recruited perform language related task stimuli carefully curated researchers elicit aspect language behavior interest. Experimental approaches language research heavily influenced procedures adapted psychology. link logical language central area study cognitive psychology. approach looks much like white-coat science made reference earlier , quantitative research, now taken advantage data analysis tool belt collect organize much larger quantities data conduct statistically robust analysis procedures communicate findings efficiently.Observational approaches bit mixed bag terms rationale study; may either start testable hypothesis cases may start open-ended research question explore. fundamental distinction two drawn amount control researcher contexts conditions language behavior data collected produced. Observational approaches seek records language behavior produced language speakers communicative purposes natural(istic) contexts. may take place labs (language development, language disorders, etc.), often , language collected sources speakers performing language part daily lives –whether posting social media, speaking telephone, making political speeches, writing class essays, reporting latest news newspaper, crafting next novel destined New York Times best-seller. , data collected ‘wild’ varies structure relative data collected experimental approaches requires number steps prepare data synch data analysis tool belt.liken distinction experimental observational data collection difference farming foraging. Experimental approaches like farming; groundwork research plan designed, much field prepared seeding, researcher performs series tasks produce data, just farmer waters cares crops, results process bear fruit, data case, data harvested. Observational approaches like foraging; researcher scans available environmental landscape viable sources data naturally existing sources, sources assessed usefulness value address research question, viable selected, data collected.data acquired approaches trade-offs, just farming foraging. Experimental approaches directly elicit language behavior highly controlled conditions. directness level control benefit allowing researchers precisely track particular experimental conditions effect language behavior. conditions explicit part design therefore resulting language behavior can precisely attributed experimental manipulation. primary shortcoming experimental approaches level artificialness directness control. Whether language materials used task, task , fact procedure takes place supervision language behavior elicited can diverge quite significantly language behavior performed natural communicative settings. Observational approaches show complementary strengths shortcomings. Whereas experimental approaches may diverge natural language use, observational approaches strive identify collected language behavior data natural, uncontrolled, unmonitored contexts. way observational approaches question extent language behavior data performed natural communicative act. flipside, contexts natural language communication take place complex relative experimental contexts. Language collected natural contexts nested within complex workings complex world inevitably include host factors conditions can prove challenging disentangle language phenomenon interest must addressed order draw reliable associations conclusions.upshot, , twofold: (1) data collection methods matter research design interpretation (2) single best approach data collection, strengths shortcomings. ideal, robust science language include insight experimental observational approaches (Gilquin & Gries, 2009). evermore greater appreciation complementary nature experimental observational approaches growing body research highlights recognition. Given particular trade-offs observational data often used exploratory starting point help build insight form predictions can submitted experimental conditions. way studies based observational data serve exploratory tool gather better externally valid view language use can serve make prediction can explore precision experimental paradigm. However, always case. Observational data also often used hypothesis-testing contexts well. furthermore, language-related fields, hypothesis-testing ultimate goal deriving knowledge insight.","code":""},{"path":"data-language-and-text-analysis.html","id":"text-analysis","chapter":"1 Data, language, and text analysis","heading":"1.4 Text analysis","text":"Text analysis application data analysis procedures data science derive insight textual data collected observational methods. deliberately chosen term ‘text analysis’ avoid see pitfalls using common terms literature Corpus Linguistics, Computational Linguistics, Digital Humanities. plenty learning resources focus specifically one three fields discussing quantitative analysis text. perspective missing resource underscores fact text analysis research methods employed span across wide variety academic fields applications industry. coursebook aims introduce areas lens data analysis procedures particular field. approach, hope, provides wider view potential applications using text data inspires either employ quantitative text analysis research / raise awareness advantages text analysis making sense language-related linguistic-based phenomenon.applications text analysis? public facing applications stem Computational Linguistic research, often known Natural Language Processing practitioners, well-known applications text analysis. Whether using search engines, online translators, submitting paper plagiarism detection software, etc. text analysis methods cover play. uses text analysis production-level applications big money behind developing evermore robust text analysis methods.academia use quantitative text analysis even widespread, despite lack public fanfare. Let’s run select studies give idea areas employing text analysis, researchers text analysis, whet interest conducting text analysis project.sample studies include research areas translation, stylistics, language variation, dialectology, psychology, psycholinguistics, political science, sociolinguistics highlights diversity fields subareas employ quantitative text analysis. Text analysis center studies share set common goals:detect retrieve patterns text subtle numerous done handTo challenge assumptions /provide views textual sourcesTo explore new questions /provide novel insightLet’s now turn last section chapter provide overview rationale learning text analysis, structure content covered, justification approach take perform text analysis.","code":""},{"path":"data-language-and-text-analysis.html","id":"coursebook-overview","chapter":"1 Data, language, and text analysis","heading":"1.5 Coursebook overview","text":"section provide general overview rest coursebook motivating general structure sequencing well setting foundation programmatic approaches data analysis. Let highlight think valuable area study, hope gain coursebook, structure coursebook configured help scaffold conceptual practical knowledge text analysis.target learning outcomes coursebook following:Data LiteracyResearch SkillsProgramming SkillsData Literacy refers ability interpret, assess, contextualize findings based data. Throughout coursebook explore topics help understand data analysis methods derive insight data. process encouraged critically evaluate connections across linguistic language-related disciplines using data analysis knowledge skills. Data Literacy invaluable skillset academics professionals (cite) also indispensable aptitude 21st century citizens navigate actively participate ‘Information Age’ live (Carmi, Yates, Lockley, & Pawluczuk, 2020).Research skills covers ability conduct original research, communicate findings, make meaningful connections findings literature field. target area differ significantly, spirit, common learning outcomes research methods course: identify area investigation, develop viable research question hypothesis, collect relevant data, analyze data relevant statistical methods, interpret communicate findings. However, working text incur series key steps selection, collection, preparation data unique text analysis projects. addition, stress importance research documentation creating reproducible research integral part modern scientific inquiry (Buckheit & Donoho, 1995).Programming skills aims develop ability implement research skills programmatically produce research replicable collaborative. Modern data analysis, extension, text analysis conducted using programming. various key reasons : (1) programming affords researchers unlimited research freedom –can envision , can program . said --shelf software either proprietary unmaintained –. (2) programming underlies well-documented reproducible research –documenting button clicks menu option selections leads research readily reproduced, either researcher future self! (3) programming forces researchers engage intimately data methods analysis. familiar data methods likely produce higher quality work.Now let turn learning goals integrate shape structure sequencing following chapters.Part II “Orientation” build Data Literacy skills working data insight. progression visualized Figure 1.3.4\nFigure 1.3: Data Insight Hierarchy (DIKI)\nDIKI Hierarchy highlights stages intermediate steps required derive insight data. Chapter 2 “Understanding data” cover Data Information covering conceptual topics populations versus samples language data samples converted information forms can take. Chapter 3 “Approaching analysis” discuss distinction descriptive analytic statistics. brief important data analysis, descriptive statistics serve sanity check dataset submitting interrogation –goal analytic statistics. also cover main distinctions analytics approaches including inference-, exploration-, prediction-based methods. fundamental understanding data, information, knowledge move Chapter 4 “Framing research” discuss develop research plan, call ‘research blueprint.’ point directly address Research Skills elaborate research really comes together; bring speed literature topic, develop research goal hypothesis, select data viable address research goal hypothesis, determine necessary information appropriate measures prepare analysis, perform diagnostic statistics data make adjustments analysis, select perform relevant analytic statistics given research goals, report findings, finally, structure project well-documented reproducible.Part III “Preparation” Part IV “Modeling” serve practical detailed guides R programming strategies conduct text analysis research develop Programming Skills. Chapter 5 “Acquire data” discuss three main strategies accessing data: direct downloads, Automatic Programming Interfaces (APIs), web scraping. Chapter 6 “Curate data” outline process converting augmenting acquired data structured format, therefore creating information. include organizing linguistic non-linguistic metadata one dataset. Chapter 7 “Transform data” describe work curated dataset derive detailed information appropriate dataset structures appropriate upcoming analysis.Chapters 8 “Inference”, 9 “Prediction”, 10 “Exploration” focus different categories statistical analysis associated distinct research goals. Inference deals analysis methods associated standard hypothesis-testing. include common statistical models employed text analysis: chi-squared, logistic regression, linear regression. Prediction covers methods modeling associations data aim accurately predict outcomes new textual data. cover standard methods text classification including Näive Bayes, k-nearest neighbors (k-NN), decisions tree random forest models. Exploration covers variety analysis methods association measures, clustering, topic modeling, vector-space models. methods aligned research goals aim interpret patterns arise data .","code":""},{"path":"data-language-and-text-analysis.html","id":"summary","chapter":"1 Data, language, and text analysis","heading":"Summary","text":"chapter started general observations difficulty making sense complex world. standard approach overcoming inherent human limitations sense making science. 21st century toolbelt scientific research exploration grown terms amount data available, statistical methods analyzing data, computational power manage, store, share data, methods, results quantitative research. methods tools deriving insight data made significant inroads outside academia, increasingly figure quantitative investigation language. Text analysis particular branch enterprise based observational data real-world language used wide variety fields. coursebook aims develop knowledge skills three fundamental areas: Data Literacy, Research Skills, Programming Skills.end hope enjoy exploration text analysis. Although learning curve times may seem steep –experience gain improve data literacy, research skills, programmings skills also enhance appreciation richness human language important role everyday lives.","code":""},{"path":"orientation-overview.html","id":"orientation-overview","chapter":"Overview","heading":"Overview","text":"ORIENTATIONBefore begin working specifics data project, important establish fundamental understanding characteristics levels DIKI Hierarchy (Figure 1.3) roles levels deriving insight data. Chapter 2 explore Data Information levels drawing distinction two main types data (populations samples) cover data structured transformed generate information (datasets) fit statistical analysis. Chapter 3 outline importance distinct types statistical procedures (descriptive analytic) commonly used text analysis. Chapter 4 aims tie concepts together cover required steps preparing research blueprint conduct original text analysis project.","code":""},{"path":"understanding-data.html","id":"understanding-data","chapter":"2 Understanding data","heading":"2 Understanding data","text":"\nDRAFT\nplural anecdote data.— Marc Bekoff\nessential questions chapter :\n\ndistinct types data differ?\n\ninformation form take?\n\nimportance documentation quantitative research?\nchapter cover starting concepts journey understand derive insight data, illustrated DIKI Hierarchy (Figure 1.3), focusing specifically first two levels: Data Information. see commonly referred ‘data’ everyday uses broken three distinct categories, two referred data third known information. also cover importance documentation data datasets quantitative research.","code":""},{"path":"understanding-data.html","id":"data","chapter":"2 Understanding data","heading":"2.1 Data","text":"Data data, right? term ‘data’ common popular vernacular easy assume know mean say ‘data.’ things, common assumptions important details require careful consideration. Let’s turn first key distinction need make start break term ‘data’: difference populations samples.","code":""},{"path":"understanding-data.html","id":"populations","chapter":"2 Understanding data","heading":"2.1.1 Populations","text":"first thing comes many people’s mind term population used human populations.  Say example –’s population Milwuakee? speak population terms talking total sum people living within geographical boundaries Milwaukee. concrete terms, population objective make idealized set objects events reality. Key terms objective idealized. Although can look US Census report Milwaukee retrieve figure population, truly population. ? Well, whatever method used derive numerical figure surely incomplete. incomplete, time someone recorded figure number residents Milwaukee moved , moved , born, passed away –figure longer true population.Likewise talk populations terms language dealing objective idealized aspect reality. Let’s take words English language analog previous example population. case words people English bounding characteristic. Just people, words move , move , born, pass away. compendium words English moment almost instananeously incomplete. true populations, save bounding characteristics select narrow slice reality objectively measurable whose membership fixed (complete works Shakespeare, example).sum, () populations amorphous moving targets. objectively hold exist, practical terms often nail specifics populations. researchers go studying populations theoretically impossible access directly? strategy employed called sampling.","code":""},{"path":"understanding-data.html","id":"sampling","chapter":"2 Understanding data","heading":"2.1.2 Sampling","text":"sample product subjective process selecting finite set observations objective population goal capturing relevant characteristics target population. Although strategies minimize mismatch characteristics subjective sample objective population, important note almost certainly true given sample diverges population aims represent degree. aim, however, employ series sampling decisions, collectively known sampling frame, maximize chance representing population.common sampling strategies? First sample size. larger sample always representative smaller sample. Sample size, however, enough. hard imagine large sample chance captures subset features population. next step enhance sample representativeness apply random sampling. Together large random sample even better chance reflecting main characteristics population better large random sample. , random random , still run risk acquiring skewed sample (.e sample mirror target population).help mitigate issues, two strategies can applied improve sample representativeness. Note, however, size random samples can applied sample little information internal characteristics population, next two strategies require decisions depend presumed internal characteristics population. first informed sampling strategies called stratified sampling. Stratified samples make (educated) assumptions sub-components within population interest. sub-populations mind, large random samples acquired sub-population, strata. minimum, stratified samples can less representative random sampling alone, chances sample better increases. Can problems approach? Yes, two fronts. First knowledge internal components population often based limited incomplete knowledge population. words, strata selected subjectively researchers using various heuristics based sense ‘common knowledge.’ second front stratified sampling can err concerns relative sizes sub-components relative whole population. Even relevant sub-components identified, relative size adds another challenge researchers must face order maximize representativeness sample. attempt align, balance, relative sizes samples strata second population-informed sampling strategy.key feature sample purposely selected. Samples simply collection set data population. Samples rigorously selected explicit target population mind. text analysis purposely sampled collection texts, type defined , known corpus. reason set texts documents selected along purposely selected sampling frame corpus. sampling frame, therefore populations modeled, given corpus likely vary reason safe assumption given corpus equally applicable every research question. Corpus development (.e. sampling) purposeful, characteristics corpus development process made explicit documentation. Therefore vetting corpus sample applicability research goal key step research must take ensure integrity research findings.\nFigure 2.1: Brown Corpus Written American English\n","code":""},{"path":"understanding-data.html","id":"corpora","chapter":"2 Understanding data","heading":"2.1.3 Corpora","text":"","code":""},{"path":"understanding-data.html","id":"types","chapter":"2 Understanding data","heading":"2.1.3.1 Types","text":"notion sampling frames mind, corpora compiled aim general purpose (general reference corpora), much specialized sampling frames (specialized corpora). example, American National Corpus (ANC) British National Corpus (BNC) corpora aim model (represent/ reflect) general characteristics English language, former American English later British English. ambitious projects, require significant investments time corpus design implementation (continued development) usually undertaken research teams (Ädel, 2020).Specialized corpora aim represent specific populations. Santa Barbara Corpus Spoken American English (SBCSAE), can imagine name resource, aims model spoken American English. claim written English included. even specific types corpora attempt model types sub-populations scientific writing, computer-mediated communication (CMC), language use specific regions world, country, region, etc.Another set specialized corpora resources aim compile texts different languages different language varieties direct indirect comparison. Corpora directly comparable, include source translated texts, called parallel corpora. Parallel corpora include different languages language varieties indexed aligned linguistic level (.e. word, phrase, sentence, paragraph, document), see OPUS. Corpora compiled different languages language varieties directly aligned called comparable corpora. comparable language language varieties sampled similar sampling frame, example Brown LOB corpora.aim quantitative text researcher select corpus corpora (plural corpus) best aligns purpose research. Therefore general corpus ANC may better suited address question dealing way American English works, general resource may lack detail certain areas, medical language, may vital research project aimed understanding changes medical terminology.","code":""},{"path":"understanding-data.html","id":"sources","chapter":"2 Understanding data","heading":"2.1.3.2 Sources","text":"common source data used contemporary quantitative research internet. web investigator can access corpora published research purposes language used natural settings can coerced investigator corpus. Many organizations exist around globe provide access corpora browsable catalogs, repositories. repositories dedicated language research, general, Language Data Consortium specific language domains, language acquisition repository TalkBank. always advisable start looking available language data repository. advantage beginning data search repositories repository, especially geared towards linguistic community, make identifying language corpora faster general web search. Furthermore, repositories often require certain standards corpus format documentation publication. standardized resource many times easier interpret evaluate appropriateness particular research project.table ’ve compiled list corpus repositories help get started.Table 2.1: list corpus repositoriesRepositories means source corpora web. Researchers around world provide access corpora data sources sites data sharing platforms. Corpora various sizes scopes often accessible dedicated homepage appear homepage sponsoring institution. Finding resources matter web search word ‘corpus’ list desired attributes, including language, modality, register, etc. part general movement towards reproducibility corpora available web ever . Therefore data sharing platforms supporting reproducible research, GitHub, Zenodo, Re3data, OSF, etc., good place look well, searching repositories targeted web searches yield results.table find list corpus resources datasets.Table 2.2: Corpora language datasets.Language corpora prepared researchers research groups listed repositories hosted researchers often first place look data. web, however, contains wealth language language-related data can accessed researcher compile corpus. two primary ways attain language data web. first process web scraping. Web scraping process harvesting data web either manually (semi-)automatically actual public-facing web. second way acquire data web Application Programming Interface (API). APIs , title suggests, programming interfaces allow access, certain conditions, information website database accessible via web contains.table lists R packages serve interface language data directly R.Table 2.3: R Package interfaces language corpora datasets.Data language research limited (primary) text sources. sources may include processed data previous research; word lists, linguistic features, etc.. Alone combination text sources data can rich viable source data research project.’ve included processed language resources.Table 2.4: Language data previous research meta-studies.list data available language research constantly growing. ’ve document wide variety resources. ’ve included attempts others provide summary corpus data language resources available.Table 2.5: Lists corpus resources.","code":""},{"path":"understanding-data.html","id":"formats","chapter":"2 Understanding data","heading":"2.1.3.3 Formats","text":"corpus often include various types non-linguistic attributes, meta-data, well. Ideally include information regarding source(s) data, dates acquired published, author speaker information. may also include number attributes identified potentially important order appropriately document target population. , key match available meta-data goals research. cases corpus may ideal aspects contain key information address research question. may mean need compile corpus fundamental attributes missing. consider compiling corpus, however, worth investigating possibility augmenting available corpus bring inline particular goals. may include adding new language sources, harnessing software linguistic annotation (part--speech, syntactic structure, named entities, etc.), linking available corpus meta-data resources, linguistic non-linguistic.Corpora come various formats, main three : running text, structured documents, databases. format corpus often influenced characteristics data may also reflect author’s individual preferences well. typical corpora meta-data characteristics take form running text.Running text sample Europarle Parallel Corpus.corpora meta-data, header may appended top running text document meta-data may contained separate file appropriate coding coordinate meta-data attributes text corpus.Meta-data header sample Switchboard Dialog Act Corpus.meta-data / linguistic annotation increases complexity common structure corpus document explicitly markup language XML (Extensible Markup Language) organize relationships language meta-data attributes database.XML format meta-data (linguistic annotation) Brown Corpus.Although push towards standardization corpus formats, available resources display degree idiosyncrasy. able parse structure corpus skill develop time. experience working corpora become adept identifying data stored whether content format serve needs analysis.","code":"> Resumption of the session\n> I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n> Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n> You have requested a debate on this subject in the course of the next few days, during this part-session.\n> In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n> Please rise, then, for this minute' s silence.\n> (The House rose and observed a minute' s silence)\n> Madam President, on a point of order.\n> You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n> One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.> FILENAME: 4325_1632_1519\n> TOPIC#:       323\n> DATE:     920323\n> TRANSCRIBER:  glp\n> UTT_CODER:    tc\n> DIFFICULTY:   1\n> TOPICALITY:   3\n> NATURALNESS:  2\n> ECHO_FROM_B:  1\n> ECHO_FROM_A:  4\n> STATIC_ON_A:  1\n> STATIC_ON_B:  1\n> BACKGROUND_A: 1\n> BACKGROUND_B: 2\n> REMARKS:        None.\n> \n> =========================================================================\n> \n> \n> o          A.1 utt1: Okay.  /\n> qw          A.1 utt2: {D So, }\n> \n> qy^d          B.2 utt1: [ [ I guess, +\n> \n> +          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\n> \n> +          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\n> \n> qy          A.5 utt1: Does it say something? /\n> \n> sd          B.6 utt1: I think it usually does.  /\n> ad          B.6 utt2: You might try, {F uh, }  /\n> h          B.6 utt3: I don't know,  /\n> ad          B.6 utt4: hold it down a little longer,  /\n> ad          B.6 utt5: {C and } see if it, {F uh, } -/> <TEI xmlns=\"http://www.tei-c.org/ns/1.0\"><teiHeader><fileDesc><titleStmt><title>Sample A01 from  The Atlanta Constitution<\/title><title type=\"sub\"> November 4, 1961, p.1 \"Atlanta Primary ...\"\n>  \"Hartsfield Files\"\n>  August 17, 1961, \"Urged strongly ...\"\n>  \"Sam Caldwell Joins\"\n>  March 6,1961, p.1 \"Legislators Are Moving\" by Reg Murphy\n>  \"Legislator to fight\" by Richard Ashworth\n>  \"House Due Bid...\"\n>  p.18 \"Harry Miller Wins...\"\n> <\/title><\/titleStmt><editionStmt><edition>A part  of the XML version of the Brown Corpus<\/edition><\/editionStmt><extent>1,988 words 431 (21.7%) quotes 2 symbols<\/extent><publicationStmt><idno>A01<\/idno><availability><p>Used by permission of The Atlanta ConstitutionState News Service (H), and Reg Murphy (E).<\/p><\/availability><\/publicationStmt><sourceDesc><bibl> The Atlanta Constitution<\/bibl><\/sourceDesc><\/fileDesc><encodingDesc><p>Arbitrary Hyphen: multi-million [0520]<\/p><\/encodingDesc><revisionDesc><change when=\"2008-04-27\">Header auto-generated for TEI version<\/change><\/revisionDesc><\/teiHeader>\n> <text xml:id=\"A01\" decls=\"A\">\n> <body><p><s n=\"1\"><w type=\"AT\">The<\/w> <w type=\"NP\" subtype=\"TL\">Fulton<\/w> <w type=\"NN\" subtype=\"TL\">County<\/w> <w type=\"JJ\" subtype=\"TL\">Grand<\/w> <w type=\"NN\" subtype=\"TL\">Jury<\/w> <w type=\"VBD\">said<\/w> <w type=\"NR\">Friday<\/w> <w type=\"AT\">an<\/w> <w type=\"NN\">investigation<\/w> <w type=\"IN\">of<\/w> <w type=\"NPg\">Atlanta's<\/w> <w type=\"JJ\">recent<\/w> <w type=\"NN\">primary<\/w> <w type=\"NN\">election<\/w> <w type=\"VBD\">produced<\/w> <c type=\"pct\">``<\/c> <w type=\"AT\">no<\/w> <w type=\"NN\">evidence<\/w> <c type=\"pct\">''<\/c> <w type=\"CS\">that<\/w> <w type=\"DTI\">any<\/w> <w type=\"NNS\">irregularities<\/w> <w type=\"VBD\">took<\/w> <w type=\"NN\">place<\/w> <c type=\"pct\">.<\/c> <\/s>\n> <\/p>"},{"path":"understanding-data.html","id":"information","chapter":"2 Understanding data","heading":"2.2 Information","text":"Identifying adequate corpus resource target research question first step moving quantitative text research project forward. next step select components characteristics resource relevant research move organize attributes data useful informative format. process converting corpus dataset –tabular representation information leveraged analysis.","code":""},{"path":"understanding-data.html","id":"structure","chapter":"2 Understanding data","heading":"2.2.1 Structure","text":"Data alone informative. explicit organization data way makes relationships accessible data become information. particularly salient hurdle text analysis research. textual data unstructured –, relationships used analysis yet explicitly drawn organized text make relationships meaningful useful analysis.running text Europarle Corpus, know files source text (original) files correspond target text (translation). Table 2.6 see text organized columns corresponding type sentence additional sentence_id column keep index sentences aligned.\nconventional work column names datasets R using conventions used naming objects. matter taste convention used, adopted snake case personal preference. also alternatives. Regardless convention choose, good practice consistent.\n\nalso note column names balanced meaningfulness brevity. brevity practical concern can somewhat opaque. questions meaning column values consult resource’s documentation.\nTable 2.6: First 10 source target sentences Europarle Corpus.corpus resources semi-structured –, characteristics structured, .Switchboard Dialog Act Corpus example semi-structured resource. meta-data associated 1,155 conversations corpus. Table 2.7 language-relevant sub-set meta-data associated utterance.Table 2.7: First 5 utterances Switchboard Dialog Act Corpus.Relatively fewer resources structured. cases high amount meta-data / linguistic annotation included corpus. format convention, however, varies resource resource. formats programming general (.csv, .xml, .json, etc.) others resource specific (.cha, .utt, .prd, etc.). Table 2.8 XML version Brown Corpus represented tabular format. Note along meta-data variables, also contains variable linguistic annotation grammatical category (pos part--speech) word.Table 2.8: First 10 words Brown Corpus.coursebook, selection attributes corpus juxtaposition attributes relational format, dataset, converts data information referred data curation. process data curation minimally involves creating base dataset, derived dataset, establishes main informational associations according philosophical approach outlined Wickham (2014). work, ‘tidy’ dataset refers structural (physical) informational (semantic) organization dataset. Physically, tidy dataset tabular data structure row observation column variable contains measures feature attribute observation. cell given row-column intersect contains value particular attribute particular observation particular observation-feature pair also known data point.\nFigure 2.2: Visual summary tidy format.\nSemantic value tidy dataset derived association physical structure along two dimensions rectangular format. First, column variable reflects measures particular attribute. Europarle Corpus dataset, Table 2.6, example, type column measures type text, either Source Target. Columns can contain measures qualitative quantitative, character-based numeric. Second, row observation contains variables associated primary unit observation. primary unit observation variable essential focus informational structure. dataset first observation contains type, sentence_id, sentence. dataset currently structured primary unit investigation sentence variables measures characterize value sentence.decision primary unit observation fundamentally guided research question, therefore highly specific particular research project. Say instead wanted focus words instead sentences. dataset need transformed new variable (words) created contain word corpus.Table 2.9: Europarle Paralle Corpus words primary unit investigation.values variables type sentence_id maintain necessary description word ensure required semantic relationships identify particular attributes word observation. dataset may seem redundant values type sentence_id repeated numerous times ‘redundancy’ makes relationship variable associated primary unit investigation explicit. format makes tidy dataset versatile format researchers conduct analyses powerful flexible way, see throughout coursebook.important make clear data tabular format constitute dataset, tidy sense using. Data can organized many ways make relationships variables observations explicit.\ntabular data ‘tidy’ format described . Can think examples tabular information tidy format?\n","code":""},{"path":"understanding-data.html","id":"transformation","chapter":"2 Understanding data","heading":"2.2.2 Transformation","text":"point introduced first step data curation original data converted relational dataset (derived dataset) highlighted importance informational structure setting stage data analysis. However, primary derived dataset often final organizational step proceeding statistical analysis. Many times, always, derived dataset requires manipulation transformation prepare dataset specific analysis approach taken. another level human intervention informational organization, therefore another step forward journey data insight step DIKI hierarchy. Common types transformations include cleaning variables (normalization), separating eliminating variables (recoding), creating new variables (generation), incorporating others datasets integrate existing variables (merging). results transformations build manipulate derived dataset produce analysis dataset. Let’s now turn provide select set examples transformations using datasets introduced chapter.","code":""},{"path":"understanding-data.html","id":"normalization","chapter":"2 Understanding data","heading":"2.2.2.1 Normalization","text":"process normalization aims sanitize values within variable set variables. may include removing whitespace, punctuation, numerals, special characters substituting uppercase lowercase characters, numerals word versions, acronyms full forms, irregular incorrect spelling accepted forms, removing common words (stopwords), etc.inspecting Europarle dataset (Table 2.6) see sentence lines represent actual parliment speeches. Table 2.10 see lines.Table 2.10: Non-speech lines Europarle dataset.research project aiming analyze speech want normalize dataset removing lines, seen Table 2.11.Table 2.11: Europarle dataset non-speech lines removed.Another feature dataset may require attention fact English lines include whitespace possessive nouns.Table 2.12: Lines possessives extra whitespace Europarle dataset.may affect another transformation process subsequent analysis, may good idea normalize forms removing extra whitespace.Table 2.13: Europarle dataset whitespace possessives removed.final normalization case scenario involves changing converting text lowercase. goal research count words point fact word starts sentence convention first letter capitalized result distinct counts words essence (.e. “” vs. “”).Table 2.14: Europarle dataset lowercasing applied.Note lowercasing text, normalization steps general, can come cost. example, lowercasing Europarle dataset sentences means lose potentially valuable information; namely ability identify proper names (.e. “Mr Kumar Ponnambalam”) titles (.e. “European Parliament”) directly orthographic forms. , however, transformation steps can applied aim recover ‘lost’ information situations others.","code":""},{"path":"understanding-data.html","id":"recoding","chapter":"2 Understanding data","heading":"2.2.2.2 Recoding","text":"process recoding aims recast values variable set variables new variable set variables enable direct access. may include extracting values variable, stemming lemmatization words, tokenization linguistic forms (words, ngrams, sentences, etc.), calculating lengths linguistic units, removing variables used analysis, etc.Words intuitively associate ‘base’ word can take many forms language use. example word forms ‘investigation,’ ‘investigation,’ ‘investigate,’ ‘investigated,’ etc. intuitively linked. two common methods can applied create new variable facilitate identification associations. first stemming. Stemming rule-based heuristic reduce word forms stem root form.Table 2.15: Results stemming first words Brown Corpus.things note . First number stemming algorithms individual languages distinct languages.5 Second words can stemmed alternate morphological forms (.e. “,” “,” etc.). generally related distinction closed-class (articles, prepositions, conjunctions, etc.) open-class (nouns, verbs, adjectives, etc.) grammatical categories. Third stem generated words can stemmed result forms words . Nonetheless, stems can useful easily extracting set related word forms.example, let’s identify word forms stem ‘investig.’Table 2.16: Results word_stems filter “investig” Brown Corpus.can see results Table 2.16 searching word_stems match ‘investig’ returns set stem-related forms. worth noting forms cut across number grammatical categories. instead want draw distinction grammatical categories, can apply lemmatization. process distinct stemming two important ways: (1) inflectional forms grouped grammatical category (2) resulting forms lemmas ‘base’ forms words.Table 2.17: Results lemmatization first words Brown Corpus.appreciate difference stemming lemmatization, let’s compare filter word_lemmas match ‘investigation.’Table 2.18: Results word_lemmas filter “investigation” Brown Corpus.lemma forms ‘investigate’ nouns appear. Let’s run similar search lemma ‘.’Table 2.19: Results word_lemmas filter “” Brown Corpus.words grammatical category returned. case verb ‘’ many inflectional forms ‘investigate.’Another form recoding detect pattern values existing variable create new variable whose values extracted pattern register pattern occurs / many times occurs. example, let’s count number disfluencies (‘uh’ ‘um’) occur utterance utterance_text Switchboard Dialog Act Corpus. Note ’ve simplified dataset dropping non-relevant variables example.Table 2.20: Disfluency counts first 10 utterance_text values Switchboard Corpus.One common forms recoding text analysis tokenization. Tokenization process recasting text smaller linguistic units. working text linguistically annotated, feasible linguistic tokens words, ngrams, sentences. word sentence tokens easily understandable, ngram tokens need explanation. ngram sequence either characters words n length sequence. ngram sequences drawn incrementally, bigrams (two-word sequences) sentence “input sentence.” :, , input, input sentenceWe’ve already seen word tokenization exemplified Europarle Corpus subsection Structure Table 2.9, let’s create (word) bigram tokens corpus.Table 2.21: first 10 word bigrams Europarle Corpus.just mentioned, ngrams sequences can formed characters well. character trigram (three-character) sequences.Table 2.22: first 10 character trigrams Europarle Corpus.","code":""},{"path":"understanding-data.html","id":"generation","chapter":"2 Understanding data","heading":"2.2.2.3 Generation","text":"process generation aims augment variable set variables. essence aims make implicit attributes explicit directly accessible. often targeted automatic generation linguistic annotations grammatical category (part--speech) syntactic structure.examples ’ve added linguistic annotation target (English) source (Spanish) example sentence Europarle Parallel Corpus. First, note variables added dataset correspond grammatical category. addition type sentence_id assortment variables replace sentence variable. part process annotation input text annotated sentence tokenized token indexed token_id. upos contains Universal Part Speech tags6, detailed list features included feats. syntactic annotation reflected token_id_source syntactic_relation variables. variables correspond type syntactic parsing done, case Dependency Parsing (using Universal Dependencies framework). Another common syntactic parsing framework phrase constituency parsing (Jurafsky & Martin, 2020).Table 2.23: Automatic linguistic annotation grammatical category syntactic structure example English sentence Europarle CorpusNow compare English example sentence dataset Table 2.23 parallel sentence Spanish. Note grammatical features language specific. example, Spanish gender apparent scanning feats variable.Table 2.24: Automatic linguistic annotation grammatical category syntactic structure example Spanish sentence Europarle CorpusThere much explore linguistic annotation, syntactic parsing particular, point suffice note possible augment dataset grammatical information automatically.strengths shortcomings automatic linguistic annotation research aware . First, automatic linguistic annotation provides quick access rich highly reliable linguistic information large number languages. However, part speech taggers syntactic parsers magic. resources built training computational algorithm recognize patterns manually annotated datasets producing language model. model used predict linguistic annotations new language (just previous examples). shortcomings automatic linguistic annotation first, languages trained language models second, data used train model inevitably reflect particular variety, register, modality, etc. accuracy linguistic annotation highly dependent alignment language sampling frame trained data language data automatically annotated. Many () language models available automatic linguistic annotation based language readily available languages traditionally newswire text. important aware characteristics using linguistic annotation tools.","code":""},{"path":"understanding-data.html","id":"merging","chapter":"2 Understanding data","heading":"2.2.2.4 Merging","text":"process merging aims join variable set variables another variable set variables another dataset. option merge two () datasets requires shared variable indexes aligns datasets.provide example let’s look Switchboard Diaglog Act Corpus. existing, disfluency recoded, version includes following variables.turns corpus website number meta-data files available, including files pertaining speakers topics conversations.speaker meta-data corpus caller_tab.csv file contains speaker_id variable corresponds speaker corpus potentially relevant variables language research project including sex, birth_year, dialect_area, education.Table 2.25: Speaker meta-data Switchboard Dialog Act Corpus.Since datasets contain shared index, speaker_id can merge two datasets. result found Table 2.26.Table 2.26: Merged conversations speaker meta-data Switchboard Dialog Act Corpus.example case dataset merged already structured format (.csv). Many corpus resources contain meta-data stand-files structured.cases researcher like merge information already accompany corpus resource. possible long dataset can created contains variable shared. Without shared variable index datasets merge take place.sum, transformation steps described collectively aim produce higher quality datasets relevant content structure submit analysis. process may include one previous transformations rarely linear often iterative. typical normalization generation, recoding, return normalizing, forth. process highly idiosyncratic given characteristics derived dataset ultimate goals analysis dataset.\nNote cases may convert tidy tabular dataset data formats may required particular statistic approaches times relationship variables maintained line research purpose. touch examples types data formats (e.g. Corpus Document-Term Matrix (DTM) objects R) dive particular statistical approaches require later coursebook.\n","code":"#> Rows: 5\n#> Columns: 11\n#> $ doc_id           <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\"\n#> $ speaker_id       <dbl> 1632, 1632, 1519, 1632, 1519\n#> $ topic_num        <dbl> 323, 323, 323, 323, 323\n#> $ topicality       <chr> \"3\", \"3\", \"3\", \"3\", \"3\"\n#> $ naturalness      <chr> \"2\", \"2\", \"2\", \"2\", \"2\"\n#> $ damsl_tag        <chr> \"o\", \"qw\", \"qy^d\", \"+\", \"+\"\n#> $ speaker          <chr> \"A\", \"A\", \"B\", \"A\", \"B\"\n#> $ turn_num         <chr> \"1\", \"1\", \"2\", \"3\", \"4\"\n#> $ utterance_num    <chr> \"1\", \"2\", \"1\", \"1\", \"1\"\n#> $ utterance_text   <chr> \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind …\n#> $ disfluency_count <int> 0, 0, 0, 0, 1"},{"path":"understanding-data.html","id":"documentation","chapter":"2 Understanding data","heading":"2.3 Documentation","text":"seen chapter acquiring data converting data information involves number conscious decisions implementation steps. favor researchers research community, crucial document decisions steps. makes possible retrace steps also provides guide future researchers want reproduce / build research. programmatic approach quantitative research helps ensure implementation steps documented reproducible also vital decisions made documented well. includes creation/ selection corpus data, description variables chosen corpus derived dataset, description variables created derived dataset analysis dataset.existing corpus sample acquired repository (e.g. Switchboard Dialog Act Corpus, Language Data Consortium), research group (e.g. CEDEL2), individual researcher (e.g. SMS Spam Collection), often documentation provided describing key attributes resource. documentation included acquisition corpus added research project. corpus researcher compiles , need generate documentation.curation transformation steps conducted original corpus data produce datasets also documented. steps can included programming scripts code comments (prose using literate programming strategy (e.g. RMarkdown)). structure resulting dataset include called data dictionary. table includes variable names, values contain, short prose description variable (e.g. ACTIV-ES Corpus).","code":""},{"path":"understanding-data.html","id":"summary-1","chapter":"2 Understanding data","heading":"Summary","text":"chapter focused data information –first two components DIKI Hierarchy. process visualized Figure 2.3.\nFigure 2.3: Understanding data: visual summary\nFirst distinction made populations samples, latter intentional subjective selection observations world attempt represent population interest. result process known corpus. Whether developing corpus selecting existing corpus important vet sampling frame applicability viability resource given research project.viable corpus identified, corpus converted derived dataset adopts ‘tidy’ dataset format column variable, row observation, intersection columns rows contain values. derived dataset serves establish base informational relationships research stem.derived dataset likely require transformations including normalization, recoding, generation, / merging enhance usefulness information analysis. analysis dataset result process.Although covered end chapter, documentation implemented stage process. Employing programmatic approach establishes documentation implementation steps motivation behind decisions taken content corpus data datasets generated also need documentation ensure transparent reproducible research.","code":""},{"path":"approaching-analysis.html","id":"approaching-analysis","chapter":"3 Approaching analysis","heading":"3 Approaching analysis","text":"\nDRAFT\nStatistical thinking one day necessary efficient citizenship ability read write.— H.G. Wells\nessential questions chapter :\n\nrole statistics data analysis?\n\nimportance descriptive assessment data analysis?\n\nways main approaches data analysis similar different?\nchapter build notions data information previous chapter. aim statistics quantitative analysis uncover patterns datasets. Thus statistics aimed deriving knowledge information, next step DIKI Hierarchy (Figure 2.3). creation information data involves human intervention conscious decisions, seen, deriving knowledge information involves even conscious subjective decisions information assess, method select interrogate information, ultimately interpret findings. first step conduct descriptive assessment information, individual variable level also variables, second interrogate dataset either inferential, predictive, exploratory analysis methods, third interpret report findings.","code":""},{"path":"approaching-analysis.html","id":"description","chapter":"3 Approaching analysis","heading":"3.1 Description","text":"descriptive assessment dataset includes set diagnostic measures tabular visual summaries provide researchers better understanding structure dataset, prepare researcher make decisions statistical methods / tests appropriate, safeguard false assumptions (missing data, data distributions, etc.). section first cover importance understanding informational value variables can represent move use understanding approach summarizing individual variables relationships variables.ground discussion introduce new dataset. dataset drawn Barcelona English Language Corpus (BELC), found TalkBank repository. ’ve selected “Written composition” task corpus contains writing samples second language learners English different ages. Participants given task writing 15 minutes topic “: past, present future.” Data collected many () participants four times course seven years. Table 3.1 ’ve included first 10 observations dataset reflects structural transformational steps ’ve done start tidy dataset.\nTable 3.1: First 10 observations BELC dataset demonstration.\nentire dataset includes 79 observations 36 participants. observation BELC dataset corresponds individual learner’s composition. includes participant wrote composition (participant_id), age group part time (age_group), sex (sex), number English words produced (num_tokens), number unique English words produced (num_types). final variable (ttr) calculated ratio number unique words (num_types) total words (num_tokens) composition. known Type-Token Ratio standard metric measuring lexical diversity.","code":""},{"path":"approaching-analysis.html","id":"information-values","chapter":"3 Approaching analysis","heading":"3.1.1 Information values","text":"Understanding informational value, level measurement, variable set variables key preparing analysis implications visualization techniques statistical measures can use interrogate dataset. two main levels measurement variable can take: categorical continuous. Categorical variables reflect class group values. Continuous variables reflect values measured along continuum.BELC dataset contains three categorical variables (participant_id, age_group, sex) three continuous variables (num_tokens, num_types, ttr). categorical variables identify class group membership; participant wrote composition, age group , biological sex. continuous variables measure attributes can take range values without fixed limit differences value regular. number words number unique words composition can range 1 \\(n\\) Type-Token Ratio derived two variables also continuous reason. Furthermore, differences values measures defined interval, example composition word count (num_tokens) 40 exactly two times large composition word count 20.distinction categorical continuous levels measurement, mentioned , main two statistical approaches distinction needs made conduct analysis. However, categorical continuous can broken subcategories descriptive analytic purposes distinctions important. categorical variables distinction can made variables structured relationship values . Nominal variables contain values labels denoting membership class relationship labels. Ordinal variables also contain labels classes, contrast nominal variables, relationship classes, namely one precedence relationship order. mind, categorical variables sub-classified. order values participant_id sex therefore nominal whereas values age_group ordered, value refers sequential age group, therefore ordinal.Turning continuous variables, another subdivision can made hinges existence non-arbitrary zero . Interval variables contain values difference values regular defined, measure arbitrary zero value. typically cited example interval variable temperature measurements Fahrenheit scale. value 0 scale mean 0 temperature. Ratio variables properties interval variables also include non-arbitrary definition zero. continuous variables BELC dataset (num_tokens, num_types, ttr) ratio variables value 0 indicate lack attribute.hierarchical overview relationship two main four sub-types levels measurement appear Figure 3.1.\nFigure 3.1: Levels measurement graphic representation.\nnotes practical importance; First, distinction interval ratio variables often applicable text analysis therefore often treated together continuous variables. Second, distinction ordinal interval/continuous variables clear cut may seem. variables contain values ordered relationship. definition values ordinal variable reflect regular intervals units measurement. practice interval/continuous variables defined number values (say Likert scale used survey) may treated ordinal variable may better understood reflecting class membership. Third, continuous variables can converted categorical variables, reverse true. , example, define criterion binning word counts num_tokens composition ordered classes “low,” “mid,” “high.” hand, sex (measured ) take intermediate values unfixed range. upshot variables can -typed -typed. cases preferred treat continuous variables , nature variable permits , -typing continuous data categorical data results loss information –result loss information hence statistical power may lead results obscure meaningful patterns data (Baayen, 2004).","code":""},{"path":"approaching-analysis.html","id":"summaries","chapter":"3 Approaching analysis","heading":"3.1.2 Summaries","text":"always key gain insight shape information numeric, tabular / visual summaries jumping analytic statistical approaches. appropriate form summarizing information depend number informational value(s) target variables. get sense looks, let’s continue work BELC dataset pose different questions data eye towards seeing various combinations variables descriptively explored.","code":""},{"path":"approaching-analysis.html","id":"single-variables","chapter":"3 Approaching analysis","heading":"3.1.2.1 Single variables","text":"way statistically summarize variable single measure derive measure central tendency. continuous variable common measure (arithmetic) mean, average, simply sum values divided number values. measure central tendency, however, mean can less--reliable sensitive outliers say data points variable extreme relative overall distribution values variable affect value mean depending extreme deviate. One way assess effects outliers calculate measure dispersion. common standard deviation estimates average amount variability values continuous variable. Another way assess, rather side-step, outliers calculate another measure central tendency, median. median calculated sorting values variable selecting value falls middle values. median less sensitive outliers extreme values () indirectly affect selection middle value. Another measure dispersion calculate quantiles. quantile slices data four percentile ranges providing five value numeric summary spread values continuous variable. spread first third quantile known Interquartile Range (IQR) also used single statistic summarize variability values continuous variable.list central tendency dispersion scores continuous variables BELC dataset.Variable type: numeric\ndescriptive statistics returned generated skimr package.\nsummary, see mean, standard deviation (sd), quantiles (five-number summary, p0, p25, p50, p75, p100). middle quantile (p50) median IQR listed last.important measures assessing central tendency dispersion useful reporting purposes, get better feel variable distributed, nothing beats visual summary. boxplot graphically summarizes many metrics. Figure 3.2 see three continuous variables, now graphical form.\nFigure 3.2: Boxplots continuous variables BELC dataset.\nboxplot, bold line median. surrounding box around median interquantile range. extending lines IQR mark largest lowest value within 1.5 times either 3rd (top box) 1st (bottom box). values fall outside, , extending lines considered statistical outliers marked dots (case red dots).7Boxplots provide robust visually intuitive way assessing central tendency variability continuous variable type plot can complemented looking overall distribution values terms frequencies. histogram provides visualization frequency (density case blue overlay) values across continuous variable binned regular intervals.Figure 3.3 ’ve plotted histograms top row density plots bottom row three continuous variables BELC dataset.\nFigure 3.3: Histograms density plots continuous variables BELC dataset.\nHistograms provide insight distribution data. three continuous variables, distributions happen strikingly distinct. , however, either. explore continuous variables histograms often trying assess whether skew . three general types skew, visualized Figure 3.4.\nFigure 3.4: Examples skew types density plots.\nhistograms/ density plots distribution either left right, median mean aligned. mode, indicates frequent value variable also aligned two measures. left-skewed distribution mean left median left mode whereas right-skewed distribution opposite occurs. distribution absolutely skew three measures . practice measures rarely align perfectly typical three measures approximate alignment. common enough distribution called Normal Distribution8 common real-world data.Another potentially informative way inspect normality distribution create Quantile-Quantile plots (QQ Plot). Figure 3.5 ’ve created QQ plots three continuous variables. line plot normal distribution points fall line, less likely distribution normal.\nFigure 3.5: QQ Plots continuous variables BELC dataset.\nvisual inspection can often enough detect non-normality, cases visually approximate normal distribution () can perform Shapiro-Wilk test normality. inferential test compares variable’s distribution normal distribution. likelihood distribution differs normal distribution reflected \\(p\\)-value. \\(p\\)-value .05 threshold suggests distribution non-normal. Table 3.2 see given criterion distribution num_types normally distributed.\nTable 3.2: Results Shapiro-Wilk test normality continuous variables BELC dataset.\nDownstream analytic analysis, distribution continuous variables need taken account certain statistical tests. Tests assume ‘normality’ parametric tests, non-parametric. Distributions approximate normal distribution can sometimes transformed conform normal distribution either outlier trimming statistical procedures (e.g. square root, log, inverse transformation), necessary. stage, however, important thing recognize whether distributions approximate wildly diverge normal distribution.leave continuous variables, let’s consider another approach visually summarizing single continuous variable. Empirical Cumulative Distribution Frequency, ECDF, summary cumulative proportion values continuous variable. ECDF plot can useful determining proportion values fall certain percentage data.Figure 3.6 see ECDF plots three continuous variables.\nFigure 3.6: ECDF plots continuous variables BELC dataset.\nTake, example, number tokens (num_tokens) per composition. ECDF plot tells us 50% values variable 56 words less. three variables plotted, cumulative growth quite steady. cases . , ECDF goes long way provide us glimpse key bends proportions values variable.Now let’s turn descriptive assessment categorical variables. categorical variables, central tendency can calculated well subset measures given reduced informational value categorical variables. nominal variables relationship levels central tendency simply mode. levels ordinal variables, however, relational therefore median, addition mode, can also used measure central tendency. Note variable one mode unimodal, two modes, bimmodal, variables two modes multimodal.\nget numeric value median ordinal variable levels variable need numeric well. Non-numeric levels can recoded numeric purpose necessary.\nlist central tendency metrics categorical variables BELC dataset.Variable type: factorIn practice categorical variable levels common simply summarize counts level table get overview variable. ordinal variables numerous levels, five-score summary (quantiles) can useful summarize distribution. contrast continuous variables graphical representation helpful get perspective shape distribution values, exploration single categorical variables rarely enhanced plots.","code":""},{"path":"approaching-analysis.html","id":"multiple-variables","chapter":"3 Approaching analysis","heading":"3.1.2.2 Multiple variables","text":"addition single variable summaries (univariate), useful understand two (bivariate) variables (multivariate) related add understanding shape relationships dataset. Just univariate summaries, informational values variables frame approach.explore relationship two continuous variables can statistically summarize relationship coefficient correlation measure effect size continuous variables. continuous variables approximate normal distribution Pearson’s r used, Kendall’s tau appropriate measure. correlation coefficient ranges -1 1 0 correlation -1 1 perfect correlation (either negative positive). Let’s assess correlation coefficient variables num_tokens ttr. Since variables normally distributed, use Kendall’s tau. Using measure correlation coefficient \\(-0.563\\) suggesting correlation, particularly strong one.Correlation measures important reporting really appreciate relationship best graphically represent variables scatterplot. Figure 3.7 see relationship num_tokens ttr.\nFigure 3.7: Scatterplot…\nplots ttr y-axis num_tokens x-axis. points correspond intersection variables single observation. left pane points represented. Visually (given correlation coefficient) can see negative relationship number tokens Type-Token ratio: words, tokens composition lower Type-Token Ratio. case trend quite apparent, cases may . provide additional visual cue trend line often added scatterplot. right pane ’ve added linear trend line. line demarcates optimal central tendency across relationship, assuming linear relationship. steeper line, slope, likely correlation strong. band, ribbon, around trend line indicates confidence interval means real central tendency fall anywhere within space. wider ribbon, larger variation observations. case see ribbon widens number tokens either low high. means trend line potentially drawn either steeper (strongly correlated) flatter (less strongly correlated).\nplots comparing two variables, choice variable plot x- y-axis contingent research question / statistical approach. language varies statistical approaches: inferential methods x-axis used plot known dependent variable y-axis independent variable. predictive methods dependent variable known outcome independent variable predictor. Exploratory methods draw distinctions variables along lines choice variable plot along x- y-axis often arbitrary.\nLet’s add another variable mix, case categorical variable sex, taking bivariate exploration multivariate exploration. point corresponds observation values num_tokens ttr intersect. now points given color reflects level sex associated .\nFigure 3.8: Scatterplot visualizing relationship num_tokens ttr.\nmultivariate case, scatterplot without trend line difficult interpret. trend lines levels sex help visually understand variation relationship num_tokensand ttr much better. important note multiple trend lines one slope evaluate. correlation coefficient can calculated level sex (.e. ‘male’ ‘female’) independently relationship slope can visually inspected provide important information regarding level’s relative distribution. trend lines parallel (ignoring ribbons moment), appears case, suggests relationship continuous variables stable across levels categorical variable, males showing lexical diversity females declining similar rate. lines cross, suggest cross point, potentially important difference levels categorical variable (known interaction). Now let’s consider meaning ribbons. Since ribbons reflect range real trend line fall, ribbons overlap, differences levels categorical variable likely distinct. descriptive level, visual summary suggest differences relationship num_tokens ttr distinct levels sex.Characterizing relationship two continuous variables, seen either performed correlation coefficient metric visually. approach summarizing bivariate relationship combines continuous categorical variable distinct. Since categorical variable definition class-oriented variable, descriptive evaluation can include tabular representation, type summary statistic. example, consider relationship num_tokens age_group can calculate mean num_tokens level age_group. provide metric dispersion can include either standard error mean (SEM) / confidence interval (CI).Table 3.3 see summary statistics.\nTable 3.3: Summary table tokens age_group.\nSEM metric summarizes variation based number values CI, seen, summarizes potential range mean may fall given likelihood criterion (usually \\(p\\)-value, .05).assessing categorical variable combination continuous variable table available visual summary. said , graphic summary hard beat. following figure (3.9) barplot provided includes means num_tokens level age_group. overlaid bars represent confidence interval mean score.\nFigure 3.9: Barplot comparing mean num_tokens age_group BELC dataset.\nCI ranges overlap, just ribbons scatterplots, likelihood differences levels ‘real’ diminished.gauge effect size relationship can use Spearman’s rho rank-based coefficients. score 0.708 indicating relationship age_group num_tokens quite strong.9Now, want explore multivariate relationship add sex current descriptive summary, can create summary table, let’s jump straight barplot.\nFigure 3.10: Barplot comparing mean num_tokens age_group sex BELC dataset.\nsee Figure 3.10 whole, appears general trend towards tokens composition advanced learner levels. However, non-overlap CI bars ‘12-year-olds’ levels sex (‘male’ ‘female’) suggest 12-year-old females may produce tokens per composition males –potential divergence overall trend.Barplots familiar common visualization summaries continuous variables across levels categorical variables, boxplot another useful visualization type relationship.\nFigure 3.11: Boxplot relationship age_group num_tokens BELC dataset.\nseen summarizing single continuous variables, boxplots provide rich set information concerning distribution continuous variable. case can visually compare continuous variable num_tokens categorical variable age_group. plot right pane includes ‘notches.’ Notches represent confidence interval, boxplots interval surrounds median. compared horizontally across levels categorical variable overlap notched spaces suggest true median may within range.\nAdditionally, confidence interval goes outside interquantile range (box) notches hinge back either 1st (lower) 3rd (higher) IQR range suggests variability high.can also add third variable exploration. barplot Figure 3.10, boxplot Figure 3.12 suggests overall trend towards tokens per composition learner advances experience, except ‘12-year-old’ level appears difference ‘males’ ‘females.’\nFigure 3.12: Boxplot relationship age_group, num_tokens sex BELC dataset.\npoint exploration multiple variables always included least one continuous variable. central tendency continuous variables can summarized multiple ways (mean, median, mode) calculating means medians, measures dispersion also provide helpful information summarize variability. working categorical variables, however, measures central tendency dispersion limited. ordinal variables central tendency can summarized median mode dispersion can assessed interquantile range. nominal variables mode measure central tendency dispersion applicable. reason relationships categorical variables typically summarized using contingency tables provide cross-variable counts level target categorical variables.Let’s explore relationship categorical variables sex age_group. Table 3.4 see contingency table summary counts percentages.\nTable 3.4: Contingency table age_group sex.\nsize contingency table increases, visual inspection becomes difficult. seen, graphical summary often proves helpful detect patterns.\nFigure 3.13: Barplot…\nFigure 3.13 left pane shows counts. Counts alone can tricky evaluate adjusting barplot account proportions males females group, shown right pane, provides clearer picture relationship. barplots can see females study overall particularly 12-year-olds 17-year-olds groups. gauge association strength sex age_group can calculate Cramer’s V , spirit, like correlation coefficients relationship continuous variables. Cramer’s V score relationship 0.12 low, suggesting strong association sex age_group –words, relationship stable.Let’s look complex case three categorical variables. Now dataset, , third categorical variable us explore can recast continuous num_tokens variable categorical variable bin scores groups. ’ve binned tokens three score groups equal ranges new variable called rank_tokens.Adding second categorical independent variable ups complexity analysis result visualization strategy change. numerical summary include individual two-way cross-tabulations levels third variable. case often best use variable fewest levels third variable, case sex.\nTable 3.5: Contingency table age_group, rank_tokens, sex (female).\n\nTable 3.6: Contingency table age_group, rank_tokens, sex (male).\nContingency tables many levels notoriously difficult interpret. plot often used three-way contingency table summaries mosaic plot. Figure 3.14 created mosaic plot three categorical variables previous contingency tables.\nFigure 3.14: Mosaic plot three categorical variables age_group, rank_tokens, sex BELC dataset.\nmosaic plot suggests number tokens per composition increase learner age group increases females show tokens earlier.sum, dataset information observations become numerous complex visually difficult inspect understand pattern level. descriptive methods described section indispensable providing researcher overview nature variable (potential) relationships variables dataset. Importantly, understanding derived exploration underlies subsequent investigation counted frame approach analysis regardless research goals methods employed derive substantial knowledge.","code":""},{"path":"approaching-analysis.html","id":"analysis","chapter":"3 Approaching analysis","heading":"3.2 Analysis","text":"identifying target population, selecting data sample represents population, structuring sample dataset, goals research project inform frame process. unsurprising know process selecting approach analysis also intimately linked researcher’s objectives. goal analysis, generally, generate knowledge information. type knowledge generated process generated, however, differ can broadly grouped three analysis types: inferential, predictive, exploratory. section provide overview analysis types tied research goals general goals teach type affect: (1) identify variables interest, (2) interrogate variables, (3) interpret results. structure discussion analysis types moving structured (deductive) least structured (inductive) approach deriving knowledge information aim provide enough information --researcher identify research approaches literature make appropriate decisions approach research adopt.","code":""},{"path":"approaching-analysis.html","id":"inferential-data-analysis","chapter":"3 Approaching analysis","heading":"3.2.1 Inferential data analysis","text":"commonly recognized three data analysis approaches, inferential data analysis (IDA) bread--butter science. IDA deductive, top-, approach investigation every step research stems premise, hypothesis, nature relationship world aims test whether relationship statistically supported given evidence. aim infer conclusions certain relationship population based statistical evaluation (corpus) sample. , researcher’s aim draw conclusions generalize, , analysis approach researcher take.Given fact approach aims making claims can generalized larger population, IDA approach rigorous set methodological restrictions. First foremost fact testable hypothesis must formulated research begins. hypothesis guides collection data, organization data dataset transformation, selection variables used address hypothesis, interpretation results. conduct analysis draw hypothesis conforms results known “Hypothesis Result Known” (HARKing) (Kerr, 1998) practice violates principles significance testing. second key stipulation reliability sample data, corpus text analysis, provide evidence test hypothesis must representative population. corpus used study misaligned hypothesis undermines ability researcher make valid claims population. essence, IDA good primary data based .point, let elaborate potentially counterintuitive nature hypothesis formulation testing. IDA, Null-Hypothesis Significance Testing (NHST), paradigm fact approached proposing two mutually exclusive hypotheses. first Alternative Hypothesis (\\(H_1\\)). \\(H_1\\) precise statement grounded previous literature outlining predicted relationship (cases directionality relationship). effect research aims investigate. second hypothesis Null Hypothesis (\\(H_0\\)). \\(H_0\\) flip-side hypothesis testing coin states difference relationship. Together \\(H_1\\) \\(H_0\\) cover logical outcomes.provide example consider hypothetical study aimed investigating claim men women differ terms number questions use spontaneous conversations. Alternative Hypothesis formulated way:\\(H_1\\): Men women differ frequency use questions spontaneous conversations.Null Hypothesis, , statement describing remaining logical outcomes. Formally:\\(H_0\\): difference men women use questions spontaneous conversations.Note stated way hypothesis makes prediction directionality difference men women, difference. likely scenario hypothesis stake claim direction difference. directional hypothesis look like :\\(H_1\\): Women use questions men spontaneous conversations.\\(H_0\\): difference men women use questions spontaneous conversations men use questions women.aspect may run counter expectations aim hypothesis testing find evidence support \\(H_1\\), rather aim assess likelihood can reliably reject \\(H_0\\). default assumption \\(H_0\\) true sufficient evidence reject accept \\(H_1\\), alternative. metric used determine sufficient evidence based probability given nature relationship characteristics data, likelihood difference relationship low. threshold likelihood traditionally summarized p-value statistic. Social Sciences, p-value lower .05 considered statistically significant interpreted correctly means 95% chance observed relationship predicted \\(H_0\\). Note working realm probability, absolutes, therefore analysis produces significant result prove \\(H_1\\) correct \\(H_0\\) incorrect, matter. margin error always present.Let’s now turn identification variables, statistical interrogation variables, interpretation statistical results. First, since clearly defined testable hypothesis center IDA approach, variables sense pre-defined. goal researcher select data curate data produce variables operationalized (practically measured) test hypothesis. second consideration roles variables play analysis. standard IDA one variable dependent variable one variables independent variables. dependent variable, sometimes referred outcome response variable, variable contains information predicted depend information independent variable(s). variable whose variation research study seeks explain. independent variable, sometimes referred predictor explanatory variable, variable whose variation predicted explain variation dependent variable.Returning hypothetical study use questions men women spontaneous conversation, frequency questions used speaker dependent variable biological sex speakers independent variable. hypothesis (\\(H_1\\)) states proposition speaker’s sex predict frequency questions used.hypothetical study ’ve identified two variables, one dependent one independent. important keep mind can multiple independent variables cases dependent variable’s variation predicted related multiple variables. relationship need explicitly part original hypothesis, however.Say formulate complex relationship educational level speakers also related number questions. can update hypothesis reflect scenario.\\(H_1\\): Less educated women use questions men spontaneous conversations.\\(H_0\\): difference men women use questions spontaneous conversations regardless educational level, educated women use questions less educated women, men use questions women.hypothesis described predicts known interaction; relationship independent variables predict different variational patterns dependent variable. likely can appreciate independent variables include hypothesis, extension analysis, difficult becomes interpret. Due increasing difficulty interpretation, practice, IDA studies rarely include two three independent variables analysis.Independent variables add complexity study part research focus, specifically hypothesis. , however, common include variables central focus, commonly assumed contribute explanation variation dependent variable. Let’s assume background literature suggests age speakers also plays role number questions men women use spontaneous conversation. Let’s also assume data collected includes information age speakers. like factor potential influence age use questions focus particular independent variables ’ve defined hypothesis, can include age speakers control variable. control variable added statistical analysis documented report included hypothesis interpreted results.\nFigure 3.15: Variable roles inferential analysis.\npoint let’s look main characteristics need taken account statistically interrogate variables chosen test hypothesis. type statistical test one chooses based (1) informational value dependent variable (2) number independent variables included analysis. Together two characteristics go long way determining appropriate class statistical test, considerations distribution particular variables (.e. normality), relationships variables (.e. independence), expected directionality predicted effect may condition appropriate method applied.can imagine, host combinations statistical tests apply particular scenarios, many consider given scope coursebook (see Gries (2013) Paquot & Gries (2020) exhaustive description). ’ve summarized common statistical scenarios associated tests focus juxtaposition informational values number variables, leaving aside alternative tests deal non-normal distributions, ordinal variables, non-independent variables, etc.Table 3.7 see monofactorial tests, tests one independent variable.\nTable 3.7: Common monofactorial tests.\nTable 3.8 includes listing multifactorial tests, tests one independent / control variables.\nTable 3.8: Common multifactorial tests.\nOne key point make turn interpret statistical results concerns use data IDA. contrast two analysis methods cover, data IDA used . say, entire dataset used single time statistically interrogate relationship(s) interest. resulting confidence metrics (p-values, etc.) evaluated findings interpreted. practice running multiple tests statistically significant result found called “p-hacking” (Head, Holman, Lanfear, Kahn, & Jennions, 2015) like HARKing (described earlier) violates statistical hypothesis testing practice. reason vital identify statistical approach outset research project.Now let’s consider approach interpreting results statistical test. now made reference multiple times, results statistical procedure hypothesis testing result confidence metric. standard widely used confidence metrics p-value. p-value provides probability results statistical test explained null hypothesis. probability crosses threshold .05, result considered statistically significant, otherwise ‘null result’ (.e. non-significant). However, sets binary distinction can problematic. one hand one test returns p-value .051 something ‘marginally significant?’ According standard practice results statistically significant. important note p-value sensitive sample size. small sample may return non-significant result, larger sample size underlying characteristics may well return significant result. hand, get statistically significant result, move –case closed? just pointed sample size plays role finding statistically significant results, mean results ‘important’ even small effects large samples can return significant p-value.important underscore purpose IDA draw conclusions dataset generalizable population. conclusions require rigorous measures ensure results analysis overgeneralize (suggest relationship one) balance fact don’t want undergeneralize (miss fact relationship population, analysis capable detecting ). Overgeneralization known Type error false positive undergeneralization Type II error false negative.reasons important calculate size magnitude result gauge uncertainty result standardized, sample size-independent way. performed analyzing effect size reporting confidence interval (CI) results. wider CI uncertainty surrounds statistical result, therefore likely significant p-value result Type error. non-significant p-value large effect size result Type II error. addition vetting p-value, CI effect size can help determine significant result reliable ‘important.’ Together effect size CIs aid ability realistically interpret confidence metrics statistical hypothesis testing.","code":""},{"path":"approaching-analysis.html","id":"predictive-data-analysis","chapter":"3 Approaching analysis","heading":"3.2.2 Predictive data analysis","text":"Predictive data analysis (PDA) first two types statistical approaches cover fall machine learning. branch artificial intelligence (AI), machine learning aims develop computer algorithms can essentially learn patterns data automatically. case PDA, also known supervised learning, learning process guided (supervised) directing algorithm associate patterns variable set variables single particular variable. particular variable analogous degree dependent variable IDA, machine learning literature variable known target variable. variable (often ) variables known features. goal PDA develop statistical generalization can accurately predict values target variable using values feature variables. PDA can seen mix deductive (top-) inductive (bottom-) methods target variable determined research goal feature variables choice statistical method (algorithm) fixed can vary depending usefulness effectively predicting target variable. PDA versatile method often employed derive intelligent action data, can also used hypothesis generation even hypothesis testing, certain conditions. researcher’s aim create model can perform language related task, explore association strength target variable various types combinations features, perform emerging alternative approaches hypothesis testing,10 analysis approach researcher take.point let’s consider departures inferential data analysis (IDA) approach covered last subsection important highlight orient overview PDA. First, cornerstone IDA hypothesis, PDA typically case. research question identifies source potential uncertainty area outlines strategy addressing uncertainty sufficient groundwork embark analysis. second divergence, fact data used distinct way. IDA entire dataset statistically interrogated . PDA dataset (minimally) partitioned training set test set. training set used train statistical model test set left test accuracy statistical model. training set typically constitutes larger portion data (typically around 75%) serves test bed iteratively applying one algorithms / feature combinations produce successful learning model. test set reserved final evaluation model’s performance. Depending application amount available data, third development set sometimes created pseudo test set facilitate testing multiple approaches data outside training set final evaluation test set performed. scenario proportions partitions vary, good rule thumb reserve 60% data training, 20% development, 20% testing.Let’s now turn identification variables, statistical interrogation variables, interpretation statistical results. IDA variables (features) pre-determined hypothesis informational values number variables plays significant role selecting statistical procedure (algorithm). Lacking hypothesis, PDA approach’s main goal make accurate predictions target variable free explore number features feature combinations end. target variable variable necessarily fixed light pre-determined.give example, let’s consider language task goal take text messages (SMS) develop language model predict message spam . Minimally need data includes individual text messages text message need labeled either spam legitimate messages (‘ham’ case). Table 3.9 see first ten 4837 observations SMS Spam Collection (v.1) dataset collected Almeida, G’omez Hildago, & Yamakami (2011).\nTable 3.9: First ten observations SMS Spam Collection (v.1)\nstands two variables; sms_type clearly target message contain full messages. question best transform information message variable provide algorithm useful information predict value sms_type. Since informational value sms_type categorical call values classes. process deciding transform information message useful features called feature engineering process much art science. creative side things often helpful mixture relevant domain knowledge clever hacking skills envision features may work best. logistic side things requires knowledge strengths weaknesses various learning algorithms dealing certain number informational value feature combinations.Leaving choice algorithm aside, let’s focus feature engineering. Since message value unique message, chance using message , likely help us make reliable predictions status new message (‘spam’ ‘ham’). simple first-pass approach decomposing message draw similarities distinctions classes may break message words. Now SMS messages average type text –many non-standard forms. definition word may simply character groupings broken apart whitespace. avoid confusion common-sense understanding word types character strings, often case language feature values called terms. term types may work better, n-grams, character sequences, stems/lemmas, even combinations . Certain terms may removed potentially uninformative either based class (stopwords, numerals, punctuation, etc.) due distribution. process systematic isolation terms informative others called dimensionality reduction (Kowsari et al., 2019). experience research become adept recognizing advantages potential issues alternative ways approaching creation features almost always level trial error process. Feature engineering much exploratory process. also iterative. can try set features algorithm produce language model test training set –accurate, great. , can brainstorm –free try engineer features trying new features feature measures (term weights) / change learning algorithm.\nFigure 3.16: Variable roles predictive analysis.\nLet’s now turn considerations take account selecting statistical algorithm. First, just IDA, variable informational value plays role algorithm selection, specifically informational value target variable. target variable categorical, looking classification algorithm. target variable continuous, employ regression algorithm.11 common classification algorithms listed Table 3.10.\nTable 3.10: common supervised learning algorithms.\nAnother consideration take account whether researcher aims go beyond simply using algorithm make accurate predictions, also wants understand algorithm made predictions contribution features made process. algorithms produce models allow researcher peer understand inner workings (e.g. logistic regression, naïve bayes classifiers, inter alia) (e.g. neural networks, support vector machines, inter alia). called ‘black-box’ algorithms. Neither type assures best prediction accuracy. Important trade-offs need considered, however, best prediction comes black-box method, goal research understand contribution features model’s predictions.identified target variable, engineered promising set features, selected algorithm employ meets research goals, now time interrogate dataset. first step partition dataset training test set. training set dataset use try different features / algorithms aim developing model can accurately predict target variable values training set. second step ’s done first training algorithm associate features (actual) target values. Next, resulting model applied training data, yet target variable removed, hidden, machine learner. target values predicted model observation compared actual target values. predicted actual values target variable coincide, accurate model. model shows high accuracy, ready move evaluate model test set (removing target variable). model accuracy low, ’s back drawing board either returning feature engineering / algorithm selection hopes improve model performance. way, training data can used multiple times, clear divergence standard IDA methods data interrogated analyzed .\nFigure 3.17: Phases predictive analysis.\napplications PDA interpretation prediction model includes metric metrics accuracy comparing extent models predictions actual targets align. cases inner workings model interest, researcher can dive features contributions prediction model exploratory fashion according research goals. exploration features, , varies, time let’s focus metrics prediction accuracy.standard form evaluating model’s performance differs classification models (naive bayes) regression models (linear regression). classification models, cross-tabulation predicted actual classes results contingency table can used calculate accuracy sum correctly predicted observations divided total number observations test set. addition accuracy, various measures aim assess model’s performance gain insight potential - -generalization model (Precision Recall). regression models, differences predicted actual values can assessed using coefficient correlation (typically \\(R^2\\)). , fine-grained detail model’s performance can calculated (Root Mean Square Error).Another component worthy consideration evaluating model’s performance determine performance actually good. One one hand, accuracy rates 90+% range test set usually good sign model performing well. model perform perfect accuracy, however, depending goal research particular error patterns may important, problematic, overall prediction accuracy. hand, another eventuality model performs well training set test set (new data) performance drops significantly. sign training phrase machine learning algorithm learned nuances data (‘noise’) obscure signal pattern learned. problem called overfitting avoid researchers iteratively run evaluations training data using resampling. two common resampling methods bootstrapping (resampling replacement) cross-validation (resampling without replacement). performance multiple models summarized error assessed. goal minimize performance differences models maximizing overall performance. measures go long way avoiding overfitting therefore maximizing chance training phase produce model robust.","code":""},{"path":"approaching-analysis.html","id":"exploratory-data-analysis","chapter":"3 Approaching analysis","heading":"3.2.3 Exploratory data analysis","text":"last three analysis types, exploratory data analysis (EDA) includes wide range methods whose objective identify structure datasets using data . way, EDA inductive, bottom-approach data analysis, make formal assumptions relationship(s) variables. EDA can roughly broken two subgroups analysis. Unsupervised learning, like supervised learning (PDA), subtype machine learning. However, unlike prediction, unsupervised learning include target variable guide associations. second subgroup EDA methods can seen (robust) extension descriptive analysis methods covered earlier chapter. Either unsupervised learning descriptive methods, EDA employs quantitative methods summarize, reduce, sort complex datasets statistically visually interrogate dataset order provide researcher novel perspective qualitatively assessed. qualitative assessments may prove useful generate hypotheses generate groupings used predictive analyses. , researcher’s aim probe dataset order explore potential relationships area predictions / hypotheses clearly made, analysis approach choose.contrast IDA even PDA assumptions made relationship(s) explore, EDA makes assumptions. Furthermore, given exploratory nature process, EDA approach can used make conclusive generalizations populations (corpus) sample drawn. IDA fidelity sample process selection variables utmost importance ensure statistical results reliably generalizable. Even case PDA, sample variables selected key building robust predictive model. However, contrast IDA, similar PDA, EDA methods may reuse data selecting different variables /methods research goals dictate. machine learning approach EDA adopted, dataset can partitioned training test sets, similar fashion PDA. PDA, training set used refining statistical measures test set used evaluate refined measures. Although evaluation results still used generalize, insight can taken stronger evidence potential relationship, set relationships, worthy study.Another notable point contrast concerns interpretation EDA results. Although quantitative nature, exploratory methods involve high level human interpretation. Human interpretation part stage data analysis, statistical approach, general, exploratory methods produce results require associative thinking pattern detection distinct two analysis approaches, particular, IDA., done two analysis approaches, let’s turn process variable identification, data interrogation, interpretation methods. case PDA, EDA requires research goal. PDA, research goal centered around predicting target variable. EDA, focus. research goal may fact less defined researcher may consider various relationships turn simultaneously. curation variables, however, overlap spirit process feature engineering touched creating variables predictive models. EDA measure gauge whether engineered variables good, left qualitative evaluation researcher.\nFigure 3.18: Variable roles exploratory analysis.\nillustrative purposes let’s consider State Union Corpus (SOTU) (Benoit, 2020). presidential addresses set meta-data variables included corpus. ’ve subsetted corpus include U.S. presidents since 1946. tabular preview first 10 addresses (truncated display) can found Table 3.11.\nTable 3.11: First ten addresses SOTU Corpus.\nCongress United States:Congress United States:dataset one leveraged explore many different types research questions. Key guiding engineering features, however, clarify outset research project entity study , unit analysis. IDA PDA approaches, unit analysis forms explicit part research hypothesis goal. EDA research question may multiple fronts, may reflected differing units analysis. example, based SOTU dataset, interested political rhetoric, language particular presidents, party ideology, etc. Depending perspective interested investigating, choice approach engineering features gain insight vary.token, approaches interrogating dataset can vary widely, within research project, instructive purposes can draw distinction descriptive methods unsupervised learning methods.\nTable 3.12: common EDA analysis methods.\nEDA leans heavily visual representations descriptive unsupervised learning methods. Visualizations enable humans identify extrapolate associative patterns. Visualizations range standard barplots scatterplots network graphs dendrograms . sample visualizations based SOTU Corpus found Figure 3.19.\nFigure 3.19: Sample visualizations SOTU Corpus (1946-2020).\nJust feature selection analysis method, interpretation results EDA much varied analysis methods. EDA methods provide information requires much human intervention associative interpretation. way, EDA can seen quantitatively informed qualitative assessment approach. results one approach can used input another. Findings can lead exploration probing nuances data. Speculative results exploratory methods can highly informative lead new insight inspire study directions may expected.","code":""},{"path":"approaching-analysis.html","id":"reporting","chapter":"3 Approaching analysis","heading":"3.3 Reporting","text":"Much necessary reporting analysis features prose part write-report article. include descriptive summaries, blueprint method(s) used, results. Descriptive summaries often include assessments individual variables / relationships variables (central tendency, dispersion, association strength, etc.). procedures applied diagnose correct data also included final report. information key helping readers assess results analysis. blueprint methods used describe variable selection process, variables used statistical analysis, information relevant reader understand done done. Reporting results analysis depend type analysis particular method(s) employed. inferential analyses include test statistic(s) (\\(X^2\\), \\(R^2\\), etc.) measure confidence (\\(p\\)-value, confidence interval, effect size). predictive analyses accuracy results related information need reported. exploratory analyses, reporting results vary often include visualizations metrics require human interpretation analysis types.good article write-include vital information understand procedures taken analysis, many details traditionally appear prose. research project conducted programmatically, however, programming files (scripts) used generate analysis can () shared. scripts highly useful researchers consult understand fine-grained detail steps taken, important also recognize research project well documented –organized project directory file structure well code commenting. description instructions run analysis form research compendium ensure research conducted easily understood able reproduced / enhanced researchers.","code":""},{"path":"approaching-analysis.html","id":"summary-2","chapter":"3 Approaching analysis","heading":"Summary","text":"chapter focused description analysis –third component DIKI Hierarchy. process visually summarized Figure 3.20.\nFigure 3.20: Approaching analysis: visual summary\nBuilding strategies covered Chapter 2 “Understanding data” derive rich relational dataset, chapter outlined key points approaching analysis. first key step analysis perform descriptive assessment individual variables relationships variables. select appropriate descriptive measures covered various informational values variable can take. addition providing key information reporting purposes, descriptive measures important explore researcher can get better feel dataset conducting analysis.covered three data analysis types chapter: inferential, predictive, exploratory. embodies distinct approaches deriving knowledge data. Ultimately choice analysis type highly dependent goals research. Inferential analysis centered around goal testing hypothesis, reason highly structured approach analysis. structure aimed providing mechanisms draw conclusions results can generalized target population. Predictive analysis less-ambitious times relevant goal discovering extent given relationship can extrapolated data provide model language can accurately predict outcome using new data. many times predictive analysis used perform language tasks, can also highly effective methodology applying different algorithmic approaches exploring relationships target variable various configurations variables. ability explore data multiple ways, also key strength employing exploratory analysis. least structured variable analysis types, exploratory analyses powerful approach deriving knowledge data area clear predictions made.rounded chapter short description importance reporting metrics, procedures, results analysis. Reporting, traditional form, documented prose article. reporting aims provide key information reader need understand done, done, done. information also provides necessary information reader’s critical eye understand analysis detail. Yet even detailed reporting write-still leaves many practical, key, points analysis obscured. programming approach provides procedural steps taken shared provide exact methods applied. Together write-research compendium provides scripts run analysis documentation run analysis forms integral part creating reproducible research.","code":""},{"path":"framing-research.html","id":"framing-research","chapter":"4 Framing research","heading":"4 Framing research","text":"\nDRAFT\nknew , called research, ?―–Albert Einstein\nessential questions chapter :\n\nstrategies selecting research area identifying research problem?\n\nresearch problem research aim frame development research statement?\n\n‘research blueprint’ conceptual practical steps involved developing aid researcher well scientific community?\npoint part coursebook, covered Data, Information, Knowledge Data Insight Hierarchy. goal provide orientation main building blocks text analysis. Insight last component hierarchy. However, practical terms, first step address research project goals research project influence subsequent steps. chapter discuss frame research, position research project’s findings contribute insight understanding world. cover connect literature, selecting research area identifying research problem, design research best positioned return relevant findings connect literature, establishing research aim research question. round chapter guide developing research blueprint –working plan organize conceptual practical steps implement research effectively way supports communicating research findings process findings obtained.Together research area, problem, aim question research blueprint forms conceptual practical scaffolding project ensure outset project solidly grounded main characteristics good research. characteristics, summarized Cross (2006), found Table 4.1.Table 4.1: Characteristics research (Cross, 2006).characteristics mind, let’s get started first component address –connecting literature.","code":""},{"path":"framing-research.html","id":"connect","chapter":"4 Framing research","heading":"4.1 Connect","text":"","code":""},{"path":"framing-research.html","id":"research-area","chapter":"4 Framing research","heading":"4.1.1 Research area","text":"area research first decision make terms make contribution understanding. point, aim identify general area interest researcher wants derive insight. established research trajectory language, area research address text analysis likely extension prior work. others, include new researchers researcher’s want explore new areas language research approach area language-based lens, choice area may less obvious. either case, choice research area guided desire contribute something relevant theoretical, social, / practical matter personal interest. Personal relevance goes long way developing carrying purposive inquisitive research.get started? first step reflect areas interest knowledge, academic, professional, personal. Language heart human experience therefore found fashion anywhere one seeks find . big world often general question area explore language use sometimes difficult. get ball rolling, helpful peruse disciplinary encyclopedias handbooks linguistics language-related academic fields (e.g. Encyclopedia Language Linguistics (cite), Practical Guide Electronic Resources Humanities (cite), Routledge encyclopedia translation technology (cite))personal, less academic, approach consult online forums, blogs, etc. one already frequents can accessed via online search. example, Reddit wide variety active subreddits (r/LanguageTechnology, r/Linguistics, r/corpuslinguistics, r/DigitalHumanities, etc.). Twitter Facebook also interesting posts linguistics language-related fields worth following. one social media site may find particular people maintain blog worth browsing. example, follow Julia Silge, Rachel Tatman, Ted Underwood, inter alia. Perusing resources can help spark ideas highlight kinds questions interest .Regardless whether inquiry stems academic, professional, personal interest, try connect findings academic areas research. Academic research highly structured well-documented making associations network aid subsequent steps developing research project.","code":""},{"path":"framing-research.html","id":"research-problem","chapter":"4 Framing research","heading":"4.1.2 Research problem","text":"’ve made rough-cut decision area research, now time take deeper dive subject area jump literature. rich structure disciplinary research provide aid traverse vast world academic knowledge identify research problem. research problem highlights particular topic debate uncertainty existing knowledge worthy study.Surveying relevant literature key ensuring research informed, , connected previous work. Identifying relevant research consult can bit ‘chicken egg’ problem –knowledge area necessary find relevant topics, knowledge topics necessary narrow area research. Many times way forward jump conducting searches. can world-accessible resources (e.g. Google Scholar) limited-access resources provided academic institution (e.g. Linguistics Language Behavior Abstracts), ERIC, PsycINFO, etc.). organizations academic institutions provide research guides help researcher’s access primary literature.Another avenue explore journals dedicated areas linguistics language-related research published. following tables ’ve listed number highly visable journals linguistics, digital humanities, computational linguistics.Table 4.2: list linguistics journals.Table 4.3: list humanities journals.Table 4.4: list computational linguistics journals.explore research related text analysis helpful start (sub)discipline name(s) identified selecting research area, specific terms occur key terms literature, terms ‘corpus study’ ‘corpus-based.’ results first searches may turn sources end figuring explicitly research, important skim results publications mine information can useful formulate better targeted searches. Relevant information honing searches can found throughout academic publication (article book). However, pay particular attention abstract, articles, table contents, books, cited references. Abstracts tables contents often include discipline-specific jargon commonly used field. articles even short list key terms listed abstract can extremely useful seed better precise search results. references section contain relevant influential research. Scan references publications appear narrowing topic interest treat like search right.searches begin show promising results time keep track organize references. Whether plan collect thousands references lifetime academic research aim centered around one project, software Zotero12, Mendeley, BibDesk provide powerful, flexible, easy--use tools collect, organize, annotate, search, export references. Citation management software indispensable modern research –often free!list relevant references grows, want start investigation process earnest. Begin skimming (reading) contents publications, starting relevant first13. Annotate publications using highlighting features citation management software identify: (1) stated goal(s) research, (2) data source(s) used, (3) information drawn data source(s), (4) analysis approach employed, (5) main finding(s) research pertain stated goal(s). Next, words, summarize five key areas prose adding summary notes feature citation management software. process allow efficiently gather document references relevant information guide identification research problem, guide formation problem statement, ultimately, support literature review figure project write-.preliminary annotated summaries undoubtedly start recognize overlapping contrasting aspects research literature. aspects may topical, theoretical, methodological, appear along lines. Note aspects continue conduct refine searches, annotate new references, monitor emerging patterns uncertainty debate (gaps) align research interest(s). promising pattern takes shape, time engage detailed reading references appear relevant highlighting potential gap(s) literature. point can focus energy nuanced aspects particular gap literature goal formulate problem statement. problem statement directly acknowledges gap literature puts finer point nature relevance gap understanding. statement reflects first deliberate attempt establish line inquiry. targeted, still somewhat general, statement framing gap literature guide subsequent research design decisions.","code":""},{"path":"framing-research.html","id":"findings","chapter":"4 Framing research","heading":"4.2 Findings","text":"","code":""},{"path":"framing-research.html","id":"research-aim","chapter":"4 Framing research","heading":"4.2.1 Research aim","text":"problem statement hand, now time consider goal(s) research. research aim frames type inquiry conducted. research aim explain, evaluate, explore? words, research seek test particular relationship, assess potential strength particular relationship, uncover novel relationships? can appreciate, research aim directly related analysis methods touched upon Chapter 3.gauge frame research aim, reflect literature led problem statement nature problem statement . gap center problem statement lack knowledge, research aim may exploratory. gap concerns conjecture relationship, research may take predictive approach. gap points validation relationship, research likely inferential nature. selecting research aim also helpful consult research aims primary literature led research statement. Consider research statement relates previous literature. aim test hypothesis based previous exploratory analyses? looking generate new knowledge (apparently) uncharted area?general, problem statement addresses smaller, nuanced gap tend adopt similar research aims previous literature larger, divergent gap tend adopt distinct research aim. hard rule, heuristic, however, important familiar previous literature, nature different types analysis, goals research ensure research best-positioned generate findings contribute existing body understanding principled way.","code":""},{"path":"framing-research.html","id":"research-question","chapter":"4 Framing research","heading":"4.2.2 Research question","text":"next step research design craft research question. research question clearly defined statement identifies aspect uncertainty particular relationships uncertainty concerns. research question extends narrows line inquiry established research statement research aim. research statement can seen content research aim form.form research question vary based analysis approach. inferential-based research, research question actually statement, question. statement makes testable claim nature particular relationship –.e. asserts hypothesis. illustration, let’s return one hypotheses previously sketched Chapter 3, leaving aside implicit null hypothesis.Women use questions men spontaneous conversations.predictive- exploratory-based research, research question fact question. reframing example hypothesis predictive-based research question might looks something like .Can number questions used spontaneous conversations predict speaker male female?similar exploratory-based research question take form.men women differ terms number questions use spontaneous conversations?central research interest behind hypothetical research questions , admittedly, quite basic. simplified examples, able appreciate similarities differences forms research statements correspond distinct research aims.terms content, research question make reference two key components. First, unit analysis. unit analysis entity research aims investigate. three example research aims, unit analysis , namely men women. Note, however, current unit analysis somewhat vague example research questions. precise unit analysis include information population men women drawn (e.g English speakers, American English speakers, American English speakers Southeast, etc.).second key component unit observation. unit observation primary element insight unit analysis derived way constitutes essential organization unit data collected. examples, unit observation, , unchanged spontaneous conversations. Note unit observation key identify forms organizational backbone research, common research derive variables unit provide evidence investigate research question. previous examples, identified number conversations part research question. cases researcher may seek understand aspects questions spontaneous conversations (.e type question, features questions, etc.). unit observation, however, remain .","code":""},{"path":"framing-research.html","id":"blueprint","chapter":"4 Framing research","heading":"4.3 Blueprint","text":"Efforts craft research question important aspect developing purposive, inquisitive, informed research (returning Cross’s characteristics research). Moving beyond research question project means developing laying research design way research Methodical Communicable. coursebook, method achieve goals development research blueprint. blueprint includes two components: (1) process identifying data, information, methods used (2) creation plan structure document project.Ignatow & Mihalcea (2017) point :Research design essentially concerned basic architecture research projects, designing projects systems allow theory, data, research methods interface way maximize project’s ability achieve goals …. Research design involves sequence decisions taken project’s early stages, one oversight poor decision can lead results ultimately trivial untrustworthy. Thus, critically important think carefully systematically research design committing time resources acquiring texts mastering software packages programming languages text mining project.","code":""},{"path":"framing-research.html","id":"identify","chapter":"4 Framing research","heading":"4.3.1 Identify","text":"Importance identifying documenting key aspects required conduct research understated. one hand process links concept implementation. , researcher better-positioned conduct research clear view entailed. hand, promising research question, paper, may present challenges may require modification reevaluation viability project. uncommon encounter roadblocks even dead-ends moving well-founded research question forward considering available data, researcher’s (current) technical / research skills, given time frame project. practice, process identifying data, information, methods analysis considered tandem investigative work develop research aim research question. subsection cover main characteristics consider developing research blueprint.first, important, part establishing research blueprint identify viable data source. Regardless find access data, essential vet corpus sample light research question. case research inferential nature, sampling frame corpus primary importance goal generalize findings target population. corpus resource align, extent feasible, target population. predictive exploratory research, goal generalize claim central reason freedom terms representative corpus sample target population. Ideally researcher find able model language population target interest. Since goal, however, test hypothesis, rather explore particular potential relationships, either predictive exploratory fashion, research can often continue stipulation results interpreted light characteristic available corpus sample.second step identify key variables need conduct research ensure information can derived corpus data. research question reference unit analysis unit observation, important point pinpoint key variables . unit observation spontaneous conversations. question aspects conversations used analysis. research questions presented chapter, want envision needs done generate variable measures number questions conversations. research, may features need extracted recoded address research question. variables importance may non-linguistic nature. Provided corpus required meta-data research, variables can normalized, recoded, generated corpus fit research needs. cases meta-data incomplete goals research, sometimes possible merge meta-data sources.third step identify method analysis. selection analysis approach part research aim research question goes long way narrowing methods researcher must consider. number factors make methods appropriate others. inferential research, number information values variables analyzed key importance (Gries, 2013). informational value dependent variable narrow search appropriate method. number independent variables also plays important role. example, study categorical dependent variable single categorical independent variable lead researcher Chi-squared test. study continuous dependent variable multiple independent variables lead linear regression. Another aspect note inference studies consideration distribution continuous variables –normal distribution use parametric test non-normal distribution use non-parametric test. details need nailed point, helpful radar ensure time comes analyze data, appropriate steps taken test normality apply correct test.predictive-based research, informational value target variable key deciding whether prediction classification task numeric prediction task. downstream effects comes time evaluate interpret results. Although feature engineering process predictive analyses means features need specified outset can tweaked changed needed analysis, good idea start basic sense features likely helpful developing robust predictive model. Furthermore, number informational values features (predictor variables) important selecting prediction method (algorithm) inferential analysis methods, important recognize algorithms strengths shortcomings working large numbers / types features (Lantz, 2013).Exploratory research least restricted three types analysis approaches. Although may case research able specify outset project exact analysis methods , attempt consider types analysis methods promising provide results address research question goes long way steering project right direction grounding research. analysis approaches, important aware analysis methods available type information produce light research question.sum, identification data, information, analysis methods used proposed research key ensuring research viable. sure document process prose describe strengths potential shortcomings (1) corpus data selected, (2) information extracted analysis, (3) analysis method(s) appropriate research aim evaluation method . Furthermore, every eventuality can foreseen. helpful include description aspects process may pose challenges include potential contingency plans part prose description.","code":""},{"path":"framing-research.html","id":"plan","chapter":"4 Framing research","heading":"4.3.2 Plan","text":"next step creating research blueprint consider physically implement project. includes organize files directories fashion provides researcher logical predictable structure work also ensures research Communicable. one hand, communicable research includes strong write-research, , hand, also important research reproducible. Reproducibility strategies benefit researcher (moment future) leads better work habits better teamwork makes changes project easier. Reproducibility also benefit scientific community shared reproducible research enhances replicability encourages cumulative knowledge development (Gandrud, 2015).set guiding principles accomplish goals (Gentleman & Temple Lang, 2007; Marwick, Boettiger, & Mullen, 2018).files plain text means contain formatting information whitespace.clear separation data, method, output research. apparent directory structure.separation original data derived data made. Original data treated ‘read-.’ changes original data justified, generated code, documented (see point 6).analysis file (script) represent particular, well-defined step research process.analysis script modular –, file correspond specific goal analysis procedure input output corresponding step.analysis scripts tied together ‘master’ script used coordinate execution analysis steps.Everything documented. includes analysis steps, script code comments, data description data dictionaries, information computing environment packages used conduct analysis, detailed instructions reproduce research.seven principles can physically implemented countless ways. recent years, growing number efforts create R packages templates quickly generate scaffolding tools facilitate reproducible research. notable R packages include workflowr ProjectTemplate many resources R included CRAN Task View Reproducible Research. many advantages working pre-existing frameworks savvy R programmer.coursebook, however, developed project template (available GitHub) believe simplifies makes process transparent beginning intermediate R programmers, directory structure provided .Let now describe template structure aligns seven principles quality reproducible research.files plain text (e.g. .R, .Rmd, .csv, .txt, etc.).three main directories analysis/, data/, ouput/.data/ directory contains sub-directories original (‘read-’) data derived data.analysis/ directory contains five scripts numbered correspond sequential role research process.analysis scripts designed modular; input output must explicit intermediate objects carried analysis scripts. Dataset output written read data/derived/ directory. Figures statistical results written read output/figures/ output/results respectively.analysis scripts, therefore entire project, tied _pipeline.R script. reproduce entire project script need run.Documentation takes place many levels. README.md file first file researcher consult. contains brief description project goals reproduce analysis. Analysis scripts use Rmarkdown format (.Rmd). format allows researchers interleave prose description executable code script. ensures rationale steps taken described prose, code made available consult, code comments can added every line. _sesssion-info.Rmd script merged analysis script provide information computing environment packages used conduct step analysis. template, data datasets appear. However, data acquired data curated transformed, documentation resources documented resource data dictionary along side data(set) .aspects project template described points 1-7 together form backbone reproducible research. template, however, includes additional functionality enhance efficient communicable research. _pipeline.R script executes analysis scripts analysis directory, side effect also produces working website journal-ready article publishing analysis, results, findings web. index.Rmd file splash page website good place house pre-analysis investigative work including research area, problem, aim, question document research blueprint including identification viable data resource(s), key variables analysis, analysis method, method assessment. Rmarkdown files provide functionality citing organizing references. references.bib file references stored can used include citations support research throughout project.","code":"#> ../project_template/\n#> ├── README.md\n#> ├── _pipeline.R\n#> ├── analysis\n#> │   ├── 1_acquire_data.Rmd\n#> │   ├── 2_curate_dataset.Rmd\n#> │   ├── 3_transform_dataset.Rmd\n#> │   ├── 4_analyze_dataset.Rmd\n#> │   ├── 5_generate_article.Rmd\n#> │   ├── _session-info.Rmd\n#> │   ├── _site.yml\n#> │   ├── biblio.bib\n#> │   └── index.Rmd\n#> ├── data\n#> │   ├── derived\n#> │   └── original\n#> └── output\n#>     ├── figures\n#>     └── results"},{"path":"framing-research.html","id":"prepare","chapter":"4 Framing research","heading":"4.3.3 Prepare","text":"template allow organize research design align implementation steps conduct quality reproducible research. prepare analysis, need download fork clone template GitHub repository make adjustments personalize template research.create local copy project template either:Download decompress .zip fileIf git installed machine GitHub account, fork repository GitHub account. open terminal desired location clone repository. using RStudio, can setup new RStudio Project clone using ‘New Project…’ dialog, choosing ‘Version Control,’ following steps.begin configuring adding project-specific details template. Reproduce project ‘-’ confirm builds local machine.RStudio R session Terminal application, open console root directory project. run:take time complete, prompt (>) console return. navigate open docs/index.html browser.confirmed project template builds, can begin configure template reflect project. files consider first. files places title project appear.README.md_pipeline.Ranalysis/index.RmdAfter updating files, build project make sure new changes appear like . now ready start research project!","code":"\nsource(\"_pipeline.R\")"},{"path":"framing-research.html","id":"summary-3","chapter":"4 Framing research","heading":"Summary","text":"Round-chapter …Heads-upcoming parts, implementation: preparation modeling\nFigure 4.1: Framing research: visual summary\n","code":""},{"path":"preparation-overview.html","id":"preparation-overview","chapter":"Overview","heading":"Overview","text":"Overview…","code":""},{"path":"acquire-data.html","id":"acquire-data","chapter":"5 Acquire data","heading":"5 Acquire data","text":"\nINCOMPLETE DRAFT\nscariest moment always just start.―–Stephen King\nchapter learn:\n\ngoals textbook\n\nreasoning using R programming language\n\nimportant text conventions employed textbook\nOverview… (edit)provide overview first three common strategies acquiring corpus data R: accessing corpus data data repositories individual sites. cover acquiring data different sources introduce R code help speed process, maintain consistency data, set stage reproducible workflow.three main ways acquire corpus data using R introduce : direct download, package interfaces, web scraping. chapter start directly downloading corpus straightforward process novice R programmer incurs least number steps. Along way introduce key R coding concepts including control statements custom functions.","code":""},{"path":"acquire-data.html","id":"direct-downloads","chapter":"5 Acquire data","heading":"5.1 Direct downloads","text":"","code":""},{"path":"acquire-data.html","id":"files","chapter":"5 Acquire data","heading":"5.1.1 Files","text":"… add …","code":""},{"path":"acquire-data.html","id":"compressed-files","chapter":"5 Acquire data","heading":"5.1.2 Compressed files","text":"… describe compressed files…Let’s take look works starting sample Switchboard Corpus, corpus 2,400 telephone conversations 543 speakers. First navigate site browser download file looking . case found Switchboard Corpus NLTK data repository site. often file type compressed archive file extension .zip .tz, case . Archive files make downloading multiple files easy grouping files directories one file. R can used download.file() function base R library14. number arguments function may require provide optionally. download.file() function minimally requires two: url destfile. file download location saved disk.can see looking directory structure data/ switchboard.zip file downloaded.archive file downloaded, however, file needs ‘decompressed’ reveal file structure. decompress file use unzip() function arguments zipfile pointing .zip file exdir specifying directory want files extracted .directory structure data/ now look like :point acquired data programmatically code part workflow anyone run code reproduce results. code , however, ideally efficient. Firstly switchboard.zip file strictly needed decompress occupies disk space keep . second, time run code file downloaded remote serve leading unnecessary data transfer server traffic. Let’s tackle issues turn.avoid writing switchboard.zip file disk (long-term) can use tempfile() function open temporary holding space file. space can used store file, unzip , temporary file destroyed. assign temporary space R object name temp tempfile() function. object can now used value argument destfile download.file() function. Let’s also assign web address another object url use value url argument.\nprevious code ’ve used values stored objects url temp download.file() function without specifying argument names –providing names objects. R assume values function map ordering arguments. values map ordering arguments required specify argument name value. view ordering objects hit TAB entering function name consult function documentation prefixing function name ? hitting ENTER.\npoint downloaded file stored temporarily disk can accessed decompressed target directory using temp value argument zipfile unzip() function. ’ve assigned target directory path target_dir used value argument exdir prepare us next tweak approach.directory structure now looks like :second issue raised concerns fact running code part project repeat download time. Since like good citizens avoid unnecessary traffic web nice code checked see already data disk exists, skip download, download .achieve need introduce two new functions () dir.exists(). dir.exists() takes path directory argument returns logical value, TRUE, directory exists, FALSE . () evaluates logical statements processes subsequent code based logical value passed argument. Let’s look toy example.assigned num value 1 created logical evaluation num == whose result passed argument (). statement returns TRUE code withing first set curly braces {...} run. num == 1 false, like code , code withing braces following else run.function () one various functions called control statements. Theses functions provide lot power make dynamic choices code run.get back key objective avoid downloading resources already disk, let introduce another strategy making code powerful ultimately efficient well legible –custom function. Custom functions functions user writes create set procedures can run similar contexts. ’ve created custom function named eval_num() .Let’s take closer look ’s going . function function() creates function user decides arguments necessary code perform task. case necessary argument object store numeric value evaluated. ’ve called num reflects name object toy example, nothing special name. ’s important object names consistently used. ’ve included previous code (except hard-coded assignment num) inside curly braces assigned entire code chunk eval_num.can now use function eval_num() perform task evaluating whether value num equal 1.’ve put coding strategies together previous code custom function named get_zip_data(). lot going . Take look first see can follow logic involved given now know.\nprefixed ! logical expression dir.exists(target_dir) returns opposite logical value. needed case target directory exists, expression return FALSE, TRUE, therefore proceed downloading resource.\ncouple key tweaks ’ve added provide additional functionality. one ’ve included function dir.create() create target directory data written. ’ve also added additional argument unzip() function, junkpaths = TRUE. Together additions allow user create arbitrary directory path files, files, extracted disk. discard containing directory .zip file can helpful want add multiple .zip files target directory.practical scenario applies want download data corpus contained multiple .zip files still maintain files single primary data directory. Take example Santa Barbara Corpus. corpus resource includes series interviews one .zip file, SBCorpus.zip contains transcribed interviews another .zip file, metadata.zip organizes meta-data associated speaker. Applying initial strategy download decompress data lead following directory structure:applying new custom function get_zip_data() transcriptions meta-data can better organize data.add data sources can keep logical separate allow data collection scale without creating unnecessary complexity. Let’s add Switchboard Corpus sample using get_zip_data() function see action.point need continue next step data analysis project. go, housekeeping document organize process make work reproducible. take advantage project-template directory structure, seen .First good practice separate custom functions processing scripts. can create file functions/ directory named acquire_functions.R add custom function get_zip_data() .\nNote acquire_functions.R file R script, Rmarkdown document. Therefore code chunks used .Rmd files used, R code .\nuse source() function read function current script make available use needed. good practice source functions SETUP section script.section, sum , ’ve covered access, download, organize data contained .zip files; common format language data found repositories individual sites. included introduction key R programming concepts strategies including using functions, writing custom functions, controlling program flow control statements. approach gather data also keeping mind reproducibility code. end introduced programming strategies avoiding unnecessary web traffic (downloads), scalable directory creation, data documentation.\ncustom function get_zip_data() works .zip files. many compressed file formats (e.g. .gz, .tar, .tgz), however. R package tadr accompanies coursebook, modified version get_zip_data() function, get_compressed_data(), extends logic deal wider range compressed file formats, including .zip files.\n\nExplore function’s documentation (?tadr::get_compressed_data()) / view code (tadr::get_compressed_data) better understand function.\n","code":"\n# Download .zip file and write to disk\ndownload.file(url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip\",\n    destfile = \"../data/original/switchboard.zip\")data\n├── derived\n└── original\n    └── switchboard.zip\n# Decompress .zip file and extract to our target directory\nunzip(zipfile = \"../data/original/switchboard.zip\", exdir = \"../data/original/\")data\n├── derived\n└── original\n    ├── switchboard\n    │   ├── README\n    │   ├── discourse\n    │   ├── disfluency\n    │   ├── tagged\n    │   ├── timed-transcript\n    │   └── transcript\n    └── switchboard.zip\n# Create a temporary file space for our .zip file\ntemp <- tempfile()\n# Assign our web address to `url`\nurl <- \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip\"\n# Download .zip file and write to disk\ndownload.file(url, temp)\n# Assign our target directory to `target_dir`\ntarget_dir <- \"../data/original/\"\n# Decompress .zip file and extract to our target directory\nunzip(zipfile = temp, exdir = target_dir)data\n├── derived\n└── original\n    └── switchboard\n        ├── README\n        ├── discourse\n        ├── disfluency\n        ├── tagged\n        ├── timed-transcript\n        └── transcript\nnum <- 1\nif (num == 1) {\n    cat(num, \"is 1\")\n} else {\n    cat(num, \"is not 1\")\n}\n#> 1 is 1\nnum <- 2\nif (num == 1) {\n    cat(num, \"is 1\")\n} else {\n    cat(num, \"is not 1\")\n}\n#> 2 is not 1\neval_num <- function(num) {\n    if (num == 1) {\n        cat(num, \"is 1\")\n    } else {\n        cat(num, \"is not 1\")\n    }\n}\neval_num(num = 1)\n#> 1 is 1\neval_num(num = 2)\n#> 2 is not 1\neval_num(num = 3)\n#> 3 is not 1\nget_zip_data <- function(url, target_dir) {\n    # Function: to download and decompress a .zip file to a target directory\n\n    # Check to see if the data already exists if data does not exist, download/\n    # decompress\n    if (!dir.exists(target_dir)) {\n        cat(\"Creating target data directory \\n\")  # print status message\n        dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE)  # create target data directory\n        cat(\"Downloading data... \\n\")  # print status message\n        temp <- tempfile()  # create a temporary space for the file to be written to\n        download.file(url = url, destfile = temp)  # download the data to the temp file\n        unzip(zipfile = temp, exdir = target_dir, junkpaths = TRUE)  # decompress the temp file in the target directory\n        cat(\"Data downloaded! \\n\")  # print status message\n    } else {\n        # if data exists, don't download it again\n        cat(\"Data already exists \\n\")  # print status message\n    }\n}data\n├── derived\n└── original\n    ├── SBCorpus\n    │   ├── TRN\n    │   └── __MACOSX\n    │       └── TRN\n    └── metadata\n        └── __MACOSX\n# Download corpus transcriptions\nget_zip_data(url = \"http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip\",\n    target_dir = \"../data/original/sbc/transcriptions/\")\n\n# Download corpus meta-data\nget_zip_data(url = \"http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip\",\n    target_dir = \"../data/original/sbc/meta-data/\")data\n├── derived\n└── original\n    └── sbc\n        ├── meta-data\n        └── transcriptions\n# Download corpus\nget_zip_data(url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip\",\n    target_dir = \"../data/original/scs/\")data\n├── derived\n└── original\n    ├── sbc\n    │   ├── meta-data\n    │   └── transcriptions\n    └── scs\n        ├── README\n        ├── discourse\n        ├── disfluency\n        ├── tagged\n        ├── timed-transcript\n        └── transcript├── README.md\n├── _pipeline.R\n├── analysis\n│   ├── 1_acquire_data.Rmd\n│   ├── 2_curate_dataset.Rmd\n│   ├── 3_transform_dataset.Rmd\n│   ├── 4_analyze_dataset.Rmd\n│   ├── 5_generate_article.Rmd\n│   ├── _session-info.Rmd\n│   ├── _site.yml\n│   ├── index.Rmd\n│   └── references.bib\n├── data\n│   ├── derived\n│   └── original\n│       ├── sbc\n│       └── scs\n├── functions\n└── output\n    ├── figures\n    └── results\n# Load custom functions for this project\nsource(file = \"../functions/acquire_functions.R\")"},{"path":"acquire-data.html","id":"apis","chapter":"5 Acquire data","heading":"5.2 APIs","text":"convenient alternative method acquiring data R package interfaces web services. interfaces built using R code make connections resources web Automatic Programming Interfaces (APIs). Websites Project Gutenberg, Twitter, Facebook, many others provide APIs allow access data certain conditions, limiting data collection others. Programmers (like !) R community take task wrapping calls API R code make accessing data R possible. example, gutenbergr provides access Project Gutenberg, rtweet Twitter, Rfacebook Facebook.","code":""},{"path":"acquire-data.html","id":"open-access","chapter":"5 Acquire data","heading":"5.2.1 Open access","text":"Using R package interfaces, however, often requires knowledge R objects functions. Let’s take look access data Project Gutenberg gutenbergr package. Along way touch upon various functions concepts key working R data types vectors data frames including filtering writing tabular data disk plain-text format.get started let’s install / load gutenbergr package. package part R base library, assume user package library. standard approach installing loading package using install.packages() function calling library().approach works just fine, luck R package installing loading packages! pacman package includes set functions managing packages. useful one p_load() look package system, load found, install load found. helps potentially avoid using unnecessary bandwidth install packages may already exist user’s system. , use pacman need include code install load functions install.packages() library(). ’ve included code mimic behavior p_load() installing pacman , can see elegant, luckily ’s used add SETUP section master file, _pipeline.R.Now pacman installed loaded R session, let’s use p_load() function make sure install/ load two packages need upcoming tasks. following along project_template, add code within SETUP section 1_acquire_data.Rmd file.\nNote arguments tidyverse gutenbergr comma-separated quoted using p_load(). using install.packages() install, package names need quoted (character strings). library() can take quotes quotes, one package time.\nProject Gutenberg provides access thousands texts public domain. gutenbergr package contains set tables, data frames R speak, index meta-data texts broken text (gutenberg_metadata), author (gutenberg_authors), subject (gutenberg_subjects). ’ll use glimpse() function loaded tidyverse package15 summarize structure data frames.gutenberg_metadata, gutenberg_authors, gutenberg_subjects periodically updated. check see data frame last updated run:attr(gutenberg_metadata, \"date_updated\")download text use gutenberg_download() function takes one required argument, gutenberg_id. gutenberg_download() function known ‘vectorized,’ , can take single value multiple values argument gutenberg_id. Vectorization refers process applying function elements stored vector –primary object type R. vector grouping values one various types including character (chr), integer (int), double (dbl), logical (lgl) data frame grouping vectors. gutenberg_download() function takes integer vector can manually added selected gutenberg_metadata gutenberg_subjects data frames using $ operator (e.g. gutenberg_metadata$gutenberg_id).Let’s first add manually toy example generating vector integers 1 5 assigned variable name ids.download works Project Gutenberg corresponding gutenberg_ids 1 5, pass ids object gutenberg_download() function.Two attributes returned: gutenberg_id text. text column contains values line text (delimited carriage return) 5 works downloaded. many attributes available Project Gutenberg API can accessed passing character vector attribute names argument meta_fields. column names gutenberg_metadata data frame contains available attributes.Let’s augment previous download title author works. create character vector use c() function, , quote delimit individual elements vector comma.Now, practical scenario like select values gutenberg_id principled query works specific author, language, subject. first query either gutenberg_metadata data frame gutenberg_subjects data frame. Let’s say want download random sample 10 works English Literature (Library Congress Classification, “PR”). Using dplyr::filter() function (dplyr part tidyverse package set) first extract Gutenberg ids gutenberg_subjects subject_type == \"lcc\" subject == \"PR\" assigning result ids.16\noperators = == equivalents. == used logical evaluation = alternate notation variable assignment (<-).\ngutenberg_subjects data frame contain information whether gutenberg_id associated plain-text version. limit query English Literature works text, filter gutenberg_metadata data frame ids selected ids attribute has_text gutenberg_metadata data frame.can see number works text fewer number works listed, 7100 versus 6724. Now can safely random selection 10 works, function slice_sample() confident ids select contain text take next step downloading data.point data move processing dataset preparation analysis. However, aiming reproducible workflow code conform principle modularity: subsequent step analysis depend running code first. Furthermore, running code creates issues bandwidth, previous examples direct downloads. address modularity write dataset disk plain-text format. way subsequent step analysis can access dataset locally. address bandwidth concerns, devise method checking see dataset already downloaded skip download, possible, avoid accessing Project Gutenberg server unnecessarily.write data frame disk export standard plain-text format two-dimensional datasets: CSV file (comma-separated value). CSV structure dataset look like :first line contains names columns subsequent lines observations. Data points contain commas (e.g. “Shaw, Bernard”) quoted avoid misinterpreting commas deliminators data. write dataset disk use reader::write_csv() function.avoid downloading dataset already resides disk, let’s implement similar strategy one used direct downloads (get_zip_data()). ’ve incorporated code sampling downloading data particular subject Project Gutenberg control statement check dataset file already exists function named get_gutenberg_subject(). Take look function .Adding function function script functions/acquire_functions.R, can now source function analysis/1_acquire_data.Rmd script download multiple subjects store disk file.Let’s download American Literature now (LCC code “PQ”).Applying function English American Literature datasets, data directory structure now looks like :sum, subsection provided overview acquiring data web service APIs R packages. took closer look gutenbergr package provides programmatic access works available Project Gutenberg. Working package interfaces requires knowledge R including loading/ installing packages, working vectors data frames, exporting data R session. touched programming concepts also outlined method create reproducible workflow.","code":"\ninstall.packages(\"gutenbergr\")  # install `gutenbergr` package\nlibrary(gutenbergr)  # load the `gutenbergr` package\n# Load `pacman`. If not installed, install then load.\nif (!require(\"pacman\", character.only = TRUE)) {\n    install.packages(\"pacman\")\n    library(\"pacman\", character.only = TRUE)\n}\n# Script-specific options or packages\npacman::p_load(tidyverse, gutenbergr)\nglimpse(gutenberg_metadata)  # summarize text meta-data\n#> Rows: 51,997\n#> Columns: 8\n#> $ gutenberg_id        <int> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …\n#> $ title               <chr> NA, \"The Declaration of Independence of the United…\n#> $ author              <chr> NA, \"Jefferson, Thomas\", \"United States\", \"Kennedy…\n#> $ gutenberg_author_id <int> NA, 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7,…\n#> $ language            <chr> \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n#> $ gutenberg_bookshelf <chr> NA, \"United States Law/American Revolutionary War/…\n#> $ rights              <chr> \"Public domain in the USA.\", \"Public domain in the…\n#> $ has_text            <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\nglimpse(gutenberg_authors)  # summarize authors meta-data\n#> Rows: 16,236\n#> Columns: 7\n#> $ gutenberg_author_id <int> 1, 3, 4, 5, 7, 8, 9, 10, 12, 14, 16, 17, 18, 20, 2…\n#> $ author              <chr> \"United States\", \"Lincoln, Abraham\", \"Henry, Patri…\n#> $ alias               <chr> NA, NA, NA, NA, \"Dodgson, Charles Lutwidge\", NA, \"…\n#> $ birthdate           <int> NA, 1809, 1736, NA, 1832, NA, 1819, 1860, 1805, 17…\n#> $ deathdate           <int> NA, 1865, 1799, NA, 1898, NA, 1891, 1937, 1844, 18…\n#> $ wikipedia           <chr> NA, \"http://en.wikipedia.org/wiki/Abraham_Lincoln\"…\n#> $ aliases             <chr> NA, \"United States President (1861-1865)/Lincoln, …\nglimpse(gutenberg_subjects)  # summarize subjects meta-data\n#> Rows: 140,173\n#> Columns: 3\n#> $ gutenberg_id <int> 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, …\n#> $ subject_type <chr> \"lcc\", \"lcsh\", \"lcsh\", \"lcc\", \"lcc\", \"lcsh\", \"lcsh\", \"lcc…\n#> $ subject      <chr> \"E201\", \"United States. Declaration of Independence\", \"Un…\nids <- 1:5  # integer vector of values 1 to 5\nids\n#> [1] 1 2 3 4 5\nworks_sample <- gutenberg_download(gutenberg_id = ids)  # download works with `gutenberg_id` 1-5\nglimpse(works_sample)  # summarize `works` dataset\n#> Rows: 2,959\n#> Columns: 2\n#> $ gutenberg_id <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#> $ text         <chr> \"December, 1971  [Etext #1]\", \"\", \"\", \"The Project Gutenb…\nnames(gutenberg_metadata)  # print the column names of the `gutenberg_metadata` data frame\n#> [1] \"gutenberg_id\"        \"title\"               \"author\"             \n#> [4] \"gutenberg_author_id\" \"language\"            \"gutenberg_bookshelf\"\n#> [7] \"rights\"              \"has_text\"\n# download works with `gutenberg_id` 1-5 including `title` and `author` as\n# attributes\nworks_sample <- gutenberg_download(gutenberg_id = ids, meta_fields = c(\"title\", \"author\"))\nglimpse(works_sample)  # summarize dataset\n#> Rows: 2,959\n#> Columns: 4\n#> $ gutenberg_id <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#> $ text         <chr> \"December, 1971  [Etext #1]\", \"\", \"\", \"The Project Gutenb…\n#> $ title        <chr> \"The Declaration of Independence of the United States of …\n#> $ author       <chr> \"Jefferson, Thomas\", \"Jefferson, Thomas\", \"Jefferson, Tho…\n# filter for only English literature\nids <- filter(gutenberg_subjects, subject_type == \"lcc\", subject == \"PR\")\nglimpse(ids)\n#> Rows: 7,100\n#> Columns: 3\n#> $ gutenberg_id <int> 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43, 46, 58, 60, 8…\n#> $ subject_type <chr> \"lcc\", \"lcc\", \"lcc\", \"lcc\", \"lcc\", \"lcc\", \"lcc\", \"lcc\", \"…\n#> $ subject      <chr> \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR…\n# Filter for only those works that have text\nids_has_text <- \n  filter(gutenberg_metadata, \n         gutenberg_id %in% ids$gutenberg_id, \n         has_text == TRUE)\nglimpse(ids_has_text)\n#> Rows: 6,724\n#> Columns: 8\n#> $ gutenberg_id        <int> 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43, 46, 58…\n#> $ title               <chr> \"Alice's Adventures in Wonderland\", \"Through the L…\n#> $ author              <chr> \"Carroll, Lewis\", \"Carroll, Lewis\", \"Carroll, Lewi…\n#> $ gutenberg_author_id <int> 7, 7, 7, 10, 17, 17, 23, 30, 30, 35, 35, 37, 17, 4…\n#> $ language            <chr> \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n#> $ gutenberg_bookshelf <chr> \"Children's Literature\", \"Children's Literature/Be…\n#> $ rights              <chr> \"Public domain in the USA.\", \"Public domain in the…\n#> $ has_text            <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\nset.seed(123)  # make the sampling reproducible\nids_sample <- slice_sample(ids_has_text, n = 10)  # sample 10 works\nglimpse(ids_sample)  # summarize the dataset\n#> Rows: 10\n#> Columns: 8\n#> $ gutenberg_id        <int> 10564, 10784, 9316, 1540, 24450, 13821, 7595, 3818…\n#> $ title               <chr> \"Fairy Gold\\nShip's Company, Part 4.\", \"Sentence D…\n#> $ author              <chr> \"Jacobs, W. W. (William Wymark)\", \"Jacobs, W. W. (…\n#> $ gutenberg_author_id <int> 1865, 1865, 2364, 65, 999, 2685, 761, 1317, 3564, …\n#> $ language            <chr> \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n#> $ gutenberg_bookshelf <chr> NA, NA, NA, NA, \"Adventure\", \"Fantasy\", NA, NA, NA…\n#> $ rights              <chr> \"Public domain in the USA.\", \"Public domain in the…\n#> $ has_text            <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\nworks_pr <- gutenberg_download(gutenberg_id = ids_sample$gutenberg_id, meta_fields = c(\"author\",\n    \"title\"))\nglimpse(works_pr)  # summarize the dataset\n#> Rows: 47,515\n#> Columns: 4\n#> $ gutenberg_id <int> 1540, 1540, 1540, 1540, 1540, 1540, 1540, 1540, 1540, 154…\n#> $ text         <chr> \"cover \", \"\", \"\", \"\", \"THE TEMPEST\", \"\", \"\", \"\", \"by Will…\n#> $ author       <chr> \"Shakespeare, William\", \"Shakespeare, William\", \"Shakespe…\n#> $ title        <chr> \"The Tempest\", \"The Tempest\", \"The Tempest\", \"The Tempest…\nworks_pr %>%\n    head() %>%\n    format_csv() %>%\n    cat()\n#> gutenberg_id,text,author,title\n#> 1540,cover ,\"Shakespeare, William\",The Tempest\n#> 1540,,\"Shakespeare, William\",The Tempest\n#> 1540,,\"Shakespeare, William\",The Tempest\n#> 1540,,\"Shakespeare, William\",The Tempest\n#> 1540,THE TEMPEST,\"Shakespeare, William\",The Tempest\n#> 1540,,\"Shakespeare, William\",The Tempest\nwrite_csv(works_pr, file = \"../data/original/gutenberg_works_pr.csv\")\nget_gutenberg_subject <- function(subject, target_file, sample_size = 10) {\n  # Function: to download texts from Project Gutenberg with \n  # a specific LCC subject and write the data to disk.\n  \n  pacman::p_load(tidyverse, gutenbergr) # install/load necessary packages\n  \n  # Check to see if the data already exists\n  if(!file.exists(target_file)) { # if data does not exist, download and write\n    target_dir <- dirname(target_file) # generate target directory for the .csv file\n    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE) # create target data directory\n    cat(\"Downloading data... \\n\") # print status message\n    # Select all records with a particular LCC subject\n    ids <- \n      filter(gutenberg_subjects, \n             subject_type == \"lcc\", subject == subject) # select subject\n    # Select only those records with plain text available\n    set.seed(123) # make the sampling reproducible\n    ids_sample <- \n      filter(gutenberg_metadata, \n             gutenberg_id %in% ids$gutenberg_id, # select ids in both data frames \n             has_text == TRUE) %>% # select those ids that have text\n      slice_sample(n = sample_size) # sample N works \n    # Download sample with associated `author` and `title` metadata\n    works_sample <- \n      gutenberg_download(gutenberg_id = ids_sample$gutenberg_id, \n                         meta_fields = c(\"author\", \"title\"))\n    # Write the dataset to disk in .csv format\n    write_csv(works_sample, file = target_file)\n    cat(\"Data downloaded! \\n\") # print status message\n  } else { # if data exists, don't download it again\n    cat(\"Data already exists \\n\") # print status message\n  }\n}\n# Download Project Gutenberg text for subject 'PQ' (American Literature) and\n# then write this dataset to disk in .csv format\nget_gutenberg_subject(subject = \"PQ\", target_file = \"../data/original/gutenberg/works_pq.csv\")data\n├── derived\n└── original\n    ├── gutenberg\n    │   ├── works_pq.csv\n    │   └── works_pr.csv\n    ├── sbc\n    │   ├── meta-data\n    │   └── transcriptions\n    └── scs\n        ├── README\n        ├── discourse\n        ├── disfluency\n        ├── documentation\n        ├── tagged\n        ├── timed-transcript\n        └── transcript"},{"path":"acquire-data.html","id":"authentication","chapter":"5 Acquire data","heading":"5.2.2 Authentication","text":"","code":""},{"path":"acquire-data.html","id":"web-scraping","chapter":"5 Acquire data","heading":"5.3 Web scraping","text":"many resources available direct downloads repositories individual sites R package interfaces web resources APIs, resources relatively limited amount public-facing textual data recorded web. case want acquire data webpages R can used access web programmatically process known web scraping. complexity web scrapes can vary general requires advanced knowledge R well structure language web: HTML (Hypertext Markup Language).","code":""},{"path":"acquire-data.html","id":"a-toy-example","chapter":"5 Acquire data","heading":"5.3.1 A toy example","text":"HTML cousin XML organizes web documents hierarchical format read browser navigate web. Take example toy webpage created demonstration Figure 5.1.\nFigure 5.1: Example web page.\nfile accessed browser render webpage test.html plain-text format looks like :element file delineated opening closing tag, <head><\/head>. Tags nested within tags create structural hierarchy. Tags can take class id labels distinguish tags often contain attributes dictate tag behave rendered visually browser. example, two <div> tags toy example: one label class = \"intro\" class = \"conc\". <div> tags often used separate sections webpage may require special visual formatting. <> tag, hand, creates web link. part tag’s function, requires attribute href= web protocol –case link email address mailto:francojc@wfu.edu. often , however, href= contains URL (Uniform Resource Locator). working example might look like : <href=\"https://francojc.github.io/\">homepage<\/>.aim web scrape download HTML file, parse document structure, extract elements containing relevant information wish capture. Let’s attempt extract information toy example. need rvest package. First, install/load package, , read parse HTML character vector named web_file assigning result html.read_html() parses raw HTML object class xml_document. summary output shows tags HTML structure parsed ‘elements.’ tag elements can accessed using html_elements() function specifying tag isolate.Notice html_elements(\"div\") returned div tags. isolate one tags class, add class name tag separating ..Great. Now say want drill isolate subordinate <p> nodes. can add p node filter.extract text contained within node use html_text() function.result character vector two elements corresponding text contained <p> tag. paying close attention might noticed second element vector includes extra whitespace period. trim leading trailing whitespace text can add trim = TRUE argument html_text().work organize text format want store write results disk. Let’s leave writing data disk later chapter. now keep focus working rvest acquire data html documents working practical example.","code":"\n<html>\n  <head>\n    <title>My website<\/title>\n  <\/head>\n  <body>\n    <div class=\"intro\">\n      <p>Welcome!<\/p>\n      <p>This is my first website. <\/p>\n    <\/div>\n    <table>\n      <tr>\n        <td>Contact me:<\/td>\n        <td>\n          <a href=\"mailto:francojc@wfu.edu\">francojc@wfu.edu<\/a>\n        <\/td>\n      <\/tr>\n    <\/table>\n    <div class=\"conc\">\n      <p>Good-bye!<\/p>\n    <\/div>\n  <\/body>\n<\/html>\npacman::p_load(rvest)  # install/ load `rvest`\n\nhtml <- read_html(web_file)  # read raw html and parse to xml\nhtml\n#> {html_document}\n#> <html>\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n#> [2] <body>\\n    <div class=\"intro\">\\n      <p>Welcome!<\/p>\\n      <p>This is  ...\nhtml %>%\n    html_elements(\"div\")\n#> {xml_nodeset (2)}\n#> [1] <div class=\"intro\">\\n      <p>Welcome!<\/p>\\n      <p>This is my first web ...\n#> [2] <div class=\"conc\">\\n      <p>Good-bye!<\/p>\\n    <\/div>\nhtml %>%\n    html_elements(\"div.intro\")\n#> {xml_nodeset (1)}\n#> [1] <div class=\"intro\">\\n      <p>Welcome!<\/p>\\n      <p>This is my first web ...\nhtml %>%\n    html_elements(\"div.intro p\")\n#> {xml_nodeset (2)}\n#> [1] <p>Welcome!<\/p>\n#> [2] <p>This is my first website. <\/p>\nhtml %>%\n    html_elements(\"div.intro p\") %>%\n    html_text()\n#> [1] \"Welcome!\"                   \"This is my first website. \"\nhtml %>%\n    html_elements(\"div.intro p\") %>%\n    html_text(trim = TRUE)\n#> [1] \"Welcome!\"                  \"This is my first website.\""},{"path":"acquire-data.html","id":"a-practical-example","chapter":"5 Acquire data","heading":"5.3.2 A practical example","text":"basic understanding HTML use rvest package, let’s turn realistic example. Say want acquire lyrics online music website database last.fm. first step web scrape investigate site page(s) want scrape ascertain licensing restrictions. Many, websites, include plain text file robots.txt root main URL. file declares webpages ‘robot’ (including web scraping scripts) can access. can use robotstxt package find URLs accessible.17The next step includes identifying URL want target exploring structure HTML document. Take following webpage identified, seen Figure 5.2.\nFigure 5.2: Lyrics page last.fm\ntoy example, first want feed HTML web address read_html() function parse tags elements. assign result html.point captured parsed raw HTML assigning object named html. next step identify html elements contain information want extract page. helpful use browser inspect specific elements webpage. browser equipped command can enable hovering mouse element page want target using right click select “Inspect” (Chrome) “Inspect Element” (Safari, Brave). split browser window vertical horizontally showing raw HTML underlying webpage.\nFigure 5.3: Using “Inspect Element” command explore raw html.\nFigure 5.4 see element want target contained within <><\/> tag. Now tag common don’t want extract every use class header-new-crumb specify want artist name. Using convention described toy example, can isolate artist lyrics page.can extract text html_text().Let’s extract song title way.Now inspect HTML lyrics page, notice lyrics contained <p><\/p> tags class lyrics-paragraph.\nFigure 5.4: Using “Inspect Element” command explore raw html.\nSince multiple elements want extract, need use html_elements() function instead html_element() targets one element.point, isolated extracted artist, song, lyrics webpage. elements stored character vectors R session. complete task need write data disk plain text. eye towards tidy dataset, ideal format store data CSV file column corresponds one elements scrape row observation. CSV file tabular format can write data disk let’s coerce data tabular format. use tibble() function streamline data frame creation.18 Feeding vectors artist, song, lyrics arguments tibble() creates tabular format looking .Notice seven rows data frame, one corresponding paragraph lyrics. R bias towards working vectors length. vectors (artist, song) replicated, recycled, length longest vector lyrics, length seven.good documentation let’s add object lyrics_url data frame, contains actual web link page, assign result song_lyrics.final step write data disk. use write_csv() function.","code":"\npacman::p_load(robotstxt)  # load/ install `robotstxt`\n\npaths_allowed(paths = \"https://www.last.fm/\")  # check permissions\n#> [1] TRUE\n# read and parse html as an xml object\nlyrics_url <- \"https://www.last.fm/music/Radiohead/_/Karma+Police/+lyrics\"\nhtml <- read_html(lyrics_url)  # read raw html and parse to xml\nhtml#> {html_document}\n#> <html lang=\"en\" class=\"\n#>         no-js\n#>         playbar-masthead-release-shim\n#>         youtube-provider-not-ready\n#>     \">\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n#> [2] <body>\\n<div id=\"initial-tealium-data\" data-require=\"tracking/tealium-uta ...\nhtml %>%\n    html_element(\"a.header-new-crumb\")\n#> {html_node}\n#> <a class=\"header-new-crumb\" itemprop=\"url\" href=\"/music/Radiohead\">\n#> [1] <span itemprop=\"name\">Radiohead<\/span>\nartist <- html %>%\n    html_element(\"a.header-new-crumb\") %>%\n    html_text()\nartist\n#> [1] \"Radiohead\"\nsong <- html %>%\n    html_element(\"h1.header-new-title\") %>%\n    html_text()\nsong\n#> [1] \"Karma Police\"\nlyrics <- html %>%\n    html_elements(\"p.lyrics-paragraph\") %>%\n    html_text()\nlyrics\n#> [1] \"Karma policeArrest this manHe talks in mathsHe buzzes like a fridgeHe's like a detuned radio\"      \n#> [2] \"Karma policeArrest this girlHer Hitler hairdoIs making me feel illAnd we have crashed her party\"   \n#> [3] \"This is what you'll getThis is what you'll getThis is what you'll getWhen you mess with us\"        \n#> [4] \"Karma policeI've given all I canIt's not enoughI've given all I canBut we're still on the payroll\" \n#> [5] \"This is what you'll getThis is what you'll getThis is what you'll getWhen you mess with us\"        \n#> [6] \"For a minute thereI lost myself, I lost myselfPhew, for a minute thereI lost myself, I lost myself\"\n#> [7] \"For a minute thereI lost myself, I lost myselfPhew, for a minute thereI lost myself, I lost myself\"\ntibble(artist, song, lyrics) %>%\n    glimpse()\n#> Rows: 7\n#> Columns: 3\n#> $ artist <chr> \"Radiohead\", \"Radiohead\", \"Radiohead\", \"Radiohead\", \"Radiohead\"…\n#> $ song   <chr> \"Karma Police\", \"Karma Police\", \"Karma Police\", \"Karma Police\",…\n#> $ lyrics <chr> \"Karma policeArrest this manHe talks in mathsHe buzzes like a f…\nsong_lyrics <- tibble(artist, song, lyrics, lyrics_url)\nwrite_csv(x = song_lyrics, path = \"../data/original/lyrics.csv\")"},{"path":"acquire-data.html","id":"scaling-up","chapter":"5 Acquire data","heading":"5.3.3 Scaling up","text":"point may think, ‘Great, can download data single page, downloading multiple pages?’ Good question. ’s really strength programming approach takes hold. Extracting information multiple pages fundamentally different working single page. However, require sophisticated understanding web R coding strategies, particular iteration.first step create couple functions make possible efficiently reuse code developed far:get_lyrics function wraps code scraping single lyrics webpage last.fm.get_lyrics function includes code developed previously, also includes: (1) output messages (cat()), (2) processing pause (Sys.sleep()), (3) code manage opening closing web connections (url() close()).write_content writes webscraped data local machine, including functionality create necessary directory structure target file path choose.just two functions, can take lyrics URL last.fm scrape write data disk like .Now manually search copy URLs run function pipeline. fine just particular URLs wanted scrape. want , say, scrape set lyrics grouped genre. probably want programmatic approach. good news can leverage understanding webscraping scrape last.fm harvest information needed create store links songs genre. can pass links pipeline, similar previous one, scrape lyrics many songs store results files grouped genre.Get lyrics top songs specific genre.\nFigure 5.5: Genre page last.fm\nDiving particular genre, ‘rock’ example, get listing top tracks genre.\nFigure 5.6: Tracks genre list page last.fm\nInspecting HTML elements track names Figure 5.6, can see relative URL found track. case, ‘Smells Like Teen Spirit’ Nirvana highlighted inspector. follow link track page lyrics track, notice relative URL track listings page unique information. web domain https://www.last.fm post-pended /+lyrics missing.can put together function gets track listing last.fm genre, scrapes relative URLs tracks, creates full absolute URL lyrics page.function, need identify verbatim way last.fm lists genres. Rock, rock Hip Hop, hip+hop.","code":"\nget_lyrics <- function(lyrics_url) {\n    # Function: Scrape last.fm lyrics page for: artist, song, and lyrics from a\n    # provided content link.  Return as a tibble/data.frame\n\n    cat(\"Scraping song lyrics from:\", lyrics_url, \"\\n\")\n\n    pacman::p_load(tidyverse, rvest)  # install/ load package(s)\n\n    url <- url(lyrics_url, \"rb\")  # open url connection \n    html <- read_html(url)  # read and parse html as an xml object\n    close(url)  # close url connection\n\n    artist <- html %>%\n        html_element(\"a.header-new-crumb\") %>%\n        html_text()\n\n    song <- html %>%\n        html_element(\"h1.header-new-title\") %>%\n        html_text()\n\n    lyrics <- html %>%\n        html_elements(\"p.lyrics-paragraph\") %>%\n        html_text()\n\n    cat(\"...one moment \")\n\n    Sys.sleep(1)  # sleep for 1 second to reduce server load\n\n    song_lyrics <- tibble(artist, song, lyrics, lyrics_url)\n\n    cat(\"... done! \\n\")\n\n    return(song_lyrics)\n}\nwrite_content <- function(content, target_file) {\n    # Function: Write the tibble content to disk. Create the directory if it\n    # does not already exist.\n\n    pacman::p_load(tidyverse)  # install/ load packages\n\n    target_dir <- dirname(target_file)  # identify target file directory structure\n    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE)  # create directory\n    write_csv(content, target_file)  # write csv file to target location\n\n    cat(\"Content written to disk!\\n\")\n}\nlyrics_url <- \"https://www.last.fm/music/Pixies/_/Where+Is+My+Mind%3F/+lyrics\"\n\nlyrics_url %>%\n    get_lyrics() %>%\n    write_content(target_file = \"../data/original/lastfm/lyrics.csv\")data/original/lastfm/\n└── lyrics.csv\nget_genre_lyrics_urls <- function(last_fm_genre) {\n  # Function: Scrapes a given last.fm genre title for top tracks in\n  # that genre and then creates links to the lyrics pages for these tracks\n  \n  cat(\"Scraping top songs from:\", last_fm_genre, \"genre: \\n\")\n  \n  pacman::p_load(tidyverse, rvest) # install/ load packages\n  \n  # create web url for the genre listing page\n  genre_listing_url <- \n    paste0(\"https://www.last.fm/tag/\", last_fm_genre, \"/tracks\") \n  \n  genre_lyrics_urls <- \n    read_html(genre_listing_url) %>% # read raw html and parse to xml\n    html_elements(\"td.chartlist-name a\") %>% # isolate the track elements\n    html_attr(\"href\") %>% # extract the href attribute\n    paste0(\"https://www.last.fm\", ., \"/+lyrics\") # join the domain, relative artist path, and the post-pended /+lyrics to create an absolute URL\n  \n  return(genre_lyrics_urls)\n}\nget_genre_lyrics_urls(\"hip+hop\") %>%  # get urls for top hip hop tracks\n  head(n = 10) # only display 10 tracks#> Scraping top songs from: hip+hop genre:\n#>  [1] \"https://www.last.fm/music/Juzhin/_/Charlie+Conscience+(feat.+MMAIO)/+lyrics\"\n#>  [2] \"https://www.last.fm/music/Juzhin/_/Railways/+lyrics\"                        \n#>  [3] \"https://www.last.fm/music/Juzhin/_/Coming+Down/+lyrics\"                     \n#>  [4] \"https://www.last.fm/music/Juzhin/_/Tupona/+lyrics\"                          \n#>  [5] \"https://www.last.fm/music/Juzhin/_/Sakhalin/+lyrics\"                        \n#>  [6] \"https://www.last.fm/music/Juzhin/_/3+Simple+Minutes/+lyrics\"                \n#>  [7] \"https://www.last.fm/music/Juzhin/_/Lost+Sense/+lyrics\"                      \n#>  [8] \"https://www.last.fm/music/Juzhin/_/Wonderful/+lyrics\"                       \n#>  [9] \"https://www.last.fm/music/Gina+Moryson/_/Vanilla+Smoothy+(Live)/+lyrics\"    \n#> [10] \"https://www.last.fm/music/Juzhin/_/Flunk-Down+(Juzhin+Remix)/+lyrics\""},{"path":"acquire-data.html","id":"documentation-1","chapter":"5 Acquire data","heading":"5.4 Documentation","text":"","code":""},{"path":"curate-data.html","id":"curate-data","chapter":"6 Curate data","heading":"6 Curate data","text":"\nINCOMPLETE DRAFT\nhardest bit information extract first piece.―–Robert Ferrigno\nchapter learn:\n\ngoals textbook\n\nreasoning using R programming language\n\nimportant text conventions employed textbook\nOverview…","code":""},{"path":"curate-data.html","id":"structured","chapter":"6 Curate data","heading":"6.1 Structured","text":"…","code":""},{"path":"curate-data.html","id":"semi-structured","chapter":"6 Curate data","heading":"6.2 Semi-structured","text":"…","code":""},{"path":"curate-data.html","id":"unstructured","chapter":"6 Curate data","heading":"6.3 Unstructured","text":"…","code":""},{"path":"curate-data.html","id":"documentation-2","chapter":"6 Curate data","heading":"6.4 Documentation","text":"…Notes:Data Organization Spreadsheets (Broman & Woo, 2018). Although based spreadsheets, many best practices discussed apply good data organization regardless technology.","code":""},{"path":"transform-data.html","id":"transform-data","chapter":"7 Transform data","heading":"7 Transform data","text":"\nINCOMPLETE DRAFT\n…\nchapter learn:\n\n…\n\n…\nOverview…","code":""},{"path":"transform-data.html","id":"normalize","chapter":"7 Transform data","heading":"7.1 Normalize","text":"…","code":""},{"path":"transform-data.html","id":"recode","chapter":"7 Transform data","heading":"7.2 Recode","text":"…","code":""},{"path":"transform-data.html","id":"generate","chapter":"7 Transform data","heading":"7.3 Generate","text":"…","code":""},{"path":"transform-data.html","id":"merge","chapter":"7 Transform data","heading":"7.4 Merge","text":"…","code":""},{"path":"transform-data.html","id":"documentation-3","chapter":"7 Transform data","heading":"7.5 Documentation","text":"…Notes:Cover Corpus Document-Term Matrices (DTM)s","code":""},{"path":"modeling-overview.html","id":"modeling-overview","chapter":"Overview","heading":"Overview","text":"","code":""},{"path":"inference.html","id":"inference","chapter":"8 Inference","heading":"8 Inference","text":"\nINCOMPLETE DRAFT\n…\nchapter learn:\n\ngoals textbook\n\nreasoning using R programming language\n\nimportant text conventions employed textbook\n","code":""},{"path":"inference.html","id":"section","chapter":"8 Inference","heading":"8.1 …","text":"text","code":""},{"path":"inference.html","id":"inference-packages","chapter":"8 Inference","heading":"8.1.1 Packages","text":"","code":"\n# Packages pacman::p_load(infer)"},{"path":"prediction.html","id":"prediction","chapter":"9 Prediction","heading":"9 Prediction","text":"\nINCOMPLETE DRAFT\n…\nchapter learn:\n\ngoals textbook\n\nreasoning using R programming language\n\nimportant text conventions employed textbook\ntext","code":"\n# Packages pacman::p_load(caret)"},{"path":"prediction.html","id":"section-1","chapter":"9 Prediction","heading":"9.1 …","text":"textIn Table () see top five terms class breaking messages terms counting frequencies.Figure () Let’s consider results hypothetical model text classification SMS dataset introduced subsection.accuracy (measure overall correct predictions)precision (measure quality predictions)\nPercentage predicted ‘ham’ messages correct\nPercentage predicted ‘ham’ messages correctrecall (measure quantity predictions)\nPercentage actual ‘ham’ messages correct\nPercentage actual ‘ham’ messages correctF1-score (summarizes balance precision recall)","code":"\nsms_f <- sms %>%\n    unnest_tokens(terms, message, token = \"regex\", pattern = \" \") %>%\n    count(sms_type, terms)\n\nsms_f %>%\n    arrange(sms_type, desc(n)) %>%\n    group_by(sms_type) %>%\n    slice_head(n = 5) %>%\n    select(sms_type, terms, frequency = n) %>%\n    kable(booktabs = TRUE, caption = \"Top five most frequent terms for 'ham' and 'spam'.\")\nlibrary(tidymodels)\n\nsms_split <- initial_split(sms, strata = \"sms_type\")\ntrain_data <- training(sms_split)\ntest_data <- testing(sms_split)\n\nlibrary(textrecipes)"},{"path":"prediction.html","id":"summary-4","chapter":"9 Prediction","heading":"9.2 Summary","text":"…","code":""},{"path":"exploration.html","id":"exploration","chapter":"10 Exploration","heading":"10 Exploration","text":"\nINCOMPLETE DRAFT\n…\nchapter learn:\n\ngoals textbook\n\nreasoning using R programming language\n\nimportant text conventions employed textbook\n","code":""},{"path":"exploration.html","id":"section-2","chapter":"10 Exploration","heading":"10.1 …","text":"Idea….Get Meditations Marcus Aurelius gutenbergr (gutenberg_id == 2680).12 books believed chronological order. may interesting look whether book-level similarities/ differences might suggest books similar others.sentiment analysis interesting well –books show similar/ different patterns terms sentiment?Topic modeling uncover themes books?","code":""},{"path":"exploration.html","id":"exploration-packages","chapter":"10 Exploration","heading":"10.1.1 Packages","text":"","code":"\n# Packages pacman::p_load(quanteda)"},{"path":"section-3.html","id":"section-3","chapter":"A …","heading":"A …","text":"","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
