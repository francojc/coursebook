% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={LIN 380 Coursebook},
  pdfauthor={Jerid Francom},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% Make links footnotes instead of hotlinks:
\DeclareRobustCommand{\href}[2]{#2\footnote{\url{#1}}}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% default
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{float}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

% added
\usepackage{longtable}
\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\ifxetex
  \usepackage{letltxmacro}
  \setlength{\XeTeXLinkMargin}{2pt}
  \LetLtxMacro\SavedIncludeGraphics\includegraphics
  \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
    \IncludeGraphicsAux{#1}%
  }%
  \newcommand*{\IncludeGraphicsAux}[2]{%
    \XeTeXLinkBox{%
      \SavedIncludeGraphics#1{#2}%
    }%
  }%
\fi

\newenvironment{rmdblock}[1]
  {\begin{shaded*}
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.5\height}[0pt][0pt]{
      {\setkeys{Gin}{width=2em,keepaspectratio}\includegraphics{assets/images/#1}}
    }
  }
  \item
  }
  {
  \end{itemize}
  \end{shaded*}
  }
\newenvironment{rmdkey}
  {\begin{rmdblock}{key}}
  {\end{rmdblock}}
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\newenvironment{rmdactivity}
  {\begin{rmdblock}{code}}
  {\end{rmdblock}}
\newenvironment{rmdstudy}
  {\begin{rmdblock}{paper}}
  {\end{rmdblock}}
\newenvironment{rmdquestion}
  {\begin{rmdblock}{question}}
  {\end{rmdblock}}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{LIN 380 Coursebook}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Text as Data: An introduction to quantative text analysis and reproducible research in R}
\author{Jerid Francom}
\date{July 04, 2021 (latest version)}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{welcome}{%
\section*{Welcome}\label{welcome}}
\addcontentsline{toc}{section}{Welcome}

INCOMPLETE DRAFT

This is the coursebook to accompany Linguistics 380 ``Language Use and Technology'' at Wake Forest University. The working title for this coursebook is \emph{Text as Data: An Introduction to Quantitative Text Analysis and Reproducible Research in R}. The content is currently under development. Feedback is welcome and can be provided through the \href{https://web.hypothes.is/}{hypothes.is} service. A toolbar interface to this service is located on the right sidebar. Note: you will need to register for a free account to make comments and suggestions.

\textbf{Author}

Dr.~Jerid Francom is Associate Professor of Spanish and Linguistics at Wake Forest University. His research interests are focused around quantitative approaches to language variation.

\hypertarget{license}{%
\subsection*{License}\label{license}}
\addcontentsline{toc}{subsection}{License}

This work by \href{https://francojc.github.io/}{Jerid C. Francom} is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States License.

\hypertarget{other}{%
\subsection*{Other}\label{other}}
\addcontentsline{toc}{subsection}{Other}

Icons made from Icon Fonts are licensed by CC BY 3.0

\hypertarget{course}{%
\section*{Course}\label{course}}
\addcontentsline{toc}{section}{Course}

INCOMPLETE DRAFT

\begin{quote}
The journey of a thousand miles begins with one step.
--\href{https://en.wikipedia.org/wiki/Laozi}{Lao Tzu}
\end{quote}

\begin{rmdkey}
The essential questions for this chapter are:

\begin{itemize}
\tightlist
\item
  \ldots{}
\end{itemize}
\end{rmdkey}

This chapter aims to provide a brief summary of current research trends that form the context for the rationale for this textbook. It also provides instructors and students an overview of the purpose and approach of the textbook. It will also include a description of the main components of each section and chapter and provide a guide to conventions used in the book and resources available.

\hypertarget{rationale}{%
\subsection{Rationale}\label{rationale}}

In recent years there has been a growing buzz around the term `Data Science' and related terms; data analytics, data mining, \emph{etc}. In a nutshell data science is the process by which an investigator leverages statistical methods and computational power to uncover insight from large datasets. Driven in large part by the increase in computing power available to the average individual and the increasing amount of electronic data that is now available through the internet, interest in data science has expanded to virtually all fields in academia and areas in the public sector. Data scientists are in high demand and this trend is expected to continue into the foreseeable future.

This coursebook is an introduction to the fundamental concepts and practical programming skills from Data Science that are increasingly employed in a variety of language-centered fields and sub-fields. It is geared towards advanced undergraduates and graduate students of linguistics and related fields. As quantitative research skills are quickly becoming a core aspect of many language programs, this coursebook aims to provide a fundamental understanding of theoretical concepts, programming skills, and statistical methods for doing quantitative text analysis.

\hypertarget{learning-goals}{%
\subsection{Learning goals}\label{learning-goals}}

This course you will:

\textbf{Data Literacy (DL):} learn to interpret, assess, and contextualize findings based on data.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ability to understand and apply data analysis to derive insight from data
\item
  ability to understand and apply data knowledge and skills across linguistic and language-related disciplines
\end{enumerate}

\textbf{Research Skills (RS):} learn to conduct original research (design, implementation, interpretation, and communication).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  identify an applicable area of investigation in a linguistic or language-related field
\item
  develop a viable research question or hypothesis
\item
  assess, acquire, and document data
\item
  curate and transform data for analysis
\item
  select and apply relevant analysis method
\item
  interpret and communicate findings
\end{enumerate}

\textbf{Programming Skills (PS):} learn to produce your own research and work collaboratively with others.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  demonstrate proficiency to implement research with R (RD points 3-5)
\item
  demonstrate ability to produce collaborative and reproducible research using R, RStudio, and GitHub
\end{enumerate}

In each chapter of this coursebook specific learning objectives will be specified that target these learning outcomes so it is clear what we are doing and why were are doing it.

\hypertarget{approach}{%
\subsection{Approach}\label{approach}}

Many textbooks on doing `Data Science', even those that have a domain-centric approach, such as text analysis, tend to focus on the basic `tidy' approach, seen in Figure \ref{fig:tidy-workflow-img} from \citet{Wickham2017}. However these resources tend to underrepresent the importance of leading with a research question. A big part, or perhaps the biggest part of doing quantitative research, and research in general is what is the question to be addressed. Then comes how to orient the research approach to best address this question (or questions). Then we move on to matching data sources, organizing data, modeling data, and finally reporting findings.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/01-course/standard-tidy-approach} 

}

\caption{Workflow diagram from R for Data Science.}\label{fig:tidy-workflow-img}
\end{figure}

I think a central advantage to this coursebook for language researchers is to thread the project goals from a conceptual point of view without technical implementation in mind first.

Then, after a general idea about what the data should look like, how it should be analyzed, and how the analysis will contribute to knowledge in the field, we can move towards implementing these preliminary formulations in R code. In essence this approach reflects \href{https://en.wikipedia.org/wiki/Separation_of_content_and_presentation}{the classic separation between content and format} --the content of our research should precede the format it should or will take.

This coursebook is divided into four parts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In ``Foundations'', an environmental survey of quantitative research across disciplines and orient language-based research is provided. (Provide historical and research context for quantitative text analysis)
\item
  ``Orientation'' aims to build your knowledge about what data is, how text is organized into datasets, what role statistics play in quantitative research and the types of statistical approaches that are commonly found in text analysis research, and finally how to develop a research question and a research blueprint for conducting a quantitative text analysis research project. (Develop an understanding of what quantitative research is and how it is approached)
\item
  ``Preparation'' covers a variety of implementation approaches for each stage for deriving a dataset ready for statistical analysis which includes acquiring, curating, and transforming data. (Dive into coding practices produce data ready for statistical analysis)
\item
  ``Modeling'' elaborates various statistical approaches for data analysis and contextualizes their application in for types of research questions. (Conducting statistical text analysis)
\end{enumerate}

\hypertarget{prerequisites}{%
\subsection{Prerequisites}\label{prerequisites}}

\textbf{TODOS:}

Change this subsection:

\begin{itemize}
\tightlist
\item
  Move the R, RStudio, Packages, Git, GitHub to the \texttt{tadr} package vignettes/ articles
\item
  Make reference here to the \texttt{tadr} package (Coursebook support package)
\item
\end{itemize}

Before we continue, make sure you have all the software you need for this book:

\begin{itemize}
\item
  \textbf{R}: \ldots{}
\item
  \textbf{RStudio}: RStudio is a free and open source integrated development environment (IDE) for R. \ldots{}
\item
  \textbf{R packages}: This coursebook uses a bunch of R packages.
  You can install them all at once by running:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"bookdown"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}
\item
  Coursebook support package
  \href{https://lin380.github.io/tadr/}{\texttt{tadr}} is a support R package and resource site for this coursebook. The package includes data, functions, and interactive R programming tutorials which make use of the \texttt{swirl} package. The website includes programming demonstrations called `Worked' examples and reference to documentation and other resources for doing quantitative research with R.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{)}
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"lin380/tadr"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{programming}{%
\subsection{Programming}\label{programming}}

Reasons to program:

\begin{itemize}
\tightlist
\item
  \emph{Flexibility} Graphical User Interface (GUI) based software is inherently limited. What you see is what you get. If you have another need, you need to find a tool. If another tool does not implement what you think you need, you are out of luck.
\item
  \emph{Transparency} By taking a programming approach to research analysis you make your decisions explicit and leave a breadcrumb trail to everything you do.
\item
  \emph{Reproducibility} What you do will be clearer to you but also allow you to share the process with others (including your future self!). Insight grows much faster when exposed to light. Sharing your research with collaborators or on sites such as GitHub or BitBucket brings makes your work visible and accessible to the world. Reproducibility is gaining momentum and is fueled by programmatic approaches to research.
\end{itemize}

Reasons to use R:

\begin{itemize}
\tightlist
\item
  \emph{One stop shopping} Once known specifically as a statistical programming language, R can now be a round trip tool to acquire, curate, transform, visualize, \emph{and} statistically analyze data. It also allows for robust communication in reports and data and analysis sharing (reproducibility).
\item
  \emph{You are not alone} There is a sizable R programming community, especially in academics. This has two tangible benefits; first, you will likely be able to find user contributed R packages that will satisfy many of the more sophisticated programming goals you will have and second, you will be able to get answers to any of your programming questions on popular sites like StackOverflow.
\item
  \emph{RStudio} RStudio is the envy of many other programmers. It is a very capable interface to R and provides convenient access powerful tools to allow you to be a more efficient and productive R programmer.
\end{itemize}

\hypertarget{conventions}{%
\subsection{Conventions}\label{conventions}}

This coursebook is about the concepts for understanding and the techniques for doing quantitative text analysis with R. Therefore there will be an intermingling of prose and code presented. As such, an attempt to establish consistent conventions throughout the text has been made to signal reader's attention as appropriate. As we explore concepts, R code itself will be incorporated into the text. This may be a unique textbook compared to others you have seen. It has been created using R itself --specifically using an R language package called \texttt{bookdown} \citep{R-bookdown}. This R package makes it possible to write, execute (`run'), and display code and results within the text.

For example, the following text block shows actual R code and the results that are generated when running this code. Note that the hashtag \texttt{\#} signals a \textbf{code comment}. The code follows within the same text block and a subsequent text block displays the output of the code.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add 1 plus 1}
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}
\CommentTok{\#\textgreater{} [1] 2}
\end{Highlighting}
\end{Shaded}

Inline code will be used when code blocks are short and the results are not needed for display. For example, the same code as above will sometimes appear as \texttt{1\ +\ 1}.

When necessary meta-description of code will appear. This is particularly relevant for R Markdown documents.

\begin{verbatim}
```{r test-code}
1 + 1
```
\end{verbatim}

In terms of prose, key concepts will be signaled using \textbf{\emph{bold italics}}. Terms that appear in this typeface will also appear in the {[}glossary{]} at the end of the text. Furthermore, there are four pose text blocks that will be used to signal the reader's attention: \emph{key points}, \emph{notes}, \emph{tips}, \emph{questions}, and \emph{warnings}.

Key points summarize the main points to be covered in a chapter or a subsection of the text.

\begin{rmdkey}
In this chapter you will learn:

\begin{itemize}
\tightlist
\item
  the goals of this textbook
\item
  the reasoning for using the R programming language
\item
  important text conventions employed in this textbook
\end{itemize}
\end{rmdkey}

Notes provide a bit more information on the topic or where to find more information.

\begin{rmdnote}
R is more than a powerful statistical programming language, it also can
be used to perform all the necessary steps in a data science project;
including reporting. A relatively new addition to the reporting
capabilities of R is the \texttt{bookdown} package (this textbook was
created using this very package). You can find out more
\href{https://bookdown.org/}{here}.
\end{rmdnote}

Tips are used to signal helpful hints that might otherwise be overlooked.

\begin{rmdtip}
During a the course of an exploratory work session, many R objects are
often created to test ideas. At some point inspecting the workspace
becomes difficult due to the number of objects displayed using
\texttt{ls()}.

To remove all objects from the workspace, use
\texttt{rm(list\ =\ ls())}.
\end{rmdtip}

From time to time there will be points for you to consider and questions to explore.

\begin{rmdquestion}
Consider the objectives in this course: what ways can the knowledge and
skills you will learn benefit you in your academic studies and/ or
professional and personal life?
\end{rmdquestion}

Errors will be an inevitable part of learning, but some errors can be avoided. The text will used the warning text block to highlight typical pitfalls and errors.

\begin{rmdwarning}
Hello world!\\
This is a warning.
\end{rmdwarning}

Although this is not intended to be a in-depth introduction to statistical techniques, mathematical formulas will be included in the text. These formulas will appear either inline \(1 + 1 = 2\) or as block equations.

\begin{equation}
  \hat{c} = \underset{c \in C} {\mathrm{argmax}} ~\hat{P}(c) \prod_i \hat{P}(w_i|c)
  \label{eq:example-formula}
\end{equation}

Data analysis leans heavily on graphical representations. Figures will appear numbered, as in Figure \ref{fig:test-fig}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2) }\CommentTok{\# load graphics package}
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ hp, }\AttributeTok{y =}\NormalTok{ mpg)) }\SpecialCharTok{+} \CommentTok{\# map \textquotesingle{}hp\textquotesingle{} and \textquotesingle{}mpg\textquotesingle{} to coordinate space}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# add points}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# draw linear trend line}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Horsepower"}\NormalTok{, }\CommentTok{\# label x axis}
       \AttributeTok{y =} \StringTok{"Miles per gallon"}\NormalTok{, }\CommentTok{\# label y axis}
       \AttributeTok{title =} \StringTok{"Test plot"}\NormalTok{, }\CommentTok{\# add title}
       \AttributeTok{subtitle =} \StringTok{"From mtcars dataset"}\NormalTok{) }\CommentTok{\# add subtitle}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{01-course_files/figure-latex/test-fig-1} 

}

\caption{Test plot from mtcars dataset}\label{fig:test-fig}
\end{figure}

Tables, such as Table \ref{tab:test-tab} will be numbered separately from figures.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(iris, }\DecValTok{20}\NormalTok{), }\AttributeTok{caption =} \StringTok{"Here is a nice table!"}\NormalTok{, }\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:test-tab}Here is a nice table!}
\centering
\begin{tabular}[t]{rrrrl}
\toprule
Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\
\midrule
5.1 & 3.5 & 1.4 & 0.2 & setosa\\
4.9 & 3.0 & 1.4 & 0.2 & setosa\\
4.7 & 3.2 & 1.3 & 0.2 & setosa\\
4.6 & 3.1 & 1.5 & 0.2 & setosa\\
5.0 & 3.6 & 1.4 & 0.2 & setosa\\
\addlinespace
5.4 & 3.9 & 1.7 & 0.4 & setosa\\
4.6 & 3.4 & 1.4 & 0.3 & setosa\\
5.0 & 3.4 & 1.5 & 0.2 & setosa\\
4.4 & 2.9 & 1.4 & 0.2 & setosa\\
4.9 & 3.1 & 1.5 & 0.1 & setosa\\
\addlinespace
5.4 & 3.7 & 1.5 & 0.2 & setosa\\
4.8 & 3.4 & 1.6 & 0.2 & setosa\\
4.8 & 3.0 & 1.4 & 0.1 & setosa\\
4.3 & 3.0 & 1.1 & 0.1 & setosa\\
5.8 & 4.0 & 1.2 & 0.2 & setosa\\
\addlinespace
5.7 & 4.4 & 1.5 & 0.4 & setosa\\
5.4 & 3.9 & 1.3 & 0.4 & setosa\\
5.1 & 3.5 & 1.4 & 0.3 & setosa\\
5.7 & 3.8 & 1.7 & 0.3 & setosa\\
5.1 & 3.8 & 1.5 & 0.3 & setosa\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{build-information}{%
\subsection{Build information}\label{build-information}}

This coursebook was written in \href{http://bookdown.org/}{bookdown} inside \href{http://www.rstudio.com/ide/}{RStudio}. The website is hosted with \href{https://pages.github.com/}{GitHub Pages} and the complete source is available from \href{https://github.com/lin380}{GitHub}.

This version of the coursebook was built with R version 4.0.2 (2020-06-22) and the following packages:

\begin{longtable}[]{@{}lll@{}}
\toprule
package & version & source \\
\midrule
\endhead
bookdown & 0.22 & CRAN (R 4.0.2) \\
\bottomrule
\end{longtable}

\hypertarget{part-foundations}{%
\part{Foundations}\label{part-foundations}}

\hypertarget{foundations-overview}{%
\section*{Overview}\label{foundations-overview}}
\addcontentsline{toc}{section}{Overview}

\textbf{FOUNDATIONS}

In this section the aims are to (1) provide an overview of quantitative research and their applications, by both highlighting visible applications and notable research in various fields. (2) We will under the hood a bit and consider how quantitative research contributes to language research. (3) I will layout the main types of research and situate quantitative text analysis inside these. Some attention will be given to the historical background to understand how theory (generative and usage-based grammar) has framed and to some degree continues to frame language research. (4) We will discuss how the programmatic approaches to language, which are fundamental for quantitative text analysis, also provide the opportunity to further science through process documentation and research reproducibility.

\hypertarget{data-language-and-text-analysis}{%
\section{Data, language, and text analysis}\label{data-language-and-text-analysis}}

DRAFT

\begin{quote}
Science walks forward on two feet, namely theory and experiment\ldots Sometimes it is one foot which is put forward first, sometimes the other, but continuous progress is only made by the use of both.
---\href{https://www.nobelprize.org/uploads/2018/06/millikan-lecture.pdf}{Robert A. Millikan} \citeyearpar{Millikan1923}
\end{quote}

\begin{rmdkey}
The essential questions for this chapter are:

\begin{itemize}
\tightlist
\item
  What is the role and goals of data analysis in and outside of
  academia?
\item
  In what ways is quantitative language research approached?
\item
  What are some of the applications of text analysis?
\item
  How is this coursebook structured and what are the target learning
  goals?
\end{itemize}
\end{rmdkey}

In this chapter I will aim to introduce the topic of text analysis and text analytics and frame the approach of this coursebook. The goals of this section are to work from the general field of data science/ data analysis to the particular sub-field of text analysis (where text is defined broadly as corpus). The aim is to introduce the context needed to understand how text analysis fits in a larger universe of data analysis and see the commonalities in the ever-ubiquitous field of data analysis, with attention to how language and linguistics studies employ data analysis down to the particular area of text analysis. To round out this chapter, I will provide a general overview of the rest of the coursebook motivating the general structure and sequencing as well as setting the foundation for programmatic approaches to data analysis.

\hypertarget{making-sense-of-a-complex-world}{%
\subsection{Making sense of a complex world}\label{making-sense-of-a-complex-world}}

The world around us is full of actions and interactions so numerous that it is difficult to really comprehend. Through the lens each individual sees and experiences this world. We gain knowledge about this world and build up heuristic knowledge about how it works and how we do and can interact with it. This happens regardless of your educational background. As humans we are built for this. Our minds process countless sensory inputs many of which never make it to our conscious mind. They underlie skills and abilities that we take for granted like being able to predict what will happen if you see someone about to knock a wine glass off a table and onto a concrete floor. You've never seen this object before and this is the first time you've been to this winery, but somehow and from somewhere you `instinctively' make an effort to warn the would-be-glass-breaker before it is too late. You most likely have not stopped to consider where this predictive knowledge has come from, or if you have, you may have just chalked it up to `common sense'. As common as it may be, it is an incredible display of the brain's capacity to monitor your environment, relate the events and observations that take place, and store that information all the time not making a big fuss to tell you conscious mind what it's up to.

So wait, this is a coursebook on text analytics and language, right? So what does all this have to do with that? Well, there are two points to make that are relevant for framing our journey: (1) the world is full of countless information which unfold in real-time at a scale that is daunting and (2) for all the power of the brain that works so efficiently behind the scene making sense of the world, we are one individual living one life that has a limited view of the world at large. Let me expand on these two points a little more.

First let's be clear. There is no way for any one to experience all things at all times, i.e.~omnipotence. But even extremely reduced slices of reality are still vastly outside of our experiental capacity, at least in real-time. One can make the point that since the inception of the internet an individual's ability to experience larger slices of the world has increased. But could you imagine reading, watching, and listening to every file that is currently accessible on the web? (or has been see the Wayback Machine)? Scale this back even further; let's take Wikipedia, the world's largest encyclopedia. Can you imagine reading every wiki entry? As large as a resource such as Wikipedia is \footnote{ADD: size of Wikipedia}, it is still a small fragment of the written language that is produced on the web, just the web. Consider that for a moment.

To my second framing point, which is actually two points in one. I made underscored the efficiency of our brain's capacity to make sense of the world. That efficiency comes from some clever evolutionary twists that lead our brain to take in the world but it makes some shortcuts that compress the raw experience into heuristic understanding. What that means is that the brain is not a supercomputer. It does not store every experience in raw form, we do not have access to the records of our experience like we would imagine a computer would have access to the records logged in a database. Where our brains do excel is in making associations and predictions that help us (most of the time) navigate the complex world we inhabit. This point is key --our brains are doing some amazing work, but that work can give us the impression that we understand the world in more detail that we actually do. Let's do a little thought experiment. Close your eyes and think about the last time you saw your best friend. What were they wearing? Can you remember the colors? If your like me, or any other human, you probably will have a pretty confident feeling that you know the answers to these questions and there is a chance you a right. But it has been demonstrated in numerous experiments on human memory that our confidence does not correlate with accuracy. (where were you when ..? JFK, 9/11, \ldots other example) You've experienced an event, but there is no real reason that we should be our lives on what we experienced. It's a little bit scary, for sure, but the magic is that it works `good enough' for practical purposes.

So here's the deal: as humans we are (1) clearly unable to experience large swaths of experience by the simple fact that we are individuals living individual lives and (2) the experiences we do live are not recorded with precision and therefore we cannot `trust' our intuitions, at least in an absolute sense.

What does that mean for our human curiosity about the world around us and our ability to reliably make sense of it? In short it means that we need to approach understanding our world with the tools of science. Science is so powerful because it makes strides to overcome our inherit limitations as humans (breadth of our experience and recall and relational abilities) and bring a complex world into a more digestible perspective. Science starts with question, identifies and collects data, careful selected slices of the complex world, submits this data to analysis through clearly defined and reproducible procedures, and reports the results for others to evaluate. This process is repeated, modifying, and manipulating the procedures, asking new questions and positing new explanations, all in an effort to make inroads to bring the complex into tangible view.

In essence what science does is attempt to subvert our inherent limitations in understanding by drawing on carefully and purposefully collected slices of experience and letting the analysis of this experience speak, even if it goes against our intuitions (those powerful but sometime spurious heuristics that our brains use to make sense of the world).

\hypertarget{data-analysis}{%
\subsection{Data analysis}\label{data-analysis}}

At this point I've sketched an outline strengths and limitations of humans' ability to make sense of the world and why science to address these limitations. This science I've described is the one you are familiar with and it has been an indespensible tool to make sense of the world. If you are like me, this description of science may be associated with visions of white coats, labs, and petri dishes. While science's foundation still stands strong in the 21st century, a series of intellectual and technological events mid-20th century set in motion changes that have changed aspects about how science is done, not why it is done. We could call this Science 2.0, but let's use the more popularized term ``Data Science''. The recognized beginnings of Data Science are attributed to work in the ``Statistics and Data Analysis Research'' department at Bell Labs during the 1960s. Although primarily conceptual and theoretic at the time, a framework for quantitative data analysis took shape that would anticipate what would come: sizable datasets which would ``\protect\hyperlink{section-6}{\ldots{}}require advanced statistical and computational techniques \protect\hyperlink{section-6}{\ldots{}} and the software to implement them.'' \citep{Chambers2020} This framework emphasized both the inference-based research of traditional science, but also embraced exploratory research and recognized the need to address practical considerations that would arise when working with and deriving insight from an abundance of data.

Fast-forward to the 21st century a world in which machine readable data is truly in abundance. With increased computing power and innovative uses of this technology the world wide web took flight. To put this in perspective, focusing in on language, the amount of text {[}here add stats on amount of data added to the web every day/month/year compared to all the literature from .. to ..?{]}. The data flood has not been limited to language, there are more sensors and recording devices than ever before which capture evermore swaths of the world we live in \citep{Desjardins2019}. Where increased computing power gave rise to the influx of data, it is also on of the primary methods for gathering, preparing, transforming, analyzing, and communicating insight derived from this data \citep{Donoho2017}. The vision laid out in the 1960s at Bell Labs had come to fruition.

The interest in deriving insight from the available data is now almost ubiquitous. The science of data has now reached deep into all aspects of life where making sense of the world is sought. Predicting whether a loan applicant will get a loan {[}cite{]}, whether a lump is cancerous {[}cite{]}, what films to recommend based on your previous viewing history {[}cite{]}, what players a sports team should sign \citep{Lewis2004} all now incorporate a common set of data analysis tools.

These advances, however, are not predicated on data alone. As envisioned by researchers at Bell Labs, turning data into insight it takes computing skills (i.e.~programming), knowledge of statistics, and, importantly, substantive/ domain expertise. This triad has been popularly represented in a Venn diagram \ref{fig:intro-data-science-venn}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/02-introduction/data-science-venn-paper} 

}

\caption{Data Science Venn Diagram adapted from [Drew Conway](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram).}\label{fig:intro-data-science-venn}
\end{figure}

This same toolbelt underlies well-known public-facing language applications. From the language-capable personal assistant applications, plagiarism detection software, machine translation and search, tangible results of quantitative approaches to language are becoming standard fixtures in our lives.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/02-introduction/well-known-language-applications} 

}

\caption{Well-known language applications}\label{fig:intro-language-applications}
\end{figure}

The spread of quantitative data analysis too has taken root in academia. Even in areas that on first blush don't appear to be approached in a quantitative manner such as fields in the social sciences and humanities, data science is making important and sometimes disisplinary changes to the way that academic research is conducted. This coursebook focuses in on a domain that cuts across many of these fields; namely language. At this point let's turn to quantitative approaches to language.

\hypertarget{language-analysis}{%
\subsection{Language analysis}\label{language-analysis}}

Language is a defining characteristic of our species. As such, the study of language is of key concern to a wide variety of fields, not just linguists. The goals of various fields, however, and as such approaches to language research, vary. On the one hand some language research traditions, namely those closely associated with Noam Chomsky, eschewed quantitative approaches to language research during the later half of the 20th century and instead turned to qualitative assessment of language structure through introspective methods. On the other hand many language research programs turned to and/or developed quantitative research methods either by necessity or through theoretical principles. These quantitative research trajectories share much of the common data analysis toolbox described in the previous section. This means to a large extent language analysis projects share a common research language with other language research but also with research beyond outside of language. However, there is never a one-size-fits all approach to anything --much less data analysis. And in quantitative analysis there is a key distinction in data collection that has downstream effects in terms of procedure but also in terms of interpretation.

The key distinction, that we need to make at this point, which will provide context for our exploration of text analysis, comes down to the approach to collecting language data and the nature of that data. This distinction is between experimental and observational data collection. Experimental approaches start with a intentionally designed hypothesis and lay out a research methodology with appropriate instruments and a plan to collect data that shows promise for shedding light on the validity of the hypothesis. Experimental approaches are conducted under controlled contexts, usually a lab environment, in which participants are recruited to perform a language related task with stimuli that have been carefully curated by researchers to elicit some aspect of language behavior of interest. Experimental approaches to language research are heavily influenced by procedures adapted from psychology. This link is logical as language is a central area of study in cognitive psychology. This approach looks a much like the white-coat science that we made reference to earlier but, as in most quantitative research, has now taken advantage of the data analysis tool belt to collect and organize much larger quantities of data and conduct statistically more robust analysis procedures and communicate findings more efficiently.

Observational approaches are a bit more of a mixed bag in terms of the rationale for the study; they may either start with a testable hypothesis or in other cases may start with a more open-ended research question to explore. But a more fundamental distinction between the two is drawn in the amount of control the researcher has on contexts and conditions in which the language behavior data to be collected is produced. Observational approaches seek out records of language behavior that is produced by language speakers for communicative purposes in natural(istic) contexts. This may take place in labs (language development, language disorders, etc.), but more often than not, language is collected from sources where speakers are performing language as part of their daily lives --whether that be posting on social media, speaking on the telephone, making political speeches, writing class essays, reporting the latest news for a newspaper, or crafting the next novel destined to be a New York Times best-seller. What is more, data collected from the `wild' is varies in more in structure relative to data collected in experimental approaches and requires a number of steps to prepare the data to synch up with the data analysis tool belt.

I liken this distinction between experimental and observational data collection to the difference between farming and foraging. Experimental approaches are like farming; the groundwork for a research plan is designed, much as a field is prepared for seeding, then the researcher performs as series of tasks to produce data, just as a farmer waters and cares for the crops, the results of the process bear fruit, data in our case, and this data is harvested. Observational approaches are like foraging; the researcher scans the available environmental landscape for viable sources of data from all the naturally existing sources, these sources are assessed as to their usefulness and value to address the research question, the most viable is selected, and then the data is collected.

The data acquired from both of these approaches have their trade-offs, just as farming and foraging. Experimental approaches directly elicit language behavior in highly controlled conditions. This directness and level of control has the benefit of allowing researchers to precisely track how particular experimental conditions effect language behavior. As these conditions are an explicit part of the design and therefore the resulting language behavior can be more precisely attributed to the experimental manipulation. The primary shortcoming of experimental approaches is that there is a level of artificialness to this directness and control. Whether it is the language materials used in the task, the task itself, or the fact that the procedure takes place under supervision the language behavior elicited can diverge quite significantly from language behavior performed in natural communicative settings. Observational approaches show complementary strengths and shortcomings. Whereas experimental approaches may diverge from natural language use, observational approaches strive to identify and collected language behavior data in natural, uncontrolled, and unmonitored contexts. In this way observational approaches do not have to question to what extent the language behavior data is or is not performed as a natural communicative act. On the flipside, the contexts in which natural language communication take place are complex relative to experimental contexts. Language collected from natural contexts are nested within the complex workings of a complex world and as such inevitably include a host of factors and conditions which can prove challenging to disentangle from the language phenomenon of interest but must be addressed in order to draw reliable associations and conclusions.

The upshot, then, is twofold: (1) data collection methods matter for research design and interpretation and (2) there is no single best approach to data collection, each have their strengths and shortcomings. In the ideal, a robust science of language will include insight from both experimental and observational approaches \citep{Gilquin:2009}. And evermore there is greater appreciation for the complementary nature of experimental and observational approaches and a growing body of research which highlights this recognition. Given their particular trade-offs observational data is often used as an exploratory starting point to help build insight and form predictions that can then be submitted to experimental conditions. In this way studies based on observational data serve as an exploratory tool to gather a better and more externally valid view of language use which can then serve to make prediction that can be explore with more precision in an experimental paradigm. However, this is not always the case. Observational data is also often used in hypothesis-testing contexts as well. And furthermore, some in some language-related fields, a hypothesis-testing is not the ultimate goal for deriving knowledge and insight.

\hypertarget{text-analysis}{%
\subsection{Text analysis}\label{text-analysis}}

Text analysis is the application of data analysis procedures from data science to derive insight from textual data collected through observational methods. I have deliberately chosen the term `text analysis' to avoid what I see are the pitfalls of using some other common terms in the literature such as Corpus Linguistics, Computational Linguistics, or Digital Humanities. There are plenty of learning resources that focus specifically on one of these three fields when discussing the quantitative analysis of text. But from my perspective what is missing is a resource which underscores the fact that text analysis research and the methods employed span across a wide variety of academic fields and applications in industry. This coursebook aims to introduce you to these areas through the lens of the data and analysis procedures and not through a particular field. This approach, I hope, provides a wider view of the potential applications of using text as data and inspires you to either employ quantitative text analysis in your research and/ or to raise your awareness of the advantages of text analysis for making sense of language-related and linguistic-based phenomenon.

So what are some applications of text analysis? For most the public facing applications that stem from Computational Linguistic research, often known as Natural Language Processing by practitioners, are the most well-known applications of text analysis. Whether it be using search engines, online translators, submitting your paper to plagiarism detection software, etc. the text analysis methods we will cover are at play. These uses of text analysis are production-level applications and there is big money behind developing evermore robust text analysis methods.

In academia the use of quantitative text analysis is even more widespread, despite the lack of public fanfare. Let's run through some select studies to give you an idea of the areas that are employing text analysis, of what researchers are doing with text analysis, and to whet your interest for conducting your own text analysis project.

\begin{rmdstudy}
\citet{Eisenstein2012} track the geographic spread of neologisms from city to city in the United States using Twitter data collected between 6/2009 and 5/2011. They only used tweets with geolocation data and then associated each tweet with a zipcode using the US Census. The most populous metropolitan areas were used. They also used the demographics from these areas to make associations between lexical innovations and demographic attributes. From this analysis they are able to reconstruct a network of linguistic influence. One of the main findings is that demographically-similar cities are more likely to share linguistic influence. At the individual level, there is a strong, potentially stronger role of demographics than geographical location.
\end{rmdstudy}

\begin{rmdstudy}
\citet{Voigt2017} explore potential racial disparities in officer respect in police body camera footage. The dataset is based on body camera footage from the Oakland Police Department during April 2014. At total of 981 stops by 245 different officers were included (black 682, white 299) and resulted in 36,738 officer utterances. The authors found evidence for racial disparities in respect but not formality of utterances, with less respectful language used with the black community members.
\end{rmdstudy}

\begin{rmdstudy}
\citet{Conway2012} investigate whether the established drop in language complexity of rhetoric in election seasons is associated with election outcomes. The authors used US Democratic Primary Debates from 2004. The results suggest that although there was no overall difference in complexity between winners and losers, their pattern differed over time. Winners tended to drop the complexity of their language closer to the upcoming election.
\end{rmdstudy}

\begin{rmdstudy}
\citet{Kloumann2012} explore the extent to which languages are positively, neutrally, or negatively biased. Using Twitter, Google Books (1520-2008), NY Times newspaper (1987-2007), and music lyrics (1960-2007) the authors extract the top 5,000 most frequent words from each source and have participants rate each word for happiness (9-point scale). The results show that positive words strongly outnumber negative words overall suggesting English is positive-, and pro-social- biased.
\end{rmdstudy}

\begin{rmdstudy}
\citet{Bychkovska2017} investigates possible differences between L1-English and L1-Chinese undergraduate students' use of lexical bundles, multiword sequences which are extended collocations (i.e.~as the result of), in argumentative essays. The authors used the Michigan Corpus of Upper-Level Student Papers (MICUSP) corpus using the argumentative essay section for L1-English and the Corpus of Ohio Learner and Teacher English (COLTE) for the L1-Chinese English essays. They found that L1-Chinese writers used more than 2 times as many bundle types than L1-English peers which they attribute to L1-Chinese writers attempt to avoid uncommon expressions and/or due to their lack of register awareness (conversation has more bundles than writing generally).
\end{rmdstudy}

\begin{rmdstudy}
\citet{Jaeger:2007a} use a corpus study to investigate the phenomenon of syntactic persistence, the increased tendency for speakers to use a particular syntactic form over an alternate when the syntactic form is recently processed. The authors attempt to distinguish between two competing explanations for the phenomenon: (1) transient activation, where the increased tendency is short-lived and time-bound and (2) implicit learning, where the increased tendency is a reflect of learning mechanisms. The use of a speech corpora (Switchboard and spoken BNC) were used to avoid the artificialness that typically occurs in experimental settings. The authors investigated the ditransitive alternation (NP PP/ NP NP), voice alternation (active/ passive), and complementizer/ relativizer omission. In these alternations structural bias was established by measuring the probability for a verb form to appear in one of the two syntactic forms. Then the probability that that form (target) would change given previous exposure to the alternative form (prime) was calculated; what the authors called surprisal. Distance between the prime structure and the target verb were considered in the analysis. In these alternations, the less common structure was used in the target more often when the when it corresponded to the prime form (higher surprisal) suggesting that implicit learning underlies syntactic persistence effects.
\end{rmdstudy}

\begin{rmdstudy}
\citet{Wulff2007} explore differences between British and American English at the lexico-syntactic level in the \emph{into}-causative construction (ex. `He tricked me into employing him.'). The analysis uses newspaper text (The Guardian and LA Times) and the findings suggest that American English uses this construction in verbal persuasion verbs whereas British English uses physical force verbs.
\end{rmdstudy}

\begin{rmdstudy}
\citet{Mosteller1963} provide a method for solving the authorship debate surrounding The Federalist papers \footnote{\url{https://guides.loc.gov/federalist-papers/full-text}}. They employ a probabilistic approach using the word frequency profiles of the articles with known authors to predict authorship of the disputed 12 papers. The results suggest that the disputed papers were most likely authored by Madison.
\end{rmdstudy}

\begin{rmdstudy}
\citet{Olohan2008} investigate the extent to which translated texts differ from native texts. In particular the author explores the notion of explicitation in translated texts (the tendency to make information in the source text explicit in the target translation). The study makes use of the Translational English Corpus (TEC) for translation samples and comparable sections of the British National Corpus (BNC) for the native samples. The results suggest that there is a tendency for syntactic explicitation in the translational corpus (TEC) which is assumed to be a subconscious process employed unwittingly by translators.
\end{rmdstudy}

This sample of studies include research from areas such as translation, stylistics, language variation, dialectology, psychology, psycholinguistics, political science, and sociolinguistics which highlights the diversity of fields and subareas which employ quantitative text analysis. Text analysis is at the center of these studies as they share a set of common goals:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To detect and retrieve patterns from text too subtle or too numerous to be done by hand
\item
  To challenge assumptions and/or provide other views from textual sources
\item
  To explore new questions and/or provide novel insight
\end{enumerate}

Let's now turn to the last section of this chapter which will provide an overview of the rationale for doing learning to do text analysis, the structure of the content covered, and a justification for the approach we will take to perform text analysis.

\hypertarget{coursebook-overview}{%
\subsection{Coursebook overview}\label{coursebook-overview}}

In this section I will provide a general overview of the rest of the coursebook motivating the general structure and sequencing as well as setting the foundation for programmatic approaches to data analysis. Let me highlight why I think this is a valuable area of study, what I hope you gain from this coursebook, and how the structure of this coursebook is configured to help scaffold your conceptual and practical knowledge of text analysis.

The target learning outcomes in this coursebook are the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data Literacy
\item
  Research Skills
\item
  Programming Skills
\end{enumerate}

\textbf{Data Literacy} refers to the ability to interpret, assess, and contextualize findings based on data. Throughout this coursebook we will explore topics which will help you understand how data analysis methods derive insight from data. In this process you will be encouraged to critically evaluate connections across linguistic and language-related disciplines using data analysis knowledge and skills. Data Literacy is an invaluable skillset for academics and professionals (cite) but also is an indispensable aptitude for in the 21st century citizens to navigate and actively participate in the `Information Age' in which we live \citep{Carmi2020}.

\textbf{Research skills} covers the ability to conduct original research, communicate findings, and make meaningful connections with findings in the literature of the field. This target area does not differ significantly, in spirit, from common learning outcomes in a research methods course: identify an area of investigation, develop a viable research question or hypothesis, collect relevant data, analyze data with relevant statistical methods, and interpret and communicate findings. However, working with text will incur a series of key steps in the selection, collection, and preparation of the data that are unique to text analysis projects. In addition, I will stress the importance of research documentation and creating reproducible research as an integral part of modern scientific inquiry.

\textbf{Programming skills} aims to develop your ability to implement research skills programmatically and produce research that is replicable and collaborative. Modern data analysis, and by extension, text analysis is conducted using programming. There are various key reasons for this: (1) programming affords researchers unlimited research freedom --if you can envision it, you can program it. The same cannot be said for off-the-shelf software which is either proprietary or unmaintained --or both. (2) programming underlies well-documented and reproducible research --documenting button clicks and menu option selections leads to research which is not readily reproduced, either by some other researcher or by your future self! (3) programming forces researchers to engage more intimately with the data and the methods for analysis. The more familiar you are with the data and the methods the more likely you are to produce higher quality work.

Now let me turn to how these learning goals integrate and shape the structure and sequencing of the following chapters.

In Part II ``Orientation'' we will build our Data Literacy skills working from data to insight. This progression is visualized in Figure \ref{fig:diki-hierarchy} \footnote{Adapted from \citet{Ackoff1989}}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/02-introduction/diki-hierarchy-paper} 

}

\caption{Data to Insight Hierarchy (DIKI)}\label{fig:diki-hierarchy}
\end{figure}

The DIKI Hierarchy highlights the stages and intermediate steps required to derive insight from data. Chapter 2 ``Understanding data'' will cover both Data and Information covering the conceptual topics of populations versus samples and how language data samples are converted to information and the forms that they can take. In Chapter 3 ``Statistical approaches'' I will discuss the distinction between descriptive and analytic statistics. In brief they are important for data analysis, but descriptive statistics serve as a sanity check on the dataset before submitting it to interrogation --which is the goal of analytic statistics. We will also cover some of the main distinctions between analytics approaches including inference-, exploration-, and prediction-based methods. With a fundamental understanding of data, information, and knowledge we will then move to Chapter 4 ``Framing research'' where we will discuss how to develop a research plan, or what I will call a `research blueprint'. At this point we will directly address Research Skills and elaborate on how research really comes together; how to bring yourself up to speed with the literature on a topic, how to develop a research goal or hypothesis, how to select data which is viable to address the research goal or hypothesis, how to determine the necessary information and appropriate measures to prepare for analysis, how to perform diagnostic statistics on the data and make adjustments before analysis, how to select and perform the relevant analytic statistics given the research goals, how to report your findings, and finally, how to structure your project so that it is well-documented and reproducible.

Part III ``Preparation'' and Part IV ``Modeling'' serve as practical and more detailed guides to the R programming strategies to conduct text analysis research and as such develop your Programming Skills. In Chapter 5 ``Acquire data'' I will discuss three main strategies for accessing data: direct downloads, Automatic Programming Interfaces (APIs), and web scraping. In Chapter 6 ``Curate data'' I will outline the process for converting or augmenting the acquired data into a structured format, therefore creating information. This will include organizing linguistic and non-linguistic metadata into one dataset. In Chapter 7 ``Transform data'' I describe how to work with a curated dataset to derive more detailed information and appropriate dataset structures that are appropriate for the upcoming analysis.

Chapters 8 ``Exploration'', 9 ``Inference'', and 10 ``Prediction'' focus on different categories of statistical analysis each associated with distinct research goals. Exploration covers `bottom-up'-style, or `unsupervised learning', analysis methods such as association measures, clustering, topic modeling, and vector-space models. These methods are aligned with research goals that aim to interpret patterns that arise in from the data itself. Inference deals with analysis methods associated with standard hypothesis-testing. This will include some common statistical models employed in text analysis: chi-squared, logistic regression, and linear regression. Prediction explores a set of statistical methods known as `supervised learning'. Similar to unsupervised learning, prediction models are employed in a bottom-up fashion. However, the key distinction is that the dataset includes an organizing `class' variable which the statistical methods aim to model in order to formulate a generalization that can correctly classify new textual data. I will cover some standard methods for text classification including Näive Bayes, \emph{k}-nearest neighbors (\emph{k}-NN), and decisions tree and random forest models.

\hypertarget{summary}{%
\subsection{Summary}\label{summary}}

In this chapter I started with some general observations about the difficulty of making sense of a complex world. The standard approach to overcoming inherent human limitations in sense making is science. In the 21st century the toolbelt for doing scientific research and exploration has grown in terms of the amount of data available, the statistical methods for analyzing the data, and the computational power to manage, store, and share the data, methods, and results from quantitative research. The methods and tools for deriving insight from data have made significant inroads in and outside academia, and increasingly figure in the quantitative investigation of language. Text analysis is a particular branch of this enterprise based on observational data from real-world language and is used in a wide variety of fields. This coursebook aims to develop your knowledge and skills in three fundamental areas: Data Literacy, Research Skills, and Programming Skills.

In the end I hope that you enjoy this exploration into text analysis. Although learning curve at times may seem steep --the experience you will gain will not only improve your data literacy, research skills, and programmings skills but also enhance your appreciation for the richness of human language and its important role in our everyday lives.

\hypertarget{part-orientation}{%
\part{Orientation}\label{part-orientation}}

\hypertarget{orientation-overview}{%
\section*{Overview}\label{orientation-overview}}
\addcontentsline{toc}{section}{Overview}

\textbf{ORIENTATION}

Before we begin working on the specifics of our data project, it is important to establish a fundamental understanding of the characteristics of each of the levels in the DIKI Hierarchy (Figure \ref{fig:diki-hierarchy}) and the roles each of these levels have in deriving insight from data. In Chapter 2 we will explore the Data and Information levels drawing a distinction between two main types of data (populations and samples) and then cover how data is structured and transformed to generate information (datasets) that is fit for statistical analysis. In Chapter 3 I will outline the importance and distinct types of statistical procedures (descriptive and analytic) that are commonly used in text analysis. Chapter 4 aims to tie these concepts together and cover the required steps for preparing a research blueprint to conduct an original text analysis project.

\hypertarget{understanding-data}{%
\section{Understanding data}\label{understanding-data}}

DRAFT

\begin{quote}
The plural of anecdote is not data.
―-- Marc Bekoff
\end{quote}

\begin{rmdkey}
The essential questions for this chapter are:

\begin{itemize}
\tightlist
\item
  What are the distinct types of data and how do they differ?
\item
  What is information and what form does it take?
\item
  What is the importance of documentation in quantitative research?
\end{itemize}
\end{rmdkey}

In this chapter I cover the starting concepts in our journey to understand how to derive insight from data, illustrated in the DIKI Hierarchy (Figure \ref{fig:diki-hierarchy}), focusing specifically on the first two levels: Data and Information. We will see that what is commonly referred to as `data' everyday uses is broken into three distinct categories, two of which are referred to as data and the third is known as information. We will also cover the importance of documentation of data and datasets in quantitative research.

\hypertarget{data}{%
\subsection{Data}\label{data}}

Data is data, right? The term `data' is so common in popular vernacular it is easy to assume we know what we mean when we say `data'. But as in most things, where there are common assumptions there are important details the require more careful consideration. Let's turn to the first key distinction that we need to make to start to break down the term `data': the difference between populations and samples.

\hypertarget{populations}{%
\subsubsection{Populations}\label{populations}}

The first thing that comes to many people's mind when the term population is used is human populations. Say for example --What's the population of Milwuakee? When we speak of a population in these terms we are talking about the total sum of people living within the geographical boundaries of Milwaukee. In concrete terms, a \textbf{population} is the objective make up of an idealized set of objects and events in reality (cite). Key terms here are objective and idealized. Although we can look up the US Census report for Milwaukee and retrieve a figure for the population, this cannot truly be the population. Why is that? Well, whatever method that was used to derive this numerical figure was surely incomplete. If not incomplete, by the time someone recorded the figure some number of residents of Milwaukee moved out, moved in, were born, or passed away --the figure is no longer the true population.

Likewise when we talk about populations in terms of language we dealing with an objective and idealized aspect of reality. Let's take the words of the English language as an analog to our previous example population. In this case the words are the people and English is the bounding characteristic. Just as people, words move out, move in, are born, and pass away. Any compendium of the words of English at any moment is almost instananeously incomplete. This is true for all populations, save those in which the bounding characteristics select a narrow slice of reality which is objectively measurable and whose membership is fixed (the complete works of Shakespeare, for example).

In sum, (most) populations are amorphous moving targets. We objectively hold them to exist, but in practical terms we often cannot nail down the specifics of populations. So how do researchers go about studying populations if they are theoretically impossible to access directly? The strategy employed is called sampling.

\hypertarget{sampling}{%
\subsubsection{Sampling}\label{sampling}}

A \textbf{sample} is the product of a subjective process of selecting a finite set of observations from an objective population with the goal of capturing the relevant characteristics of the target population. Although there are strategies to minimize the mismatch between the characteristics of the subjective sample and objective population, it is important to note that it is almost certainly true that any given sample diverges from the population it aims to represent to some degree. The aim, however, is to employ a series of sampling decisions, which are collectively known as a sampling frame, that maximize the chance of representing the population.

What are the most common sampling strategies? First \textbf{sample size}. A larger sample will always be more representative than a smaller sample. Sample size, however, is not enough. It is not hard to imagine a large sample which by chance captures only a subset of the features of the population. A next step to enhance sample representativeness is apply \textbf{random sampling}. Together a large random sample has an even better chance of reflecting the main characteristics of the population better than a large or random sample. But, random as random is, we still run the risk of acquiring a skewed sample (i.e a sample which does not mirror the target population).

To help mitigate these issues, there are two more strategies that can be applied to improve sample representativeness. Note, however, that while size and random samples can be applied to any sample with little information about internal characteristics of the population, these next two strategies require decisions depend on the presumed internal characteristics of the population. The first of these more informed sampling strategies is called \textbf{stratified sampling}. Stratified samples make (educated) assumptions about sub-components within the population of interest. With these sub-populations in mind, large random samples are acquired for each sub-population, or strata. At a minimum, stratified samples can be no less representative than random sampling alone, but the chances that the sample is better increases. Can there be problems in the approach? Yes, and on two fronts. First knowledge of the internal components of a population are often based on a limited or incomplete knowledge of the population. In other words, strata are selected subjectively by researchers using various heuristics some of which are based on some sense of `common knowledge'. The second front that stratified sampling can err concerns the relative sizes of the sub-components relative to the whole population. Even if the relevant sub-components are identified, their relative size adds another challenge in which researchers must face in order to maximize the representativeness of a sample. To attempt to align, or \textbf{balance}, the relative sizes of the samples for the strata is the second population-informed sampling strategy.

A key feature of a sample is that it is purposely selected. Samples are not simply a collection or set of data from the population. Samples are rigorously selected with an explicit target population in mind. In text analysis a purposely sampled collection of texts, of the type defined here, is known as a \textbf{corpus.} For this same reason a set of texts or documents which have not been selected along a purposely selected sampling frame is not a corpus. The sampling frame, and therefore the populations modeled, in any given corpus most likely will vary and for this reason it is not a safe assumption that any given corpus is equally applicable for any and every research question. Corpus development (i.e.~sampling) is purposeful, and the characteristics of the corpus development process should be made explicit through documentation. Therefore vetting a corpus sample for its applicability to a research goal is a key step in that a research must take to ensure the integrity of the research findings.

\begin{rmdquestion}
The Brown Corpus is widely recognized as one of the first large, machine-readable corpora. It was compiled by \citet{Kucera1967}. Consult the \href{http://korpus.uib.no/icame/brown/bcm.html}{documentation for this corpus}. Can you determine what language population this corpus aims to represent? Given the sampling frame for this corpus (in the documentation and summarized in Figure \ref{fig:brown-distribution}), what types of research might this corpus support or not support?
\end{rmdquestion}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{03-understanding-data_files/figure-latex/brown-distribution-1} 

}

\caption{Brown Corpus of Written American English}\label{fig:brown-distribution}
\end{figure}

\hypertarget{corpora}{%
\subsubsection{Corpora}\label{corpora}}

\hypertarget{types}{%
\paragraph{Types}\label{types}}

With the notion of sampling frames in mind, some corpora are compiled with the aim to be of general purpose (general or \textbf{reference corpora}), and some with much more specialized sampling frames (\textbf{specialized corpora}). For example, the \href{}{American National Corpus (ANC)} or the \href{}{British National Corpus (BNC)} are corpora which aim to model (represent/ reflect) the general characteristics of the English language, the former of American English and the later British English. These are ambitious projects, and require significant investments of time in corpus design and then in implementation (and continued development) that are usually undertaken by research teams \citep{Adel2020}.

Specialized corpora aim to represent more specific populations. The \href{}{Santa Barbara Corpus of Spoken American English (SBCSAE)}, as you can imagine from the name of the resource, aims to model spoken American English. No claim to written English is included. There are even more specific types of corpora which attempt to model other types of sub-populations such as scientific writing, \href{https://www.clarin.eu/resource-families/cmc-corpora}{computer-mediated communication (CMC)}, language use in specific \href{http://ice-corpora.net/ice/index.html}{regions of the world}, or \href{https://cesa.arizona.edu}{a country}, etc.

Another set of specialized corpora are resources which aim to compile texts from different languages or different language varieties for direct or indirect comparison. Corpora that are directly comparable, that is they include source and translated texts, are called \textbf{parallel corpora}. Parallel corpora include different languages or language varieties that are indexed and aligned at some linguistic level (i.e.~word, phrase, sentence, paragraph, or document) \href{}{OPUS example}. Corpora that are compiled with different languages or language varieties but are not directly aligned are called \textbf{comparable corpora}. The comparable language or language varieties are sampled with the same or similar sampling frame \href{}{Brown and LOB example}.

The aim of the quantitative text researcher is to select the corpus or corpora (plural of corpus) which best aligns with the purpose of the research. Therefore a general corpus such as the ANC may be better suited to address a question dealing with the way American English works, but this general resource may lack detail in certain areas, such as \href{http://www.hd.uib.no/icame/ij22/vihla.pdf}{medical language}, that may be vital for a research project aimed at understanding changes in medical terminology.

\hypertarget{sources}{%
\paragraph{Sources}\label{sources}}

The most common source of data used in contemporary quantitative research is the internet. On the web an investigator can access corpora published for research purposes and language used in natural settings that can be coerced by the investigator into a corpus. Many organizations exist around the globe that provide access to corpora in browsable catalogs, or \textbf{repositories}. There are repositories dedicated to language research, in general, such as the \href{https://www.ldc.upenn.edu/}{Language Data Consortium} or for specific language domains, such as the language acquisition repository \href{http://talkbank.org/}{TalkBank}. It is always advisable to start looking for the available language data in a repository. The advantage of beginning your data search in repositories is that a repository, especially those geared towards the linguistic community, will make identifying language corpora faster than through a general web search. Furthermore, repositories often require certain standards for corpus format and documentation for publication. A standardized resource many times will be easier to interpret and evaluate for its appropriateness for a particular research project.

In the table below I've compiled a list of some corpus repositories to help you get started.

\begin{table}

\caption{\label{tab:pinboard-repositories}A list of some corpus repositories}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="https://corpus.byu.edu/">BYU corpora</a> & A repository of corpora that includes billions of words of data.\\
\hline
<a href="http://corporafromtheweb.org/">COW (COrpora from the Web)</a> & A collection of linguistically processed gigatoken web corpora\\
\hline
<a href="http://wortschatz.uni-leipzig.de/en/download/">Leipzig Corpora Collection</a> & Corpora in different languages using the same format and comparable sources.\\
\hline
<a href="https://www.ldc.upenn.edu/">Linguistic Data Consortium</a> & Repository of language corpora\\
\hline
<a href="http://www.resourcebook.eu/searchll.php">LRE Map</a> & Repository of language resources collected during the submission process for the Language Resource and Evaluation Conference (LREC).\\
\hline
<a href="http://www.nltk.org/nltk\_data/">NLTK language data</a> & Repository of corpora and language datasets included with the Python package NLTK.\\
\hline
<a href="http://opus.lingfil.uu.se/">OPUS - an open source parallel corpus</a> & Repository of translated texts from the web.\\
\hline
<a href="http://talkbank.org/">TalkBank</a> & Repository of language collections dealing with conversation, acquisition, multilingualism, and clinical contexts.\\
\hline
<a href="https://corpus1.mpi.nl/ds/asv/?4">The Language Archive</a> & Various corpora and language datasets\\
\hline
<a href="http://ota.ox.ac.uk/">The Oxford Text Archive (OTA)</a> & A collection of thousands of texts in more than 25 different languages.\\
\hline
\end{tabular}
\end{table}

Repositories are by no means the only source of corpora on the web. Researchers from around the world provide access to corpora and other data sources on their own sites or through data sharing platforms. Corpora of various sizes and scopes will often be accessible on a dedicated homepage or appear on the homepage of a sponsoring institution. Finding these resources is a matter of doing a web search with the word `corpus' and a list of desired attributes, including language, modality, register, etc. As part of a general movement towards reproducibility more corpora are available on the web than ever before. Therefore data sharing platforms supporting reproducible research, such as \href{https://github.com/}{GitHub}, \href{https://zenodo.org/}{Zenodo}, \href{http://www.re3data.org/}{Re3data}, \href{https://osf.io/}{OSF}, etc., are a good place to look as well, if searching repositories and targeted web searches do not yield results.

In the table below you will find a list of corpus resources and datasets.

\begin{table}

\caption{\label{tab:pinboard-corpora}Corpora and language datasets.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="http://www.socsci.uci.edu/\textasciitilde{}lpearl/CoLaLab/CHILDESTreebank/childestreebank.html">CHILDES Treebank</a> & A corpus derived from several corpora from the American English section of CHILDES with the goal to annotate child-directed speech utterance transcriptions with phrase structure tree information.\\
\hline
<a href="http://www.cs.cornell.edu/\textasciitilde{}cristian/Cornell\_Movie-Dialogs\_Corpus.html">Cornell Movie-Dialogs Corpus</a> & A corpus containing a large metadata-rich collection of fictional conversations extracted from raw movie scripts.\\
\hline
<a href="http://www.lllf.uam.es/\textasciitilde{}fmarcos/informes/corpus/coarginl.html">Corpus Argentino</a> & Corpus of Argentine Spanish\\
\hline
<a href="https://cesa.arizona.edu/">Corpus of Spanish in Southern Arizona</a> & Spanish varieties spoken in Arizona.\\
\hline
<a href="https://www.statmt.org/europarl/">Europarl Parallel Corpus</a> & A parallel corpus extracted from the proceedings of the European Parliament Proceedings between 1996-2011.\\
\hline
<a href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html">Google Ngram Viewer</a> & Google web corpus\\
\hline
<a href="http://opus.lingfil.uu.se/OpenSubtitles\_v2.php">OpenSubtitles2011</a> & A collection of documents from http://www.opensubtitles.org/.\\
\hline
<a href="http://www.ruscorpora.ru/en/">Russian National Corpus</a> & A corpus of modern Russian language incorporating over 300 million words.\\
\hline
<a href="https://quantumstat.com//dataset">The Big Bad NLP Database - Quantum Stat</a> & NLP datasets\\
\hline
<a href="https://catalog.ldc.upenn.edu/docs/LDC97S62/">The Switchboard Dialog Act Corpus</a> & A corpus of 1155 5-minute conversations in American English, comprising 205,000 utterances and 1.4 million words, from the Switchboard corpus of telephone conversations.\\
\hline
<a href="http://langsnap.soton.ac.uk/">Welcome to LANGSNAP - LANGSNAP</a> & The aim of this repository is to promote research on the learning of French and Spanish as L2, by making parallel learner corpora for each language freely available to the research community.\\
\hline
<a href="http://www.psych.ualberta.ca/\textasciitilde{}westburylab/downloads/usenetcorpus.download.html">Westbury Lab Web Site: Usenet Corpus Download</a> & This corpus is a collection of public USENET postings. This corpus was collected between Oct 2005 and Jan 2011, and covers 47,860 English language, non-binary-file news groups (see list of newsgroups included with the corpus for details)\\
\hline
\end{tabular}
\end{table}

If your corpus search ends in a dead-end, either because a suitable resource does not appear to exist or an existing resource is unattainable given licensing restrictions or fees, it may be time to compile your own corpus. Turning to machine readable texts on the internet is usually the logical first step to access language for a new corpus. Language texts may be found on sites as uploaded files, such as pdf or doc (Word) documents, or found displayed as the primary text of a site. Given the wide variety of documents uploaded and language behavior recorded daily on social media, news sites, blogs and the like, compiling a corpus has never been easier. Having said that, how the data is structured and how much data needs to be retrieved can pose practical obstacles to collecting data from the web, particularly if the approach is to acquire the data by hand instead of automating the task. Our approach here, however, will be to automate the process as much as possible whether that means leveraging R package interfaces to language data, converting hundreds of pdf documents to plain text, or scraping content from web documents.

The table below lists some R packages that serve to interface language data directly through R.

\begin{table}

\caption{\label{tab:pinboard-apis}R Package interfaces to language corpora and datasets.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="https://ropensci.org/tutorials/arxiv\_tutorial.html">aRxiv</a> & R package interface to query arXiv, a repository of electronic preprints for computer science, mathematics, physics, quantitative biology, quantitative finance, and statistics.\\
\hline
<a href="https://github.com/ropensci/crminer">crminer</a> & R package interface focusing on getting the user full text via the Crossref search API.\\
\hline
<a href="https://github.com/ropensci/dvn">dvn</a> & R package interface to access to the Dataverse Network APIs.\\
\hline
<a href="https://ropensci.org/tutorials/fulltext\_tutorial.html">fulltext</a> & R package interface to query open access journals, such as PLOS.\\
\hline
<a href="https://ropensci.org/tutorials/gutenbergr\_tutorial.html">gutenbergr</a> & R package interface to download and process public domain works from the Project Gutenberg collection.\\
\hline
<a href="https://ropensci.org/tutorials/internetarchive\_tutorial.html">internetarchive</a> & R package interface to query the Internet Archive.\\
\hline
<a href="https://github.com/hrbrmstr/newsflash">newsflash</a> & R package interface to query the Internet Archive and GDELT Television Explorer\\
\hline
<a href="https://github.com/ropensci/oai">oai</a> & R package interface to query any OAI-PMH repository, including Zenodo.\\
\hline
<a href="https://github.com/ropensci/rfigshare">rfigshare</a> & R package interface to query the data sharing platform FigShare.\\
\hline
\end{tabular}
\end{table}

Data for language research is not limited to (primary) text sources. Other sources may include processed data from previous research; word lists, linguistic features, etc.. Alone or in combination with text sources this data can be a rich and viable source of data for a research project.

Below I've included some processed language resources.

\begin{table}

\caption{\label{tab:pinboard-experimental}Language data from previous research and meta-studies.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="http://elexicon.wustl.edu/WordStart.asp">English Lexicon Project</a> & Access to a large set of lexical characteristics, along with behavioral data from visual lexical decision and naming studies.\\
\hline
<a href="https://github.com/ropensci/lingtypology">lingtypology</a> & R package interface to connect with the Glottolog database and provides additional functionality for linguistic mapping.\\
\hline
<a href="https://nyu-mll.github.io/CoLA/">The Corpus of Linguistic Acceptability (CoLA)</a> & A corpus that consists of 10657 sentences from 23 linguistics publications, expertly annotated for acceptability (grammaticality) by their original authors.\\
\hline
<a href="http://icon.shef.ac.uk/Moby/">The Moby lexicon project</a> & Language wordlists and resources from the Moby project.\\
\hline
\end{tabular}
\end{table}

The list of data available for language research is constantly growing. I've document very few of the wide variety of resources. Below I've included attempts by others to provide a summary of the corpus data and language resources available.

\begin{table}

\caption{\label{tab:pinboard-listings}Lists of corpus resources.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="https://uclouvain.be/en/research-institutes/ilc/cecl/learner-corpora-around-the-world.html">Learner corpora around the world</a> & A listing of learner corpora around the world\\
\hline
<a href="https://paperswithcode.com/datasets">Machine Learning Datasets | Papers With Code</a> & A free and open resource with Machine Learning papers, code, and evaluation tables.\\
\hline
<a href="http://nlp.stanford.edu/links/statnlp.html\#Corpora">Stanford NLP corpora</a> & Listing of corpora and language resources aimed at the NLP community.\\
\hline
<a href="https://makingnoiseandhearingthings.com/2017/09/20/where-can-you-find-language-data-on-the-web/">Where can you find language data on the web?</a> & Listing of various corpora and language datasets.\\
\hline
\end{tabular}
\end{table}

\begin{rmdnote}
Here I can work with real or simplified research questions and have
students consider which of set of corpus resources would most likely be
the better resource.
\end{rmdnote}

\hypertarget{formats}{%
\paragraph{Formats}\label{formats}}

A corpus will often include various types of non-linguistic attributes, or \textbf{meta-data}, as well. Ideally this will include information regarding the source(s) of the data, dates when it was acquired or published, and other author or speaker information. It may also include any number of other attributes that were identified as potentially important in order to appropriately document the target population. Again, it is key to match the available meta-data with the goals of your research. In some cases a corpus may be ideal in some aspects but not contain all the key information to address your research question. This may mean you will need to compile your own corpus if there are fundamental attributes missing. Before you consider compiling your own corpus, however, it is worth investigating the possibility of augmenting an available corpus to bring it inline with your particular goals. This may include adding new language sources, harnessing software for linguistic annotation (part-of-speech, syntactic structure, named entities, etc.), or linking available corpus meta-data to other resources, linguistic or non-linguistic.

Corpora come in various formats, the main three being: running text, structured documents, and databases. The format of a corpus is often influenced by characteristics of the data but may also reflect an author's individual preferences as well. It is typical for corpora with few meta-data characteristics to take the form of running text.

Running text sample from the \href{https://www.statmt.org/europarl/}{Europarle Parallel Corpus}.

\begin{verbatim}
> Resumption of the session
> I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.
> Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.
> You have requested a debate on this subject in the course of the next few days, during this part-session.
> In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.
> Please rise, then, for this minute' s silence.
> (The House rose and observed a minute' s silence)
> Madam President, on a point of order.
> You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.
> One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.
\end{verbatim}

In corpora with more meta-data, a header may be appended to the top of each running text document or the meta-data may be contained in a separate file with appropriate coding to coordinate meta-data attributes with each text in the corpus.

Meta-data header sample from the \href{}{Switchboard Dialog Act Corpus}.

\begin{verbatim}
> FILENAME: 4325_1632_1519
> TOPIC#:       323
> DATE:     920323
> TRANSCRIBER:  glp
> UTT_CODER:    tc
> DIFFICULTY:   1
> TOPICALITY:   3
> NATURALNESS:  2
> ECHO_FROM_B:  1
> ECHO_FROM_A:  4
> STATIC_ON_A:  1
> STATIC_ON_B:  1
> BACKGROUND_A: 1
> BACKGROUND_B: 2
> REMARKS:        None.
> 
> =========================================================================
> 
> 
> o          A.1 utt1: Okay.  /
> qw          A.1 utt2: {D So, }
> 
> qy^d          B.2 utt1: [ [ I guess, +
> 
> +          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /
> 
> +          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /
> 
> qy          A.5 utt1: Does it say something? /
> 
> sd          B.6 utt1: I think it usually does.  /
> ad          B.6 utt2: You might try, {F uh, }  /
> h          B.6 utt3: I don't know,  /
> ad          B.6 utt4: hold it down a little longer,  /
> ad          B.6 utt5: {C and } see if it, {F uh, } -/
\end{verbatim}

When meta-data and/ or linguistic annotation increases in complexity it is common to structure each corpus document more explicitly with a markup language such as XML (Extensible Markup Language) or organize relationships between language and meta-data attributes in a database.

XML format for meta-data (and linguistic annotation) from the \href{http://www.nltk.org/nltk_data/}{Brown Corpus}.

\begin{verbatim}
> <TEI xmlns="http://www.tei-c.org/ns/1.0"><teiHeader><fileDesc><titleStmt><title>Sample A01 from  The Atlanta Constitution</title><title type="sub"> November 4, 1961, p.1 "Atlanta Primary ..."
>  "Hartsfield Files"
>  August 17, 1961, "Urged strongly ..."
>  "Sam Caldwell Joins"
>  March 6,1961, p.1 "Legislators Are Moving" by Reg Murphy
>  "Legislator to fight" by Richard Ashworth
>  "House Due Bid..."
>  p.18 "Harry Miller Wins..."
> </title></titleStmt><editionStmt><edition>A part  of the XML version of the Brown Corpus</edition></editionStmt><extent>1,988 words 431 (21.7%) quotes 2 symbols</extent><publicationStmt><idno>A01</idno><availability><p>Used by permission of The Atlanta ConstitutionState News Service (H), and Reg Murphy (E).</p></availability></publicationStmt><sourceDesc><bibl> The Atlanta Constitution</bibl></sourceDesc></fileDesc><encodingDesc><p>Arbitrary Hyphen: multi-million [0520]</p></encodingDesc><revisionDesc><change when="2008-04-27">Header auto-generated for TEI version</change></revisionDesc></teiHeader>
> <text xml:id="A01" decls="A">
> <body><p><s n="1"><w type="AT">The</w> <w type="NP" subtype="TL">Fulton</w> <w type="NN" subtype="TL">County</w> <w type="JJ" subtype="TL">Grand</w> <w type="NN" subtype="TL">Jury</w> <w type="VBD">said</w> <w type="NR">Friday</w> <w type="AT">an</w> <w type="NN">investigation</w> <w type="IN">of</w> <w type="NPg">Atlanta's</w> <w type="JJ">recent</w> <w type="NN">primary</w> <w type="NN">election</w> <w type="VBD">produced</w> <c type="pct">``</c> <w type="AT">no</w> <w type="NN">evidence</w> <c type="pct">''</c> <w type="CS">that</w> <w type="DTI">any</w> <w type="NNS">irregularities</w> <w type="VBD">took</w> <w type="NN">place</w> <c type="pct">.</c> </s>
> </p>
\end{verbatim}

Although there has been a push towards standardization of corpus formats, most available resources display some degree of idiosyncrasy. Being able to parse the structure of a corpus is a skill that will develop with time. With more experience working with corpora you will become more adept at identifying how the data is stored and whether its content and format will serve the needs of your analysis.

\hypertarget{information}{%
\subsection{Information}\label{information}}

Identifying an adequate corpus resource for the target research question is the first step in moving a quantitative text research project forward. The next step is to select the components or characteristics of this resource that are relevant for the research and then move to organize the attributes of this data into a more useful and informative format. This is the process of converting a corpus into a \textbf{dataset} --a tabular representation of the information to be leveraged in the analysis.

\hypertarget{structure}{%
\subsubsection{Structure}\label{structure}}

Data alone is not informative. Only through explicit organization of the data in a way that makes relationships accessible does the data become information. This is a particularly salient hurdle in text analysis research. Some textual data is \emph{unstructured} --that is, the relationships that will be used in the analysis have yet to be explicitly drawn and organized from the text to make the relationships meaningful and useful for analysis.

For the running text in the Europarle Corpus, we know that there are files which are the source text (original) and files that correspond to the target text (translation). In Table \ref{tab:structure-europarle} we see that this text has been organized so that there are columns corresponding to the \texttt{type} and \texttt{sentence} with an additional \texttt{sentence\_id} column to keep an index of how the sentences are aligned.

\begin{rmdtip}
It is conventional to work with column names for datasets in R using the
same conventions that are used for naming objects. It is a matter of
taste which convention is used, but I have adopted
\href{https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/names.html\#snake_case}{snake
case} as my personal preference. There are also
\href{https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/names.html}{alternatives}.
Regardless of the convention you choose, it is good practice to be
consistent.

It is also of note that the column names should be balanced for
meaningfulness and brevity. This brevity is of practical concern but can
be somewhat opaque. For questions into the meaning of the column and is
values consult the resource's documentation.
\end{rmdtip}

\begin{table}

\caption{\label{tab:structure-europarle}First 10 source and target sentences in the Europarle Corpus.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Target & 1 & Resumption of the session\\
Source & 1 & Reanudación del período de sesiones\\
Target & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Source & 2 & Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\\
Target & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
\addlinespace
Source & 3 & Como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\
Target & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
Source & 4 & Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\
Target & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Source & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\\
\addlinespace
Target & 6 & Please rise, then, for this minute' s silence.\\
Source & 6 & Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
Target & 7 & (The House rose and observed a minute' s silence)\\
Source & 7 & (El Parlamento, de pie, guarda un minuto de silencio)\\
Target & 8 & Madam President, on a point of order.\\
\addlinespace
Source & 8 & Señora Presidenta, una cuestión de procedimiento.\\
Target & 9 & You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\\
Source & 9 & Sabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en Sri Lanka.\\
Target & 10 & One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\\
Source & 10 & Una de las personas que recientemente han asesinado en Sri Lanka ha sido al Sr. Kumar Ponnambalam, quien hace pocos meses visitó el Parlamento Europeo.\\
\bottomrule
\end{tabular}
\end{table}

Other corpus resources are \emph{semi-structured} --that is, there are some characteristics which are structured, but other which are not.

The Switchboard Dialog Act Corpus is an example of a semi-structured resource. It has meta-data associated with each of the 1,155 conversations in the corpus. In Table \ref{tab:structure-swda} a language-relevant sub-set of the meta-data is associated with each utterance.

\begin{table}

\caption{\label{tab:structure-swda}First 5 utterances from the Switchboard Dialog Act Corpus.}
\centering
\begin{tabular}[t]{lrrllllll}
\toprule
doc\_id & speaker\_id & topic\_num & topicality & naturalness & damsl\_tag & speaker & utterance\_num & utterance\_text\\
\midrule
4325 & 1632 & 323 & 3 & 2 & o & A & 1 & Okay.  /\\
4325 & 1632 & 323 & 3 & 2 & qw & A & 2 & \{D So, \}\\
4325 & 1519 & 323 & 3 & 2 & qy\textasciicircum{}d & B & 1 & {}[ [ I guess, +\\
4325 & 1632 & 323 & 3 & 2 & + & A & 1 & What kind of experience [ do you, + do you ] have, then with child care? /\\
4325 & 1519 & 323 & 3 & 2 & + & B & 1 & I think, ] + \{F uh, \} I wonder ] if that worked. /\\
\bottomrule
\end{tabular}
\end{table}

Relatively fewer resources are \emph{structured}. In these cases a high amount of meta-data and/ or linguistic annotation is included in the corpus. The format convention, however, varies from resource to resource. Some of the formats are programming general (.csv, .xml, .json, etc.) and others are resource specific (.cha, .utt, .prd, etc.). In Table \ref{tab:structure-brown} the XML version of the Brown Corpus is represented in tabular format. Note that along with other meta-data variables, it also contains a variable with linguistic annotation for grammatical category (\texttt{pos} part-of-speech) of each word.

\begin{table}

\caption{\label{tab:structure-brown}First 10 words from the Brown Corpus.}
\centering
\begin{tabular}[t]{llll}
\toprule
document\_id & category & words & pos\\
\midrule
01 & A & The & AT\\
01 & A & Fulton & NP\\
01 & A & County & NN\\
01 & A & Grand & JJ\\
01 & A & Jury & NN\\
\addlinespace
01 & A & said & VBD\\
01 & A & Friday & NR\\
01 & A & an & AT\\
01 & A & investigation & NN\\
01 & A & of & IN\\
\bottomrule
\end{tabular}
\end{table}

In this coursebook, the selection of the attributes from a corpus and the juxtaposition of these attributes in a relational format, or dataset, that converts data into information will be referred to as \textbf{data curation}. The process of data curation minimally involves creating a base dataset, or \emph{derived dataset}, which establishes the main informational associations according to philosophical approach outlined by \citet{Wickham2014a}. In this work, a `tidy' dataset refers both to the structural (physical) and informational (semantic) organization of the dataset. Physically, a tidy dataset is a tabular data structure where each \emph{row} is an observation and each \emph{column} is a variable that contains measures of a feature or attribute of each observation. Each cell where a given row-column intersect contains a \emph{value} which is a particular attribute of a particular observation for the particular observation-feature pair also known as a \emph{data point}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/03-understanding-data/tidy-format-paper} 

}

\caption{Visual summary of the tidy format.}\label{fig:tidy-format-image}
\end{figure}

Semantic value in a tidy dataset is derived from the association of this physical structure along the two dimensions of this rectangular format. First, each column is a \textbf{variable} which reflects measures for a particular attribute. In the Europarle Corpus dataset, in Table \ref{tab:structure-europarle}, for example, the \texttt{type} column measures the type of text, either \texttt{Source} or \texttt{Target}. Columns can contain measures which are qualitative or quantitative, that is character-based or numeric. Second, each row is an \textbf{observation} that contains all of the variables associated with the primary unit of observation. The primary unit of observation the variable that is the essential focus of the informational structure. In this same dataset the first observation contains the \texttt{type}, \texttt{sentence\_id}, and the \texttt{sentence}. As this dataset is currently structured the primary unit of investigation is the \texttt{sentence} as each of the other variables have measures that characterize each value of \texttt{sentence}.

The decision as to what the primary unit of observation is is fundamentally guided by the research question, and therefore highly specific to the particular research project. Say instead we wanted to focus on words instead of sentences. The dataset would need to be transformed such that a new variable (\texttt{words}) would be created to contain each word in the corpus.

\begin{table}

\caption{\label{tab:tidy-words-europarle}Europarle Paralle Corpus with `words` as primary unit of investigation.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & words\\
\midrule
Target & 1 & Resumption\\
Target & 1 & of\\
Target & 1 & the\\
Target & 1 & session\\
Source & 1 & Reanudación\\
\addlinespace
Source & 1 & del\\
Source & 1 & período\\
Source & 1 & de\\
Source & 1 & sesiones\\
\bottomrule
\end{tabular}
\end{table}

The values for the variables \texttt{type} and \texttt{sentence\_id} maintain the necessary description for each \texttt{word} to ensure the required semantic relationships to identify the particular attributes for each word observation. This dataset may seem redundant in that the values for \texttt{type} and \texttt{sentence\_id} are repeated numerous times but this `redundancy' makes the relationship between each variable associated with the primary unit of investigation explicit. This format makes a tidy dataset a versatile format for researchers to conduct analyses in a powerful and flexible way, as we will see throughout this coursebook.

It is important to make clear that data in tabular format in itself does not constitute a dataset, in the tidy sense we will be using. Data can be organized in many ways which do not make relationships between variables and observations explicit.

\emph{Consider adding some `messy' data and/ or summary tables which do not reflect the relational structure we are aiming to create to base our research on.}

\begin{rmdtip}
Note in some cases we may convert our tidy tabular dataset to other data
formats that may be required for some particular statistic approaches
but at all times the relationship between the variables should be
maintained in line with our research purpose. We will touch on examples
of other types of data formats when we dive into particular statistical
approaches that require them later in the series (i.e.~Corpus and
Document-Term Matrix (DTM) objects in R).
\end{rmdtip}

\hypertarget{transformation}{%
\subsubsection{Transformation}\label{transformation}}

At this point have introduced the first step in data curation in which the original data is converted into a relational dataset (derived dataset) and highlighted the importance of this informational structure for setting the stage for data analysis. However, the primary derived dataset is often not the final organizational step before proceeding to statistical analysis. Many times, if not always, the derived dataset requires some manipulation or transformation to prepare the dataset for the specific analysis approach to be taken. This is another level of human intervention and informational organization, and therefore another step forward in our journey from data to insight and as such a step up in the DIKI hierarchy. Common types of transformations include cleaning variables (normalization), separating or eliminating variables (recoding), creating new variables (generation), or incorporating others datasets which integrate with the existing variables (merging). The results of these transformations build on and manipulate the derived dataset and produce an \emph{analysis dataset}. Let's now turn to provide a select set of examples of each of these transformations using the datasets we have introduced in this chapter.

\hypertarget{normalization}{%
\paragraph{Normalization}\label{normalization}}

The process of normalization aims to \emph{sanitize} the values within a variable or set of variables. This may include removing whitespace, punctuation, numerals, or special characters or substituting uppercase for lowercase characters, numerals for word versions, acronyms for their full forms, irregular or incorrect spelling for accepted forms, or removing common words (stopwords), etc.

On inspecting the Europarle dataset (Table \ref{tab:structure-europarle}) we will see that there are sentence lines which do not represent actual parliment speeches. In Table \ref{tab:normalize-non-speech-identify-europarle} we see these lines.

\begin{table}

\caption{\label{tab:normalize-non-speech-identify-europarle}Non-speech lines in the Europarle dataset.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Target & 1 & Resumption of the session\\
Source & 1 & Reanudación del período de sesiones\\
Target & 7 & (The House rose and observed a minute' s silence)\\
Source & 7 & (El Parlamento, de pie, guarda un minuto de silencio)\\
\bottomrule
\end{tabular}
\end{table}

A research project aiming to analyze speech would want to normalize this dataset removing these lines, as seen in Table \ref{tab:normalize-non-speech-remove-europarle}.

\begin{table}

\caption{\label{tab:normalize-non-speech-remove-europarle}The Europarle dataset with non-speech lines removed.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Target & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Source & 2 & Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\\
Target & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Source & 3 & Como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\
Target & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
\addlinespace
Source & 4 & Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\
Target & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Source & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\\
Target & 6 & Please rise, then, for this minute' s silence.\\
Source & 6 & Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
\addlinespace
Target & 8 & Madam President, on a point of order.\\
Source & 8 & Señora Presidenta, una cuestión de procedimiento.\\
Target & 9 & You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\\
Source & 9 & Sabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en Sri Lanka.\\
Target & 10 & One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\\
\addlinespace
Source & 10 & Una de las personas que recientemente han asesinado en Sri Lanka ha sido al Sr. Kumar Ponnambalam, quien hace pocos meses visitó el Parlamento Europeo.\\
\bottomrule
\end{tabular}
\end{table}

Another feature of this dataset which may require attention is the fact that the English lines include whitespace between possessive nouns.

\begin{table}

\caption{\label{tab:normalize-whitespace-identify-europarle}Lines with possessives with extra whitespace in the Europarle dataset.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Target & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Target & 6 & Please rise, then, for this minute' s silence.\\
\bottomrule
\end{tabular}
\end{table}

This may affect another transformation process or subsequent analysis, so it may be a good idea to normalize these forms by removing the extra whitespace.

\begin{table}

\caption{\label{tab:normalize-whitespace-remove-europarle}The Europarle dataset with whitespace from possessives removed.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Target & 5 & In the meantime, I should like to observe a minute's silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Target & 6 & Please rise, then, for this minute's silence.\\
\bottomrule
\end{tabular}
\end{table}

A final normalization case scenario involves changing converting all the text to lowercase. If the goal for the research is to count words at some point the fact that a word starts a sentence and by convention the first letter is capitalized will result distinct counts for words that are in essence the same (i.e.~``In'' vs.~``in'').

\begin{table}

\caption{\label{tab:normalize-lowercase-europarle}The Europarle dataset with lowercasing applied.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Target & 2 & i declare resumed the session of the european parliament adjourned on friday 17 december 1999, and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Source & 2 & declaro reanudado el período de sesiones del parlamento europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a sus señorías mi deseo de que hayan tenido unas buenas vacaciones.\\
Target & 3 & although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Source & 3 & como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. en cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\
Target & 4 & you have requested a debate on this subject in the course of the next few days, during this part-session.\\
\addlinespace
Source & 4 & sus señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\
Target & 5 & in the meantime, i should like to observe a minute's silence, as a number of members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the european union.\\
Source & 5 & a la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la unión europea afectados.\\
Target & 6 & please rise, then, for this minute's silence.\\
Source & 6 & invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
\addlinespace
Target & 8 & madam president, on a point of order.\\
Source & 8 & señora presidenta, una cuestión de procedimiento.\\
Target & 9 & you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka.\\
Source & 9 & sabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en sri lanka.\\
Target & 10 & one of the people assassinated very recently in sri lanka was mr kumar ponnambalam, who had visited the european parliament just a few months ago.\\
\addlinespace
Source & 10 & una de las personas que recientemente han asesinado en sri lanka ha sido al sr. kumar ponnambalam, quien hace pocos meses visitó el parlamento europeo.\\
\bottomrule
\end{tabular}
\end{table}

Note that lowercasing text, and normalization steps in general, can come at a cost. For example, lowercasing the Europarle dataset sentences means we lose potentially valuable information; namely the ability to identify proper names (i.e.~``Mr Kumar Ponnambalam'') and titles (i.e.~``European Parliament'') directly from the orthographic forms. There are, however, transformation steps that can be applied which aim to recover `lost' information in situations such as this and others.

\hypertarget{recoding}{%
\paragraph{Recoding}\label{recoding}}

The process of recoding aims to \emph{recast} the values of a variable or set of variables to a new variable or set of variables to enable more direct access. This may include extracting values from a variable, stemming or lemmatization of words, tokenization of linguistic forms (words, ngrams, sentences, etc.), calculating the lengths of linguistic units, removing variables that will not be used in the analysis, etc.

Words that we intuitively associate with a `base' word can take many forms in language use. For example the word forms `investigation', `investigation', `investigate', `investigated', etc. are intuitively linked. There are two common methods that can be applied to create a new variable to facilitate the identification of these associations. The first is stemming. Stemming is a rule-based heuristic to reduce word forms to their stem or root form.

\begin{table}

\caption{\label{tab:recoding-stemming-brown-example}Results for stemming the first words in the Brown Corpus.}
\centering
\begin{tabular}[t]{lllll}
\toprule
document\_id & category & words & pos & word\_stems\\
\midrule
01 & A & The & AT & The\\
01 & A & Fulton & NP & Fulton\\
01 & A & County & NN & Counti\\
01 & A & Grand & JJ & Grand\\
01 & A & Jury & NN & Juri\\
\addlinespace
01 & A & said & VBD & said\\
01 & A & Friday & NR & Fridai\\
01 & A & an & AT & an\\
01 & A & investigation & NN & investig\\
01 & A & of & IN & of\\
\bottomrule
\end{tabular}
\end{table}

A few things to note here. First there are a number of stemming algorithms both for individual languages and distinct languages \footnote{\url{https://snowballstem.org/algorithms/}}. Second not all words can be stemmed as they do not have derivative forms (i.e.~``The'', ``of'', etc.). This generally related to the distinction between closed-class (articles, prepositions, conjunctions, etc.) and open-class (nouns, verbs, adjectives, etc.) grammatical categories. Third the stem generated for those words that can be stemmed result in forms that are not words themselves. Nonetheless, stems can be very useful for more easily extracting a set of related word forms.

As an example, let's identify all the word forms for the stem `investig'.

\textbackslash begin\{table\}

\textbackslash caption\{\label{tab:recoding-stemming-brown-search}Results for a \texttt{word\_stems} filter for ``investig'' in the Brown Corpus.\}
\centering

\begin{tabular}[t]{lllll}
\toprule
document\_id & category & words & pos & word\_stems\\
\midrule
01 & A & investigation & NN & investig\\
01 & A & investigate & VB & investig\\
03 & A & investigation & NN & investig\\
03 & A & investigation & NN & investig\\
07 & A & investigations & NNS & investig\\
\addlinespace
07 & A & investigate & VB & investig\\
08 & A & investigation & NN & investig\\
09 & A & investigation & NN & investig\\
09 & A & investigating & VBG & investig\\
09 & A & investigation & NN & investig\\
\bottomrule
\end{tabular}

\textbackslash end\{table\}

We can see from the results in Table \ref{tab:recoding-stemming-brown-search} that searching for \texttt{word\_stems} that match `investig' returns a set of stem-related forms. But it is worth noting that these forms cut across a number of grammatical categories. If instead you want to draw a distinction between grammatical categories, we can apply lemmatization. This process is distinct from stemming in two important ways: (1) derivative forms are grouped by grammatical category and (2) the resulting forms are lemmas or `base' forms of words.

\begin{table}

\caption{\label{tab:recoding-lemmatization-brown-example}Results for lemmatization of the first words in the Brown Corpus.}
\centering
\begin{tabular}[t]{lllll}
\toprule
document\_id & category & words & pos & word\_lemmas\\
\midrule
01 & A & The & AT & The\\
01 & A & Fulton & NP & Fulton\\
01 & A & County & NN & County\\
01 & A & Grand & JJ & Grand\\
01 & A & Jury & NN & Jury\\
\addlinespace
01 & A & said & VBD & say\\
01 & A & Friday & NR & Friday\\
01 & A & an & AT & a\\
01 & A & investigation & NN & investigation\\
01 & A & of & IN & of\\
\bottomrule
\end{tabular}
\end{table}

To appreciate the difference between stemming and lemmatization, let's compare a filter for \texttt{word\_lemmas} which match `investigation'.

\textbackslash begin\{table\}

\textbackslash caption\{\label{tab:recoding-lemmatization-brown-investigation}Results for a \texttt{word\_lemmas} filter for ``investigation'' in the Brown Corpus.\}
\centering

\begin{tabular}[t]{lllll}
\toprule
document\_id & category & words & pos & word\_lemmas\\
\midrule
01 & A & investigation & NN & investigation\\
03 & A & investigation & NN & investigation\\
03 & A & investigation & NN & investigation\\
07 & A & investigations & NNS & investigation\\
08 & A & investigation & NN & investigation\\
\addlinespace
09 & A & investigation & NN & investigation\\
09 & A & investigation & NN & investigation\\
23 & A & investigation & NN & investigation\\
25 & A & investigation & NN & investigation\\
41 & A & investigation & NN & investigation\\
\bottomrule
\end{tabular}

\textbackslash end\{table\}

Only lemma forms of `investigate' which are nouns appear. Let's run a similar search but for the lemma `be'.

\textbackslash begin\{table\}

\textbackslash caption\{\label{tab:recoding-lemmatization-brown-be}Results for a \texttt{word\_lemmas} filter for ``be'' in the Brown Corpus.\}
\centering

\begin{tabular}[t]{lllll}
\toprule
document\_id & category & words & pos & word\_lemmas\\
\midrule
01 & A & was & BEDZ & be\\
01 & A & been & BEN & be\\
01 & A & was & BEDZ & be\\
01 & A & was & BEDZ & be\\
01 & A & are & BER & be\\
\addlinespace
01 & A & are & BER & be\\
01 & A & be & BE & be\\
01 & A & is & BEZ & be\\
01 & A & was & BEDZ & be\\
01 & A & be & BE & be\\
\bottomrule
\end{tabular}

\textbackslash end\{table\}

Again only words of the same grammatical category are returned. In this case the verb `be' has many more derivative forms than `investigate'.

Another form of recoding is to detect a pattern in the values of an existing variable and create a new variable whose values are the extracted pattern or register that the pattern occurs and/ or how many times it occurs. As an example, let's count the number of disfluencies (`uh' or `um') that occur in each utterance in \texttt{utterance\_text} from the Switchboard Dialog Act Corpus. \emph{Note I've simplified the dataset dropping the non-relevant variables for this example.}

\textbackslash begin\{table\}

\textbackslash caption\{\label{tab:recoding-extract-switchboard}Disfluency counts in the first 10 \texttt{utterance\_text} values from the Switchboard Corpus.\}
\centering

\begin{tabular}[t]{lr}
\toprule
utterance\_text & disfluency\_count\\
\midrule
Okay.  / & 0\\
\{D So, \} & 0\\
{}[ [ I guess, + & 0\\
What kind of experience [ do you, + do you ] have, then with child care? / & 0\\
I think, ] + \{F uh, \} I wonder ] if that worked. / & 1\\
\addlinespace
Does it say something? / & 0\\
I think it usually does.  / & 0\\
You might try, \{F uh, \}  / & 1\\
I don't know,  / & 0\\
hold it down a little longer,  / & 0\\
\bottomrule
\end{tabular}

\textbackslash end\{table\}

One of the most common forms of recoding in text analysis is tokenization. Tokenization is the process of recasting the text into smaller linguistic units. When working with text that has not been linguistically annotated, the most feasible linguistic tokens are words, ngrams, and sentences. While word and sentence tokens are easily understandable, ngram tokens need some explanation. An ngram is a sequence of either characters or words where \emph{n} is the length of this sequence. The ngram sequences are drawn incrementally, so the bigrams (two-word sequences) for the sentence ``This is an input sentence.'' are:

this is, is an, an input, input sentence

We've already seen word tokenization exemplified with the Europarle Corpus in subsection \protect\hyperlink{structure}{Structure} in Table \ref{tab:tidy-words-europarle}, so let's create (word) bigram tokens for this corpus.

\begin{table}

\caption{\label{tab:recoding-tokenization-europarle-bigram-words}The first 10 word bigrams of the Europarle Corpus.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & word\_bigrams\\
\midrule
Target & 2 & i declare\\
Target & 2 & declare resumed\\
Target & 2 & resumed the\\
Target & 2 & the session\\
Target & 2 & session of\\
\addlinespace
Target & 2 & of the\\
Target & 2 & the european\\
Target & 2 & european parliament\\
Target & 2 & parliament adjourned\\
Target & 2 & adjourned on\\
\bottomrule
\end{tabular}
\end{table}

As I just mentioned, ngrams sequences can be formed of characters as well. Here are character trigram (three-character) sequences.

\begin{table}

\caption{\label{tab:recoding-tokenization-europarle-trigram-chars}The first 10 character trigrams of the Europarle Corpus.}
\centering
\begin{tabular}[t]{lrl}
\toprule
type & sentence\_id & char\_trigrams\\
\midrule
Target & 2 & ide\\
Target & 2 & dec\\
Target & 2 & ecl\\
Target & 2 & cla\\
Target & 2 & lar\\
\addlinespace
Target & 2 & are\\
Target & 2 & rer\\
Target & 2 & ere\\
Target & 2 & res\\
Target & 2 & esu\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{generation}{%
\paragraph{Generation}\label{generation}}

The process of generation aims to \emph{augment} a variable or set of variables. In essence this aims to make implicit attributes explicit to that they are directly accessible. This often targeted at the automatic generation of linguistic annotations such as grammatical category (part-of-speech) or syntactic structure.

In the examples below I've added linguistic annotation to a target (English) and source (Spanish) example sentence from the Europarle Parallel Corpus. First, note the variables that are added to our dataset that correspond to grammatical category. In addition to the \texttt{type} and \texttt{sentence\_id} we have an assortment of variables which replace the \texttt{sentence} variable. As part of the process of annotation the input text to be annotated \texttt{sentence} is tokenized \texttt{token} and indexed \texttt{token\_id}. Then \texttt{upos} contains the Universal Part of Speech tags\footnote{\href{https://universaldependencies.org/u/pos/}{Descriptions of the UPOS tagset}}, and a detailed list of features is included in \texttt{feats}. The syntactic annotation is reflected in the \texttt{token\_id\_source} and \texttt{syntactic\_relation} variables. These variables correspond to the type of syntactic parsing that has been done, in this case Dependency Parsing (using the \href{https://universaldependencies.org/}{Universal Dependencies} framework). Another common syntactic parsing framework is phrase constituency parsing \citep{Jurafsky2020}.

\begin{table}

\caption{\label{tab:generation-europarle-en-example}Automatic linguistic annotation for grammatical category and syntactic structure for an example English sentence from the Europarle Corpus}
\centering
\begin{tabular}[t]{lrllllll}
\toprule
type & sentence\_id & token\_id & token & upos & feats & token\_id\_source & syntactic\_relation\\
\midrule
Target & 6 & 1 & Please & INTJ & NA & 2 & discourse\\
Target & 6 & 2 & rise & VERB & Mood=Imp|VerbForm=Fin & 0 & root\\
Target & 6 & 3 & , & PUNCT & NA & 2 & punct\\
Target & 6 & 4 & then & ADV & PronType=Dem & 10 & advmod\\
Target & 6 & 5 & , & PUNCT & NA & 10 & punct\\
\addlinespace
Target & 6 & 6 & for & ADP & NA & 10 & case\\
Target & 6 & 7 & this & DET & Number=Sing|PronType=Dem & 8 & det\\
Target & 6 & 8 & minute & NOUN & Number=Sing & 10 & nmod:poss\\
Target & 6 & 9 & 's & PART & NA & 8 & case\\
Target & 6 & 10 & silence & NOUN & Number=Sing & 2 & conj\\
\addlinespace
Target & 6 & 11 & . & PUNCT & NA & 2 & punct\\
\bottomrule
\end{tabular}
\end{table}

Now compare the English example sentence dataset in Table \ref{tab:generation-europarle-en-example} with the parallel sentence in Spanish. Note that the grammatical features are language specific. For example, Spanish has gender which is apparent when scanning the \texttt{feats} variable.

\begin{table}

\caption{\label{tab:generation-europarle-es-example}Automatic linguistic annotation for grammatical category and syntactic structure for an example Spanish sentence from the Europarle Corpus}
\centering
\begin{tabular}[t]{lrllllll}
\toprule
type & sentence\_id & token\_id & token & upos & feats & token\_id\_source & syntactic\_relation\\
\midrule
Source & 6 & 1 & Invito & VERB & Gender=Masc|Number=Sing|VerbForm=Fin & 0 & root\\
Source & 6 & 2 & a & ADP & NA & 3 & case\\
Source & 6 & 3 & todos & PRON & Gender=Masc|Number=Plur|PronType=Tot & 1 & obj\\
Source & 6 & 4 & a & ADP & NA & 7 & mark\\
Source & 6 & 5 & que & SCONJ & NA & 4 & fixed\\
\addlinespace
Source & 6 & 6 & nos & PRON & Case=Acc,Dat|Number=Plur|Person=1|PrepCase=Npr|PronType=Prs|Reflex=Yes & 7 & iobj\\
Source & 6 & 7 & pongamos & VERB & Mood=Ind|Number=Plur|Person=1|Tense=Pres|VerbForm=Fin & 1 & advcl\\
Source & 6 & 8 & de & ADP & NA & 9 & case\\
Source & 6 & 9 & pie & NOUN & Gender=Masc|Number=Sing & 7 & obl\\
Source & 6 & 10 & para & ADP & NA & 11 & mark\\
\addlinespace
Source & 6 & 11 & guardar & VERB & VerbForm=Inf & 1 & advcl\\
Source & 6 & 12 & un & DET & Definite=Ind|Gender=Masc|Number=Sing|PronType=Art & 13 & det\\
Source & 6 & 13 & minuto & NOUN & Gender=Masc|Number=Sing & 11 & obj\\
Source & 6 & 14 & de & ADP & NA & 15 & case\\
Source & 6 & 15 & silencio & NOUN & Gender=Masc|Number=Sing & 13 & nmod\\
\addlinespace
Source & 6 & 16 & . & PUNCT & NA & 1 & punct\\
\bottomrule
\end{tabular}
\end{table}

There is much more to explore with linguistic annotation, and syntactic parsing in particular, but at this point it will suffice to note that it is possible to augment a dataset with grammatical information automatically.

There are strengths and shortcomings with automatic linguistic annotation that a research should be aware of. First, automatic linguistic annotation provides quick access to rich and highly reliable linguistic information for a large number of languages. However, part of speech taggers and syntactic parsers are not magic. They are resources that are built by training a computational algorithm to recognize patterns in manually annotated datasets producing a language model. This model is then used to predict the linguistic annotations for new language (as we just did in the previous examples). The shortcomings of automatic linguistic annotation is first, not all languages have trained language models and second, the data used to train the model inevitably reflect a particular variety, register, modality, etc. The accuracy of the linguistic annotation is highly dependent on alignment between the language sampling frame of the trained data and the language data to be automatically annotated. Many (most) of the language models available for automatic linguistic annotation are based on language that is most readily available and for most languages this has traditionally been newswire text. It is important to be aware of these characteristics when using linguistic annotation tools.

\emph{Consider adding `creating measures' here}

\hypertarget{merging}{%
\paragraph{Merging}\label{merging}}

The process of merging aims to \emph{join} a variable or set of variables with another variable or set of variables from another dataset. The option to merge two (or more) datasets requires that there is a shared variable that indexes and aligns the datasets.

To provide an example let's look at the Switchboard Diaglog Act Corpus. Our existing, disfluency recoded, version includes the following variables.

\begin{verbatim}
#> Rows: 5
#> Columns: 11
#> $ doc_id           <chr> "4325", "4325", "4325", "4325", "4325"
#> $ speaker_id       <dbl> 1632, 1632, 1519, 1632, 1519
#> $ topic_num        <dbl> 323, 323, 323, 323, 323
#> $ topicality       <chr> "3", "3", "3", "3", "3"
#> $ naturalness      <chr> "2", "2", "2", "2", "2"
#> $ damsl_tag        <chr> "o", "qw", "qy^d", "+", "+"
#> $ speaker          <chr> "A", "A", "B", "A", "B"
#> $ turn_num         <chr> "1", "1", "2", "3", "4"
#> $ utterance_num    <chr> "1", "2", "1", "1", "1"
#> $ utterance_text   <chr> "Okay.  /", "{D So, }", "[ [ I guess, +", "What kind ~
#> $ disfluency_count <int> 0, 0, 0, 0, 1
\end{verbatim}

It turns out that on the \href{https://catalog.ldc.upenn.edu/docs/LDC97S62/}{corpus website} a number of meta-data files are available, including files pertaining to speakers and the topics of the conversations.

The speaker meta-data for this corpus is in a the \texttt{caller\_tab.csv} file and contains a \texttt{speaker\_id} variable which corresponds to each speaker in the corpus and other potentially relevant variables for a language research project including \texttt{sex}, \texttt{birth\_year}, \texttt{dialect\_area}, and \texttt{education}.

\begin{table}

\caption{\label{tab:merging-swda-speaker-table}Speaker meta-data for the Switchboard Dialog Act Corpus.}
\centering
\begin{tabular}[t]{rlrlr}
\toprule
speaker\_id & sex & birth\_year & dialect\_area & education\\
\midrule
1632 & FEMALE & 1962 & WESTERN & 2\\
1632 & FEMALE & 1962 & WESTERN & 2\\
1519 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
1632 & FEMALE & 1962 & WESTERN & 2\\
1519 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
\bottomrule
\end{tabular}
\end{table}

Since both datasets contain a shared index, \texttt{speaker\_id} we can merge these two datasets. The result is found in Table \ref{tab:merging-swda-speaker-added}.

\begin{table}

\caption{\label{tab:merging-swda-speaker-added}Merged conversations and speaker meta-data for the Switchboard Dialog Act Corpus.}
\centering
\begin{tabular}[t]{lrlrlrrlllllllr}
\toprule
doc\_id & speaker\_id & sex & birth\_year & dialect\_area & education & topic\_num & topicality & naturalness & damsl\_tag & speaker & turn\_num & utterance\_num & utterance\_text & disfluency\_count\\
\midrule
4325 & 1632 & FEMALE & 1962 & WESTERN & 2 & 323 & 3 & 2 & o & A & 1 & 1 & Okay.  / & 0\\
4325 & 1632 & FEMALE & 1962 & WESTERN & 2 & 323 & 3 & 2 & qw & A & 1 & 2 & \{D So, \} & 0\\
4325 & 1519 & FEMALE & 1971 & SOUTH MIDLAND & 1 & 323 & 3 & 2 & qy\textasciicircum{}d & B & 2 & 1 & {}[ [ I guess, + & 0\\
4325 & 1632 & FEMALE & 1962 & WESTERN & 2 & 323 & 3 & 2 & + & A & 3 & 1 & What kind of experience [ do you, + do you ] have, then with child care? / & 0\\
4325 & 1519 & FEMALE & 1971 & SOUTH MIDLAND & 1 & 323 & 3 & 2 & + & B & 4 & 1 & I think, ] + \{F uh, \} I wonder ] if that worked. / & 1\\
\bottomrule
\end{tabular}
\end{table}

In this example case the dataset that was merged was already in a structured format (.csv). Many corpus resources contain meta-data in stand-off files that are structured.

In some cases a researcher would like to merge information that does not already accompany the corpus resource. This is possible as long as a dataset can be created that contains a variable that is shared. Without a shared variable to index the datasets the merge cannot take place.

In sum, the transformation steps described here collectively aim to produce higher quality datasets that are relevant in content and structure to submit to analysis. The process may include one or more of the previous transformations but is rarely linear and is most often iterative. It is typical to do some normalization then generation, then recoding, and then return to normalizing, and so forth. This process is highly idiosyncratic given the characteristics of the derived dataset and the ultimate goals for the analysis dataset.

\hypertarget{documentation}{%
\subsection{Documentation}\label{documentation}}

As we have seen in this chapter that acquiring data and converting that data into information involves a number of conscious decisions and implementation steps. As a favor to ourselves as researchers and to the research community, it is crucial to document these decisions and steps. This makes it both possible to retrace our own steps and also provides a guide for future researchers that want to reproduce and/ or build on your research. A programmatic approach to quantitative research helps ensure that the implementation steps are documented and reproducible but it is also vital that the decisions that are made are documented as well. This includes the creation/ selection of the corpus data, the description of the variables chosen from the corpus for the derived dataset, and the description of the variables created from the derived dataset for the analysis dataset.

\emph{Consider adding more specifics on the characteristics and formats for documenting corpus data and datasets; data dictionaries --examples in R packages and in spreadsheets, or Rmarkdown files}

\hypertarget{summary-1}{%
\subsection{Summary}\label{summary-1}}

In this chapter we have focused on data and information --the first two components of DIKI Hierarchy. This process is visualized in Figure \ref{fig:understanding-data-vis-sum}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/03-understanding-data/understanding-data_visual-summary-paper} 

}

\caption{Understanding data: visual summary}\label{fig:understanding-data-vis-sum}
\end{figure}

First a distinction is made between populations and samples, the latter being a intentional and subjective selection of observations from the world which attempt to represent the population of interest. The result of this process is known as a corpus. Whether developing a corpus or selecting an existing a corpus it is important to vet the sampling frame for its applicability and viability as a resource for a given research project.

Once a viable corpus is identified, then that corpus is converted into a derived dataset which adopts the `tidy' dataset format where each column is a variable, each row is an observation, and the intersection of columns and rows contain values. This derived dataset serves to establish the base informational relationships from which your research will stem.

The derived dataset will most likely require transformations including normalization, recoding, generation, and/ or merging to enhance the usefulness of the information to analysis. An analysis dataset is the result of this process.

Although covered at the end of this chapter, documentation should be implemented at each stage of the process. Employing a programmatic approach establishes documentation of the implementation steps but the motivation behind the decisions taken and the content of the corpus data and datasets generated also need documentation to ensure transparent and reproducible research.

\hypertarget{approaching-analysis}{%
\section{Approaching analysis}\label{approaching-analysis}}

INCOMPLETE DRAFT

\begin{quote}
Lies, damn lies, and statistics\\
---Benjamin Disraeli, popularized by Mark Twain
\end{quote}

\begin{rmdkey}
The essential questions for this chapter are:

\begin{itemize}
\tightlist
\item
  What is the role of statistics in data analysis?
\item
  What is the importance of descriptive assessment in data analysis?
\item
  In what ways are main approaches to data analysis similar and
  different?
\end{itemize}
\end{rmdkey}

In this chapter I will build on the notions of data and information from the previous chapter. The aim of statistics in quantitative analysis is to uncover patterns in datasets. Thus statistics is aimed at deriving knowledge from information, the next step in the DIKI Hierarchy (Figure \ref{fig:understanding-data-vis-sum}). Where the creation of information from data involves human intervention and conscious decisions, as we have seen, deriving knowledge from information involves even more conscious subjective decisions on what information to assess, and what method to select to interrogate the information, and ultimately how to interpret the findings. The first step is to conduct a descriptive assessment of the information, both at the individual variable level and also between variables, the second is to interrogate the dataset either through inferential, predictive, or exploratory analysis methods, and the third is to interpret and report the findings.

\hypertarget{description}{%
\subsection{Description}\label{description}}

A descriptive assessment of the dataset includes a set of diagnostic measures and tabular and visual summaries which provide researchers a better understanding of the structure of a dataset, prepare the researcher to make decisions about which statistical methods and/ or tests are most appropriate, and to safeguard against false assumptions (missing data, data distributions, etc.). In this section we will first cover the importance of understanding the informational value that variables can represent and then move to use this understanding to approach summarizing individual variables and relationships between variables.

To ground this discussion I will introduce a new dataset. This dataset is drawn from the \href{https://slabank.talkbank.org/access/English/BELC.html}{Barcelona English Language Corpus (BELC)}, which is found in the \href{http://talkbank.org/}{TalkBank repository}. I've selected the ``Written composition'' task from this corpus which contains writing samples from second language learners of English at different ages. Participants were given the task of writing for 15 minutes on the topic of ``Me: my past, present and future''. Data was collected for many (but not all) participants up to four times over the course of seven years. In Table \ref{tab:belc-overview} I've included the first 10 observations from the dataset which reflects structural and transformational steps I've done so we start with a tidy dataset.

\begin{table}

\caption{\label{tab:belc-overview}First 10 observations of the BELC dataset for demonstration.}
\centering
\begin{tabular}[t]{lllrrr}
\toprule
participant\_id & age\_group & sex & num\_tokens & num\_types & ttr\\
\midrule
L02 & 10-year-olds & female & 12 & 12 & 1.000\\
L05 & 10-year-olds & female & 18 & 15 & 0.833\\
L10 & 10-year-olds & female & 36 & 26 & 0.722\\
L11 & 10-year-olds & female & 10 & 8 & 0.800\\
L12 & 10-year-olds & female & 41 & 23 & 0.561\\
\addlinespace
L16 & 10-year-olds & female & 13 & 12 & 0.923\\
L22 & 10-year-olds & female & 47 & 30 & 0.638\\
L27 & 10-year-olds & female & 8 & 8 & 1.000\\
L28 & 10-year-olds & female & 84 & 34 & 0.405\\
L29 & 10-year-olds & female & 53 & 34 & 0.642\\
\bottomrule
\end{tabular}
\end{table}

The entire dataset includes 79 observations from 36 participants. Each observation in the BELC dataset corresponds to an individual learner's composition. It includes which participant wrote the composition (\texttt{participant\_id}), the age group they were part of at the time (\texttt{age\_group}), their sex (\texttt{sex}), and the number of English words they produced (\texttt{num\_tokens}), the number of unique English words they produced (\texttt{num\_types}). The final variable (\texttt{ttr}) is the calculated ratio of number of unique words (\texttt{num\_types}) to total words (\texttt{num\_tokens}) for each composition. This is known as the Type-Token Ratio and it is a standard metric for measuring lexical diversity.

\hypertarget{information-values}{%
\subsubsection{Information values}\label{information-values}}

Understanding the informational value, or \textbf{level of measurement}, of a variable or set of variables in key to preparing for analysis as it has implications for what visualization techniques and statistical measures we can use to interrogate the dataset. There are two main levels of measurement a variable can take: categorical and continuous. \textbf{Categorical variables} reflect class or group values. \textbf{Continuous variables} reflect values that are measured along a continuum.

The BELC dataset contains three categorical variables (\texttt{participant\_id}, \texttt{age\_group}, and \texttt{sex}) and three continuous variables (\texttt{num\_tokens}, \texttt{num\_types}, and \texttt{ttr}). The categorical variables identify class or group membership; which participant wrote the composition, what age group they were in, and their biological sex. The continuous variables measure attributes that can take a range of values without a fixed limit and the differences between each value are regular. The number of words and number of unique words for each composition can range from 1 to \(n\) and the Type-Token Ratio being derived from these two variables is also continuous for the same reason. Furthermore, the differences between the each of values of these measures is on a defined interval, so for example a composition which has a word count (\texttt{num\_tokens}) of 40 is exactly two times as large as a composition with a word count of 20.

The distinction between categorical an continuous levels of measurement, as mentioned above, are the main two and for some statistical approaches the only distinction that needs to be made to conduct an analysis. However, categorical and continuous can each be broken down into subcategories and for some descriptive and analytic purposes these distinctions are important. For categorical variables a distinction can be made between variables in which there is a structured relationship between the values and those in which there is not. \emph{Nominal variables} contain values which are labels denoting the membership in a class in which there is no relationship between the labels. \emph{Ordinal variables} also contain labels of classes, but in contrast to nominal variables, there is a relationship between the classes, namely one in which there is a precedence relationship or order. With this in mind, our categorical variables be sub-classified. There is no order between the values of \texttt{participant\_id} and \texttt{sex} and they are therefore nominal whereas the values of \texttt{age\_group} are ordered, each value refers to a sequential age group, and therefore it is ordinal.

Turning to continuous variables, another subdivision can be made which hinges on the existence of a non-arbitrary zero or not. \emph{Interval variables} contain values in which the difference between the values is regular and defined, but the measure has an arbitrary zero value. A typically cited example of an interval variable is temperature measurements on the Fahrenheit scale. A value of 0 on this scale does not mean there is 0 temperature. \emph{Ratio variables} have all the properties of interval variables but also include a non-arbitrary definition of zero. All of the continuous variables in the BELC dataset (\texttt{num\_tokens}, \texttt{num\_types}, and \texttt{ttr}) are ratio variables as a value of 0 would indicate the lack of this attribute.

An hierarchical overview of the relationship between the two main and four sub-types of levels of measurement appear in Figure \ref{fig:info-values-graphic}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/04-approaching-analysis/Informational-values-paper} 

}

\caption{Levels of measurement graphic representation.}\label{fig:info-values-graphic}
\end{figure}

A few notes of practical importance; First, the distinction between interval and ratio variables is often not applicable in text analysis and therefore often treated together as continuous variables. Second, the distinction between ordinal and interval/continuous variables is not as clear cut as it may seem. Both variables contain values which have an ordered relationship. By definition the values of an ordinal variable do not reflect regular intervals between the units of measurement. But in practice interval/continuous variables with a defined number of values (say from a Likert scale used on a survey) may be treated as an ordinal variable as they may be better understood as reflecting class membership. Third, all continuous variables can be converted to categorical variables, but the reverse is not true. We could, for example, define a criterion for binning the word counts in \texttt{num\_tokens} for each composition into ordered classes such as ``low'', ``mid'', ``high''. On the other hand, \texttt{sex} (as it has been measured here) cannot take intermediate values on a unfixed range. The upshot is that variables can be down-typed but not up-typed. In most cases it is preferred to treat continuous variables as such, if the nature of the variable permits it, as the down-typing of continuous data to categorical data results in a loss of information --which will result in a loss of information and hence statistical power which may lead to results that obscure meaningful patterns in the data \citep{Baayen2004}.

\hypertarget{summaries}{%
\subsubsection{Summaries}\label{summaries}}

It is always key to gain insight into shape of the information through numeric, tabular and/ or visual summaries before jumping in to analytic statistical approaches. The most appropriate form of summarizing information will depend on the number and informational value(s) of our target variables. To get a sense of how this looks, let's continue to work with the BELC dataset and pose different questions to the data with an eye towards seeing how various combinations of variables are descriptively explored.

\hypertarget{single-variables}{%
\paragraph{Single variables}\label{single-variables}}

The way to statistically summarize a variable into a single measure is to derive a \textbf{measure of central tendency}. For a continuous variable the most common measure is the (arithmetic) \emph{mean}, or avergage, which is simply the sum of all the values divided by the number of values. As a measure of central tendency, however, the mean can be less-than-reliable as it is sensitive to outliers which is to say that data points in the variable that are extreme relative to the overall distribution of the other values in the variable affect the value of the mean depending on how extreme the deviate. One way to assess the effects of outliers is to calculate a \textbf{measure of dispersion}. The most common of these is the \emph{standard deviation} which estimates the average amount of variability between the values in a continuous variable. Another way to assess, or rather side-step, outliers is to calculate another measure of central tendency, the \emph{median}. A median is calculated by sorting all the values in the variable and then selecting the value which falls in the middle of all the other values. A median is less sensitive to outliers as extreme values (if there are few) only indirectly affect the selection of the middle value. Another measure of dispersion is to calculate quantiles. A \emph{quantile} slices the data in four percentile ranges providing a five value numeric summary of the spread of the values in a continuous variable. The spread between the first and third quantile is known as the Interquartile Range (IQR) and is also used as a single statistic to summarize variability between values in a continuous variable.

Below is a list of central tendency and dispersion scores for the continuous variables in the BELC dataset.

\textbf{Variable type: numeric}

\begin{tabular}{l|r|r|r|r|r|r|r|r|r|r}
\hline
skim\_variable & n\_missing & complete\_rate & mean & sd & p0 & p25 & p50 & p75 & p100 & iqr\\
\hline
num\_tokens & 0 & 1 & 66.23 & 43.90 & 1.00 & 29.00 & 55.00 & 90.00 & 185 & 61.00\\
\hline
num\_types & 0 & 1 & 40.25 & 22.80 & 1.00 & 22.00 & 38.00 & 54.00 & 97 & 32.00\\
\hline
ttr & 0 & 1 & 0.67 & 0.13 & 0.41 & 0.57 & 0.64 & 0.73 & 1 & 0.16\\
\hline
\end{tabular}

\begin{rmdtip}
The descriptive statistics returned above were generated by the
\texttt{skimr} package.
\end{rmdtip}

In the above summary, we see the mean, standard deviation (sd), and the quantiles (the five-number summary, p0, p25, p50, p75, and p100). The middle quantile (p50) is the median and the IQR is listed last.

These are important measures for assessing the central tendency and dispersion and will be useful for reporting purposes, but to get a better feel of how a variable is distributed, nothing beats a visual summary. A boxplot graphically summarizes many of these metrics. In Figure \ref{fig:summaries-boxplots-belc} we see the same three continuous variables, but now in graphical form.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-boxplots-belc-1} 

}

\caption{Boxplots for each of the continuous variables in the BELC dataset.}\label{fig:summaries-boxplots-belc}
\end{figure}

In a boxplot, the bold line is the median. The surrounding box around the median is the interquantile range. The extending lines above and below the IQR mark the largest and lowest value that is within 1.5 times either the 3rd (top of the box) or 1st (bottom of the box). Any values that fall outside, above or below, the extending lines are considered statistical outliers and are marked as dots (in this case red dots). \footnote{Note that each of these three variables are to be considered separately here (vertically). Later we will see the use of boxplots to compare a continuous variable across levels of a categorical variable (horizontally).}

Boxplots provide a robust and visually intuitive way of assessing central tendency and variability in a continuous variable but this type of plot can be complemented by looking at the overall distribution of the values in terms of their frequencies. A histogram provides a visualization of the frequency (and density in this case with the blue overlay) of the values across a continuous variable binned at regular intervals.

In Figure \ref{fig:summaries-histograms-belc} I've plotted histograms in the top row and density plots in the bottom row for the same three continuous variables from the BELC dataset.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-histograms-belc-1} 

}

\caption{Histograms and density plots for the continuous variables in the BELC dataset.}\label{fig:summaries-histograms-belc}
\end{figure}

Histograms provide insight into the distribution of the data. For our three continuous variables, the distributions happen not to be too strikingly distinct. They are, however, not the same either. When we explore continuous variables with histograms we are often trying to assess whether there is skew or not. There are three general types of skew, visualized in Figure \ref{fig:summaries-skew-graphic}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/04-approaching-analysis/skew-types-paper} 

}

\caption{Examples of skew types in density plots.}\label{fig:summaries-skew-graphic}
\end{figure}

In histograms/ density plots in which the distribution is either left or right, the median and the mean are not aligned. The \emph{mode}, which indicates the most frequent value in the variable is also not aligned with the other two measures. In a left-skewed distribution the mean will be to the left of the median which is left of the mode whereas in a right-skewed distribution the opposite occurs. In a distribution with absolutely no skew these three measures are the same. In practice these measures rarely align perfectly but it is very typical for these three measures to approximate alignment. It is common enough that this distribution is called the Normal Distribution \footnote{formally known as a Gaussian Distribution} as it is very common in real-world data.

Another and potentially more informative way to inspect the normality of a distribution is to create Quantile-Quantile plots (QQ Plot). In Figure \ref{fig:summaries-qqnorm-plot-belc} I've created QQ plots for our three continuous variables. The line in each plot is the normal distribution and the more points that fall off of this line, the less likely that the distribution is normal.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-qqnorm-plot-belc-1} 

}

\caption{QQ Plots for the continuous variables in the BELC dataset.}\label{fig:summaries-qqnorm-plot-belc}
\end{figure}

A visual inspection can often be enough to detect non-normality, but in cases which visually approximate the normal distribution (such as these) we can perform the Shapiro-Wilk test of normality. This is an inferential test that compares a variable's distribution to the normal distribution. The likelihood that the distribution differs from the normal distribution is reflected in a \(p\)-value. A \(p\)-value below the .05 threshold suggests the distribution is non-normal. In Table \ref{tab:summaries-normality-test-belc} we see that given this criterion only the distribution of \texttt{num\_types} is normally distributed.

\begin{table}

\caption{\label{tab:summaries-normality-test-belc}Results from Shapiro-Wilk test of normality for continuous variables in the BELC dataset.}
\centering
\begin{tabular}[t]{lrr}
\toprule
variable & statistic & p\_value\\
\midrule
Number of tokens & 0.942 & 0.001\\
Number of types & 0.970 & 0.058\\
Type-Token Ratio & 0.947 & 0.003\\
\bottomrule
\end{tabular}
\end{table}

Downstream in the analytic analysis, the distribution of continuous variables will need to be taken into account for certain statistical tests. Tests that assume `normality' are parametric tests, those that do not are non-parametric. Distributions which approximate the normal distribution can sometimes be transformed to conform to the normal distribution either by outlier trimming or through statistical procedures (i.e.~square root, log, or inverse transformation), if necessary. At this stage, however, the most important thing is to recognize whether the distributions approximate or wildly diverge from the normal distribution.

Before we leave continuous variables, let's consider another approach for visually summarizing a single continuous variable. The Empirical Cumulative Distribution Frequency, or \emph{ECDF}, is a summary of the cumulative proportion of each of the values of a continuous variable. An ECDF plot can be useful in determining what proportion of the values fall above or below a certain percentage of the data.

In Figure \ref{fig:summarize-ecdf-belc} we see ECDF plots for our three continuous variables.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summarize-ecdf-belc-1} 

}

\caption{ECDF plots for the continuous variables in the BELC dataset.}\label{fig:summarize-ecdf-belc}
\end{figure}

Take, for example, the number of tokens (\texttt{num\_tokens}) per composition. The ECDF plot tells us that 50\% of the values in this variable are 56 words or less. In the three variables plotted, the cumulative growth is quite steady. In some cases it is not. When it is not, an ECDF goes a long way to provide us a glimpse into key bends in the proportions of values in a variable.

Now let's turn to the descriptive assessment of categorical variables. For categorical variables, central tendency can be calculated as well but only a subset of measures given the reduced informational value of categorical variables. For nominal variables where there is no relationship between the levels the central tendency is simply the mode. The levels of ordinal variables, however, are relational and therefore the median, in addition to the mode, can also be used as a measure of central tendency. Note that a variable with one mode is unimodal, two modes, bimmodal, and in variables that have two or more modes multimodal.

\begin{rmdwarning}
To get numeric value of the median for an ordinal variable the levels of
the variable will need to be numeric as well. Non-numeric levels can be
recoded to numeric for this purpose if necessary.
\end{rmdwarning}

Below is a list of the central tendency metrics for the categorical variables in the BELC dataset.

\textbf{Variable type: factor}

\begin{tabular}{l|r|r|l|r|l}
\hline
skim\_variable & n\_missing & complete\_rate & ordered & n\_unique & top\_counts\\
\hline
participant\_id & 0 & 1 & FALSE & 36 & L05: 3, L10: 3, L11: 3, L12: 3\\
\hline
age\_group & 0 & 1 & TRUE & 4 & 10-: 24, 16-: 24, 12-: 16, 17-: 15\\
\hline
sex & 0 & 1 & FALSE & 2 & fem: 48, mal: 31\\
\hline
\end{tabular}

In practice when a categorical variable has few levels it is common to simply summarize the counts of each level in a table to get an overview of the variable. With ordinal variables with more numerous levels, the five-score summary (quantiles) can be useful to summarize the distribution. In contrast to continuous variables where a graphical representation is very helpful to get perspective on the shape of the distribution of the values, the exploration of single categorical variables is rarely enhanced by plots.

\hypertarget{multiple-variables}{%
\paragraph{Multiple variables}\label{multiple-variables}}

In addition to the single variable summaries (univariate), it is very useful to understand how two (bivariate) or more variables (multivariate) are related to add to our understanding of the shape of the relationships in the dataset. Just as with univariate summaries, the informational values of the variables frame our approach.

To explore the relationship between two continuous variables we can statistically summarize a relationship with a \textbf{coefficient of correlation} which is a measure of \textbf{effect size} between continuous variables. If the continuous variables approximate the normal distribution \emph{Pearson's r} is used, if not \emph{Kendall's tau} is the appropriate measure. A correlation coefficient ranges from -1 to 1 where 0 is no correlation and -1 or 1 is perfect correlation (either negative or positive). Let's assess the correlation coefficient for the variables \texttt{num\_tokens} and \texttt{ttr}. Since these variables are not normally distributed, we use Kendall's tau. Using this measure the correlation coefficient is \(-0.563\) suggesting there is a correlation, but not a particularly strong one.

Correlation measures are important for reporting but to really appreciate a relationship it is best to graphically represent the variables in a \emph{scatterplot}. In Figure \ref{fig:summaries-bivariate-scatterplot-belc} we see the relationship between \texttt{num\_tokens} and \texttt{ttr}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-bivariate-scatterplot-belc-1} 

}

\caption{Scatterplot...}\label{fig:summaries-bivariate-scatterplot-belc}
\end{figure}

In both plots \texttt{ttr} is on the y-axis and \texttt{num\_tokens} on the x-axis. The points correspond to the intersection between each of these variables for a single observation. In the left pane only the points are represented. Visually (and given the correlation coefficient) we can see that there is a negative relationship between the number of tokens and the Type-Token ratio: in other words, the more tokens a composition has the lower the Type-Token Ratio. In this case this trend is quite apparent, but in other cases is may not be. To provide an additional visual cue a trend line is often added to a scatterplot. In the right pane I've added a linear trend line. This line demarcates the optimal central tendency across the relationship, assuming a linear relationship. The steeper the line, or slope, the more likely the correlation is strong. The band, or ribbon, around this trend line indicates the \textbf{confidence interval} which means that real central tendency could fall anywhere within this space. The wider the ribbon, the larger the variation between the observations. In this case we see that the ribbon widens when the number of tokens is either low or high. This means that the trend line could be potentially be drawn either steeper (more strongly correlated) or flatter (less strongly correlated).

\begin{rmdtip}
In plots comparing two or more variables, the choice of which variable
to plot on the x- and y-axis is contingent on the research question and/
or the statistical approach. The language varies between statistical
approaches: in inferential methods the x-axis is used to plot what is
known as the dependent variable and the y-axis an independent variable.
In predictive methods the dependent variable is known as the outcome and
the independent variable a predictor. Exploratory methods do not draw
distinctions between variables along these lines so the choice between
which variable to plot along the x- and y-axis is often arbitrary.
\end{rmdtip}

Let's add another variable to the mix, in this case the categorical variable \texttt{sex}, taking our bivariate exploration to a multivariate exploration. Again each point corresponds to an observation where the values for \texttt{num\_tokens} and \texttt{ttr} intersect. But now each of these points is given a color that reflects which level of \texttt{sex} it is associated with.

\textbackslash begin\{figure\}

\{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-multivariate-scatterplot-belc-1}

\}

\textbackslash caption\{Scatterplot visualizing the relationship between \texttt{num\_tokens} and \texttt{ttr}.\}\label{fig:summaries-multivariate-scatterplot-belc}
\textbackslash end\{figure\}

In this multivariate case, the scatterplot without the trend line is more difficult to interpret. The trend lines for the levels of \texttt{sex} help visually understand the variation of the relationship of \texttt{num\_tokens}and \texttt{ttr} much better. But it is important to note that when there are multiple trend lines there is more than one slope to evaluate. The correlation coefficient can be calculated for each level of \texttt{sex} (i.e.~`male' and `female') independently but the relationship between the each slope can be visually inspected and provide important information regarding each level's relative distribution. If the trend lines are parallel (ignoring the ribbons for the moment), as it appears in this case, this suggests that the relationship between the continuous variables is stable across the levels of the categorical variable, with males showing more lexical diversity than females declining at a similar rate. If the lines were to cross, or suggest that they would cross at some point, then there would be a potentially important difference between the levels of the categorical variable (known as an interaction). Now let's consider the meaning of the ribbons. Since the ribbons reflect the range in which the real trend line could fall, and these ribbons overlap, the differences between the levels of our categorical variable are likely not distinct. So at a descriptive level, this visual summary would suggest that there are no differences between the relationship between \texttt{num\_tokens} and \texttt{ttr} for the distinct levels of \texttt{sex}.

Characterizing the relationship between two continuous variables, as we have seen is either performed through a correlation coefficient metric or visually. The approach for summarizing a bivariate relationship which combines a continuous and categorical variable is distinct. Since a categorical variable is by definition a class-oriented variable, a descriptive evaluation can include a tabular representation, with some type of summary statistic. For example, if we consider the relationship between \texttt{num\_tokens} and \texttt{age\_group} we can calculate the mean for \texttt{num\_tokens} for each level of \texttt{age\_group}. To provide a metric of dispersion we can include either the standard error of the mean (SEM) and/ or the confidence interval (CI).

In Table \ref{tab:summarize-bivariate-cont-cat-table} we see each of these summary statistics.

\textbackslash begin\{table\}

\textbackslash caption\{\label{tab:summarize-bivariate-cont-cat-table}Summary table for \texttt{tokens} by \texttt{age\_group}.\}
\centering

\begin{tabular}[t]{lrrr}
\toprule
age\_group & mean\_num\_tokens & sem & ci\\
\midrule
10-year-olds & 27.8 & 3.69 & 6.07\\
12-year-olds & 57.4 & 7.12 & 11.71\\
16-year-olds & 81.7 & 6.15 & 10.11\\
17-year-olds & 112.4 & 12.98 & 21.35\\
\bottomrule
\end{tabular}

\textbackslash end\{table\}

The SEM is a metric which summarizes variation based on the number of values and the CI, as we have seen, summarizes the potential range of in which the mean may fall given a likelihood criterion (usually the same as the \(p\)-value, .05).

Because we are assessing a categorical variable in combination with a continuous variable a table is an available visual summary. But as I have said before, a graphic summary is hard to beat. In the following figure (\ref{fig:summaries-bivariate-barplot-belc}) a barplot is provided which includes the means of \texttt{num\_tokens} for each level of \texttt{age\_group}. The overlaid bars represent the confidence interval for each mean score.

\textbackslash begin\{figure\}

\{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-bivariate-barplot-belc-1}

\}

\textbackslash caption\{Barplot comparing the mean \texttt{num\_tokens} by \texttt{age\_group} from the BELC dataset.\}\label{fig:summaries-bivariate-barplot-belc}
\textbackslash end\{figure\}

When CI ranges overlap, just as with ribbons in scatterplots, the likelihood that the differences between levels are `real' is diminished.

To gauge the effect size of this relationship we can use \emph{Spearman's rho} for rank-based coefficients. The score is 0.708 indicating that the relationship between \texttt{age\_group} and \texttt{num\_tokens} is quite strong. \footnote{To calculate effect sizes for the difference between two means, \emph{Cohen's d} is used.}

Now, if we want to explore a multivariate relationship and add \texttt{sex} to the current descriptive summary, we can create a summary table, but let's jump straight to a barplot.

\textbackslash begin\{figure\}

\{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-multivariate-barplot-belc-1}

\}

\textbackslash caption\{Barplot comparing the mean \texttt{num\_tokens} by \texttt{age\_group} and \texttt{sex} from the BELC dataset.\}\label{fig:summaries-multivariate-barplot-belc}
\textbackslash end\{figure\}

We see in Figure \ref{fig:summaries-multivariate-barplot-belc} that on the whole, the appears to be general trend towards more tokens in a composition for more advanced learner levels. However, the non-overlap in CI bars for the `12-year-olds' for the levels of \texttt{sex} (`male' and `female') suggest that 12-year-old females may produce more tokens per composition than males --a potential divergence from the overall trend.

Barplots are a familiar and common visualization for summaries of continuous variables across levels of categorical variables, but a boxplot is another useful visualization of this type of relationship.

\textbackslash begin\{figure\}

\{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-bivariate-boxplots-belc-1}

\}

\textbackslash caption\{Boxplot of the relationship between \texttt{age\_group} and \texttt{num\_tokens} from the BELC dataset.\}\label{fig:summaries-bivariate-boxplots-belc}
\textbackslash end\{figure\}

As seen when summarizing single continuous variables, boxplots provide a rich set of information concerning the distribution of a continuous variable. In this case we can visually compare the continuous variable \texttt{num\_tokens} with the categorical variable \texttt{age\_group}. The plot in the right pane includes `notches'. Notches represent the confidence interval, in boxplots this interval surrounds the median. When compared horizontally across levels of a categorical variable the overlap of notched spaces suggest that the true median may be within the same range.
Additionally, when the confidence interval goes outside the interquantile range (the box) the notches hinge back to the either the 1st (lower) or the 3rd (higher) IQR range and suggests that the variability is high.

We can also add a third variable to our exploration. As in the barplot in Figure \ref{fig:summaries-multivariate-barplot-belc}, the boxplot in Figure \ref{fig:summaries-multivariate-boxplots-belc} suggests that there is an overall trend towards more tokens per composition as a learner advances in experience, except at the `12-year-old' level where there appears to be a difference between `males' and `females'.

\textbackslash begin\{figure\}

\{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-multivariate-boxplots-belc-1}

\}

\textbackslash caption\{Boxplot of the relationship between \texttt{age\_group}, \texttt{num\_tokens} and \texttt{sex} from the BELC dataset.\}\label{fig:summaries-multivariate-boxplots-belc}
\textbackslash end\{figure\}

Up to this point in our exploration of multiple variables we have always included at least one continuous variable. The central tendency for continuous variables can be summarized in multiple ways (mean, median, and mode) and when calculating means and medians, measures of dispersion are also provide helpful information summarize variability. When working with categorical variables, however, measures of central tendency and dispersion are more limited. For ordinal variables central tendency can be summarized by the median or mode and dispersion can be assessed with an interquantile range. For nominal variables the mode is the only measure of central tendency and dispersion is not applicable. For this reason relationships between categorical variables are typically summarized using \textbf{contingency tables} which provide cross-variable counts for each level of the target categorical variables.

Let's explore the relationship between the categorical variables \texttt{sex} and \texttt{age\_group}. In Table \ref{tab:summaries-bivariate-categorical-table-belc} we see the contingency table with summary counts and percentages.

\textbackslash begin\{table\}

\textbackslash caption\{\label{tab:summaries-bivariate-categorical-table-belc}Contingency table for \texttt{age\_group} and \texttt{sex}.\}
\centering

\begin{tabular}[t]{llllll}
\toprule
sex/age\_group & 10-year-olds & 12-year-olds & 16-year-olds & 17-year-olds & Total\\
\midrule
female & 58\% (14) & 69\% (11) & 54\% (13) & 67\% (10) & 61\% (48)\\
male & 42\% (10) & 31\%  (5) & 46\% (11) & 33\%  (5) & 39\% (31)\\
Total & 100\% (24) & 100\% (16) & 100\% (24) & 100\% (15) & 100\% (79)\\
\bottomrule
\end{tabular}

\textbackslash end\{table\}

As the size of the contingency table increases, visual inspection becomes more difficult. As we have seen, a graphical summary often proves more helpful to detect patterns.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-bivariate-categorical-barplot-belc-1} 

}

\caption{Barplot...}\label{fig:summaries-bivariate-categorical-barplot-belc}
\end{figure}

In Figure \ref{fig:summaries-bivariate-categorical-barplot-belc} the left pane shows the counts. Counts alone can be tricky to evaluate and adjusting the barplot to account for the proportions of males to females in each group, as shown in the right pane, provides a clearer picture of the relationship. From these barplots we can see there were more females in the study overall and particularly in the 12-year-olds and 17-year-olds groups. To gauge the association strength between \texttt{sex} and \texttt{age\_group} we can calculate \emph{Cramer's V} which, in spirit, is like our correlation coefficients for the relationship between continuous variables. The Cramer's V score for this relationship is 0.12 which is low, suggesting that there is not a strong association between \texttt{sex} and \texttt{age\_group} --in other words, the relationship is stable.

Let's look at a more complex case in which we have three categorical variables. Now the dataset, as is, does not have a third categorical variable for us to explore but we can recast the continuous \texttt{num\_tokens} variable as a categorical variable if we bin the scores into groups. I've binned tokens into three score groups with equal ranges in a new variable called \texttt{rank\_tokens}.

Adding a second categorical independent variable ups the complexity of our analysis and as a result our visualization strategy will change. Our numerical summary will include individual two-way cross-tabulations for each of the levels for the third variable. In this case it is often best to use the variable with the fewest levels as the third variable, in this case \texttt{sex}.

\textbackslash begin\{table\}

\textbackslash caption\{\label{tab:summaries-multivariate-categorical-table-belc-female}Contingency table for \texttt{age\_group}, \texttt{rank\_tokens}, and \texttt{sex} (female).\}
\centering

\begin{tabular}[t]{llllll}
\toprule
rank\_tokens/age\_group & 10-year-olds & 12-year-olds & 16-year-olds & 17-year-olds & Total\\
\midrule
low & 27\% (13) & 10\%  (5) & 4\%  (2) & 6\%  (3) & 48\% (23)\\
mid & 2\%  (1) & 13\%  (6) & 21\% (10) & 6\%  (3) & 42\% (20)\\
high & 0\%  (0) & 0\%  (0) & 2\%  (1) & 8\%  (4) & 10\%  (5)\\
Total & 29\% (14) & 23\% (11) & 27\% (13) & 21\% (10) & 100\% (48)\\
\bottomrule
\end{tabular}

\textbackslash end\{table\}

\textbackslash begin\{table\}

\textbackslash caption\{\label{tab:summaries-multivariate-categorical-table-belc-male}Contingency table for \texttt{age\_group}, \texttt{rank\_tokens}, and \texttt{sex} (male).\}
\centering

\begin{tabular}[t]{llllll}
\toprule
rank\_tokens/age\_group & 10-year-olds & 12-year-olds & 16-year-olds & 17-year-olds & Total\\
\midrule
low & 32\% (10) & 13\% (4) & 13\%  (4) & 3\% (1) & 61\% (19)\\
mid & 0\%  (0) & 3\% (1) & 23\%  (7) & 6\% (2) & 32\% (10)\\
high & 0\%  (0) & 0\% (0) & 0\%  (0) & 6\% (2) & 6\%  (2)\\
Total & 32\% (10) & 16\% (5) & 35\% (11) & 16\% (5) & 100\% (31)\\
\bottomrule
\end{tabular}

\textbackslash end\{table\}

Contingency tables with this many levels are notoriously difficult to interpret. A plot that is often used for three-way contingency table summaries is a mosaic plot. In Figure \ref{fig:summaries-multivariate-mosaic-belc} I have created a mosaic plot for the three categorical variables in the previous contingency tables.

\textbackslash begin\{figure\}

\{\centering \includegraphics[width=0.9\linewidth]{04-approaching-analysis_files/figure-latex/summaries-multivariate-mosaic-belc-1}

\}

\textbackslash caption\{Mosaic plot for three categorical variables \texttt{age\_group}, \texttt{rank\_tokens}, and \texttt{sex} in the BELC dataset.\}\label{fig:summaries-multivariate-mosaic-belc}
\textbackslash end\{figure\}
The mosaic plot suggests that the number of tokens per composition increase as the learner age group increases and that females show more tokens earlier.

In sum, a dataset is information but when the observations become numerous or complex they are visually difficult to inspect and understand at a pattern level. The descriptive methods described in this section are indispensable for providing the researcher an overview of the nature of each variable and any (potential) relationships between variables in a dataset. Importantly, the understanding derived from this exploration underlies all subsequent investigation and will counted on to frame your approach to analysis regardless of the research goals and the methods employed to derive more substantial knowledge.

\emph{Consider adding a table with informational level, central tendency measure, dispersion measure, visualization??}

\hypertarget{analysis}{%
\subsection{Analysis}\label{analysis}}

Overview\ldots{}

From identifying a target population, selecting a data sample that represents that population, and to structuring the sample into a dataset, the goals of a research project inform and frame the process. So it will be unsurprising to know that the process of selecting an approach to analysis is also intimately linked with a researcher's objectives. In this section we will cover three main analysis types: inferential, predictive, and exploratory analysis. The contrasts between the three hinge on (1) how to \emph{identify} the variables of interest, (2) how to \emph{interrogate} these variables, and (3) how to \emph{interpret} the results. I will structure the discussion of these analysis types moving from the most structured (deductive) to least structured (inductive) approach to deriving knowledge from information.

Goals of analysis: identify variables that characterize the population in relevant ways, statistically interrogate these variables evaluating and assessing relationships, and interpret the results from the statistical procedures in terms of the research design and research goals.

\hypertarget{inferential-data-analysis-ida}{%
\subsubsection{Inferential data analysis (IDA)}\label{inferential-data-analysis-ida}}

(deductive)

Statistical hypothesis testing/ Null-Hypothesis Significance Testing Procedure (NHSTP)

\begin{itemize}
\item
  Top-down approach, hypothesis testing, deriving verifying insight from data
\item
  Inferential: aim to infer conclusions about a particular relationship in the dataset that can be generalized, under some assumption of reliability, to the target population
\end{itemize}

pattern testing, top-down, (deductive),

\emph{Identify}: pre-determined and operationalized (practically measured) set of variables, to provide confirmatory evidence regarding a hypothesis.

\begin{itemize}
\tightlist
\item
  observational unit
\item
  variable roles: dependent variable, independent variable(s)
\end{itemize}

\emph{Interrogate}: use statistical procedures to deduce/ evaluate the likelihood that the patterns in the data represent true patterns in the sample

\begin{itemize}
\tightlist
\item
  choose appropriate statistical procedure:

  \begin{itemize}
  \tightlist
  \item
    number and informational values of the variables
  \item
    assumptions about the nature of the variables (independence, normality, etc.)
  \end{itemize}
\item
  use of the data

  \begin{itemize}
  \tightlist
  \item
    entire dataset
  \item
    may include boostrapping (resampling with replacement)
  \end{itemize}
\end{itemize}

\emph{Interpret}: conclude whether the patterns are reliably generalizable to the population

\begin{itemize}
\tightlist
\item
  parameter estimates
\item
  confidence measure (p-value, confidence intervals)
\item
  effect size (association strength)
\end{itemize}

Also commonly known as hypothesis testing or confirmation, statistical inference aims to establish whether there is a reliable and generalizable relationship given patterns in the data. The approach makes the starting assumption that there is no relationship, or that the null hypothesis (\(H_0\)) is true. A relationship is only reliable, or \emph{significant}, if the chance that the null hypothesis is false is less than some predetermined threshold; in which case we accept the alternative hypothesis (\(H_1\)). The standard threshold used in the Social Sciences, Linguistic included, is the famous p-value \(p < .05\). Without digging into the deeper meaning of a p-value, in a nutshell a p-value is a confidence measure to suggest that the relationship you are investigating is robust and reliable given the data.

There are two considerations to keep in mind when conducting IDA. First, in this approach all the data is used and is used \emph{only} once. This is not the case for the other two categories fo statistical approaches. For this reason it is vital to identify your statistical approach from the outset of your research project. Second, failing to establish a clear hypothesis and testable hypothesis and then sticking to that hypothesis can lead researchers to engage in ``p-hacking''; a practice of running multiple tests and/or parameters on the same data (i.e.~reusing the data) until evidence for the alternative hypothesis appears.

\begin{itemize}
\tightlist
\item
  ? Include methods, visualizations, examples/ applications/ studies?
\end{itemize}

\hypertarget{predictive-data-analysis-pda}{%
\subsubsection{Predictive data analysis (PDA)}\label{predictive-data-analysis-pda}}

(deductive/ inductive)

\begin{itemize}
\item
  Mixed approach, can be used for the generation of hypotheses or to test hypotheses, deriving intelligent action from data, discovering and leveraging patterns
\item
  Predictive: pattern associating (deductive) and leveraging (inductive)
\end{itemize}

\emph{Identify}:

\begin{itemize}
\tightlist
\item
  observational unit
\item
  variable roles: outcome variable, predictor variable(s)
\end{itemize}

\emph{Interrogate}:

\begin{itemize}
\tightlist
\item
  choose appropriate statistical procedure:

  \begin{itemize}
  \tightlist
  \item
    number and informational values of the variables
  \end{itemize}
\item
  use of the data

  \begin{itemize}
  \tightlist
  \item
    training/ testing split
  \item
    may inlcude boostrapping (resampling with replacement) or cross-validation (resampling without replacement)
  \end{itemize}
\end{itemize}

\emph{Interpret}:

\begin{itemize}
\tightlist
\item
  accuracy

  \begin{itemize}
  \tightlist
  \item
    contingency table
  \item
    precision and recall
  \end{itemize}
\end{itemize}

The other statistical learning approach, Prediction, aims to uncover relationships in our data as they pertain to a particular outcome variable. This approach is known as \textbf{supervised learning}. Similar to Exploration in many ways, this approach also makes no assumptions about the potential relationships between variables in our data and the data can be used multiple times to refine our statistical tests in order to tease out the most effective method for our goals. Where an exploratory analysis aims to uncover meaningful patterns of any sort, prediction, however, is more focused in that the main aim is to ascertain the extent to which the variables in the data pattern, individually or together, in such a way to make reliable associations to a particular outcome variable in unseen data. To evaluate the robustness of a prediction model the data is partitioned into training and validation sets. Depending on the application and the amount of available data, a third `development' set is sometimes created as a pseudo test set to facilitate the testing of multiple approaches before the final evaluation. The proportions vary, but it a good rule of thumb is to reserve 60\% of the data for training, 20\% for development, and 20\% for validation.

\begin{itemize}
\item
  ? Include methods, visualizations, examples/ applications/ studies?
\item
  ? overfitting, a model that captures noise in training data obscuring the target pattern that is revealed when the model makes systematic errors on the testing data (new data)
\end{itemize}

\hypertarget{exploratory-data-analysis-eda}{%
\subsubsection{Exploratory data analysis (EDA)}\label{exploratory-data-analysis-eda}}

(inductive)

\begin{itemize}
\item
  Bottom-up approach, hypothesis generating, deriving tentative insight from data, discovering patterns
\item
  Exploratory: pattern discovery, bottom-up, (inductive)

  \begin{itemize}
  \tightlist
  \item
    reduce, summarize, sort
  \item
    can be seen as an extension of descriptive methods
  \end{itemize}
\end{itemize}

\emph{Identify}:

\begin{itemize}
\tightlist
\item
  observational unit
\item
  variable roles: predictor variable(s)
\end{itemize}

\emph{Interrogate}:

\begin{itemize}
\tightlist
\item
  choose appropriate statistical procedure:

  \begin{itemize}
  \tightlist
  \item
    number and informational values of the variables
  \end{itemize}
\item
  use of the data

  \begin{itemize}
  \tightlist
  \item
    training/ testing split
  \item
    may inlcude boostrapping (resampling with replacement) or cross-validation (resampling without replacement)
  \end{itemize}
\end{itemize}

\emph{Interpret}:

\begin{itemize}
\tightlist
\item
  quantitatively informed qualitative assessment
\item
  supervised reassessment (semi-supervised)
\end{itemize}

One of two statistical learning approaches, this statistical approach is used to uncover potential relationships in the data and gain new insight in an area where predictions and hypotheses cannot be clearly made. In statistical learning, exploration is a type of \textbf{unsupervised learning}. Supervision here, and for Prediction, refers to the presence or absence of an outcome variable. By choosing exploration as our approach we make no assumptions (or hypotheses) about the relationships between any of the particular variables in the data. Rather we aims to investigate the extent to which we can induce meaningful patterns wherever they may lie.

Findings from exploratory analyses can provide valuable insight for future study but they cannot be safely used to generalize to the larger population, which is why exploratory analyses are often known as hypothesis generating analyses (rather than hypothesis confirming). Given our generalizing power is curtailed, the data \emph{can} be reused multiple times trying out various tests.

While it is not strictly required, data for exploratory analysis is often partitioned into two sets, training and validation, at roughly an 80\%/20\% split. The training set is used for refining statistical measures and the test set is used to evaluate the refined measures. Although the evaluation results still cannot be used to generalize, the insight can be taken as stronger evidence that there is a potential relationship, or set of relationships, worthy of further study.

\emph{Although quantitative in nature, exploratory methods involve a high level of human interpretation. Human interpretation is a part of each stage of data analysis, and each statistical approach, in particular, but exploratory methods produce results that require associative thinking and pattern detection which is distinct from the other two statistical approaches, in particular, IDA.}

\begin{itemize}
\tightlist
\item
  ? Include methods, visualizations, examples/ applications/ studies?

  \begin{itemize}
  \tightlist
  \item
    Keyword analysis
  \item
    Clustering
  \item
    Topic modeling
  \end{itemize}
\item
  Note that these methods are document-level, or in terms of \citet{Egbert2020} ``linguistic descriptive'' in nature.
\end{itemize}

\hypertarget{reporting}{%
\subsection{Reporting}\label{reporting}}

Much of the necessary reporting for an analysis features in prose as part of the write-up of a report or article.

\begin{itemize}
\tightlist
\item
  Descriptive assessment

  \begin{itemize}
  \tightlist
  \item
    Key summaries
  \item
    Procedures to diagnose and correct
  \end{itemize}
\item
  Analysis results

  \begin{itemize}
  \tightlist
  \item
    Statistical procedures

    \begin{itemize}
    \tightlist
    \item
      in appropriate forms
    \end{itemize}
  \item
    Statistical results

    \begin{itemize}
    \tightlist
    \item
      in appropriate forms
    \end{itemize}
  \end{itemize}
\end{itemize}

(include the fact that although this reporting should be detailed in prose, some decisions and many implementation steps are not. Replicable and documented code fills this gap {[}code book vs.~data dictionary from Chapter 2{]})

\hypertarget{framing-research}{%
\section{Framing research}\label{framing-research}}

INCOMPLETE DRAFT

\begin{quote}
It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts.
―---Sir Arthur Conan Doyle, Sherlock Holmes
\end{quote}

\begin{rmdkey}
In this chapter you will learn:

\begin{itemize}
\tightlist
\item
  the goals of this textbook
\item
  the reasoning for using the R programming language
\item
  important text conventions employed in this textbook
\end{itemize}
\end{rmdkey}

Before jumping into the code, every researcher must come to a project with a clear idea about the purpose of the analysis. This means doing your homework in order to understand what it is exactly that you want to achieve; that is, you need to identify a research question. The first step is become versed in the previous literature on the topic. What has been written? What are the main findings? Secondly, it is important to become familiar with the standard methods for approaching the topic of interest. How has the topic been approached methodologically? What are the types, sources, and quality of data employed? What have been the statistical approaches employed? What particular statistical tests have been chosen? Getting an overview not only of the domain-specific findings in the literature but also the methodological choices will help you identify promising plan for carrying out your research.

\hypertarget{chapter-subsection}{%
\subsection{\ldots chapter subsection}\label{chapter-subsection}}

text

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Packages}
\end{Highlighting}
\end{Shaded}

\hypertarget{annotated-readdings}{%
\subsection{Annotated readdings}\label{annotated-readdings}}

\textbf{Ignatow, G., \& Mihalcea, R. (2017). An introduction to text mining: Research design, data collection, and analysis. Sage Publications.}
\citep{Ignatow2017}

Chapter 5 ``Designing your research project''

\begin{quote}
Research design is essentially concerned with the basic architecture of research projects, with designing projects as systems that allow theory, data, and research methods to interface in such a way as to maximize a project's ability to achieve its goals (see Figure 5.1). Research design involves a sequence of decisions that have to be taken in a project's early stages, when one oversight or poor decision can lead to results that are ultimately trivial or untrustworthy. Thus, it is critically important to think carefully and systematically about research design before committing time and resources to acquiring texts or mastering software packages or programming languages for your text mining project.
\end{quote}

\includegraphics{images/05-framing-research/Ignatow2017-research-design.png}
\textbf{Egbert, J., Larsson, T., \& Biber, D. (2020). Doing Linguistics with a Corpus: Methodological Considerations for the Everyday User. Cambridge University Press.} \citep{Egbert2020}

Chapter 3 ``Research Designs: Linguistically Meaningful Research Questions, Observational Units, Variables, and Dispersion''

\begin{itemize}
\tightlist
\item
  Research questions should drive decisions about the choice of observational unit, how variables are defined, and the choice of research design.
\item
  Observational units can be defined at the level of the linguistic feature,
  the text, or the corpus.
\item
  Variables can be measured qualitatively, according to variants of a
  linguistic feature, or quantitatively, using rates of occurrence for features.
\end{itemize}

Chapter 7 ``Interpreting Quantitative Results''

\begin{itemize}
\tightlist
\item
  Linguistics is done by linguists, not by computers.
\item
  In order to be useful, quantitative corpus linguistic analysis should be
  coupled with sound qualitative interpretation.
\item
  Researchers can rely on linguistic context, text-external context, and
  linguistic theory to guide their interpretation of quantitative corpus findings.
\end{itemize}

\hypertarget{part-preparation}{%
\part{Preparation}\label{part-preparation}}

\hypertarget{preparation-overview}{%
\section*{Overview}\label{preparation-overview}}
\addcontentsline{toc}{section}{Overview}

Overview\ldots{}

\hypertarget{acquire-data}{%
\section{Acquire data}\label{acquire-data}}

INCOMPLETE DRAFT

\begin{quote}
\ldots{}
\end{quote}

\begin{rmdkey}
In this chapter you will learn:

\begin{itemize}
\tightlist
\item
  the goals of this textbook
\item
  the reasoning for using the R programming language
\item
  important text conventions employed in this textbook
\end{itemize}
\end{rmdkey}

Overview\ldots{}

\hypertarget{section}{%
\subsection{\ldots{}}\label{section}}

text

\hypertarget{acquire-data-packages}{%
\subsubsection{Packages}\label{acquire-data-packages}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rvest)  }\CommentTok{\# full{-}fleged web scraping}
\FunctionTok{library}\NormalTok{(datapasta)  }\CommentTok{\# copy/paste approach to HTML tables}
\end{Highlighting}
\end{Shaded}

\hypertarget{curate-data}{%
\section{Curate data}\label{curate-data}}

INCOMPLETE DRAFT

\begin{quote}
\ldots{}
\end{quote}

\begin{rmdkey}
In this chapter you will learn:

\begin{itemize}
\tightlist
\item
  the goals of this textbook
\item
  the reasoning for using the R programming language
\item
  important text conventions employed in this textbook
\end{itemize}
\end{rmdkey}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Packages}
\end{Highlighting}
\end{Shaded}

Overview\ldots{}

\hypertarget{section-1}{%
\subsection{\ldots{}}\label{section-1}}

text

Data Organization in Spreadsheets \citep{Broman2018}. Although based on spreadsheets, many of the best practices discussed apply to good data organization regardless of the technology.

\hypertarget{transform-data}{%
\section{Transform data}\label{transform-data}}

INCOMPLETE DRAFT

\begin{quote}
\ldots{}
\end{quote}

\begin{rmdkey}
In this chapter you will learn:

\begin{itemize}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{itemize}
\end{rmdkey}

Overivew \ldots{}

\hypertarget{section-2}{%
\subsection{\ldots{}}\label{section-2}}

text

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Packages}
\end{Highlighting}
\end{Shaded}

NOTE:

\begin{itemize}
\tightlist
\item
  Cover Corpus and Document-Term Matrices (DTM)s
\end{itemize}

\hypertarget{part-modeling}{%
\part{Modeling}\label{part-modeling}}

\hypertarget{modeling-overview}{%
\section*{Overview}\label{modeling-overview}}
\addcontentsline{toc}{section}{Overview}

\hypertarget{exploration}{%
\section{Exploration}\label{exploration}}

INCOMPLETE DRAFT

\begin{quote}
\ldots{}
\end{quote}

\begin{rmdkey}
In this chapter you will learn:

\begin{itemize}
\tightlist
\item
  the goals of this textbook
\item
  the reasoning for using the R programming language
\item
  important text conventions employed in this textbook
\end{itemize}
\end{rmdkey}

text

\hypertarget{section-3}{%
\subsection{\ldots{}}\label{section-3}}

text

\hypertarget{exploration-packages}{%
\subsubsection{Packages}\label{exploration-packages}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Packages}
\end{Highlighting}
\end{Shaded}

\hypertarget{inference}{%
\section{Inference}\label{inference}}

INCOMPLETE DRAFT

\begin{quote}
\ldots{}
\end{quote}

\begin{rmdkey}
In this chapter you will learn:

\begin{itemize}
\tightlist
\item
  the goals of this textbook
\item
  the reasoning for using the R programming language
\item
  important text conventions employed in this textbook
\end{itemize}
\end{rmdkey}

text

\hypertarget{section-4}{%
\subsection{\ldots{}}\label{section-4}}

text

\hypertarget{inference-packages}{%
\subsubsection{Packages}\label{inference-packages}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Packages}
\end{Highlighting}
\end{Shaded}

\hypertarget{prediction}{%
\section{Prediction}\label{prediction}}

INCOMPLETE DRAFT

\begin{quote}
\ldots{}
\end{quote}

\begin{rmdkey}
In this chapter you will learn:

\begin{itemize}
\tightlist
\item
  the goals of this textbook
\item
  the reasoning for using the R programming language
\item
  important text conventions employed in this textbook
\end{itemize}
\end{rmdkey}

text

\hypertarget{section-5}{%
\subsection{\ldots{}}\label{section-5}}

text

\hypertarget{prediction-packages}{%
\subsubsection{Packages}\label{prediction-packages}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Packages}
\end{Highlighting}
\end{Shaded}

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{section-6}{%
\section{\ldots{}}\label{section-6}}

  \bibliography{coursebook.bib,packages.bib}

\end{document}
